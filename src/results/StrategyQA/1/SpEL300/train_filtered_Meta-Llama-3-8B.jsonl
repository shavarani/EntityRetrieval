{"question": "are Lentil able to withstand a wind storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Lentil able to withstand frost?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are drain flies able to survive bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "would an oil miner be interested in the Persian Gulf?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Persian_Gulf"}
{"question": "would an aerodynamic cactus dessicate more quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "would an aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a more aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need water more frequently?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need more water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do apple farmers benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do apples benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Tokyo Tower repainted often enough that someone can do the work seven times during their career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Tokyo_Tower"}
{"question": "can a worker repaint Tokyo Tower seven times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Tokyo_Tower"}
{"question": "can a worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tokyo_Tower"}
{"question": "can the same worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Tokyo_Tower"}
{"question": "can the same worker repaint Tokyo Tower seven or more times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Tokyo_Tower"}
{"question": "can cheap cinnamon sticks break a coffee grinder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would an aerodynamic cactus get thirsty more often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "when en route from Beijing to Paris, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Beijing;Paris"}
{"question": "when en route from Beijing to London, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Beijing;London"}
{"question": "when en route from London to Beijing, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "London;Beijing"}
{"question": "when en route from London to Beijing, will pilots convert the foot to metric?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "London;Beijing"}
{"question": "does a new pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Moscow"}
{"question": "does a pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Moscow"}
{"question": "would a pilot leaving Moscow during the 2018 Fifa World Cup report altitude in feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Moscow;2018_FIFA_World_Cup"}
{"question": "is growing cucumber with seeds bad for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with a bee phobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "can someone deathly allergic to bees safely grow cucumber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "C", "extracted_entity": null}
{"question": "would suspension bridges be better able to withstand an earthquake than a beam bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Beam_bridge"}
{"question": "does a cotton ball provide nutrition for humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a classic xbox emulator be popular in the future?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a classic xbox emulator sell well in 2040?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Miley Cyrus sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Miley_Cyrus;Iggy_Pop"}
{"question": "would selina gomez sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Iggy_Pop"}
{"question": "would Iggy Pop cover a Justin Bieber song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "would Iggy Pop cover a Sean Mendez song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Iggy_Pop"}
{"question": "would Sean Mendez join Iggy Pop on stage?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Iggy_Pop"}
{"question": "do Alan Turing and Jeff Bezos have the same alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alan_Turing;Jeff_Bezos"}
{"question": "would Justin Bieber join Iggy Pop at the Warp Tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "would Justin Bieber join Iggy Pop on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "would Iggy Popi invite Justin Bieber on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Bieber"}
{"question": "would Iggy Popi invite Justin Bieber to jam?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Bieber"}
{"question": "would Iggy Pop and Justin Bieber rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "will Iggy Pop invite Justin Bieber to rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ig", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "could Alan Turing have used a slide rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alan_Turing;Slide_rule"}
{"question": "will Iggy Pop invite Justin Bieber to rock?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ig", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "will Iggy Pop invite Justin Bieber to play together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "will Justin Bieber play on stage with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "would Justin Bieber play songs by Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Justin", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "is the Mona Lisa within one kilometre of the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mona_Lisa;Venus_de_Milo"}
{"question": "would Iggy Pop play Justin Bieber songs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ig", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "would Justin Bieber jam with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "would Justin Bieber tour with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;Iggy_Pop"}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "can you make cheese from domestic yak milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "could a snowy owl survive on a diet of mice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Snowy_owl"}
{"question": "hypothetically, could a snowy owl survive the climate of Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snowy_owl;Antarctica"}
{"question": "is overfeeding Lactobacillus unwise for people afraid of dentists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "do nerds like Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Naruto been a bad influence on people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Naruto"}
{"question": "have people risked their lives because of Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Naruto"}
{"question": "has Naruto inspired dangerous activities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can you hold bitcoin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can bitcoin fit in your hand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bitcoin"}
{"question": "can bitcoin fit in wallets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bitcoin"}
{"question": "can bitcoin buy vaporizers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bitcoin"}
{"question": "are meatballs popular in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Meatball;Italian_cuisine"}
{"question": "can meatballs be used in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Meatball;Italian_cuisine"}
{"question": "is the color green associated with a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there wild giant pandas south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there wild giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there wild giant pandas in France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "France"}
{"question": "did Buzz Aldrin walk on the moon in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Buzz_Aldrin"}
{"question": "is honey associated with a kind of queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are chinchillas a kind of bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinchilla"}
{"question": "did Charles Manson die at home?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charles_Manson"}
{"question": "is the Milky Way galaxy on a collision course?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Milky_Way"}
{"question": "are the Constitution and Bill of Rights related?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can the Constitution be updated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Constitution_of_Romania"}
{"question": "can the Constitution be changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Constitution_of_Pakistan"}
{"question": "are infants a possible result of sex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do infants need diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Mount Emei located to the west of the Alps??", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Mount_Emei;Alps"}
{"question": "is Mount Emei one of the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Mount_Emei;Andes"}
{"question": "is Mount Emei one of the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mount_Emei;Rocky_Mountains"}
{"question": "is Mount Emei one of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Mount_Emei;Alps"}
{"question": "is Mount Emei part of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Mount_Emei;Alps"}
{"question": "do eastern grey kangaroos live in Arizona?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Arizona"}
{"question": "do eastern grey kangaroos live in the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Eastern_grey_kangaroo;Mojave_Desert"}
{"question": "is the original Communist leader of China dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "People's_Republic_of_China"}
{"question": "is latitude related to the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Equator"}
{"question": "is June associated with a seasonal change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is June associated with a patriotic American symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "United_States"}
{"question": "is Orange County near the border with Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Orange_County,_California;Canada"}
{"question": "are Douglas firs native to the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Douglas_fir;Mojave_Desert"}
{"question": " Is cactus fruit an important menu item for a restaurant inspired by Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant themed around Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": " Is cactus fruit a good menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "should a Tenochtitlan cuisine-style restaurant serve cactus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is amnesia probably why a student of the class of 2016 forgot 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "September_11_attacks"}
{"question": "is amnesia why a student of the class of 2017 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Because", "extracted_entity": "September_11_attacks"}
{"question": "is amnesia likely why someone born in 1999 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "does the Giant panda live near Shanghai?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Shanghai"}
{"question": "is the giant panda vegetarian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Gulf_of_Finland;Scandinavia"}
{"question": "are hamburgers usually made with ham?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do wild camels live near Las Vegas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Las_Vegas,_Nevada"}
{"question": "are camels native to the Mojave Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Mojave_Desert"}
{"question": "does Simon Cowell still host American Idol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Simon_Cowell;American_Idol"}
{"question": "is Europa one of the first few moons of Jupiter discovered by humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Moons_of_Jupiter"}
{"question": "is Europa the smallest moon of Jupiter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Europa_(moon);Jupiter"}
{"question": "can a person donate a kidney while still alive?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does an organ donor need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does a human need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Amazon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Amazon_River"}
{"question": "do eastern grey kangaroos live in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Africa"}
{"question": "does Snow White live with an even number of dwarves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Richard III die in old age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Richard_III_of_England"}
{"question": "did reptiles live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Silurian"}
{"question": "did birds live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Silurian"}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Silurian"}
{"question": "does Jennifer Lawrence have significantly more Academy Award nominations than actual wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jennifer_Lawrence;Academy_Award"}
{"question": "was Los Angeles Memorial Sports Arena home to an NBA team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Los_Angeles_Memorial_Sports_Arena;National_Basketball_Association"}
{"question": "is amnesia likely why someone born in 1999 forgets 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is surfing popular in Switzerland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Switzerland"}
{"question": "is surfing popular in Quebec?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Quebec"}
{"question": "is surfing popular in Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iowa"}
{"question": "did Amy Winehouse vote for Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Amy_Winehouse;Donald_Trump"}
{"question": "did Amy Winehouse vote for President Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Amy", "extracted_entity": "Amy_Winehouse;Donald_Trump"}
{"question": "is there a parody song about tabloid journalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the current Speaker of the House a Republican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Speaker_of_the_United_States_House_of_Representatives;Republican_Party_(United_States)"}
{"question": "is the Antarctic Peninsula south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Antarctic_Peninsula;Equator"}
{"question": "was natural rubber originally produced in South America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "South_America"}
{"question": "can a Porsche lift off the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Porsche"}
{"question": "is the central processing unit part of a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Central_processing_unit;Motherboard"}
{"question": "is the central processing unit attached to a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Central_processing_unit;Motherboard"}
{"question": "does the Easter Bunny appear during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Easter Bunny associated with eggs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Easter_Bunny"}
{"question": "can the freedom of speech be legally limited in the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "are the Illuminati involved in the plot of a movie based on a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Illuminati"}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Gulf_of_Finland;Scandinavia"}
{"question": "were Dr. Seuss poems performed on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dr._Seuss;Broadway_theatre"}
{"question": "were Dr. Seuss books adapted into a stage play?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dr._Seuss"}
{"question": "did Dr. Seuss poetry become adapted for stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dr._Seuss"}
{"question": "is there a named elephant character in a book by Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dr._Seuss"}
{"question": "did Dr. Seuss create an elephant character in one of his books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dr._Seuss"}
{"question": "is amber important in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does amber play an important role in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does amber play an important role in the Jurassic Park series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jurassic_Park"}
{"question": "is Elijah part of an important Jewish ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jews"}
{"question": "is Elijah part of an important Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Elijah"}
{"question": "is Elijah part of an important Jewish tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Columbus"}
{"question": "did Columbus sail his voyages representing a different country than his original home country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus"}
{"question": "was the current Parc de Princes built on the site of a previous stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Parc_des_Princes"}
{"question": "were the British directly involved in the Arab-Israeli conflict during the 1940s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_Kingdom"}
{"question": "was Athena's statue from the Parthenon originally sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Athena;Parthenon;Michelangelo"}
{"question": "was Athena's statue from the Parthenon sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Athena;Parthenon;Michelangelo"}
{"question": "is Athena's statue from the Parthenon currently in Asia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Athena;Parthenon;Asia"}
{"question": "is Athena's statue from the Parthenon currently on display?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Athena;Parthenon"}
{"question": "is Athena's statue from the Parthenon currently on display in the Louvre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Athena;Parthenon;Louvre"}
{"question": "is Athena's statue from the Parthenon currently on display in the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Athena;Parthenon;United_Kingdom"}
{"question": "do wild emu live in the Everglades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Emu;Everglades"}
{"question": "do wild emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Emu;Mojave_Desert"}
{"question": "do emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Emu;Mojave_Desert"}
{"question": "were the 2018 Winter Olympics held in the Western Hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "2018_Winter_Olympics;Western_Hemisphere"}
{"question": "can olives be grown in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Bern located north of Turin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bern", "extracted_entity": "Bern;Turin"}
{"question": "is Bern located south of Munich?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bern;Munich"}
{"question": "is Bern located south of Berlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bern;Berlin"}
{"question": "is Bern located to the east of the Swiss-French border?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bern"}
{"question": "is Bern located between the Swiss borders to Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bern;Switzerland;Germany;Italy"}
{"question": "is Bern located between Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bern;Germany;Italy"}
{"question": "was \"D'oh\" first said during the 1980's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you fit the Eifel Tower under the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Eiffel_Tower;Golden_Gate_Bridge"}
{"question": "are there multiple Zorro featured by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Zorro;The_Walt_Disney_Company"}
{"question": "is the rank of lieutenant different across groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Michael Dorn ever been a Lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Dorn"}
{"question": "are common carp protected species?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do common carp eat sharks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Common_carp"}
{"question": "was Gandalf present at the death of Sauron?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Gand", "extracted_entity": "Gandalf;Sauron"}
{"question": "was Gandalf involved in the defeat of the Witch King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Gand", "extracted_entity": "Gandalf;Witch-king_of_Angmar"}
{"question": "is Mercedes-Benz a car company from the Eastern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mercedes-Benz;Eastern_Hemisphere"}
{"question": "is Bucharest located south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Equator"}
{"question": "would intelligent design be inappropriate to teach in public school?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Intelligent_design"}
{"question": "is the New Testament taught in kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Testament"}
{"question": "is the New Testament taught in most kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Testament"}
{"question": "could the Playstation 4 play Super Smash Brothers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "PlayStation_4;Super_Smash_Bros."}
{"question": "is DDR the most anticipated game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "PlayStation_4"}
{"question": "is DDR the most hyped game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "PlayStation_4"}
{"question": "does a Jewish holiday include a tradition regarding Elijah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Elijah"}
{"question": "were all of the performers at Aretha Franklin's funeral black?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Aretha_Franklin"}
{"question": "is World of Warcraft typically played on the Nintendo Switch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_of_Warcraft;Nintendo_Switch"}
{"question": "do popcorn and pretzels use different kinds of salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable at a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tonsillitis"}
{"question": "do popcorn and pretzels use different salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tonsillitis;Thanksgiving_(United_States)"}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tonsillitis;Thanksgiving_(United_States)"}
{"question": "is homelessness more difficult in cold places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does racism contribute to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which ice cream came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can coming out of the closet as gay lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can coming out of the closet lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can coming out lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Black_Canadians"}
{"question": "did soy milk originate from the same country in which the compass came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do more people experiencing homelessness go to warm places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is homelessness more prevalent in warmer places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which gunpowder came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "China", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which noodles came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over regular milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Soy_milk"}
{"question": "would an environmentalist support soy milk production over dairy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can salt ruin farmland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can manta rays and oysters live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Manta_ray;Oyster"}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Manta_ray"}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Manta_ray"}
{"question": "can manta rays and Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Manta_ray"}
{"question": "can manta rays and the bacteria Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Manta_ray"}
{"question": "was Confucius a citizen of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Confucius;Portuguese_Empire"}
{"question": "could a blow below your parietal bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Parietal_bone"}
{"question": "could a blow to your occipital bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do all USPS mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do all mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are there citizens who cannot vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;United_States_presidential_election"}
{"question": "could some seniors in High School vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;United_States_presidential_election"}
{"question": "can some high schoolers vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;United_States_presidential_election"}
{"question": "can some high school students vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_presidential_election"}
{"question": "can a coin be useful in assessing your tire condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can siblings develop in the wombs of multiple women?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can siblings develop in different wombs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Percy Jackson series a good introduction go people like Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "do Rick Riordan fans know about Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "do Percy Jackson fans know of Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Apollo"}
{"question": "did Christina Aguilera do a duet with another disney star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christina", "extracted_entity": "Christina_Aguilera"}
{"question": "is Christina Aguilera on a tv show with the singer who sang Honey Bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Christina_Aguilera"}
{"question": "is Christina Aguilera on a tv show with a country singer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christina_Aguilera"}
{"question": "is Christina Aguilera on a tv show with Blake Shelton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Christina_Aguilera;Blake_Shelton"}
{"question": "can Christina Aguilera eat bacon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christina_Aguilera"}
{"question": "would a person with Anorexia nervosa suffer with weak bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Anorexia_nervosa"}
{"question": "could Lisa Simpson have held a copy of the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lisa_Simpson;Toronto_Star"}
{"question": "can \"meat sweats\" cause a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "should you shower if you have a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you trip from a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Ronda Rousey held hands with The Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ronda_Rousey;Dwayne_Johnson"}
{"question": "has someone in China been hit by Ronda Rousey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "People's_Republic_of_China;Ronda_Rousey"}
{"question": "has Ronda Rousey punched someone in China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ronda_Rousey;People's_Republic_of_China"}
{"question": "has Ronda Rousey traveled to Greece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ronda_Rousey;Greece"}
{"question": "has Ronda Rousey competed on the global stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ronda_Rousey"}
{"question": "does \"cheers\" in the Estonian language get confused with \"terrible sex\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Estonian_language"}
{"question": "can you buy a kidney from a hospital for use in organ transplantation in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States"}
{"question": "does Christmas always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christmas"}
{"question": "does actor Geoffrey Rush play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Geoffrey_Rush"}
{"question": "does actor Chow Yun Fat play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chow_Yun-fat"}
{"question": "does actor Johnny Depp play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Johnny_Depp"}
{"question": "is the Pakistan Air Force based near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Pakistan_Air_Force;India"}
{"question": "are French people part of the European Union?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "France;European_Union"}
{"question": "are some hams supposed to be incredibly salty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are some Alternative Medicines actually poisonous? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are some alternative medicine practices capable of causing more harm than good?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alternative_medicine"}
{"question": "is Norman, Oklahoma known for it's corn production?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Norman,_Oklahoma"}
{"question": "is Norman, Oklahoma home to the champions of the 2019 Women's NCAA? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Norman,_Oklahoma"}
{"question": "is Norman, Oklahoma home to the university who won the 2019 Women's NCAA champions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Norman,_Oklahoma;NCAA_Division_I_women's_basketball_tournament;2019_NCAA_Division_I_women's_basketball_tournament"}
{"question": "are infants the result of labor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Vulcan related to Hephaestus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vulcan_(mythology);Hephaestus"}
{"question": "do moose live near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\u00c9cole_nationale_v\u00e9t\u00e9rinaire_d'Alfort"}
{"question": "are there wild date palms in Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica"}
{"question": "are there wild date palms in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canada"}
{"question": "are date palms native to Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Date_palm;Canada"}
{"question": "is the famous Hollywood sign in Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Hollywood;Los_Angeles_County,_California"}
{"question": "was Andrew Johnson alive during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Andrew_Johnson;American_Civil_War"}
{"question": "is Brooklyn near Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Manhattan"}
{"question": "does a Douglas fir keep its leaves during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Douglas_fir"}
{"question": "are people likely to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Burning_Man"}
{"question": "is there a city in California known for its homelessness problems?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "California"}
{"question": "can Curiosity interact with its environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Curiosity_(rover)"}
{"question": "would a victim of Jack the Ripper be likely to be hard to recognize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jack_the_Ripper"}
{"question": "did Curiosity outlive its expected lifespan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Curiosity_(rover)"}
{"question": "would using a firearm be outside of Jack the Ripper's MO?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jack_the_Ripper"}
{"question": "do the Israelis have a powerful country as an ally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Israel"}
{"question": "did Christopher Columbus use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus;Unicode"}
{"question": "does the country with largest population also consume the most peas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a gray whale be able to crush a human?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gray_whale"}
{"question": "would a house with a Swastika on it be likely to be vandalized? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are those who work in Oceanography likely to be SCUBA trained?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oceanography;Scuba_diving"}
{"question": "is SCUBA certification useful to those working in Oceanography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Scuba_diving;Oceanography"}
{"question": "could paralysis be caused by a spinal cord injury?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Avengers merch be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Avengers_(comics);DC_Comics"}
{"question": "is higher education necessary to become an artist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a seasoned Sea Captain take Dramamine? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you become an artist without formal education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are some types of pancakes named after currency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "if your pancake batter resembles crepe batter, did you make it wrong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do most people only memorize half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do most people use only part of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "ZIP_Code"}
{"question": "would you be likely to see a Honey badger in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Honey_badger;Australia"}
{"question": "does pancake batter have less liquid than crepe batter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "panc", "extracted_entity": "Pancake"}
{"question": "does hanging have multiple modes of lethality?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hanging"}
{"question": "has the Joker tortured anyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do roles meant for Egyptians often go to white actors in Hollywood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES", "extracted_entity": "Egyptians;Hollywood"}
{"question": "are historically relevant, non-white Egyptians often depicted in film with white skin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egyptians"}
{"question": "are historically relevant, non-white Egyptians often depicted in film as being white?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egyptians"}
{"question": "is Egyptian culture often whitewashed when it is put on film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Egypt"}
{"question": "does whitewashing affect Egyptians in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "are Egyptian Pharaohs often whitewashed in the media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ancient_Egypt"}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in film media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Ancient_Egypt"}
{"question": "do Egyptians face the problem of Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "do Egyptians face the problem of Hollywood Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "are Egyptians often misrepresented in film culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Egyptians"}
{"question": "are Egyptians often misrepresented in pop culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "did Henry Ford's name remain well known after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henry_Ford"}
{"question": "does Henry Ford's name live on despite him being dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henry_Ford"}
{"question": "is it unusual to enjoy pain with sex?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Othello"}
{"question": "should you keep jalapeno peppers away from small children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from infants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are Mexican restaurants likely to have jalapenos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the texture of Jalapenos change after roasting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can acne be hidden without makeup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is the Book of Joshua a dangerous item for North Koreans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Book_of_Joshua;North_Korea"}
{"question": "is it dangerous for North Koreans to speak of or have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "North_Korea;Book_of_Joshua"}
{"question": "is it dangerous for North Koreans to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "North_Korea;Book_of_Joshua"}
{"question": "is it dangerous for North Koreans to openly have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "North_Korea"}
{"question": "would it be strange for North Koreans to know of the book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "North_Korea;Book_of_Joshua"}
{"question": "would it be unsafe to have a copy of the book of Joshua as a North Korean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Book_of_Joshua;North_Korea"}
{"question": "would it be unsafe for a North Korean citizen to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "North_Korea;Book_of_Joshua"}
{"question": "would it be unsafe for a North Korean to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "North_Korea;Book_of_Joshua"}
{"question": "could a sea turtle have eaten Herrerasaurus eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Herrerasaurus"}
{"question": "could a Herrerasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Herrerasaurus"}
{"question": "could a Guaibasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sea_turtle"}
{"question": "could an ichthyosaur have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sea_turtle"}
{"question": "did Romani people obtain a slur about themselves based on misinformation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Romani_people"}
{"question": "are Romani people stereotyped as being gypsies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romani_people"}
{"question": "are Romani people stereotyped as being nomadic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Romani_people;Nomad"}
{"question": "would a high school student have been taught about Chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Chlorophyll"}
{"question": "do yellow plants have less chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Chlorophyll"}
{"question": "would someone from the US think some components of breakfast in British cuisine are odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;British_cuisine"}
{"question": "would Americans find breakfast in British Cuisine odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;British_cuisine"}
{"question": "is the British Cuisine version of breakfast unfamiliar to people in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "British_cuisine;United_States"}
{"question": "do Youtubers often recommend audiobooks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could a cheeseburger give someone with lactose intolerance a stomach ache?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lactose_intolerance"}
{"question": "can you find soup dumplings in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Chinatown,_Manhattan"}
{"question": "are women's eyelashes subject to societal expectations and judgment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "are there societal pressures for how eyelashes should look?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is your circulatory system important in fighting infections?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a cat outrun a chipmunk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chipmunk"}
{"question": "can hunger make you unkind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did people in the Warsaw Ghetto live in fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Warsaw_Ghetto"}
{"question": "would it be unwise to give a fairy your name?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "only", "extracted_entity": null}
{"question": "is there recourse for those who feel they've had an unfair trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If", "extracted_entity": null}
{"question": "does human behavior harm honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would sunblock be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sunscreen"}
{"question": "would sunscreen be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sunscreen"}
{"question": "are pears and onions harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "p", "extracted_entity": null}
{"question": "are pears and apples harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are pears and apples harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are pears and pumpkins harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is cast iron good for cooking on a flat top stove?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cast", "extracted_entity": null}
{"question": "would someone who is deaf be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QR_code"}
{"question": "are you likely to see pastel colors near the Easter Bunny?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Easter_Bunny"}
{"question": "would someone who is blind be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QR_code"}
{"question": "would a child be strong enough to pick up a goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can goldfish be kept in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jiangxi"}
{"question": "can the goldfish live in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jiangxi"}
{"question": "can the typical goldfish survive in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jiangxi"}
{"question": "can the typical goldfish survive in the world's largest ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do companies usually provide employees a day off to celebrate Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Thanksgiving"}
{"question": "is the number 5 known to be a lucky number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "have family members ever raced together in the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Daytona_500"}
{"question": "is the Daytona 500 a day of mourning for some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Daytona_500"}
{"question": "would a Jewish cemetery refuse the body of Justin Bieber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Jews;Justin_Bieber"}
{"question": "did someone related to the Backstreet Boys inspire Justin Bieber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Backstreet_Boys;Justin_Bieber"}
{"question": "do vegetarians participate in smoking foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is smoking pork belly on a stove top possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is smoking pork belly indoors possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do camels have a body part that is of comedic value on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a joke about camels be good on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are camel jokes appropriately made on Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Armenians have a tragedy in their past as a collective?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Armenians"}
{"question": "can some chiropractic maneuvers kill you?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you play some Nintendo DS Lite games on the Nintendo Switch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nintendo_DS_Lite;Nintendo_Switch"}
{"question": "do you produce adrenaline during good and bad experiences?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can having a lot of adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can too much adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would David Miscavige be unlikely to befriend Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "David_Miscavige;Nicole_Kidman"}
{"question": "would Nicole Kidman be likely to avoid David Miscavige?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nicole_Kidman;David_Miscavige"}
{"question": "would David Miscavige consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "David_Miscavige;Nicole_Kidman"}
{"question": "would David Miscavige Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "David", "extracted_entity": "David_Miscavige;Nicole_Kidman"}
{"question": "would the Church of Scientology consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nicole_Kidman;Suppressive_Person"}
{"question": "does Nicole Kidman know about the concept of thetans? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Nicole_Kidman"}
{"question": "would fans of pirate movies know of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "East_India_Company"}
{"question": "have fans of the Pirates of the Caribbean heard of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pirates_of_the_Caribbean;East_India_Company"}
{"question": "are commas used differently from country to country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Comma"}
{"question": "should you feed an infant a spoon of cinnamon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan"}
{"question": "would capturing the Japanese bulk carrier market be ideal for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Japan"}
{"question": "have rivers historically enabled trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a joke about camels be relevant on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Japanese a useful language for bulk carrier buyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Japanese", "extracted_entity": "Japanese_language"}
{"question": "could you watch The Powerpuff Girls in 4K Ultra HD?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Powerpuff_Girls;4K_resolution"}
{"question": "would tea made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do people typically buy new album releases on Mondays?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Stork;Robin_(comics)"}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Stork;Robin_(comics)"}
{"question": "would Edgar Allan Poe be considered 'straight edge'?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Edgar_Allan_Poe"}
{"question": "is Kim Jung Un the leader of a democracy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kim_Jong-un"}
{"question": "can you teach at a University without a Doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University"}
{"question": "does all of the cream end up in the butter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do most domestic cats take their excretions outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do most house cats practice outdoor excretion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are pickled cucumbers in Seoul flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Seoul"}
{"question": "are pickled cucumbers in South Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "South_Korea"}
{"question": "are pickled cucumbers in Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Korea"}
{"question": "would an art dealer prize a print of a picasso? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Well", "extracted_entity": null}
{"question": "can you cook hotter with olive oil than with butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a video of a Yeti make the news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Yeti"}
{"question": "would a photograph of a yeti be news-worthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Yeti"}
{"question": "are there means for deaf people to participate in surveying?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Boat Race held in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Boat_Race;Canada"}
{"question": "did early humans eat citrus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Skype used on Windows XP?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Skype;Windows_XP"}
{"question": "could you see a Manta Ray in Lake Michigan? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Lake_Michigan"}
{"question": "are homo sapiens capable of using tools?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Homo_sapiens"}
{"question": "are homo sapiens capable of language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Homo_sapiens"}
{"question": "did homo sapiens originate in Africa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Africa"}
{"question": "does the Wonder Woman movie take place in the same universe as Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wonder_Woman;Superman"}
{"question": "is the Wonder Woman movie related to Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wonder_Woman;Superman"}
{"question": "was Iggy Pop in a Nintendo game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Iggy_Pop;Nintendo"}
{"question": "was Iggy Pop in a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Iggy_Pop"}
{"question": "did the Nepalese Civil War take place in Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nepalese_Civil_War;Asia"}
{"question": "are saltwater crocodiles related to sharks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are saltwater crocodiles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saltwater_crocodile"}
{"question": "are saltwater crocodiles vertebrates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saltwater_crocodile"}
{"question": "does cognition involve the brain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are apes the animals most closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there multiple genii of apes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does circumference measure the sides of a square?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Circumference"}
{"question": "was swing music popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Swing_music;Chicago"}
{"question": "was swing popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Swing_music;Chicago"}
{"question": "was swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "New_Orleans"}
{"question": "is swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "New_Orleans"}
{"question": "do all American government holidays fall on Monday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States"}
{"question": "is handedness unevenly distributed among humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are the blues related to jazz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Blues"}
{"question": "can the blues be played by a new musician in training?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Blues"}
{"question": "does the North Sea border Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "North_Sea;Antarctica"}
{"question": "is there a relationship between Intel and Microsoft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Intel;Microsoft"}
{"question": "do wild sea turtles eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do octopi eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do lions eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do black coral live in the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Black_coral;Rocky_Mountains"}
{"question": "are diamonds useful in the manufacturing industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is binary related to hexadecimal numbering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is binary a good number system to use in computing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Binary_number"}
{"question": "is the Detroit River located in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Detroit_River;Europe"}
{"question": "do frigates travel in the ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Frigate"}
{"question": "are there snowboarding competitions in Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "is Eve possibly associated with apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Eve in an incestuous relationship with Adam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Adam"}
{"question": "would Socrates make a good candidate for a Doctor of Philosophy program?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Socrates;Doctor_of_Philosophy"}
{"question": "could all of the people who pass through 30th Street Station in a year live in Chicago?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "30th_Street_Station;Chicago"}
{"question": "was the year 2000 a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can I find a chick-fil-a with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chick-fil-A"}
{"question": "was Michael Crichton involved in the Jurassic World sequel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Crichton"}
{"question": "is Zika virus prominent in Third World countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Third_World"}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mrs._Claus"}
{"question": "were gladiators associated with the Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Colosseum"}
{"question": "would a compass based on Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Earth's_magnetic_field"}
{"question": "would a compass attuned to Earth's magnetic field be useless for Santa Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Santa_Claus"}
{"question": "would a compass attuned to Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Earth's_magnetic_field;Mrs._Claus"}
{"question": "does iron ore help sea turtles use Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Michael an unpopular name?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can dog owners without a compass find home with Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a map?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can dog owners use Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do sea turtles use a rock mineral to navigate Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform turtles in the American South?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Southern_United_States"}
{"question": "does the Earth's magnetic field inform chickens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would an ancient visitor to Persia consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Iran;Crocus"}
{"question": "do camels live in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canada"}
{"question": "was the DS Lite made by the same company as the Game Boy Advance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nintendo_DS_Lite;Game_Boy_Advance"}
{"question": "is keyboard typing is an important part of software engineering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Prime_Minister_of_Turkey;New_Delhi"}
{"question": "does the Prime Minister of Turkey work in New Delhi with the Earl of Sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Prime_Minister_of_Turkey;New_Delhi"}
{"question": "is New Delhi controlled by the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Delhi"}
{"question": "does the Earl of Sandwich and the Prime Minister of Turkey preside over New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Montagu,_4th_Earl_of_Sandwich;New_Delhi"}
{"question": "does the Earl of Sandwich manage the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Prime_Minister_of_Turkey"}
{"question": "does the Earl of Sandwich have power over the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Prime_Minister_of_Turkey"}
{"question": "will the Prime Minister of Turkey vote in the US presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Prime_Minister_of_Turkey;United_States"}
{"question": "will the Prime Minister of Turkey attend the UN convention in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Prime_Minister_of_Turkey"}
{"question": "did the Taco Bell dog star in any movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Taco_Bell"}
{"question": "can you buy macaroni and cheese at taco bell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at tacobell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did UV light have nothing to do with Bob Marley's death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bob_Marley"}
{"question": "would a cauliflower farmer prosper in Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cauliflower;Alaska"}
{"question": "would a cauliflower farmer prosper in Juneau, Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Juneau,_Alaska;Alaska"}
{"question": "are there multiple actors who played James Bond in the movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "James_Bond"}
{"question": "are photons weaponized in Star Trek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Star_Trek"}
{"question": "is Elon Musk associated with space flight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elon_Musk"}
{"question": "is Cartagena located south of Buenos Aires?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cartagena,_Colombia;Buenos_Aires"}
{"question": "is Cartagena located to the north of the Panama Canal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cartagena,_Colombia;Panama_Canal"}
{"question": "is Cartagena located to the north of Cancun?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Cartagena,_Colombia;Canc\u00fan"}
{"question": "is Cartagena in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cartagena,_Colombia;North_America"}
{"question": "is Cartagena in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cartagena,_Spain;Europe"}
{"question": "were the Egyptian pyramids built during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Egypt"}
{"question": "are Snow White's dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Snow_White_and_the_Seven_Dwarfs_(1937_film);Snow_White"}
{"question": "are Snow White's group of dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is ice prominently featured in a Disney animated movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "does Disney have a movie about ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "was the JPEG format developed on the original MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "JPEG;MS-DOS"}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "JPEG;MS-DOS"}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "JPEG;MS-DOS"}
{"question": "is JAXA involved with the International Space Station?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "JAXA;International_Space_Station"}
{"question": "is there a popular musical about missionaries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is karaoke commonly performed in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Karaoke;People's_Republic_of_China"}
{"question": "is karaoke commonly performed in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saudi_Arabia"}
{"question": "is karaoke popular in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Karaoke;People's_Republic_of_China"}
{"question": "is karaoke popular in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saudi_Arabia"}
{"question": "is classical music an important part of Italian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Italy"}
{"question": "did Emperor Nero lead Rome during the Gallic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nero;Rome;Gallic_Wars"}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Silurian"}
{"question": "is Alitalia headquartered in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alitalia;Europe"}
{"question": "does taking ukemi always halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does a triangle need to have at least two acute angles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Triangle"}
{"question": "is dessert generally unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has white pigment historically been produced with a fermented liquid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has white pigment historically been produced with manure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does fencing require special equipment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could a bite from a honey badger be easily differentiated from one by a wolverine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Wolverine"}
{"question": "is there life on Saturn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saturn"}
{"question": "are students of cell biology likely to understand kreb's cycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cell_biology"}
{"question": "was Ringo Starr's first band active in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ringo_Starr;United_States"}
{"question": "was Ringo Starr's first band active in Norwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ringo_Starr;Norwich"}
{"question": "was Ringo Starr one of the Beatles' major writers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ringo_Starr;The_Beatles"}
{"question": "was Ringo Starr the original drummer for the Beatles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ringo_Starr;The_Beatles"}
{"question": "is ectopic pregnancy dangerous for the mother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ectopic_pregnancy"}
{"question": "does ectopic pregnancy result in miscarriage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ectopic_pregnancy;Miscarriage"}
{"question": "has a woman ever held the office of Prime Minister of New Zealand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Prime_Minister_of_New_Zealand"}
{"question": "can a bhikkhu play for the Chicago Red Stars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "University_of_Chicago;Chicago_Red_Stars"}
{"question": "can a bhikkhu join Hadassah?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hadassah_Women's_Zionist_Organization_of_America"}
{"question": "is there a musical based on a Mark Twain novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mark_Twain"}
{"question": "can a bhikkhu attend Saint Mary's College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a bhikkhu attend Cedar Crest College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cedar_Crest_College"}
{"question": "can a bhikkhu attend Bernard College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fredericton_High_School;Fort_William_College"}
{"question": "is trigonometry related to geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Ray Charles play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ray_Charles;Nintendo_DS_Lite"}
{"question": "can Miles O'Brien play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nintendo_DS_Lite"}
{"question": "can Bethany Hamilton play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bethany_Hamilton;Nintendo_DS_Lite"}
{"question": "can Nicholas James Vujicic play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nintendo_DS_Lite"}
{"question": "can Kyle Maynard play a Nintendo DS Lite right from the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nintendo_DS_Lite"}
{"question": "can Kyle Maynard play an unmodified Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gray_Maynard;Nintendo_DS_Lite"}
{"question": "did Carol Yager suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Anorexia_nervosa"}
{"question": "were compact discs used as the game medium in a major home console?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the team that lost Super Bowl 50 still without any actual Super Bowl victories?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Super_Bowl_50;Super_Bowl"}
{"question": "was the Great Wall of China built by the People's Republic of China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Great_Wall_of_China;People's_Republic_of_China"}
{"question": "does someone born in San Antonio have US voting rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "San_Antonio;United_States"}
{"question": "would a painter avoid red from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "would a painter refuse pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a painter decline pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a large, rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Horoscope"}
{"question": "would an astrologer consult the trajectory of a rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would an astrologer focus on a rocky planet to make predictions about Friday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was the New Deal primarily an economy-oriented series of programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_Deal"}
{"question": "are helmets required when playing lacrosse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a person suffer multiple strokes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the science of meteorology prominently featured on TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Meteorology"}
{"question": "is Earth Day celebrated during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Earth_Day"}
{"question": "does the Metropolitan Museum of Art contain an ancient building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Metropolitan_Museum_of_Art"}
{"question": "did Amy Winehouse hear the news about Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amy_Winehouse;Brexit"}
{"question": "does England border on the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "England;Indian_Ocean"}
{"question": "were dinosaurs alive during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bronze_Age"}
{"question": "was Jack the Ripper a serial killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jack_the_Ripper"}
{"question": "are geisha ladies part of East Asian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Geisha;East_Asia"}
{"question": "did John Williams score a Warner Bros. movie based on an English fantasy novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "John_Williams;Warner_Bros.;England"}
{"question": "is the Dark Knight based on DC Comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DC_Comics"}
{"question": "is the Dark Knight about a DC comics superhero?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "DC_Comics"}
{"question": "was Julius Caesar assassinated during summer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Julius_Caesar"}
{"question": "was Julius Caesar an Emperor of Rome?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julius_Caesar;Roman_emperor"}
{"question": "is there a significant difference between a baby's skull and that of an adult other than size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "is there a seasonal change during June?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you order a cheeseburger at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Taco_Bell"}
{"question": "can Atlantic salmon survive in both saltwater and freshwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Atlantic_salmon"}
{"question": "could you buy music players at Toys R Us?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do small children in America know what crucifixion is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "can Aron Ralston play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aron_Ralston;Nintendo_DS_Lite"}
{"question": "is cancer caused by swallowed poison?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Bern a major European city?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bern;Europe"}
{"question": "was Andrew Johnson loyal to his state during the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Andrew_Johnson;American_Civil_War"}
{"question": "did Jennifer Lawrence portray a character from Marvel comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jennifer_Lawrence;Marvel_Comics"}
{"question": "is Fiat Chrysler gaining a new corporate identity post-merger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Fiat Chrysler have two headquarters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Fiat_Chrysler_Automobiles"}
{"question": "did Rush Limbaugh vote for Donald Trump in the 2016 Republican primary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rush_Limbaugh;Donald_Trump;Republican_Party_presidential_primaries,_2016"}
{"question": "did Rush Limbaugh vote for Hillary Clinton?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rush", "extracted_entity": "Rush_Limbaugh;Hillary_Rodham_Clinton"}
{"question": "did Barack Obama's father vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Barack_Obama"}
{"question": "did Barack Obama's mother vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama"}
{"question": "did Barack Obama's mother vote for him during his presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama"}
{"question": "did Barack Obama's mother live in the White House?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;White_House"}
{"question": "did Barack Obama's visit the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Barack_Obama;White_House"}
{"question": "did Barack Obama's mother live in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;White_House"}
{"question": "is it possible for an amoeba to get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Barack Obama's mother visit her son in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;White_House"}
{"question": "was Peter Jackson's King Kong the original movie featuring the famous gorilla?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Peter_Jackson;King_Kong"}
{"question": "is pancreatic cancer a particularly dangerous cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is pancreatic cancer one of the deadliest forms of cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pancreatic_cancer"}
{"question": "does the Beatles song While My Guitar Gently Weeps feature a guest guitarist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "While", "extracted_entity": "The_Beatles;While_My_Guitar_Gently_Weeps"}
{"question": "has Anthrax been used in terrorism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anthrax"}
{"question": "has weaponized Anthrax caused deaths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anthrax_(American_band)"}
{"question": "have there been deaths from Anthrax specifically after its use as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anthrax_(American_band)"}
{"question": "are African leopards the biggest wild cats in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Africa"}
{"question": "was Chuck Hagel born and raised in the American Midwest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chuck_Hagel;Midwestern_United_States"}
{"question": "was Saint Vincent and the Grenadines once a European colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saint_Vincent_and_the_Grenadines;Europe"}
{"question": "can dogs eat brownies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Excalibur wielded by King Henry VIII?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Excalibur;Henry_VIII_of_England"}
{"question": "is menthol associated with oral hygiene?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is there a Star Wars movie that was only ever shown once?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Star", "extracted_entity": "Star_Wars"}
{"question": "have multiple actors portrayed the character of Anakin Skywalker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Darth_Vader"}
{"question": "is cholera a viral infection of the respiratory system?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was New England involved in the American Revolutionary War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_England;American_Revolutionary_War"}
{"question": "could Lil Wayne's children ride on a motorcycle together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lil_Wayne"}
{"question": "could the ingredients in the Powerpuff Girls make a pumpkin pie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in the 1940's, were mood disorders treated in dangerous ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mood_disorder"}
{"question": "would someone with a mood disorder in the 1940's be at risk for dangerous treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there judges working at the top level of the United States federal government?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;Federal_government_of_the_United_States"}
{"question": "would a peregrine falcon enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Peregrine_falcon;Pear"}
{"question": "would a cat enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pear"}
{"question": "does United Airlines operate a hub in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_Airlines;Europe"}
{"question": "do judges wear distinctive outfits while serving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Kane lose an election to be a mayor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the East Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "East_Coast_of_the_United_States"}
{"question": "does Kane hold political office in a state on the West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "West_Coast_of_the_United_States"}
{"question": "would you hire Michael Faraday to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Michael_Faraday"}
{"question": "did the Philadelphia Museum of Art as an institution predate the opening of its main building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Philadelphia_Museum_of_Art"}
{"question": "does childbirth cause the mother to become a parent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Robert De Niro have at least double the number of Oscar nominations as actual Oscar wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Robert_De_Niro"}
{"question": "did Robert De Niro win an Oscar during the 1970s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Robert_De_Niro;Academy_Award"}
{"question": "is Boris Johnson in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": "Boris_Johnson;Scottish_independence"}
{"question": "was Achilles considered royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does a butler appear in an American household in a popular sitcom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Butler;United_States"}
{"question": "would a podiatrist be familiar with Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Achilles"}
{"question": "is there a DC superhero who has a butler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "DC_Comics"}
{"question": "was the Enigma machine used by the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Nazi_Germany"}
{"question": "was Fred Rogers involved in the Cuban Missile Crisis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fred_Rogers;Cuban_Missile_Crisis"}
{"question": "if you enjoy Stephen King, are you likely to enjoy American Horror Story?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Stephen_King;American_Horror_Story"}
{"question": "does Wikipedia contain more knowledge than an encyclopedia set?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wikipedia"}
{"question": "did the Russian Revolution result in the establishment of the USSR?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Russian_Revolution;Soviet_Union"}
{"question": "is Qantas headquarted on the eastern side of Australia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qantas;Australia"}
{"question": "is Qantas headquarted in the Southern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qantas;Southern_Hemisphere"}
{"question": "are sweet potatoes a variety of fruit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sweet_potato"}
{"question": "is it possible to score over a hundred points in a single turn in Scrabble?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is colitis a gastrointestinal disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "col", "extracted_entity": null}
{"question": "is a complete Monty Python reunion show possible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Monty_Python"}
{"question": "is French toast made using potatoes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "French_toast"}
{"question": "did Sojourner Truth climb to the top of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Washington_Monument"}
{"question": "was Sojourner Truth present the first time the Washington Monument opened?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sojourner_Truth;Washington_Monument"}
{"question": "was Sojourner Truth present at the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sojourner_Truth;Washington_Monument"}
{"question": "did Sojourner Truth attend the opening of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sojourner_Truth;Washington_Monument"}
{"question": "did William Henry Harrison climb the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "William_Henry_Harrison;Washington_Monument"}
{"question": "did President William Henry Harrison place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "William_Henry_Harrison;Washington_Monument"}
{"question": "did President William Henry Harrison place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "William_Henry_Harrison;Washington_Monument"}
{"question": "did President Washington place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_Washington;Washington_Monument"}
{"question": "did President Washington lay the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_Washington;Washington_Monument"}
{"question": "did President Washington dedicate the Washington Monument to himself?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "George_Washington;Washington_Monument"}
{"question": "could Stephen Hawking climb the stairs of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stephen_Hawking;Washington_Monument"}
{"question": "could Zhang Zuolin write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Zhang_Zuolin"}
{"question": "could Jean Jacques Dessalines write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jean-Jacques_Dessalines"}
{"question": "did Kit Carson create a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kit_Carson"}
{"question": "would Ronald Reagan have appointed a secretary for the US Department of Education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ronald_Reagan;United_States_Department_of_Education"}
{"question": "would a geisha be good at a  soir\u00e9e?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Geisha"}
{"question": "can someone walk from New York to New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "New_York_City;New_Delhi"}
{"question": "does New Delhi experience snow?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_Delhi"}
{"question": "can fleas survive on hamsters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can hamsters get fleas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would it be easier to carry a dozen clementines than a dozen grapefruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Ringo Starr perform a concert in Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Ringo_Starr;Japan"}
{"question": "did Ringo Starr perform at the Budokan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ringo_Starr;Nippon_Budokan"}
{"question": "did Ringo Starr perform at Shea Stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ringo_Starr;Shea_Stadium"}
{"question": "would someone with curly hair dislike weather with high humidity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "did Howard Taft place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "William_Howard_Taft;Washington_Monument"}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is the Force an important part of a movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Force"}
{"question": "is the Force prominently featured in a popular movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Force"}
{"question": "does oceanography involve multiple scientific disciplines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Oceanography"}
{"question": "was the 1980 presidential election won by a Republican?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "are there guns at the United States Military Academy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Military_Academy"}
{"question": "is it considered rude not to say Hello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the Silk Road pass through the Middle East?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Silk_Road;Middle_East"}
{"question": "are elk popular targets of hunters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Elk"}
{"question": "is World War III likely to involve nuclear weapons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_War_III"}
{"question": "can paparazzi be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Paparazzi"}
{"question": "is The Hague in the Middle East?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hague;Middle_East"}
{"question": "do watchmakers need an understanding of basic mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do gray seals live on the North American coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Grey_seal;North_America"}
{"question": "is the Mona Lisa currently on display in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mona_Lisa;Europe"}
{"question": "are the English currently directly involved in the Arab-Israeli conflict?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "England"}
{"question": "is surfing featured in a Disney cartoon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "are snowshoes commonly worn in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Australia"}
{"question": "are snowshoes commonly worn in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Texas"}
{"question": "are goldfish good as a child's pet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "gold", "extracted_entity": "Goldfish"}
{"question": "are there slot machines in Maryland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Maryland"}
{"question": "did Allosaurus live at the same time as Neanderthals?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Allosaurus;Neanderthal"}
{"question": "is Call of Duty available on the N64?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nintendo_64"}
{"question": "was the flag flying prominently during the Battle of Baltimore in 1814?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Battle_of_Baltimore"}
{"question": "does the flag change often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does part of the flag represent the American colonies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "did Sojourner Truth attend the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sojourner_Truth;Washington_Monument"}
{"question": "is the Mona Lisa a subject of historical debate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mona_Lisa"}
{"question": "is the Order of the British Empire associated with knighthood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Order_of_the_British_Empire"}
{"question": "is there a title associated with members of the Order of the British Empire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Order_of_the_British_Empire"}
{"question": "is Austria the best alpine skiing team at the Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Austria", "extracted_entity": "Austria;Olympic_Mountains"}
{"question": "does Felicity Huffman have an Oscar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fel", "extracted_entity": "Felicity_Huffman;Academy_Award"}
{"question": "is Phobos beyond the Asteroid belt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Phobos_(moon)"}
{"question": "is tourism beneficial for the host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "tourism", "extracted_entity": null}
{"question": "is Kobe in eastern Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kobe"}
{"question": "are lifeboats used in airplanes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Mercury have sunsets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Mercury_(planet)"}
{"question": "has the Indian Ocean garbage patch completed a full rotation of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Goofy have eaten white chocolate in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Goofy"}
{"question": "is Goofy's age in human years an abundant number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Goofy"}
{"question": "is Goofy's age calculated in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy"}
{"question": "is Goofy's age in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy"}
{"question": "is Goofy's age in dog years a semiprime number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Goofy"}
{"question": "would a biologist concurrently study Adelie penguins and the Western honey bee in the field?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ad\u00e9lie_penguin;Western_honey_bee"}
{"question": "could a Western honeybee sting an Adelie penguin's egg?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Western_honey_bee;Ad\u00e9lie_penguin"}
{"question": "could a Western honeybee sting an Adelie penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Western_honey_bee;Ad\u00e9lie_penguin"}
{"question": "are there three countries touching the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gulf_of_Mexico"}
{"question": "does the Gulf of Mexico border three countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gulf_of_Mexico"}
{"question": "is there a music video about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a popular song about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ner", "extracted_entity": null}
{"question": "did the word nerd first appear in a children's book?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nerd"}
{"question": "did the word nerd first appear in literature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nerd"}
{"question": "is the Golden Gate Bridge on San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Golden_Gate_Bridge;San_Francisco_Bay"}
{"question": "can a citizen of United States buy morphine in a pharmacy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;Morphine"}
{"question": "were tanks invented in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "were tanks used in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "American_Civil_War"}
{"question": "are the Great Lakes connected to the Atlantic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "do hippos prefer fresh water to salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did land vertebrates live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Silurian"}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Silurian"}
{"question": "are hospitals exempt from displays of institutional racism?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Institutional_racism"}
{"question": "can representations of goats cause some religious people anxiety?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are some images of goats concerning to people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are all rice puddings white traditionally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Ariana Grande's signature look combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande"}
{"question": "would you have luck pearl finding in Lake Champlain? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lake_Champlain"}
{"question": "is homeopathy real medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is homeopathy medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Homeopathy"}
{"question": "does the cuisine of Hawaii embrace foods considered distasteful in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Cuisine_of_Hawaii;Contiguous_United_States;United_States"}
{"question": "does the Cuisine of Hawaii have similarities with the cuisine of korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Cuisine_of_Hawaii;Korea"}
{"question": "does the cuisine of Hawaii have any commonalities with that of South Korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Hawaii;South_Korea"}
{"question": "did Donald Trump invent the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donald_Trump;New_York_Harbor"}
{"question": "do American's think chief executive officers have skills to make good US Presidents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States"}
{"question": "did the Great Depression affect the entirety of some people's childhoods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Great_Depression"}
{"question": "does the cast of Finding Dory feature people from marginalized groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Finding_Dory"}
{"question": "is it possible to get frostbite in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_York_City"}
{"question": "do infants rely on different senses than toddlers to identify their parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do infants perceive their parents differently than toddlers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are some breakfast items in British cuisine considered BBQ food in America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "British_cuisine;United_States"}
{"question": "would Arnold Schwarzenegger be ineligible to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Arnold_Schwarzenegger;President_of_the_United_States;United_States"}
{"question": "is green skin a complaint caused by nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is moss a fungus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a modern central processing unit fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Central_processing_unit"}
{"question": "does an abacus have a central processing unit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Abacus;Central_processing_unit"}
{"question": "do Russian Cosmonauts go to hospitals on the moon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Roscosmos_State_Corporation"}
{"question": "do astronauts go to hospitals on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is there a hospital on the Moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are hearts sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "are elephant tusks sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States"}
{"question": "are elephant tusks sold in retail stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was the first example of astrophotography a daguerreotype? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Astrophotography;Daguerreotype"}
{"question": "was Isaac Newton good at astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Isaac_Newton;Astrophotography"}
{"question": "is a mountaintop a good place for astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Astrophotography"}
{"question": "do Crane hunt in packs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did DARPA pave the way for the atom bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "DARPA"}
{"question": "was the Atom Bomb funded by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "did DARPA help build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "did DARPA build the first television?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "did DARPA recruit Albert Einstein?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA;Albert_Einstein"}
{"question": "did Albert Einstein apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Albert_Einstein;DARPA"}
{"question": "did Thomas Edison apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Thomas_Edison;DARPA"}
{"question": "did Thomas Edison create DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thomas_Edison;DARPA"}
{"question": "was Thomas Edison the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thomas_Edison;DARPA"}
{"question": "was Albert Einstein the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Albert_Einstein;DARPA"}
{"question": "did Albert Einstein work for DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Albert_Einstein;DARPA"}
{"question": "was the Atom Bomb invented by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "did DARPA invent the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "DARPA"}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA"}
{"question": "was the atom bomb created by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "DARPA"}
{"question": "would Huckleberry Finn have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Huckleberry_Finn"}
{"question": "is eleventh grade required to buy alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Tom Sawyer have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tom_Sawyer"}
{"question": "is eleventh grade required to graduate elementary school?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do other animals use agriculture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can agriculture be illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Benadryl cause similar side effects to those of morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Benadryl;Morphine"}
{"question": "does morphine cause similar side effects as Benadryl?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Morphine;Benadryl"}
{"question": "has Ivan the Terrible flown on an airplane?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible"}
{"question": "has Donald Trump met with Ivan the Terrible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donald_Trump;Ivan_the_Terrible"}
{"question": "did Ivan the Terrible fight the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible;Nazi_Germany"}
{"question": "did Ivan the Terrible fight against Hitler?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible;Adolf_Hitler"}
{"question": "did Ivan the Terrible fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible;World_War_I"}
{"question": "did Ivan the Terrible fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ivan_the_Terrible;World_War_II"}
{"question": "would Mount Rushmore be completely submerged in the Sea of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mount_Rushmore;Sea_of_Japan"}
{"question": "does the Sea of Japan have multiple names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sea_of_Japan"}
{"question": "did Isaac Newton read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Isaac_Newton;Jean-Paul_Sartre"}
{"question": "are there more hours in a day than roman numeral symbols?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there fewer roman numeral symbols than letters in the alphabet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can roman numerals spell out words?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a laptop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Buzz_Aldrin"}
{"question": "can microscopic objects be Small Solar System bodies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Small_Solar_System_body"}
{"question": "can flour float on water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do amoebas descend from a variety of ancestors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a serious disease caused by amoebas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do the people in Nepal build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nepal"}
{"question": "do the people in Liechtenstein build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein"}
{"question": "do the people in Uzbekistan build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Uzbekistan"}
{"question": "was Aristotle considered to be a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aristotle"}
{"question": "is one of the important places in Irish mythology located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "Irish_mythology"}
{"question": "is \"The Land of the Young\" a place name in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Irish_mythology"}
{"question": "does Irish mythology have the concept of an underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Irish_mythology"}
{"question": "is someone who owns a cloak fastened by eight stones an important figure in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Irish_mythology"}
{"question": "was dynamite patented after the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "American_Civil_War"}
{"question": "was dynamite invented during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dynamite;American_Civil_War"}
{"question": "was General Pompey a governor of a region that contains what is now Basque Country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pompey;Basque_Country_(autonomous_community)"}
{"question": "was General Pompey a governor of a region where the capital was Corduba?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Pompey;Juba"}
{"question": "was General Pompey a contemporary of a queen who died of poison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pompey"}
{"question": "was General Pompey a contemporary of Cleopatra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pompey;Cleopatra"}
{"question": "do Asian black bear eat avacado? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Asian_black_bear"}
{"question": "do Asian black bear eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Asian_black_bear;Penguin"}
{"question": "can you order sea otter from the menu at Long John Silvers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you order sea otter at seafood restaurants? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not", "extracted_entity": null}
{"question": "do sea otters eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sea_otter;Penguin"}
{"question": "can you buy sea otter meat at the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do sea otters lay eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sea_otter"}
{"question": "can a sloth stand on a skateboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are fish living in a man-made environment in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are freshwater fish in an aquarium living in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Aquarium"}
{"question": "is rest at home the best treatment for suspected strokes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is spinach popular in breakfast drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spinach"}
{"question": "could mandatory birth control prevent human overpopulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Human_overpopulation"}
{"question": "does the human body produce vitamin C?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Vitamin_C"}
{"question": "could you work as a bagger at Aldi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aldi"}
{"question": "would Charles Darwin be considered a \"spoonie\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Charles_Darwin"}
{"question": "are all suicide attempts done with intent to die?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has a sports game ever featured Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Alcatraz been replicated in any video games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alcatraz_Island"}
{"question": "would Tony Hawk game fans be familiar with the layout of Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tony_Hawk;Alcatraz_Island"}
{"question": "are proteins negatively impacted by fevers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did prisoners at Auschwitz struggle with hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Auschwitz_concentration_camp"}
{"question": "could a craft be made with cloves and apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Excalibur featured in the Zelda games?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Excalibur"}
{"question": "was Charles Manson familiar with self mutilation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Charles_Manson"}
{"question": "would it be hard to confuse a poblano pepper for a jalapeno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are teenagers a big demographic for acne products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "if you're born in the autumn portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Libra_(constellation)"}
{"question": "if you're born in the fall portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Libra_(constellation)"}
{"question": "are there cities where the leaves stay green all autumn? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have a mental health condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have SAD?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are relationships with two husbands considered immoral by some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Wake County NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wake_County,_North_Carolina"}
{"question": "is snowboarding a popular winter sport in Raleigh, NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Raleigh,_North_Carolina"}
{"question": "can conditioner fix damaged hair?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "in Doctor Who, is River Song the Doctor's daughter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Doctor_Who;River_Song_(Doctor_Who)"}
{"question": "is it normal to be waiting for tax forms from your employer in March?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would pears, cheese, and lettuce be served on a platter in some locations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a Chihuahua with a deer head be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chihuahua_(dog)"}
{"question": "would a Chihuahua with a longer nose be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chihuahua_(dog)"}
{"question": "would an Apple Head Chihuahua be likely cost more in vet bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would you be more likely to have higher vet bills with an apple head chihuahua? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would students at University of North Carolina at Chapel Hill want air conditioning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill"}
{"question": "does Kesha include details in her video that people associate with the Illuminati?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Kesha;Illuminati"}
{"question": "are multiple claims by chiropractic practitioners unfounded?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Chiropractic"}
{"question": "did Alaskan soldiers fight in the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alaska;American_Civil_War"}
{"question": "would Peter Dinklage be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Peter_Dinklage;Paralympic_Games"}
{"question": "can someone with Down Syndrome compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Down_syndrome;Paralympic_Games"}
{"question": "can a legally blind person compete in swimming at the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paralympic_Games"}
{"question": "can someone who is blind compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paralympic_Games"}
{"question": "is Snoopy considered \"man's best friend\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Snoopy"}
{"question": "does the Amazon rainforest have Colobinae?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amazon_rainforest"}
{"question": "does the Amazon rainforest have Pygathrix?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon_rainforest"}
{"question": "do Pygathrix live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon_rainforest"}
{"question": "do Cercopithecoidea live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Old_World_monkey;Amazon_rainforest"}
{"question": "do talapoin live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon_rainforest"}
{"question": "do  Cercopithecidae live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amazon_rainforest"}
{"question": "is the Amazon rainforest a completely wild forest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon_rainforest"}
{"question": "was the Amazon Rainforest one of the places discovered by Norse sailors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Amazon_rainforest;Norsemen"}
{"question": "was the Amazon Rainforest one of the places discovered by Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Amazon_rainforest;Christopher_Columbus"}
{"question": "was the Amazon Rainforest one of the places discovered by Marco Polo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Amazon_rainforest;Marco_Polo"}
{"question": "did Marco Polo travel to the Amazon Rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Marco_Polo;Amazon_rainforest"}
{"question": "could Bart Simpson have owned comics with The Joker in them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bart_Simpson;Joker_(comics);Joker_(character)"}
{"question": "has the character of the Joker lasted longer than the character of Ninja-K?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Joker_(character)"}
{"question": "does a double-blind trial weaken the placebo effect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the use of a placebo considered somewhat unethical?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you get negative effects from a placebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Placebo"}
{"question": "has Broadway featured performances set with the Napoleonic Wars as part of the background?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Napoleonic_Wars"}
{"question": "have several broadway musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Napoleonic_Wars"}
{"question": "have several musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Napoleonic_Wars"}
{"question": "have multiple musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Napoleonic_Wars"}
{"question": "can people taking their BTEC's be parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Business_and_Technology_Education_Council"}
{"question": "is Michael Scott from The Office known for his leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael", "extracted_entity": "Michael_Scott_(The_Office);The_Office_(U.S._TV_series)"}
{"question": "is Michael Scott from The Office known for his strong leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael", "extracted_entity": "Michael_Scott_(The_Office);The_Office_(U.S._TV_series)"}
{"question": "did the Brazilian navy fight Portugal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Brazil;Portugal"}
{"question": "can Python scripts call the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Python_(programming_language);MediaWiki"}
{"question": "can Bulbapedia be accessed with the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "MediaWiki"}
{"question": "was Hamlet first shown in an IMAX 3D theater?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hamlet;IMAX"}
{"question": "can the MediaWiki API be connected directly to Wikipedia articles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "MediaWiki"}
{"question": "at Christmastime, are there reminders of groundhog day coming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the United States Department of Defense have a balanced budget?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Department_of_Defense"}
{"question": "does Bojack Horseman have an ongoing parody of penguin books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "BoJack_Horseman"}
{"question": "would a Doctor of Medicine in orthopedics be appropriate for a compound fracture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Doctor_of_Medicine"}
{"question": "is Redbird Smith a religious leader for the Cherokee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cherokee"}
{"question": "was Sasha Obama friends with John Lennon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Lennon"}
{"question": "if you've passed your A Levels, are you aware of the Constitution of the United Kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "GCE_Advanced_Level;Constitution_of_the_United_Kingdom"}
{"question": "is the city that hosts the French Open also known as the Windy City?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "French_Open"}
{"question": "did Lionel Messi ever play in the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lionel_Messi;French_Open"}
{"question": "did Lionel Messi win at the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lionel_Messi;French_Open"}
{"question": "do DJ's for weddings typically have Bryan Adams in their playlist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bryan_Adams"}
{"question": "is Bryan Adams frequently played at weddings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bryan_Adams"}
{"question": "could a farmer make both soy milk and tempeh from the same crop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Tempeh"}
{"question": "did President Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ronald_Reagan;League_of_Legends"}
{"question": "did Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ronald_Reagan;League_of_Legends"}
{"question": "can I buy League of Legends in the Android App Store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "League_of_Legends"}
{"question": "can I play League of Legends on a Nintendo Entertainment System?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "League_of_Legends;Nintendo_Entertainment_System"}
{"question": "are there enough shogi game pieces to replace missing chess pieces?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Kim Kardashian shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kim_Kardashian;Sainsbury's"}
{"question": "does Malia Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Family_of_Barack_Obama;Sainsbury's"}
{"question": "does President Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;Sainsbury's"}
{"question": "did the Byzantine calendar have a different new year than the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julian_calendar"}
{"question": "wouldna hoverboard take beyond two days to finish the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Daytona_500"}
{"question": "would being in a lifeboat after a wreck be unsafe during a tropical storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "did biologist Rachel Carson write critiques of the FDA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Rachel_Carson"}
{"question": "can an Alaskan gardener sow eggplant directly outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alaska"}
{"question": "would lolcats and roflcopters be known to World of Warcraft players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lolcat;World_of_Warcraft"}
{"question": "is coca leaf good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Coca"}
{"question": "is coca leaf a good diet supplement for weight gain?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Coca"}
{"question": "would cultural anthropologists document historic cultivation of coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cultural_anthropology;Coca"}
{"question": "would botanists document coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is copper a good choice for an artwork that changes color over time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "copper", "extracted_entity": null}
{"question": "can copper inhibit parasitic spores?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a painter use copper so images of trees remain vibrant in sunlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Elizabeth I of England wear gowns of viscose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elizabeth_I_of_England"}
{"question": "would a fungal life-form be wary of green pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a derivative of copper end a fungal infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can goldfish breed in saltwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are goldfish friendly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are paratroopers good at search and rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Higher Education required to teach high school in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_York"}
{"question": "would spaghetti in the Philippines be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Philippines"}
{"question": "can Spaghetti be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spaghetti"}
{"question": "can onion peel be used for things?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can onion peel be repurposed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would it be inappropriate to put a Flag of the United States on the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Flag_of_the_United_States"}
{"question": "should someone doing mixed martial arts have health insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mixed_martial_arts"}
{"question": "is Futurama a movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Futurama"}
{"question": "can you book a ticket on a fighter jet on British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "British_Airways"}
{"question": "can you book a ticket on a fighter jet through British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "British_Airways"}
{"question": "does British Airways operate a fleet of trains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "British_Airways"}
{"question": "has British Airways flown in space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "British_Airways"}
{"question": "can human overpopulation be stopped?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Human_overpopulation"}
{"question": "is Ariana Grande old enough to drive a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande"}
{"question": "can water be bad for a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can cows get Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cholera"}
{"question": "do authorities always respond calmly to sit-ins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you survive Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cholera"}
{"question": "is Niagara Falls a national park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Niagara_Falls"}
{"question": "is Niagara falls the tallest waterfall in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canada"}
{"question": "is Niagara falls the tallest waterfall in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "North_America"}
{"question": "was Niagara Falls formed by a meteorite? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "alse", "extracted_entity": "Niagara_Falls"}
{"question": "is Niagara falls in mexico?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Niagara_Falls;Mexico"}
{"question": "do salmon migrate up Niagara Falls to spawn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Niagara_Falls"}
{"question": "can salmon swim up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Niagara_Falls"}
{"question": "do salmon travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Niagara_Falls"}
{"question": "do ships travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Niagara_Falls"}
{"question": "are there various types of barn floors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a tailor adjust a kimono?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kimono"}
{"question": "is October a month during which people seek out fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Halloween", "extracted_entity": null}
{"question": "is fear a common emotion experienced at Universal Studios Orlando?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Universal_Orlando"}
{"question": "did the Moon landing occur before the first episode of Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do people who attend AA meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do people who attend drug abuse counseling meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do people who attend AlAnon meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the Sahara desert have a big variety of plant life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sahara"}
{"question": "was there a famous director who came from Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "George", "extracted_entity": "Happy_Days"}
{"question": "was a Happy Days actor also in Little Nicky?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Adam", "extracted_entity": "Happy_Days"}
{"question": "was a member of the Happy Days cast also in movies with Adam Sandler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Henry", "extracted_entity": "Adam_Sandler"}
{"question": "can you kayak to Saint Vincent and the Grenadines from Montreal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saint_Vincent_and_the_Grenadines;Montreal"}
{"question": "are goldfish bottom feeders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Goldfish"}
{"question": "for movies with a 'G' rating, are teens an appropriate audience?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Teens", "extracted_entity": null}
{"question": "is cellulose important for bowel maintenance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ell", "extracted_entity": "Cellulose"}
{"question": "can cellulose help with constipation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cellulose"}
{"question": "can a rainbow be manmade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can rainbows occur without rain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Crab"}
{"question": "would a vet use dual-energy X-ray absorptiometry on a jellyfish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jellyfish"}
{"question": "would a vet use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lobster"}
{"question": "would a veterinarian use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lobster"}
{"question": "can all plants withstand frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the United States Secretary of State a woman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Secretary_of_State"}
{"question": "will people vote United States Secretary of State this November?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States_Secretary_of_State"}
{"question": "are all parts of the aloe plant tasty?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Aloe"}
{"question": "could you keep a bengal fox as a pet in New York?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bengal_fox;New_York_City"}
{"question": "is a Haiku a type of painting?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Haiku"}
{"question": "are fashion design jobs only available to women?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Dungeons and Dragons be played with the participants in different homes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dungeons_&_Dragons"}
{"question": "is Kayaking possible for double arm amputees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do Snow Leopards have valuable parts that make them likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do Snow Leopards have attributes that make them more likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a crime scene cleanup crew have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a medical sanitation company have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can Short-Eared Dogs still get ear mites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can green makeup help reduce rosacea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Green", "extracted_entity": null}
{"question": "would a spiral fracture require medical imaging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Medical_imaging"}
{"question": "did Claude Monet fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Claude_Monet;World_War_II"}
{"question": "was Claude Monet famous for playing basketball?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Claude_Monet"}
{"question": "did Claude Monet teach Salvador Dali how to paint?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Claude_Monet;Salvador_Dal\u00ed"}
{"question": "did Claude Monet take painting classes from Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Claude_Monet;Leonardo_da_Vinci"}
{"question": "was the the Pony Express was used to deliver mail in 1901?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pony_Express"}
{"question": "is it strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Happy_hardcore"}
{"question": "would it be strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Happy_hardcore"}
{"question": "would it be strange to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet when flying overhead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Orange_County,_California;California"}
{"question": "would an Arctic fox stand out against a pile of roasted coffee beans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arctic_fox"}
{"question": "would an Arctic fox stand out against a pile of coal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Alan Turing be looked down upon at Pride parades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alan_Turing;Pride_parade"}
{"question": "can you go ice fishing for anchovy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "i", "extracted_entity": "Ice_fishing;Anchovy"}
{"question": "is murder something the Joker would consider?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can Anchovy walk on land?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a chiropractor be able to treat dystonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "chiropr", "extracted_entity": null}
{"question": "do people often give ducks food that is bad for them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Alan Rickman spend many hours in a black robe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Rickman"}
{"question": "would the 10th Doctor on Doctor Who refuse a pear salad?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Tenth_Doctor;Doctor_Who"}
{"question": "would an explosion at a gunpowder storage facility result in a sonic boom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Russian minor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Twelve-tone_technique"}
{"question": "would it be pointless to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Indian_classical_music"}
{"question": "would it be impossible to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indian_classical_music"}
{"question": "would it be impossible to use the twelve-tone technique in Saba scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Samuel_Sebastian_Wesley"}
{"question": "would it be unusual to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would it be unusual to play happycore at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would it be unusual to play happycore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "could over reproduction of some species be unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the pentatonic scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Han Dynasty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Han_dynasty"}
{"question": "would it be impossible to use the twelve-tone technique in the Kuiper Belt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kuiper_belt"}
{"question": "would it be impossible to use the twelve-tone technique on Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mars"}
{"question": "would it have been impossible to use the twelve-tone technique in New Amsterdam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "impossible", "extracted_entity": "New_Amsterdam"}
{"question": "would it have been impossible for Jane Austen to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jane_Austen"}
{"question": "would it have been impossible for Neanderthals to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Neanderthal"}
{"question": "would it have been impossible for Shakespeare to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "William_Shakespeare"}
{"question": "would it have been impossible for Beethoven to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ludwig_van_Beethoven"}
{"question": "could A. D. Condo enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "sure", "extracted_entity": null}
{"question": "could a World War I veteran enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "World_War_I"}
{"question": "could a World War I veteran enjoy the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "World_War_I"}
{"question": "could Chinook salmon benefit someone with a major mood disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Chinook salmon benefit persons with heart disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinook_salmon"}
{"question": "did Auguste Comte define the study of society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Auguste_Comte"}
{"question": "could Oprah Winfrey buy multiple private islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oprah_Winfrey"}
{"question": "is Oprah Winfrey part of the largest generation of the 20th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oprah_Winfrey"}
{"question": "did Spirit go further than the winner of the Birell Prague Grand Prix?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Spirit go further than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Spirit cover more ground than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are fossil fuels costing fishermen jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gulf_of_Mexico"}
{"question": "could people sing songs from the Sound of Music before 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Sound_of_Music_(film)"}
{"question": "does b\u00f6rek share an origin with the Dolmabah\u00e7e Palace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Dolmabah\u00e7e_Palace"}
{"question": "would it be unusual to play Happy hardcore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can someone with lactose intolerance easily enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can someone with lactose intolerance enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lactose_intolerance"}
{"question": "has the Toyota Supra lost the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Toyota_Supra;Kentucky_Derby"}
{"question": "has the Toyota Supra won the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Toyota_Supra;Kentucky_Derby"}
{"question": "was the Forbidden City constructed using a backhoe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Backhoe"}
{"question": "was the Forbidden City constructed using a bulldozer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Forbidden_City"}
{"question": "is hunger more unsafe for diabetics than health people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would sloths be at great risk in a forest fire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that shows female breasts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that has nudity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would someone with multiple lip piercings pay more in dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a fan of the Pokemon games be likely to like Nintendo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "obably", "extracted_entity": "Pok\u00e9mon;Nintendo"}
{"question": "is it likely that Dan Harmon would enjoy parody?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Dan_Harmon"}
{"question": "was the United States Secretary of Defense enlisted for fewer years than the man who replaced Barry Goldwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Secretary_of_Defense;Barry_Goldwater"}
{"question": "could a smallpox vaccine have changed the outcome of the American Indian Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "American_Indian_Wars"}
{"question": "do farmers and cargo ships sometimes work the same waterways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes use the same area for work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Aristotle ever see Cats the musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aristotle;Cats_(musical)"}
{"question": "was Martin Luther King Jr able to vote because of the 23rd amendment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Martin_Luther_King_Jr."}
{"question": "did the 23rd Amendment help Washington become president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the 23rd Amendment help Teddy Roosevelt get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Theodore_Roosevelt"}
{"question": "did the 23rd Amendment help JFK get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "John_F._Kennedy"}
{"question": "could Washington D.C. residents vote for John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Washington,_D.C.;John_F._Kennedy"}
{"question": "is the amendent granting DC residents the right to vote part of the Bill of Rights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States_Bill_of_Rights"}
{"question": "is Pan a child of Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pan_(god);Zeus"}
{"question": "would a student in ancient Thessaly have learned about Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Thessaly"}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an Emmy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jack_Black;Bear_McCreary;Emmy_Award"}
{"question": "is Jack Black unlikely to compete with John Williams for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jack_Black;John_Williams"}
{"question": "can I visit Saint Vincent and the Grenadines by airplane?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Saint_Vincent_and_the_Grenadines;Saint_Vincent_(Antilles)"}
{"question": "can I visit Saint Vincent and the Grenadines on a boat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saint_Vincent_(Antilles);Saint_Vincent_and_the_Grenadines"}
{"question": "can the Abitur be granted to German Shepherds?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Germany"}
{"question": "can German shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Germany"}
{"question": "can German Shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Germany"}
{"question": "can German Shepherds acquire the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Germany"}
{"question": "could John Key be inaugurated as president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key be inaugurated as president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key serve a four-year term as the Commander in Chief of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key serve a four-year term as America's Commander in Chief?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key"}
{"question": "could John Key serve a four-year term as America's president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key serve a four-year term as the president of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;President_of_the_United_States;United_States"}
{"question": "could John Key serve a four-year term as the president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key serve a four-year term as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key serve a four-year term as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Key;President_of_the_United_States;United_States"}
{"question": "could John Key serve as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;President_of_the_United_States;United_States"}
{"question": "could John Key serve as the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "John_Key"}
{"question": "could John Key serve as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key become the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key;United_States"}
{"question": "could John Key become the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key"}
{"question": "could John Key become the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "John_Key;United_States"}
{"question": "would diepoxybutane be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would butane diepoxide be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "would butane diepoxide be present in the ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would the ideal drinking water contain butane diepoxide?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would the ideal drinking water contain strychnine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Strychnine"}
{"question": "would the ideal drinking water contain anthrax?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Steve Ballmer unable to use a keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Steve_Ballmer"}
{"question": "is it unnecessary to purchase food for a Lolcat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would members of the Communist Party USA find a billionaire class reprehensible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Communist_Party_USA"}
{"question": "does the musical Rent feature New Years day twice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Rent_(musical)"}
{"question": "would Bird Person be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bird", "extracted_entity": "Pluto;Rick_and_Morty"}
{"question": "would Summer Smith be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Summer", "extracted_entity": "Summer_Smith;Pluto;Rick_and_Morty"}
{"question": "would Summer be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pluto;Rick_and_Morty"}
{"question": "would Rick be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Rick_Sanchez;Pluto"}
{"question": "does Rick and Morty discuss the status of Pluto as a planet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pluto"}
{"question": "did the Big Bang create both matter and antimatter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Big_Bang;Antimatter"}
{"question": "can the time of the Big Bang be determined by telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Big_Bang"}
{"question": "could Jackie Chan learn Jujutsu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Jackie_Chan;Jujutsu"}
{"question": "would the Ku Klux Klan welcome Alicia Garza into their group?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ku_Klux_Klan;Alicia_Garza"}
{"question": "would the Ku Klux Klan accept Alicia Garza?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ku_Klux_Klan;Alicia_Garza"}
{"question": "would Alicia Garza be appreciated by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alicia_Garza;Ku_Klux_Klan"}
{"question": "would Alicia Garza be accepted into the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alicia_Garza;Ku_Klux_Klan"}
{"question": "would Alicia Garza be accepted by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alicia_Garza;Ku_Klux_Klan"}
{"question": "can paratroopers be used on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the United States use paratroopers in World War One?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "would only warm weather attire be acceptable on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mercury_(planet)"}
{"question": "did J. D. Salinger ever ask his parents for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "J._D._Salinger"}
{"question": "did J. D. Salinger ever celebrate a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "J._D._Salinger"}
{"question": "did J. D. Salinger have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "J._D._Salinger"}
{"question": "did J. D. Salinger ever have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "J._D._Salinger"}
{"question": "would it be very unlikely for Christopher Hitchens to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Hitchens"}
{"question": "hypothetically, could the entire 2018 class at Princeton watch a match at Wembley Arena?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Princeton_University;Wembley_Arena"}
{"question": "does acid rain damage the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Louvre"}
{"question": "was the American Constitution signed before the Louvre was built?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "United_States_Constitution;Louvre"}
{"question": "did Olympia, Washington become a city after the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Olympia,_Washington;American_Civil_War"}
{"question": "was Olympia, Washington around during the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Olympia", "extracted_entity": "Olympia,_Washington;Washington_(U.S._state);American_Civil_War"}
{"question": "could a drag race be held on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a truck drive on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Kim Kardashian the first guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kim_Kardashian"}
{"question": "is President Trump a guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donald_Trump;Guru"}
{"question": "is street art allowed under the First Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "First_Amendment_to_the_United_States_Constitution"}
{"question": "was Alaska part of the Union during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Union_(American_Civil_War);American_Civil_War"}
{"question": "can League of Legends be played on a DSL connection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "League_of_Legends"}
{"question": "would the League of Legends download file fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "League_of_Legends"}
{"question": "would the League of Legends download fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "League_of_Legends"}
{"question": "would the League of Legends download fit on a Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "League_of_Legends"}
{"question": "could a high school senior in 2020 have played League of Legends when it was new?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "League_of_Legends"}
{"question": "has Leage of Legends been around more years than Fortnite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Fortnite"}
{"question": "is the Bible considered to be a parody?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bible"}
{"question": "are goldfish friendly to eachother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Goldfish"}
{"question": "can you buy chlorine at a supermarket?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would honey have been accessible to ancient romans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "honey", "extracted_entity": null}
{"question": "would honey have been accessible to Julius Caesar? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Julius_Caesar"}
{"question": "is some of the weight of a papaya inedible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Papaya"}
{"question": "is Dustin Hoffman unlikely to support Mike Pence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dustin_Hoffman;Mike_Pence"}
{"question": "did Christopher Nolan meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Nolan;Dwight_D._Eisenhower"}
{"question": "did Christopher Nolan ever meet  former President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Nolan;Dwight_D._Eisenhower"}
{"question": "did Christopher Nolan ever meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Nolan;Dwight_D._Eisenhower"}
{"question": "did Christopher Nolan ever meet Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Nolan;Dwight_D._Eisenhower"}
{"question": "did Christopher Nolan ever meet Judy Garland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Christopher_Nolan;Judy_Garland"}
{"question": "did Christopher Nolan ever meet President Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Nolan;John_F._Kennedy"}
{"question": "are goldfish bowls bad for goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Nolan;John_F._Kennedy"}
{"question": "did Christopher Nolan ever meet John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Nolan;John_F._Kennedy"}
{"question": "could Aristotle have gone to school to be a railroad engineer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Aristotle"}
{"question": "was Saint Vincent and the Grenadines a colony when Elton John was born?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saint_Vincent_and_the_Grenadines;Elton_John"}
{"question": "was Saint Vincent and the Grenadines a colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Saint_Vincent_and_the_Grenadines"}
{"question": "does Bill Gates experience menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bill_Gates"}
{"question": "does Bill Gates have trouble dealing with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bill_Gates"}
{"question": "does Bill Gates have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Gates"}
{"question": "does Bill Gates frequently have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bill_Gates"}
{"question": "does the rover Spirit have a spirit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spirit_(rover)"}
{"question": "is a curling iron helpful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Curling"}
{"question": "is a curling iron useful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Curling"}
{"question": "is a musket required for the French Defence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "France"}
{"question": "would a casino goer's '7' charm bracelet be likely for luck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "does Stephen King disagree with J.K Rowling on trans issues?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Stephen", "extracted_entity": "Stephen_King;J._K._Rowling"}
{"question": "are there still prisoners being kept in Alcatraz island?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Alcatraz_Island"}
{"question": "can you see Alcatraz Island from the Atlantic City boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alcatraz_Island;Atlantic_City,_New_Jersey"}
{"question": "can you see Alcatraz Island from the Jersey Shore boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alcatraz_Island;Jersey_Shore"}
{"question": "would Lord Voldemort have been permitted into Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hogwarts"}
{"question": "would sandals be unsafe at a construction site?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Doctor Who represent interracial couples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Doctor_Who"}
{"question": "in Doctor Who, can statues be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Doctor_Who"}
{"question": "in Doctor Who, can statues be threatening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Doctor_Who"}
{"question": "can people meet each other out of order in Doctor Who?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada;Doctor_Who"}
{"question": "are there family planning options without hormones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": null}
{"question": "did Gorbachev influence the Russian Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Russian_Revolution"}
{"question": "would Tom Cruise consider his Katie Holms a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Cruise"}
{"question": "would Tom Cruise consider his ex-wife a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tom_Cruise"}
{"question": "would Tom Cruise be unlikely to support psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Cruise"}
{"question": "is Tom Cruise opposed to psychiatric medication??", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Cruise"}
{"question": "is Tom Cruise opposed to psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Cruise"}
{"question": "can you complete watercolor paintings without a cup of water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do poop and hot dog filling have anything in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They", "extracted_entity": null}
{"question": "was Lamarck's evolution theory compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jean-Baptiste_Lamarck;Charles_Darwin"}
{"question": "was Lamarckian evolution compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jean-Baptiste_Lamarck;Charles_Darwin"}
{"question": "would a poisoner have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like french beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Homer_Simpson"}
{"question": "would Homer Simpson be likely to like beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Homer_Simpson;Beignet"}
{"question": "does the San Francisco Bay have more of a homeless issue than Albany New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "San", "extracted_entity": "San_Francisco_Bay;Albany,_New_York;New_York"}
{"question": "was there ever a South Park episode about someone who has a song about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "South_Park;Louvre"}
{"question": "did South Park make a parody of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "South_Park;Louvre"}
{"question": "did South Park make fun of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "South_Park;Louvre"}
{"question": "is a lot of famous New York food from Jewish cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_York_City"}
{"question": "does New York City have a lot of famous Jewish Cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_York_City"}
{"question": "would it be uncommon fora teenager to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "is agriculture without irrigation difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about teaching romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "Robin_Williams"}
{"question": "has Robin Williams ever been involved in a film about romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Robin_Williams"}
{"question": "is sunlight important in pigmentation of asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Asparagus"}
{"question": "does light matter in pigmentation of Asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Asparagus"}
{"question": "are there Chinese imports in all of the Aldi stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "People's_Republic_of_China;Aldi"}
{"question": "would fans of Nine Inch Nails be likely to know of Johnny Cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nine_Inch_Nails;Johnny_Cash"}
{"question": "are there red varieties of picked cucumber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there times that fear causes euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Fear;Euphoria"}
{"question": "would it be difficult to sleep in the world's most quiet room?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could a mole of glucose fit inside the Empire State Building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Empire_State_Building"}
{"question": "could Johnny Carson's children form a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Johnny_Carson"}
{"question": "would a silicon shortage be harmful to Intel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Intel"}
{"question": "was Mozart inspired by Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wolfgang_Amadeus_Mozart;Richard_Wagner"}
{"question": "are there romantic Doctor Who moments near Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Doctor_Who;Big_Ben"}
{"question": "does Hank Hill sell items useful for grilling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could lots of Long Island Iced Teas cause Liver Disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "has anyone from Texas ever served as Vice President of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Texas;Vice_President_of_the_United_States"}
{"question": "are pecans and peanuts grown in the same part of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "have oil companies contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would the Neanderthal people have found snowshoes useful?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Neanderthal"}
{"question": "are all canidae good house pets?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Canidae"}
{"question": "do students at Berlin University of the Arts recite the Pledge of Allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Berlin_University_of_the_Arts;Pledge_of_Allegiance"}
{"question": "could Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jeanne_Moreau;AEW_Dynamite"}
{"question": "did Jeanne Moreau see AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jeanne_Moreau;AEW_Dynamite"}
{"question": "did Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jeanne_Moreau;AEW_Dynamite"}
{"question": "is it likely that Katy Perry's baby will look like Russell Brand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Katy_Perry;Russell_Brand"}
{"question": "do all drag kings take testosterone injections?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it possible to confuse acne and chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Acne_vulgaris"}
{"question": "could acne scars be mistaken for chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chickenpox"}
{"question": "would someone on foodstamps be less likely to own a christian dior bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Bill Clinton use a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Clinton;PlayStation_4;Oval_Office"}
{"question": "did Bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bill_Clinton;PlayStation_4;Oval_Office"}
{"question": "did bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Clinton;PlayStation_4;Oval_Office"}
{"question": "did Richard Nixon play Call Of Duty on a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Nixon;PlayStation_4"}
{"question": "did Richard Nixon play Call Of Duty on a  Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Richard_Nixon;PlayStation_4"}
{"question": "did Richard Nixon use a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Nixon;PlayStation_4"}
{"question": "could Jesse Owens have won a medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jesse_Owens;1936_Summer_Olympics"}
{"question": "did Thierry Rey win a gold medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "1936_Summer_Olympics"}
{"question": "are oysters harvested to make jewelry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "has the COVID-19 virus been used for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "COVID-19;Whole_genome_sequencing"}
{"question": "is the COVID-19 virus a good candidate for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Whole_genome_sequencing"}
{"question": "is it dangerous to walk to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Very_Large_Telescope"}
{"question": "is Edward Snowden in hiding?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Edward_Snowden"}
{"question": "did Elvis popularize smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elvis_Presley;Smooth_jazz"}
{"question": "did Elvis invent smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Elvis_Presley;Smooth_jazz"}
{"question": "would oak burn on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the media sugar coat the health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would something with a high viscosity be hard to fit through a plastic straw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would bringing a Torah to a Mosque be unusual?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Torah;Mosque"}
{"question": "are LGBT Pride events due, in part, to the work of Drag Queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "were Drag Queens involved in the creation of Pride events?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do many American stores have lots of notebook on sale in August?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "is handling some turtles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is handling a snapping turtle dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Common_snapping_turtle"}
{"question": "should a doctor's office have a ramp for wheelchairs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are some children very excited to meet Robert Downey Jr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Robert_Downey_Jr."}
{"question": "is some adverting hidden in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you see sea lions in the San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "San_Francisco_Bay"}
{"question": "are there groups within the movement of feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Feminism"}
{"question": "is there infighting within feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Feminism"}
{"question": "do some artists support themselves through patronage alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a linguistics major take SQL courses?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it difficult to interview Edward Snowden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Edward_Snowden"}
{"question": "can street art be used as a form of protest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you kayak from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_England;England"}
{"question": "can you walk from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_England;England"}
{"question": "is Vice Media a peer reviewed journal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Vice_Media"}
{"question": "could you go to New York Public Library and the Adirondacks in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_York_Public_Library;Adirondack_Mountains"}
{"question": "do nurses watch anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "in treatment, would someone with anoxrexia nervosa be watched?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If", "extracted_entity": null}
{"question": "can you store hamburgers in the car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "you", "extracted_entity": null}
{"question": "does Elon Musk act as an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elon_Musk"}
{"question": "is Billy Joel considered high risk for COVID-19?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Billy_Joel;COVID-19"}
{"question": "in Tetris99 do Newbies play with Pro players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "List_of_Tetris_variants"}
{"question": "did P. G. Wodehouse like cartoon shows as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "P._G._Wodehouse"}
{"question": "did P. G. Wodehouse watch Rick and Morty as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "P._G._Wodehouse;Rick_and_Morty"}
{"question": "is it likely some paleontologists were inspired by Michael Crichton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Crichton"}
{"question": "could Michael Crichton have inspired some paleontologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Crichton"}
{"question": "do some people feel nauseous after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the Antarctic coast have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Antarctic"}
{"question": "does Antarctica have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica"}
{"question": "is homelessness generally voluntary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Homelessness"}
{"question": "would Paypal be a good way to pay for pornography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "PayPal"}
{"question": "are some scientific journals less credible than others?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the author of the Da Vinci Code ever view depictions of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Da_Vinci_Code;Last_Supper"}
{"question": "has the author Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dan_Brown;Last_Supper"}
{"question": "has Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dan_Brown"}
{"question": "do people find reason to protest the London Zoo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "London_Zoo"}
{"question": "have there been zombie adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jane_Austen"}
{"question": "have there been horror adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jane_Austen"}
{"question": "do different parts of the globe celebrate New Year's Day on different calendar dates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_Year's_Day"}
{"question": "were people in Utah worried about damage from Hurricane Harvey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Utah;Hurricane_Harvey"}
{"question": "are Indigenous peoples in Canada still fighting for equal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aboriginal_peoples_in_Canada"}
{"question": "are all pickles made with lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lakes in the Adirondack Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Adirondack_Park"}
{"question": "could you swim with Manta Ray in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could you swim with Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Great_Lakes"}
{"question": "would you be likely to see Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "has Subway had ties with any child predators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is pain in the torso a potential sign of appendicitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a kidney infection cause pain in the torso?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can you get xanax from a pharmacy without a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "X", "extracted_entity": null}
{"question": "are there substitutes available for human legs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do students with dyscalculia need more help with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would a task of documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a job documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a job about statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "could someone with dyscalculia struggle with documenting statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would someone with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a student with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a student with dyscalcula struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Dyscalculia"}
{"question": "can judges be sent to jail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are most people the result of ectopic pregnancies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ectopic_pregnancy"}
{"question": "could you see the Eiffel Tower from the top of the Vatican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eiffel_Tower;Vatican_City"}
{"question": "can you see a themed King Kong ride in Universal Studios?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Universal_Studios"}
{"question": "can you see a themed King Kong ride in Florida?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do desert hares live in the Balkans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Balkans"}
{"question": "do the Balkans experience monsoons?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Balkans"}
{"question": "can you smell argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you view argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you pick up argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Johnny Carson make phone calls while driving?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Johnny_Carson"}
{"question": "is Bruce Lee making a new movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bruce_Lee"}
{"question": "can you shake Bruce Lee's hand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bruce_Lee"}
{"question": "can someone survive without a pancreas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can you say hello to Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hello", "extracted_entity": "Aretha_Franklin"}
{"question": "can you meet Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aretha_Franklin"}
{"question": "do drag queens sometimes have a hard time finding jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do white blood cells keep people alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "White_blood_cell"}
{"question": "do guitarists have trouble with their fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does a guitarist have callused fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Julia Roberts pay taxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julia_Roberts"}
{"question": "did Douglas Adams use the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Douglas_Adams"}
{"question": "can sweet corn make diabetes worse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sweet_corn"}
{"question": "did Mario Andretti win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mario_Andretti;Space_Race"}
{"question": "was Europa important to Stanley Kubrick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "El_Pa\u00eds;Stanley_Kubrick"}
{"question": "are drag queens harassed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Claude Monet's widow receive aid from the French government after his death?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Claude_Monet;France"}
{"question": "should Koi and beta fish be kept together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Koi"}
{"question": "would children be disappointed with a deciduous Christmas tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are the blossoms of a peach tree equal in length to cherry tree blossoms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Peach;Cherry"}
{"question": "will a peach pit always grow into a peach tree?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "with the right makeup, can clowns be frightening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would nuclear attacks improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nuclear", "extracted_entity": "United_States"}
{"question": "would nuclear escalation improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "would fans of Christian Rock enjoy Shaggy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christian_rock;Shaggy_(musician)"}
{"question": "would a Galapagos penguin be out of place on the Antarctic Peninsula?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Antarctic_Peninsula"}
{"question": "is it ever necessary to bathe a sloth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you make soup without a pot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you make soup without heat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a blender be useful for making some soups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Blender"}
{"question": "are stim toys marketed at children with learning disabilities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Learning_disability"}
{"question": "are children with learning disabilities big consumers of stim toys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Learning_disability"}
{"question": "would someone who doesn't own a gun have a need for gunpowder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do anime fans enjoy detective fiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Anime"}
{"question": "are most milkshakes high in glucose?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could someone from 1920 have gotten HIV from a transfusion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can peppermint oil cure pink eye?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would you need a boat to get from Peru to the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Peru;Andes"}
{"question": "is it normal to be nervous before spinal chord procedures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a new mother need more time off from work than a childless person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a mother require more time off than a childless woman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would it be acceptable to have cats that live in your barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "are wild boar hunted by indigenous peoples in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Wild_boar;Aboriginal_peoples_in_Canada"}
{"question": "do indigenous peoples in Canada hunt wild boar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Aboriginal_peoples_in_Canada;Wild_boar"}
{"question": "is it likely that you'd hear disco music at Mazi Nightclub?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "have there been any Greeks in space?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Tom Cruise know what an e-meter is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tom_Cruise;E-meter"}
{"question": "was Mozart inspired by Richard Wagner's music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Wolfgang_Amadeus_Mozart;Richard_Wagner"}
{"question": "can you see the stars in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Wembley_Arena"}
{"question": "does Marvel sell myths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a leftist be likely to refuse lunch from Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Chick-fil-A"}
{"question": "are there materials that are small enough to pass through cell walls?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are transgender teens less likely to read J.K. Rowling's writing than cisgender teens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "transgender", "extracted_entity": "Transgender;J._K._Rowling"}
{"question": "do some trees keep their leaves during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Some", "extracted_entity": null}
{"question": "do some trees keep their foliage during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you still see flowers in autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do an ocelot's eyes glow under a flashlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ocelot"}
{"question": "is Wasabi paste permitted for religious feasts in Judaism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Judaism"}
{"question": "can you travel by boat from Bern to Albuquerque?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bern;Albuquerque,_New_Mexico"}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Casablanca?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vietnam_War"}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Across the Universe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vietnam_War"}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Forest Gump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnam_War;Forrest_Gump"}
{"question": "is Santa Claus a nerd?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Santa_Claus;Nerd"}
{"question": "is there likely to be frost on the morning of Boxing Day in Johannesburg? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boxing_Day;Johannesburg"}
{"question": "is there likely to be frost on the morning of a Canberra Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canberra"}
{"question": "are most feminists proponents of conservatism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Rick and Morty be shocking to someone from the 1970's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "are fans of Community likely to enjoy Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they", "extracted_entity": null}
{"question": "would fans of Osmosis Jones know what white blood cells are?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Osmosis_Jones"}
{"question": "is material from an aloe plant found in bathroom cupboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Aloe"}
{"question": "could an astronomer detect noise from the beginning of the universe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could an astronomer warn of an imminent electrical blackout?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Astronomer"}
{"question": "would an abused child benefit from talking to a school counselor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it impossible to play \"Happy Birthday\" using the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Happy_Birthday_to_You"}
{"question": "are white blood cells a useful diagnostic tool for doctors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "will most attendees of groundhog day in Vermont be wearing jackets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vermont"}
{"question": "is it wise to wear a jacket on groundhog day in Vermont?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vermont"}
{"question": "is the weather usually chilly on groundhog day in the Northeastern US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Northeastern_United_States"}
{"question": "is it usually cold in Vermont on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vermont"}
{"question": "is it usually cold in Canada on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada"}
{"question": "is xenophobia counter productive to world peace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Xenophobia"}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulently made?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kurt_Cobain"}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Kurt_Cobain"}
{"question": "would Barack Obama be unlikely to be a friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Barack_Obama;Glenn_Beck"}
{"question": "would Barack Obama be an unlikely friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;Glenn_Beck"}
{"question": "would a guitarist feel less pain during a finger prick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have long terms?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Supreme_Court_of_the_United_States"}
{"question": "did the Disney company always have power over the Star Wars movies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Walt_Disney_Company;Star_Wars"}
{"question": "would an environmentalist advocate for preventing domestic feline reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Environmentalist"}
{"question": "would an environmentalist advocate for preventing house cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Environmentalist"}
{"question": "is it environmentally responsible to prevent domestic cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can some porches be inaccessible for disabled people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Jay-Z get free tickets to a Beyonce concert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jay_Z;Beyonc\u00e9"}
{"question": "is the average person at less risk than a celebrity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do celebrities feel less safe than most people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does being a celebrity make one's life less safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does being a celebrity put one in danger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is joining the United States Air Force an option for someone who takes Seroquel daily?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You", "extracted_entity": "United_States_Air_Force"}
{"question": "would someone on a daily regimen of an SSRI be able to enter the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Selective_serotonin_reuptake_inhibitor;United_States_Air_Force"}
{"question": "would someone on a daily regimen of an SSRI be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Selective_serotonin_reuptake_inhibitor;United_States_Air_Force"}
{"question": "would someone on a daily regimen of psych meds be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Air_Force"}
{"question": "would someone on a daily regimen of Valium be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Diazepam;United_States_Air_Force"}
{"question": "are animal rights advocates likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bullfighting"}
{"question": "are PETA members likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "People_for_the_Ethical_Treatment_of_Animals"}
{"question": "in Dead Poet's Society, do the boys in the English class enjoy poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "are the students on Dead Poets Society fans of Romantic Poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dead_Poets_Society;Romantic_poetry"}
{"question": "would a novice singer be nervous to perform for Simon Cowell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Simon_Cowell"}
{"question": "are people less skilled at climbing steel angles than animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are there animals capable of climbing more extreme angles than people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Sigmund Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sigmund_Freud"}
{"question": "would Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Freud", "extracted_entity": "Sigmund_Freud"}
{"question": "did Tony Bennett's children outnumber his wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tony_Bennett"}
{"question": "would firefighters respond to a nearby gunshot victim?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were some employees from the WTC in 2000 the victims of a terror attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a cook appreciate the gift of a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a cook appreciate a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sous-vide"}
{"question": "would people be disappointed in chicken on a Thanksgiving table?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Thanksgiving"}
{"question": "are there disparities in access to preventive healthcare in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "would your restroom be more odorous if you ate asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Asparagus"}
{"question": "would modern Marxists disapprove of capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a neglected child benefit from psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Psychotherapy"}
{"question": "are do it yourself tattoos more likely to become infected than shop tattoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "should students study Old English for the SAT?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Old_English;SAT"}
{"question": "would you see squid in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would you see squis in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are all translations of hello in spoken language?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are all translations of hello audible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Alcatraz Island significant to Native American history?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alcatraz_Island;Native_Americans_in_the_United_States"}
{"question": "would car travel from Brooklyn to Canada in a day be feasible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Brooklyn;Canada"}
{"question": "would car travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Brooklyn;Canada"}
{"question": "would it be unsafe for an aircraft pilot to have untreated narcolepsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Narcolepsy"}
{"question": "would narcolepsy make being an aircraft pilot impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot nearly impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would lily pads grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gobi_Desert"}
{"question": "would cattails grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gobi_Desert"}
{"question": "would cattails thrive in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "c", "extracted_entity": "Gobi_Desert"}
{"question": "are Kangaroos the smallest animal with a pouch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are Kangaroos the only animals with a pouch on the front of their body?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are right wing extremists opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Marxism"}
{"question": "do Jehova's Witnesses skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jehovah's_Witnesses;Easter"}
{"question": "would travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Brooklyn;Canada"}
{"question": "would an endive farmer worry if their crop was covered in frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a farmer worry if they saw frost on their endive plants?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "could someone who isn't mentally ill have a reason to seek psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would someone with a BA in English be well suited for copy editing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "Bachelor_of_Arts;English_language"}
{"question": "does inflammation of the brain require medical attention?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are all crustaceans on restaurant menus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is handedness unimportant in guitar playing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "have many fans of Spongebob seen Dustin Hoffman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dustin_Hoffman"}
{"question": "did you get a bad deal if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ubuntu_(operating_system)"}
{"question": "did you get ripped off if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ubuntu_(operating_system)"}
{"question": "would you be likely to see a tour guide in the Adirondack National Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Adirondack_Park"}
{"question": "would Goku from Dragon Ball Z work with a mercenary?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Goku", "extracted_entity": "Goku;Dragon_Ball_Z"}
{"question": "would a fan of  The Lord of the Rings books be likely to have read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Lord_of_the_Rings;The_Hobbit"}
{"question": "would a fan of  The Lord of the Rings books be likely to read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "The_Lord_of_the_Rings;The_Hobbit"}
{"question": "did the Attack on Pearl Harbor lead to the world's first nuclear attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Attack_on_Pearl_Harbor"}
{"question": "do System of a Down push for the recognition of the genocide against Armenians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "System_of_a_Down;Armenians"}
{"question": "do Armenians still have to fight for recognition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Armenians"}
{"question": "do System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "System_of_a_Down;Armenian_Genocide"}
{"question": "so System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "System_of_a_Down;Armenian_Genocide"}
{"question": "did Christopher Columbus play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus;Scrabble"}
{"question": "can Pandas play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Scrabble"}
{"question": "can a lobster procreate in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it a good idea to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does it make sense to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does it make sense to drink water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would you notice a difference between a shallot and an onion in french onion soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You", "extracted_entity": "Shallot;France"}
{"question": "could a garlic clove be confused for a shallot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "garlic", "extracted_entity": "Shallot"}
{"question": "could a shallot be confused for a garlic clove?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shallot"}
{"question": "could a shallot be confused for a cocktail onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Shallot"}
{"question": "does handedness determine how you use ASL?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are advocates of right wing extremism likely to support Proud Boys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Proud_Boys"}
{"question": "are advocates of right wing extremism against gender neutral restrooms? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would someone practicing right wing extremism donate to an abortion clinic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would an broken hand make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would an broken arm make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "do most PCMH have a file system?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can hyena's on steak alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Hyena"}
{"question": "can a cable ferry use horsehair for stabilization?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can a cable ferry be made from plastic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a cable ferry be made from cedar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Unitarian Universalists welcoming to people who align with agnosticism? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Unitarian_Universalism"}
{"question": "do Vegans refuse products made with honey bee products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Some", "extracted_entity": null}
{"question": "would it be unusual for a strict vegan to keep Honey Bees for harvest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would it be unusual for a vegan to keep honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Honey_bee"}
{"question": "would a honey bee's honey be unappealing to a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Honey_bee;Veganism"}
{"question": "is Hanoi considered large in Vietnam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hanoi;Vietnam"}
{"question": "did Eddie Murphy own smartphone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eddie_Murphy"}
{"question": "did Eddie Murphy own an iPhone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Eddie_Murphy;IPhone"}
{"question": "would someone have a hard time seeing a snowy owl against the sydney opera house?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "would Amish parents warn against feelings of lust?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Albert Einstein watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Albert_Einstein;Modern_Family"}
{"question": "did Donald Trump watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Donald_Trump;Modern_Family"}
{"question": "did Homer watch Modern Family?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Homer_Simpson;Modern_Family"}
{"question": "can Homer Simpson serve as President of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Homer_Simpson;President_of_the_United_States"}
{"question": "can Robert De Niro become President of South Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Robert_De_Niro;President_of_South_Africa"}
{"question": "can Robert De Niro run for President of France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Robert_De_Niro;President_of_France"}
{"question": "was the antagonist of the Lego Movie portrayed by a Saturday Night Live alumnus? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Lego_Movie;Saturday_Night_Live"}
{"question": "can parachuting cause loss of life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the Gallic Wars have an effect on France?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gallic_Wars;France"}
{"question": "were tanks used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gallic_Wars"}
{"question": "were guns used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gallic_Wars"}
{"question": "is Florida a well known curling state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Florida"}
{"question": "are all hot dogs created with smoking as a step?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is panoramic photography possible with a polaroid? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Panorama;Panoramic_photography"}
{"question": "is it fair to be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the upcoming new year worry people in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were people hoarding food and supplies out of fear in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do people opposed to gentrification butt heads with urban planners often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gentrification;Urban_planner"}
{"question": "on Venice beach, would you be likely to hear music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Venice"}
{"question": "are there countries where people opposed to fascism are considered terrorists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Fascism"}
{"question": "should you expect to be hungry before surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is it common to be hungry after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could a rhinoceros accidentally be included in a list of mythical creatures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could a rhinoceros be a playable animal in Dungeons and Dragons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Rhinoceros;Dungeons_&_Dragons"}
{"question": "does the United States Department of Education oversee undocumented students?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Department_of_Education"}
{"question": "would myofascitis be diagnosed by dual-energy X-ray absorptiometry?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can salt protect a garden that has spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Salt", "extracted_entity": "Spinach"}
{"question": "were people concerned about the new year in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "people", "extracted_entity": null}
{"question": "were computers a hot topic in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can mammals survive with only one kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz crystals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Reiki;Quartz"}
{"question": "at the office of a reiki master, would you be likely to see quartz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "obably", "extracted_entity": "Quartz"}
{"question": "are cucumbers a low maintenance plant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do The World's Billionaires deny pizzagate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would it be impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is it impossible to go snowboarding on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Venus"}
{"question": "is it impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Venus"}
{"question": "after COVID-19 came to the US, did Chinese Americans face discrimination?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "COVID-19;United_States;Chinese_Americans"}
{"question": "would someone with COVID-19 have a higher level of blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "COVID-19"}
{"question": "did Ralph Macchio make a fighting movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Ralph_Macchio;Jackie_Chan"}
{"question": "did Ralph Macchio make a karate movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Ralph_Macchio;Jackie_Chan"}
{"question": "did Putin help Russia win the space race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Vladimir_Putin;Russia"}
{"question": "has Osama Bin Laden been influencing the Trump Administration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Osama_bin_Laden;Presidency_of_Donald_Trump"}
{"question": "is sweet potato with marshmallow a holiday traditional dish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "during COVID-19, have more people been using Skype?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "COVID-19;Skype"}
{"question": "does Channon Rose use her platform to speak against abortion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO", "extracted_entity": null}
{"question": "can you get on the AirTrain JFK with a metropass?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "AirTrain_JFK"}
{"question": "do you need a Metropass to ride the AirTraine JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "AirTrain_JFK"}
{"question": "do you need a Metropass to ride the AirTraine at JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "John_F._Kennedy_International_Airport"}
{"question": "is Dr. Joseph Mengele a part of the history of psychology that is celebrated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Josef_Mengele"}
{"question": "for breast cancer patients, is plastic surgery considered reconstructive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Plastic_surgery"}
{"question": "did Europeans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Europe"}
{"question": "did Germans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Germany"}
{"question": "would someone in a MAGA hat consider accusations of Trump's islamophobia to be invalid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Make_America_Great_Again;Islamophobia"}
{"question": "would someone in a MAGA hat ignore claims about Trump of Islamophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Islamophobia"}
{"question": "do humans find the bite of a horse fly to be upsetting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "in 1960, did people know what the surface of the moon was like?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you go to the Mall of America and the Metropolitan Museum of Art in the same day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mall_of_America;Metropolitan_Museum_of_Art"}
{"question": "did Al Capone own an iPhone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al_Capone;IPhone"}
{"question": "do Scientologists celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Eid_al-Fitr"}
{"question": "do Mormons have a big Eid al-Fitr celebration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mormons;Eid_al-Fitr"}
{"question": "do Mormons celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mormons;Eid_al-Fitr"}
{"question": "do Christians worship multiple Gods?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a person survive by eating Ammonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has a Christmas character ever tried to save the New Year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the Forbidden City the seat of China's government?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Forbidden_City;People's_Republic_of_China"}
{"question": "did Woodrow Wilson have access to a telephone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Woodrow_Wilson"}
{"question": "can someone eat a Dodo today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dodo"}
{"question": "is the Dodo reproducing today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dodo"}
{"question": "can Cholera be treated successfully?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Dale Earnheardt Sr. crash due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dale_Earnhardt"}
{"question": "would it be bad for a Member of Parliament to be photographed with Jeffrey Epstein?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Member_of_parliament;Jeffrey_Epstein"}
{"question": "would fans of Weird Al be likely to enjoy The Onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Onion"}
{"question": "does Mozilla offer free educational programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Mozilla"}
{"question": "can you learn web development for free with Mozilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mozilla"}
{"question": "would it be harder for Kaitlyn Dobrow to do calligraphy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a Catholic missionary meet resistance with Tibetan people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Catholic_Church;Tibetan_people"}
{"question": "do guitarist's have finger tips that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "do finger blood tests typically hurt guitarist's less?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do guitarist's finger tips typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do guitarist's fingers typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "if Whole Foods is too expensive for you, would Aldi be a good option?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ld", "extracted_entity": "Whole_Foods_Market;Aldi"}
{"question": "would you be disappointed if you went to bojangles for a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does tourism harm the environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the off season?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do Mormons and 7th Day Adventists disagree about if Sunday is a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mormons"}
{"question": "do Christians disagree about whether Sunday is the day of rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is there disagreement among Christians regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is there argument regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is David A.R. White an advocate for acknowledging the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is David A.R. White known for arguing for the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did The Three Stooges get popular on Saturday Night Live?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Three_Stooges;Saturday_Night_Live"}
{"question": "were The Three Stooges inspired by Saturday Night Live skits?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Three_Stooges;Saturday_Night_Live"}
{"question": "would Bobby Fischer be likely to know of the French Defense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bobby_Fischer;France"}
{"question": "would a chocolate brownie from a marijuana dispensary be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would a chocolate brownie from a dispensary in Colorado be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Colorado"}
{"question": "would it have been wise to keep alcohol away from Amy Winehouse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amy_Winehouse"}
{"question": "if somebody wants a coolata is Starbucks a good place to go?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Starbucks"}
{"question": "did Jane Austen ever make the New York Times Bestseller list in her lifetime?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jane_Austen;The_New_York_Times_Best_Seller_list"}
{"question": "could someone take Northwest Airlines to the Beijing Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Northwest", "extracted_entity": "Northwest_Airlines;2008_Summer_Olympics"}
{"question": "could someone take Northwest Airlines to the Rio Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Northwest_Airlines;2016_Summer_Olympics"}
{"question": "would Benito Mussolini have success in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Benito_Mussolini;National_Basketball_Association"}
{"question": "would the leaves in Auburn, New York be changing colors in in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Auburn,_New_York"}
{"question": "would the leaves in Auburn, New York be changing colors in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Auburn,_New_York"}
{"question": "would you be able to watch the leaves change in the fall in Auburn, NY?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Auburn,_New_York"}
{"question": "do people often think that Frankenstein was a monster?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Frankenstein"}
{"question": "would Firefighters be included in a 9/11 memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do any games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there places with slot machines that you can get free drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does St. Jude's hospital help patients with Leukemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "St._Jude_Children's_Research_Hospital;Leukemia"}
{"question": "when you convince a child that they have no power, does it diminish their confidence? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "are there women who get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a woman get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "at a latino restaurant, would you find the dish coq au vin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Coq_au_vin"}
{"question": "is Call of Duty more popular than Doom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Call", "extracted_entity": "Call_of_Duty"}
{"question": "is lithium considered a good treatment for the mood disorder major depressive disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Major_depressive_disorder"}
{"question": "by junior high school, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "by their A-level exams, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "GCE_Advanced_Level"}
{"question": "by their A-levels, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "GCE_Advanced_Level"}
{"question": "are any colors on flag of the United States shared by flags of genocide perpetrators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Flag_of_the_United_States"}
{"question": "did Bonanza have greater longevity than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bonanza"}
{"question": "did Bonanza have greater staying power than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bonanza"}
{"question": "was Alexander Graham Bell's telephone made by motorola?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alexander_Graham_Bell"}
{"question": "are there people for whom the correct pronouns are 'he/him' who experience menstruation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Heracles have any full siblings that were immortal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Heracles"}
{"question": "has Aretha Franklin ever collaborated with someone that later committed suicide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aretha_Franklin"}
{"question": "are elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Photosynthesis;Atmosphere_of_Mars"}
{"question": "were several of Spartacus's companions from area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Spartacus;Gallic_Wars"}
{"question": "were any of Spartacus's companions born in area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Spartacus;Gallic_Wars"}
{"question": "were any of Spartacus's companions from the area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spartacus;Gallic_Wars"}
{"question": "were there any famous musicians with more letter a's in their name than Isaac Newton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Isaac_Newton"}
{"question": "were cannons built during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bronze_Age"}
{"question": "were cannons wielded during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bronze_Age"}
{"question": "were cannons used during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bronze_Age"}
{"question": "would firefighters respond to a heart attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Myocardial_infarction"}
{"question": "would firefighters respond to a stabbing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Firefighter"}
{"question": "did Shane Dawson work with any creators who have Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shane_Dawson;Anorexia_nervosa"}
{"question": "are any current world leaders former KGB members?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Vladimir", "extracted_entity": "KGB"}
{"question": "will you see the doctor a lot more during pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES", "extracted_entity": null}
{"question": "will someone have more doctor's appointments during their pregnancy than normal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can pregnancy put strain on friendships?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can pregnancy ruin a friendship?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you eat some kinds of citrus on birth control?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of iron earrings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Rosalind Franklin studying in Molecular Biology?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rosalind_Franklin"}
{"question": "would a chiropractic care center be more helpful for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "would a chiropractic care center be more appropriate for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do Doctor Who fans enjoy taking photos in pay phone booths?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Doctor_Who"}
{"question": "did students in 1999 have a different understanding of Pluto than those in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pluto"}
{"question": "would Kale be better to eat than Spinach for someone with scurvy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kale", "extracted_entity": null}
{"question": "is Kim Kardashian a supermodel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kim_Kardashian"}
{"question": "did Gladiator's weapon of choice require less hands than a Zweihander?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gladiator_(2000_film);Zweih\u00e4nder"}
{"question": "eminem's daughter spells her name differently than star of 1961 Disney film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "eminem's daughter spells her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "The_Parent_Trap_(1961_film)"}
{"question": "does Eminem's daughter spell her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eminem;The_Parent_Trap_(1961_film)"}
{"question": "does Eminem's daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eminem;The_Parent_Trap_(1961_film)"}
{"question": "does Eminem's biological daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eminem;The_Parent_Trap_(1961_film)"}
{"question": "did the early settlers of the U.S celebrate the first halloween?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "has any Doctor Who actor won a razzie award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Doctor_Who"}
{"question": "would a psychiatrist be likely to recommend Prozac to someone with bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fluoxetine;Bipolar_disorder"}
{"question": "has any Doctor Who actor won a Golden Globe award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Golden_Globe_Award"}
{"question": "would an SSRI be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bipolar_disorder"}
{"question": "would Zoloft be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sertraline;Bipolar_disorder"}
{"question": "has any Duke University graduate married a Grammy winner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Duke_University;Grammy_Award"}
{"question": "do pharmacy employees need to ask for customer ID every time?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Academy_Award;John_Kerry"}
{"question": "did any Oscar winners attend John Kerry's college?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Academy_Award;John_Kerry"}
{"question": "does the Torah have more books than Emily Bronte wrote?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Torah"}
{"question": "in Mexico, is Tuesday the 13th is considered more unlucky than Friday the 13th? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mexico;Friday_the_13th"}
{"question": "does the Torah have more books than the Hunger Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Torah;The_Hunger_Games"}
{"question": "does the Torah have more books than the Katniss Everdeen series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Torah;Katniss_Everdeen"}
{"question": "lil Wayne similar real name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "lil Wayne similar name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "lil Wayne needs over 4 times Grammy awards to equal rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "lil Wayne needs over 4 times Grammy awards to equal with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "lil Wayne double Grammy awards trails rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "does Lil Wayne need more Grammy awards to match rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "do Lil Wayne's Grammy awards trail rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "do Lil Wayne's Grammy awards lag behind rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "did the author of \"A Doll's House\" have a mentally disabled son?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "A_Doll's_House"}
{"question": "was Charles Manson's body count surpassed by any Marilyn Manson band member name origin killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charles_Manson;Marilyn_Manson_(band)"}
{"question": "can a bumblebee suffer from spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bumblebee"}
{"question": "can a bumblebee get spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bumblebee"}
{"question": "can a bumblebee get a blood clot?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bumblebee"}
{"question": "is the Numerologist featured on the podcast Oh No Ross and Carrie well known?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would someone opening a brewery need to get barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the Maori people build kayaks in the 1300's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "M\u0101ori_people"}
{"question": "spain native spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the Maori people reach New Zealand by kayak?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "M\u0101ori_people;New_Zealand"}
{"question": "spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "English_language"}
{"question": "does Spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Spanish_language;English_language"}
{"question": "do you need to add a consonant to September in order to get the Spanish spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Spanish_language"}
{"question": "did the Inuit fish trout from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Inuit"}
{"question": "did the Inuit hunt coyotes from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Inuit;Coyote"}
{"question": "any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Islam and Christianity have at least five prophets in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Islam;Christianity"}
{"question": "does pi exceed square root of pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Livonia"}
{"question": "is 5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Livonia"}
{"question": "is Pluto origin mythology still to be explored in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pluto;God_of_War_(franchise)"}
{"question": "pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "pl", "extracted_entity": "God_of_War_(franchise)"}
{"question": "is Pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pluto;God_of_War_(franchise)"}
{"question": "have any plays based on graphic novels won awards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Graphic_novel"}
{"question": "did the author of \"The Haunting of Hill House\" see the second film adaptation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see both of the film adaptations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Haunting_of_Hill_House"}
{"question": "if someone is wearing a Jack Skellington jacket, is it likely that they enjoy grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Jack_Skellington"}
{"question": "would Nightmare Before Christmas fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Nightmare_Before_Christmas"}
{"question": "would Tim Burton fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tim_Burton"}
{"question": "would Tim Burton fans be likely to embrace grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tim_Burton"}
{"question": "were modern assembly lines inspired by the work of Henry Ford?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henry_Ford"}
{"question": "do restaurant waiting staff in South Korea expect a tip?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "South_Korea"}
{"question": "do restaurant waiting staff in South Korea expect a gratuity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "South_Korea"}
{"question": "does a wedding designer typically make the floral arrangements?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "was John Kerry less active in the military than John McCain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Kerry;John_McCain"}
{"question": "is J.K. Rowling considered an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She", "extracted_entity": "J._K._Rowling"}
{"question": "does the LGBT community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "J._K._Rowling"}
{"question": "does the transgender community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "J._K._Rowling"}
{"question": "in teenagers with depression, are SSRI medications as safe as they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Selective_serotonin_reuptake_inhibitor"}
{"question": "would a teen with depression be at higher risk for adverse effects on a medication like Zoloft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sertraline"}
{"question": "have violent crowd control tactics been used against anyone in a sit-in?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sit-in"}
{"question": "is tobacco a major crop in the Southern United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Southern_United_States"}
{"question": "are there turtles that are dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would someone who was good at guitar hero be good at rock band?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do tailors predate the sewing machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tailor;Sewing_machine"}
{"question": "would Go be considerably more difficult for someone who was blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Go be considerably more difficult if you were blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would fauna in a tropical climate be very different from those in a temperate climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Tropical_climate;Temperate_climate"}
{"question": "does the Pope live in an area close to Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pope;Italy"}
{"question": "is country ham bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Country", "extracted_entity": "Hypertension"}
{"question": "would country ham be a bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ham", "extracted_entity": "Hypertension"}
{"question": "should someone watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "should people watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Country", "extracted_entity": null}
{"question": "does country ham taste different than city ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Will Ferrell feature in multiple films featuring elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Will_Ferrell"}
{"question": "has Will Ferrell been in multiple movies that feature elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Will_Ferrell"}
{"question": "can anyone with a driver's license work in transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can anyone with a driver's license work in bulk transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a comparison of a murder scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jackson_Pollock"}
{"question": "would a comparison of a crime scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jackson_Pollock"}
{"question": "at a presentation about PTSD, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande"}
{"question": "in 2020, is door-to-door advertising considered unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising socially unacceptable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising frowned upon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Victor Hugo's novel about the French Revolution ever become a Broadway show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Victor_Hugo;French_Revolution;Broadway_theatre"}
{"question": "did the Voyages of Christopher Columbus involve coming ashore on the West coast of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Voyages_of_Christopher_Columbus;United_States"}
{"question": "was George Washington aboard one of the ships during the voyages of Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_Washington;Christopher_Columbus"}
{"question": "was Jamestown the landing place for members of the Voyages of Christopher Columubus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jamestown,_Virginia"}
{"question": "was Michael Scott still on The Office when Will Ferrell's character came onto the show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Scott_(The_Office);The_Office_(U.S._TV_series);Will_Ferrell"}
{"question": "should Peter Griffin be familiar with the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Peter_Griffin"}
{"question": "was the death of the CEO of Apple caused by pancreatic cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Apple_Inc."}
{"question": "were Furbys popular during the Presidency of Bill Clinton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Clinton"}
{"question": "do students studying Shakespeare learn about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "William_Shakespeare"}
{"question": "would a Professor of a Shakespeare class have to teach the students about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "William_Shakespeare"}
{"question": "would a Professor of a Shakespeare class have to teach about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "William_Shakespeare"}
{"question": "did Kate Winslet and Leonardo Dicaprio star in a tragedy together?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kate_Winslet;Leonardo_DiCaprio"}
{"question": "is Yale University beyond 5000 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond 400 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond 100 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond 30 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ale", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond 20 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond a mile from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "is Yale University beyond a quick walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ale", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "did lead singer of The Police ever collaborate with an Arab?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Police;Arab_people"}
{"question": "did lead singer of The Police ever collaborate with an Algerian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Police;Algeria"}
{"question": "did Shaggy ever collaborate with a Blood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shaggy_(musician)"}
{"question": "are there any spirits in the Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qur'an"}
{"question": "does the average male foot fail to reach a foot (unit) on a ruler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Home Depot a good place to shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Home_Depot"}
{"question": "is Yale University beyond a walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ale", "extracted_entity": "Yale_University;University_of_New_Haven"}
{"question": "did Reza Shah's grandfather's grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reza_Shah;Constantinople"}
{"question": "did Reza Shah's great great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Reza_Shah;Constantinople"}
{"question": "did Reza Shah's great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Reza_Shah;Constantinople"}
{"question": "did Reza Shah's great great grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Reza_Shah;Constantinople"}
{"question": "did Reza Shah's grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Reza_Shah;Constantinople"}
{"question": "is anyone that was at the Last Supper important to Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Last_Supper;Islam"}
{"question": "are chocolate brownies hypothetically fatal to Harry Potter's Fang?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Harry_Potter_(character)"}
{"question": "can a chocolate brownie be fatal to a member of Canidae?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canidae"}
{"question": "can a chocolate brownie be fatal to a dog?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have protesters ever rallied for a governor to veto a bill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the Sun (United Kingdom) published in every United Kingdom country?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Sun_(United_Kingdom);United_Kingdom"}
{"question": "national Transportation Safety Board has more abbreviation letters than Mulder's X-Files agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "National_Transportation_Safety_Board"}
{"question": "do vendors ever claim that Quartz has healing properties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were Republican voters pleased with the outcome of  the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Republican_Party_(United_States);United_States_presidential_election,_1980"}
{"question": "did any states bordering California vote for Jimmy Carter in the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lifornia", "extracted_entity": "California;Jimmy_Carter;United_States_presidential_election,_1980"}
{"question": "were any of the blue states in the 1980's election on the East Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "East_Coast_of_the_United_States"}
{"question": "were any of the blue states in the 1980's election on the West Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "California", "extracted_entity": "West_Coast_of_the_United_States"}
{"question": "was Super Bowl 50 devoid of NFL founding teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Super_Bowl_50"}
{"question": "was Christopher Columbus a poor example of religious tolerance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher", "extracted_entity": "Christopher_Columbus"}
{"question": "does Burning Man effigy share similarities with The Wicker Man movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Wicker_Man"}
{"question": "is saying Melania Trump is from a third world country inaccurate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Melania_Trump"}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same gender Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Harvey_Milk"}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same sex Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Harvey_Milk"}
{"question": "can you reach the UNICEF headquarters from Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "UNICEF;Hoboken,_New_Jersey"}
{"question": "can you reach the UNICEF headquarters by Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "UNICEF;Hoboken,_New_Jersey"}
{"question": "has the executive director of UNICEF spent time in New England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "UNICEF;New_England"}
{"question": "has the executive director of UNICEF spent time in Massachusetts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "UNICEF;Massachusetts"}
{"question": "is the executive director of UNICEF familiar with a bra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "UNICEF;Bra"}
{"question": "would a bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Millie_Bobby_Brown"}
{"question": "did people used to believe bagel seeds were from same drug family as morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Morphine"}
{"question": "would a bartender make a cocktail for a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would a bartender accept an order from a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does country ham taste different than spiral ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would someone with celiac disease eat cookies baked with all purpose flour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Coeliac_disease"}
{"question": "would a bartender accept an order from a 2nd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a bartender accept an order from a 3rd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a bartender accept an order from a 4th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a bartender accept an order from Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jacob_Tremblay"}
{"question": "would a bartender provide service to Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jacob_Tremblay"}
{"question": "would a bartender provide service to Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Noah_Schnapp"}
{"question": "would a bartender make a White Russian for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Noah_Schnapp"}
{"question": "would a bartender make a Mojito for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mojito;Noah_Schnapp"}
{"question": "would a bartender make a cocktail for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Noah_Schnapp"}
{"question": "has name God Save the Queen been used in things other than royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Cynthia Powell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Lennon;Cynthia_Lennon"}
{"question": "did John Lennon celebrate a silver anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Lennon;Yoko_Ono"}
{"question": "did John Lennon celebrate a diamond anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Lennon;Yoko_Ono"}
{"question": "did John Lennon celebrate a golden anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Lennon;Yoko_Ono"}
{"question": "is RoboCop director a Razzie award legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "RoboCop"}
{"question": "would it be inaccurate to call Eve an orphan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eve"}
{"question": "was Hulk Hogan a good guy in one organization and bad in another?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hulk_Hogan"}
{"question": "can you be be denied penance for a deadly sin posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his deadly sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "William_Howard_Taft"}
{"question": "would William Howard Taft be denied penance today for his sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "William_Howard_Taft"}
{"question": "would William Howard Taft be denied penance today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "William_Howard_Taft"}
{"question": "would William Howard Taft be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "William_Howard_Taft"}
{"question": "would Jack the Ripper be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jack_the_Ripper"}
{"question": "would Jack the Ripper be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jack_the_Ripper"}
{"question": "would Jeffrey Dahmer be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jeffrey_Dahmer"}
{"question": "when Elizabeth Glaser died was penance impossible for Jeffrey Dahmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elizabeth_Reaser;Jeffrey_Dahmer"}
{"question": "penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Elizabeth", "extracted_entity": "Jeffrey_Dahmer;Elizabeth_Reaser"}
{"question": "was penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jeffrey_Dahmer;Elizabeth_Reaser"}
{"question": "was penance impossible for Jeffrey Dahmer in year Layne Staley died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jeffrey_Dahmer;Layne_Staley"}
{"question": "was penance impossible for Jeffrey Dahmer in last month of 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jeffrey_Dahmer"}
{"question": "was penance out of reach for Jeffrey Dahmer in December 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jeffrey_Dahmer"}
{"question": "have any of Kurt Cobain's bandmates had more success than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kurt_Cobain"}
{"question": "will Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Atlantic_salmon"}
{"question": "do Atlantic salmon hang out in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Atlantic_salmon;Lucille_Bogan"}
{"question": "are Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you spot Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do Atlantic salmon ever swim near Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lucille_Bogan"}
{"question": "is Summer Triangle band named after grizzly bear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Ginger something that a naturopath would recommend for a pregnant woman? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Naturopathy"}
{"question": "is Olivia Newton John's 1980 film named after summer home of Mongols?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Olivia_Newton-John;Mongols"}
{"question": "have there been any blood eagle killings in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Common_Era"}
{"question": "have there been any crucifixions in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Common_Era"}
{"question": "has League of Legends esports players surpassed DOTA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "League_of_Legends"}
{"question": "is cancer treatment a preventative for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "COVID-19"}
{"question": "did Popeye eat all of Bugs Bunny's food source?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Popeye;Bugs_Bunny"}
{"question": "are all colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Marlboro_Township,_New_Jersey"}
{"question": "do majority of Tibetan people in their ancestral homeland believe in bodhisattvas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tibetan_people"}
{"question": "is Islamophobia against Cyprus majority religion be misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Islamophobia;Cyprus"}
{"question": "would a member of the Communist Party of the Soviet Union be likely to have read Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Communist_Party_of_the_Soviet_Union;Karl_Marx"}
{"question": "is electrolysis possible on the human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Facebook take steps to limit fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Facebook"}
{"question": "did Confederate States Army have similar fashion sense to West Point cadets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Confederate_States_Army;United_States_Military_Academy"}
{"question": "is Fake News currently uncensored on major social media sites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would someone feel safer from fake news on facebook than on the deep web?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the deep web somewhere that you could encounter unchecked fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Ahura Mazda live above Abaddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ahura_Mazda"}
{"question": "can Boards of Canada perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Boards_of_Canada"}
{"question": "can MGMT perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "MGMT"}
{"question": "can Destiny's Child perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch harm hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Starch;Hyperglycemia"}
{"question": "can starch be deadly to hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Starch;Hyperglycemia"}
{"question": "hyperglycemics should avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hyperglycemia;Starch"}
{"question": "should hyperglycemics avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hyperglycemia;Starch"}
{"question": "did Alan Rickman's final film outearn his first?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Alan_Rickman"}
{"question": "was Alan Rickman's final film a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Rickman"}
{"question": "did Alan Rickman outearn his debut movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alan_Rickman"}
{"question": "did Alan Rickman ever surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Rickman"}
{"question": "did an Alan Rickman film surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Rickman"}
{"question": "was Alan Rickman's first movie a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alan_Rickman"}
{"question": "has Simo Hayha killed any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Red_Army"}
{"question": "did Simo Hayha kill any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Red_Army"}
{"question": "has any of Nancy Pelosi's children followed similar profession?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nancy_Pelosi"}
{"question": "has any of Nancy Pelosi's brats followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nancy_Pelosi"}
{"question": "has any of Nancy Pelosi's kids followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nancy_Pelosi"}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nancy_Pelosi"}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps career wise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nancy_Pelosi"}
{"question": "was 2014 governor of New Jersey fattest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Jersey"}
{"question": "was 2014 governor of New Jersey heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Jersey"}
{"question": "did any belligerent in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Portuguese_Colonial_War"}
{"question": "would clown be a bad occupational fit for Ebenezer Scrooge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ebenezer_Scrooge"}
{"question": "do jumping spiders hunt to survive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are tap shoes required for breakdancing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does a Drow reach top of a shelf before The Hobbit's hero can?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drow_(Dungeons_&_Dragons);The_Hobbit"}
{"question": "yayoi era Japanese people didn't worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "y", "extracted_entity": "Yayoi_period;Japan;Kami"}
{"question": "did Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Yayoi_period;Japan;Kami"}
{"question": "did  Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Yayoi_period;Japan;Kami"}
{"question": "did Giovanni Battista Cybo serve his position longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_A._Garfield"}
{"question": "was Giovanni Battista Cybo in office longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "James_A._Garfield"}
{"question": "is Alexandria Ocasio-Cortez a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alexandria_Ocasio-Cortez"}
{"question": "was the first Vice President of the United States a Segregationist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vice_President_of_the_United_States;Racial_segregation_in_the_United_States"}
{"question": "have any Star Wars Ewok actors been featured in a Val Kilmer project?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Star_Wars;Ewok;Val_Kilmer"}
{"question": "have any Star Wars Ewok actors worked with Val Kilmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Star_Wars;Ewok;Val_Kilmer"}
{"question": "have any Star Wars Ewok actors worked with Bowie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Star_Wars;Ewok;David_Bowie"}
{"question": "have any Star Wars Ewok actors worked with Jennifer Connelly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Star_Wars;Ewok;Jennifer_Connelly"}
{"question": "have any Star Wars Ewok actors worked with Jim Henson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Star_Wars;Ewok;Jim_Henson"}
{"question": "does Lines on the Antiquity of Microbes lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Haiku"}
{"question": "does the shortest poem lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Haiku"}
{"question": "does the shortest poem lack necessary component of a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does the shortest poem lack the structure to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Haiku"}
{"question": "does the shortest poem written fail the criteria to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a haiku longer than the shortest poem written?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Haiku"}
{"question": "are there Scientologists who know a lot about Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nicole_Kidman"}
{"question": "did Katy Perry and Taylor Swift appear together in a music video?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Katy_Perry;Taylor_Swift"}
{"question": "did Katy Perry create more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Katy_Perry;Avril_Lavigne"}
{"question": "did Katy Perry sell more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Katy_Perry;Avril_Lavigne"}
{"question": "are there people who carry drums at the Macy's Day Parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Macy's_Thanksgiving_Day_Parade"}
{"question": "can coffee help with diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "coffee", "extracted_entity": null}
{"question": "would the plant Spartina Patens thrive in the Sahara?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sahara"}
{"question": "during the neolithic period, did people know how to hunt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Neolithic"}
{"question": "during the neolithic period, was hunting a valued skill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Neolithic"}
{"question": "are there accessibility options for blind users who encounter CAPTCHAs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a student in AP Chemistry have a need for scientific journals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Florida"}
{"question": "have there been any actors from musicals who have played Noah in a film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Noah"}
{"question": "has anyone from a film about Noah been in a movie about magic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Noah"}
{"question": "are any of the actors from the Harry Potter film series in any films about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Harry_Potter_(film_series);Noah"}
{"question": "have any of the actors from Harry Potter participated in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Harry_Potter;Noah"}
{"question": "does Russell Crowe know about the story of Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Russell_Crowe;Noah"}
{"question": "has Russell Crowe ever acted in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Russell_Crowe;Noah"}
{"question": "did Michael Jackson own a lot of bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Jackson;Bitcoin"}
{"question": "did Michael Jackson invest in Bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Jackson;Bitcoin"}
{"question": "are goldfish found on the menu of Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it recommended to put goldfish in the same tank as beta fish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it recommended for goldfish and beta fish to share a tank?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Native American tribes teach Colonizers how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Native_Americans_in_the_United_States"}
{"question": "are there types of plastic that you can't put in the recycling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did James Roday play a private investigator on the USA network?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "James_Roday"}
{"question": "did James Roday play a private investigator on a TV show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Roday"}
{"question": "is Lance Armstrong known for riding a bicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lance_Armstrong"}
{"question": "if someone is a fan of Dale Earnheardt, would they know what the Daytona 500 was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dale_Earnhardt;Daytona_500"}
{"question": "before it sunk, did the Titanic have too few lifeboats for all of the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "RMS_Titanic"}
{"question": "can you go water skiing in the Adirondacks? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "in the movie The Titanic, were there too few many lifeboats for the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": null}
{"question": "has Saturday Night Live been on air longer than American Idol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saturday_Night_Live;American_Idol"}
{"question": "was Ashlee Simpson lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ashlee_Simpson;Saturday_Night_Live"}
{"question": "have any performers been caught lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saturday_Night_Live"}
{"question": "can a person with triskaidekophobia succeed at Sudoku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sudoku"}
{"question": "have any of the Doctor (Doctor Who) actors had famous offspring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Doctor_(Doctor_Who);Doctor_Who"}
{"question": "did any of the amazons on Xena: Warrior Princess star in a Fox TV series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xena:_Warrior_Princess;Fox_Broadcasting_Company"}
{"question": "would Library of Alexandria hypothetically need less shelf pace than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Library_of_Alexandria;Library_of_Congress"}
{"question": "was Library of Alexandria commissioned by a Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Library_of_Alexandria;Greece"}
{"question": "is it impossible for Siddhartha Gautama to become a Bodhisattva?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gautama_Buddha;Bodhisattva"}
{"question": "did a mystery ever happen at Tower of London?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tower_of_London"}
{"question": "is Tower of London relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Willis_Tower;Robert_Stack"}
{"question": "would Tower of London be relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tower_of_London;Robert_Stack"}
{"question": "would Tower of London be relevant to Robert Stack's longest running show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tower_of_London;Robert_Stack"}
{"question": "george Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life zoology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "George_W._Trendle"}
{"question": "is George Trendle's hornet a misnomer in real life taxonomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_W._Trendle"}
{"question": "in animal world is George Trendle's hornet a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "George_W._Trendle"}
{"question": "did Barack Obama and Osama Bin Laden go to school together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Barack_Obama;Osama_bin_Laden"}
{"question": "if someone was hungry for a sandwich, would they be likely to go to taco bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Aloe plants be mailed within the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aloe;United_States"}
{"question": "would Aloe thrive more in a dry climate than a swampy climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aloe"}
{"question": "would Aloe grow better in a drier area than a marshy area?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Mark-Paul Gosselaar ever play the role of a student?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mark-Paul_Gosselaar"}
{"question": "would Drew Pinsky be someone to talk to about mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Drew_Pinsky"}
{"question": "is Drew Pinsky qualified to help people with mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Drew_Pinsky"}
{"question": "is Dr. Drew Pinsky qualified to treat mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drew_Pinsky"}
{"question": "would Steve Martin turn down a dish of shrimp risotto? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Steve_Martin"}
{"question": "did any of the founding fathers get frequent migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do any of the actors in Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Wicked_(musical)"}
{"question": "does anyone from the cast of Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Wicked_(musical)"}
{"question": "are there any famous singers with migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "there", "extracted_entity": null}
{"question": "would a mammogram at a new doctor's office be considered preventive healthcare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Preventive_healthcare"}
{"question": "is it inappropriate to open a closed casket at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "are there live performances at celebrity funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were there celebrities performing at the funeral of Aretha Franklin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aretha_Franklin"}
{"question": "was Aretha Franklin's funeral attended by celebrity guests?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Aretha_Franklin"}
{"question": "would a Heath Ledger fan be interested in seeing The Dark Knight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Heath_Ledger;Dark_Knight"}
{"question": "is The Dark Knight associated with any violence in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Dark_Knight"}
{"question": "do Northern Fur Seals encounter penguins naturally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would it be unusual to see a Northern Fur Seal in the south pole?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Northern_fur_seal"}
{"question": "are there country artists who also integrate rapping into their work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did any actors from a musical play in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_Bond"}
{"question": "did any actors from Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Bond"}
{"question": "did anyone from the film Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mamma_Mia_(song);Mamma_Mia!_(film);James_Bond"}
{"question": "did anyone from the movie Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mamma_Mia_(song);James_Bond"}
{"question": "has there been a song by Billie Eilish that has been used in a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Billie_Eilish;James_Bond"}
{"question": "is it normal for prisoners to encounter situations that inspire disgust in prison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Russia to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sainsbury's;Russia"}
{"question": "would Sainsbury's be too far away for someone in Arkansas to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "S", "extracted_entity": "Sainsbury's;Arkansas"}
{"question": "do cactus plants thrive more in dry environments than wet ones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cactus"}
{"question": "do cactus plants require less water than dafodills? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "C", "extracted_entity": "Cactus"}
{"question": "are there varieties of cactus that are unsafe to touch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "have there been suicides to protest war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would it be normal for a teacher to tell kindergartners about the golden rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Halle Berry eat crustaceans often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Halle_Berry"}
{"question": "is Halle Berry prohibited from eating crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Halle_Berry"}
{"question": "is some public art illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Public_art"}
{"question": "is Harry Styles currently knighted in England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_Styles"}
{"question": "do Razor brand kick scooters have gears?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Britney Spears receiving treatment for her bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Britney_Spears;Bipolar_disorder"}
{"question": "would a prosthetic limb work as a replacement for a human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Germany already established during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Germany;French_Revolution"}
{"question": "was Hugh Jackman alive during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hugh_Jackman;French_Revolution"}
{"question": "was the movie Les Miserables filmed during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Les_Mis\u00e9rables;French_Revolution"}
{"question": "are deep sea fishermen considered experts in oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Jane Goodall be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jane_Goodall"}
{"question": "would Jane Austen be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jane_Austen"}
{"question": "has Iggy Pop been in a giallo film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Iggy_Pop;Giallo"}
{"question": "has Iggy Pop worked with a Giallo director?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Iggy_Pop;Giallo"}
{"question": "do Coen brothers take up more car space than Scott directing brothers in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Coen_brothers;Ridley_Scott"}
{"question": "are there more Coen brothers alive in 2020 than Scott directing brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Coen_brothers;Tony_Scott"}
{"question": "did Robin Williams have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Robin_Williams"}
{"question": "did Michael Jackson suffer from a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Jackson"}
{"question": "did Christoper Reeves have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is John Cazale's 1970s partner yet to debut in a Coen brothers movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Cazale;Coen_brothers"}
{"question": "is the Quran regarded as a holy text by Jewish people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Qur'an;Jews"}
{"question": "do practicing Presbyterians read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Presbyterianism;Qur'an"}
{"question": "do practicing Catholics read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Catholic_Church;Qur'an"}
{"question": "did Kangaroo Court originate in High Court of Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Kangaroo_court;High_Court_of_Australia"}
{"question": "did mongoose come from later geologic period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "nATO excludes double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would NATO exclude double triangle flag country to a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "NATO"}
{"question": "is double triangle flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "NATO"}
{"question": "is double-pennon flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "NATO"}
{"question": "is Saugat Malla's home delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "NATO"}
{"question": "is Saugat Malla's home country delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "NATO"}
{"question": "is Saugat Malla's home country delegate barred from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "NATO"}
{"question": "are squid considered invertebrates? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Squid"}
{"question": "are squid found in salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there raw preparations of squid for eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the white part of the citrus fruit less sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would most people discard the outside of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is citrus good for the body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is citrus useful in promoting health?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is calling creator of Gorillaz a one hit wonder ludicrous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gorillaz"}
{"question": "are house cats a potential source of danger for chipmunks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Alexander Klevan's birthplace safe from Zika virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did same villain kill Batman and Superman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Batman;Superman"}
{"question": "is Cerebrospinal fluid contents totally different than Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cerebrospinal_fluid;Gatorade"}
{"question": "is Cerebrospinal fluid contents similar to Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Cerebrospinal_fluid;Gatorade"}
{"question": "are there no shared ingredients between Cerebrospinal fluid and Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Gatorade"}
{"question": "did Billie Eilish perform a song for a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Billie_Eilish;James_Bond"}
{"question": "are there paintings of Salvador Dali's on both coasts of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Salvador_Dal\u00ed;United_States"}
{"question": "can you see Salvador Dali's work in multiple cities of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Salvador_Dal\u00ed;United_States"}
{"question": "can you see Salvador Dali's work in both New York and Georgia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salvador_Dal\u00ed;New_York_City;Georgia_(U.S._state)"}
{"question": "does the Bronx Zoo have an exhibit of the Dodo bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bronx_Zoo;Dodo"}
{"question": "are prisoners who practice winemaking allowed to do so?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so by their guards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "did Richard III miss out on Parcheesi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Richard_III_of_England"}
{"question": "sunday name origin people enemy of Christians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "will Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Sunday name origin disturb Serena Williams's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Serena_Williams"}
{"question": "would Sunday name origin disturb Prince's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Sunday name origin disturb a Jehovah Witness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jehovah's_Witnesses"}
{"question": "would a Jehovah's Witness denounce name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jehovah's_Witnesses;Sunday"}
{"question": "would a Jehovah's Witness reject name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jehovah's_Witnesses;Sunday"}
{"question": "would a Jehovah's Witness be upset by name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jehovah's_Witnesses"}
{"question": "are all bicycles able to go off road?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are two names in Albany, Georgia common as country and city names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Albany,_Georgia"}
{"question": "does Camp Rock feature a lot of smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Camp_Rock"}
{"question": "is the Song of Roland a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Song_of_Roland"}
{"question": "can the Song of Roland be considered a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Song_of_Roland"}
{"question": "is KFC more global than Roy Rogers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "K", "extracted_entity": "KFC;Roy_Rogers"}
{"question": "would a Dodo tower over Pauline Musters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dodo"}
{"question": "would a Dodo be above Pauline Musters's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dodo"}
{"question": "would a Dodo be above Chandra Bahadur Dangi's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dodo"}
{"question": "would a Dodo tower over Chandra Bahadur Dangi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dodo_(prince)"}
{"question": "can you play a Nintendo game in your car CD player?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Nintendo"}
{"question": "are there any theme park attractions featuring King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "King_Kong"}
{"question": "is there a ride at Universal Studios Orlando about King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Universal_Orlando;King_Kong"}
{"question": "were there fifty English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Kingdom_of_England"}
{"question": "were there at least fifty different English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kingdom_of_England;Middle_Ages"}
{"question": "did Pre-Raphaelites have a profound influence on Claude Monet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Claude_Monet"}
{"question": "was Claude Monet influenced by Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Claude_Monet;Pre-Raphaelite_Brotherhood"}
{"question": "was Claude Monet a Pre-Raphaelite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Claude_Monet;Pre-Raphaelite_Brotherhood"}
{"question": "would Claude Monet value art style of the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Claude_Monet"}
{"question": "did Claude Monet have a similar art style to the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Claude_Monet;Pre-Raphaelite_Brotherhood"}
{"question": "while on the voice, did Adam Levine quit Maroon 5?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Adam_Levine;Maroon_5"}
{"question": "would someone with a back fracture go to a chiropractic center for treatment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "should cinnamon be eaten straight from the container?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Microsoft Excel make slideshow presentations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Microsoft_Excel"}
{"question": "is In God We Trust part of the pledge of allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "In_God_We_Trust"}
{"question": "did Ariana Grande do a duet of Give it Up with Victoria Justice?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Ariana_Grande;Victoria_Justice"}
{"question": "was Ariana Grande's hair in Victorious natural?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ariana_Grande"}
{"question": "did Ariana Grande get popular on show created by Disney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ariana_Grande;The_Walt_Disney_Company"}
{"question": "did Ariana Grande get popular on a Disney Channel Original Series?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ariana_Grande;List_of_Disney_Channel_series"}
{"question": "while on Broadway, did Ariana Grande audition for The Spongebob Musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Broadway_theatre;Ariana_Grande"}
{"question": "is Chris Hemsworth qualified to perform surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chris_Hemsworth"}
{"question": "in WW2, were there people who did surgery on the battlefield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "World_War_II"}
{"question": "can a Sphynx cat make wool?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sphynx_cat"}
{"question": "in New York, is snowboarding a popular summer activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_York"}
{"question": "has an appendix been successfully transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Appendix_(anatomy)"}
{"question": "do most nuclear families in America have Bengal Cats?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Bengal_cat"}
{"question": "can lacto fermentation preserve cucumbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can cucumbers be preserved for longer storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do you pass through Wyoming following Route 66?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wyoming;U.S._Route_66_in_Arizona"}
{"question": "is building design the responsibility of a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are turtles vulnerable to Raccoons? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Raccoon"}
{"question": "could you see turtles at the Boston Aquarium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Boston_Athen\u00e6um"}
{"question": "did Ada Lovelace know the python coding language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ada_Lovelace"}
{"question": "would a person with arachnophobia fear spiders? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arachnophobia"}
{"question": "were the family of Donald Shea angry with Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Charles_Manson"}
{"question": "would a mastectomy be a treatment for colorectal cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mastectomy;Colorectal_cancer"}
{"question": "do most gynecologists have to see the firsthand health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a home garden be growing cucumbers during the winter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cucumber"}
{"question": "can you order pancakes at Dennys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are Zebra common sights in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Z", "extracted_entity": "Zebra;Illinois"}
{"question": "does Greek goddess Venus was named after have a Roman equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Greece;Venus_(mythology)"}
{"question": "are bench trials performed before a jury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jury"}
{"question": "would the Temple of Kom Ombo feature Egyptian hieroglyphs? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Egypt"}
{"question": "as of 2020 does John Key have more PM's succeed than precede him?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "John_Key"}
{"question": "did Methuselah outlive Sarah's and Abraham combined?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Methuselah;Sarah;Abraham"}
{"question": "did Methuselah outlive Sarah's lifetime over 5 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Methuselah"}
{"question": "did Methuselah outlive Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Methuselah;Sarah"}
{"question": "did Charlemagne's father fight in the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Charlemagne;Battle_of_Tours"}
{"question": "did Charlemagne's father win the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charlemagne;Battle_of_Tours"}
{"question": "has J.K. Rowling written mysteries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "J._K._Rowling"}
{"question": "do any of Powerpuff Girls share name with character in Princess Bride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Butter", "extracted_entity": "The_Princess_Bride_(film)"}
{"question": "is Newt Gingrich's nickname a type of reptile?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Newt_Gingrich"}
{"question": "would adherents to Ahimsa be appalled at Paul the Apostle's cause of death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paul_the_Apostle"}
{"question": "was Florence Nightingale's death a better example of Ahimsa than Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Florence_Nightingale;Paul_the_Apostle"}
{"question": "would people prefer Florence Nightingale's cause of death to Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Florence_Nightingale;Paul_the_Apostle"}
{"question": "was Paul the Apostle's death more violent than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paul_the_Apostle;Florence_Nightingale"}
{"question": "was Paul the Apostle's death crueler than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paul_the_Apostle;Florence_Nightingale"}
{"question": "was Paul the Apostle's death more eventful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paul_the_Apostle;Florence_Nightingale"}
{"question": "was Paul the Apostle's death more painful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Paul_the_Apostle;Florence_Nightingale"}
{"question": "is number of US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;President_of_the_United_States;Hong_Kong"}
{"question": "does the band Marilyn Manson get their entire name from Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Marilyn_Manson_(band);Charles_Manson"}
{"question": "did Pre-Colonial America have Ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pre-Columbian_era"}
{"question": "are vanity plates important to any of Stephen King's children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Stephen_King"}
{"question": "are any of Stephen King's children familiar with vanity plates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Stephen_King"}
{"question": "have any of Stephen King's offspring embraced their father's genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "his", "extracted_entity": "Stephen_King"}
{"question": "have any of Stephen King's offspring followed in his footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Stephen_King"}
{"question": "did members of singer Prince's Christian sect celebrate Easter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Prince_(musician);Easter"}
{"question": "do more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Taco_Bell;Roy_Rogers"}
{"question": "will Gremlins tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gremlins;The_Matrix"}
{"question": "is Gremlins set to tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Gremlins;The_Matrix_(franchise)"}
{"question": "are Gremlins sequels less prevalent than Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Gremlins;The_Matrix"}
{"question": "are there more Matrix sequels than Gremlins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Matrix_(franchise);Gremlins"}
{"question": "is it possible for a mortal to escape the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Alexander Graham Bell have supported Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alexander_Graham_Bell"}
{"question": "can Halle Berry eat crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Halle_Berry;Crustacean"}
{"question": "can letters in first line of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "QWERTY;Palindrome"}
{"question": "does the letter B's place in alphabet exceed number of Henry VIII male king heirs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Henry_VIII_of_England"}
{"question": "does the letter B's place in alphabet exceed number of sons Sofia Vergara has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sof\u00eda_Vergara"}
{"question": "does the letter B's place in alphabet exceed number of sons of Bruce Dern?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bruce_Dern"}
{"question": "does the letter B's place in alphabet exceed number of sons Adam Sandler has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Adam_Sandler"}
{"question": "does the letter B's place in alphabet exceed number of sons Bruce Willis has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bruce_Willis"}
{"question": "does Richard Dawkins donate to Science Partners?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Dawkins"}
{"question": "is Love Song from November 11, 2000 an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Richard Dawkins donate to the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Richard_Dawkins"}
{"question": "would Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Dawkins"}
{"question": "is Richard Dawkins joining the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Dawkins"}
{"question": "would Holy Wood's Love Song be an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "is Holy Wood's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Manson's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Marilyn_Manson"}
{"question": "will Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Richard_Dawkins"}
{"question": "is Narcissus Narcosis an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Narcissus Narcosis an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Target Audience an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Christopher Columbus receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus"}
{"question": "can a brain be transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do internet forums require natural gas to operate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are wind turbines instrumental in making internet forums available to everyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was the tenth Amendment written on a mobile phone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mobile_phone"}
{"question": "was the tenth Amendment written on a typewriter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Typewriter"}
{"question": "are Atlantic Cod a source of income for fishermen in California?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cape_Cod;California"}
{"question": "did John Lewis work alongside Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "John_Lewis;Martin_Luther_King_Jr."}
{"question": "did John Lewis march with Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Lewis;Martin_Luther_King_Jr."}
{"question": "does Bill Clinton have the ability to post in internet forums?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bill_Clinton"}
{"question": "would Goofy hypothetically enjoy Milk Bone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy"}
{"question": "is wheat important to make unleavened bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is wheat essential for bread making?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ludacris"}
{"question": "did Ludacris' rap career start before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lud", "extracted_entity": "Ludacris"}
{"question": "did Ludacris' rap career begin before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludacris"}
{"question": "did Ludacris' rap career begin before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludacris"}
{"question": "was Clark Gable handy with a wrench when he was young?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Clark_Gable"}
{"question": "was Clark Gable familiar with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Clark_Gable"}
{"question": "was Clark Gable handy with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Clark_Gable"}
{"question": "is week old chlorine water safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "would Teri Hatcher have most pay stubs from Desperate Housewives in her career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Teri_Hatcher;Desperate_Housewives"}
{"question": "was Desperate Housewives Teri Hatcher's greatest marathon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Desperate_Housewives;Teri_Hatcher"}
{"question": "was Desperate Housewives Teri Hatcher's longest tenure as actress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Desperate_Housewives;Teri_Hatcher"}
{"question": "did Desperate Housewives provide Teri Hatcher her longest acting paycheck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Desperate_Housewives;Teri_Hatcher"}
{"question": "are there more games after FInal Fantasy VI than before in its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is silicon better for making wedding rings than bromine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bromine"}
{"question": "can a comatose person produce art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do any adherents to Christianity in China have different saints than Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christianity_in_China;Catholic_Church"}
{"question": "it it illegal to pump your own gas in Morris County New Jersey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Morris_County,_New_Jersey;New_Jersey"}
{"question": "has September shifted to later month from the oldest calendar it appeared on?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": null}
{"question": "are Sram's kin found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_of_Warcraft"}
{"question": "is Sram's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "World_of_Warcraft"}
{"question": "is Balthor the Defiled's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_of_Warcraft"}
{"question": "is Balthor the Defiled's race represented in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "World_of_Warcraft"}
{"question": "is Balthor the Defiled's race found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "World_of_Warcraft"}
{"question": "was surveyor of Norman, Oklahoma a viking?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Norman,_Oklahoma"}
{"question": "do Snow White dwarves fill out entire The Hobbit dwarves roster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Snow_White;The_Hobbit"}
{"question": "are there enough Snow White dwarves to play all dwarves in The Hobbit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Snow_White;The_Hobbit"}
{"question": "does Eli Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Eli_Manning;Oval_Office"}
{"question": "does Peyton Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Peyton_Manning;Oval_Office"}
{"question": "is the Hague interested in geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Hague"}
{"question": "does the Hague care about geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Hague"}
{"question": "will Wednesday origin religion followers be denied Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Wednesday traditionally a pagan day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "cab Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is impaired Cognition affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Cognition impairment affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Cognition impairment influenced by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does citrus grow in the Arctic circle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Arctic_Circle"}
{"question": "can you watch Rick and Morty at the bottom of Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Mariana_Trench"}
{"question": "is Mark Cuban visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mark_Cuban;Northern_Mariana_Islands"}
{"question": "can I talk to Vishnu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vishnu"}
{"question": "is President Trump xenophobic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "False", "extracted_entity": "Donald_Trump"}
{"question": "do any countries that practice capitalism still have homelessness problem?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is homelessness still a problem in countries that practice capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book ever been in same project with Eva Green?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Oscar_Wilde;Eva_Green"}
{"question": "has character from Oscar Wilde's most famous book endured through other media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Oscar", "extracted_entity": "Oscar_Wilde"}
{"question": "can Michael Jordan lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Jordan;Northern_Mariana_Islands"}
{"question": "can Mark Cuban lease land in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Mark_Cuban;Northern_Mariana_Islands"}
{"question": "does human with no gear die capturing giant squid in natural habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Western Honey bee important to US brand Nice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Western_honey_bee;United_States;Nice"}
{"question": "are crustaceans consumed raw by people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Crustacean"}
{"question": "in New York, is snowboarding a popular July activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_York"}
{"question": "is Freya a composite of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Athena;Aphrodite"}
{"question": "are there many similarities between Athena and Freya?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They", "extracted_entity": "Athena;Freyja"}
{"question": "did Ivan the Terrible have harshest nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible"}
{"question": "did Ivan the Terrible have the worst nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible"}
{"question": "do placozoa have learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Placozoa"}
{"question": "can an accordion player benefit from eye surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Alexander the Great conquer part of Land of Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alexander_the_Great;Land_of_Israel"}
{"question": "can a minotaur hypothetically break a tibia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Minotaur;Tibia"}
{"question": "does Alex Rodriguez have fewer championship memories than Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael", "extracted_entity": "Alex_Rodriguez;Michael_Jordan"}
{"question": "does Michael Jordan have more championship memories than Alex Rodriguez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael", "extracted_entity": "Michael_Jordan;Alex_Rodriguez"}
{"question": "is John Kerry familiar with cross checking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John", "extracted_entity": "John_Kerry"}
{"question": "does table tennis make use of prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Table_tennis"}
{"question": "would 7 zucchini's satisfy potassium USDA recommended dailyrecommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States_Department_of_Agriculture"}
{"question": "can Mark Cuban lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mark_Cuban;Northern_Mariana_Islands"}
{"question": "is Pan Satan-like in appearance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pan_(god)"}
{"question": "did The King of Rock'n Roll's snack with bananas come from plants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did The King of Rock'n Roll love a snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was one of the fruits in apple banana bread in The King of Rock'n Roll's favorite snacks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the The King of Rock'n Roll love one of the fruits in apple banana bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "ABBA"}
{"question": "could the members of ABBA play a mixed doubles table tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "ABBA"}
{"question": "could the members of ABBA play a mixed doubles table tennis game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "ABBA"}
{"question": "would it be impossible for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mount_Emei"}
{"question": "does Jabberwocky have the length to be a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jabberwocky;Sonnet"}
{"question": "has one EGOT category eluded Justin Timberlake for nominations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Justin_Timberlake"}
{"question": "did Benjamin Butler attend a college that has the first name of a UFC welterweight fighter in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ultimate_Fighting_Championship"}
{"question": "can a Nissan product beat a Ferrari in a race? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan;Ferrari"}
{"question": "can a Nissan win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan;Ferrari"}
{"question": "does Nissan have any products that can win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan;Ferrari"}
{"question": "does Nissan have any products that can win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan;Ferrari"}
{"question": "did Nissan build any products that win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan;Ferrari"}
{"question": "was the Land of Israel under the control of a muslim empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Land_of_Israel;Islam"}
{"question": "did a Turkish University of North Carolina at Chapel Hill professor win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill;Nobel_Prize"}
{"question": "did a University of North Carolina at Chapel Hill professor from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill;Turkey;Nobel_Prize"}
{"question": "did a University of North Carolina at Chapel Hill biochemist from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill;Nobel_Prize"}
{"question": "did a 6-time NBA champion graduate from the University of North Carolina at Chapel Hill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "National_Basketball_Association;University_of_North_Carolina_at_Chapel_Hill"}
{"question": "did University of North Carolina at Chapel Hill give scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill;Michael_Jordan"}
{"question": "did University of North Carolina at Chapel Hill offer scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University_of_North_Carolina_at_Chapel_Hill;Michael_Jordan"}
{"question": "was Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Morris_County,_New_Jersey"}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Morris_County,_New_Jersey"}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Morris_County,_New_Jersey"}
{"question": "is there a wide range between smallest and largest asteroid size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after being fired from the band?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dave", "extracted_entity": "Metallica"}
{"question": "did founder of Skype win a race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Skype"}
{"question": "did Eiffel Tower contribute to the Allied victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eiffel_Tower"}
{"question": "does soy milk increase chances of heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in restaurant in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Drew", "extracted_entity": "Drew_Barrymore;United_States"}
{"question": "can Drew Barrymore work as a cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Drew", "extracted_entity": "Drew_Barrymore;United_States"}
{"question": "would a model be a good choice to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if", "extracted_entity": null}
{"question": "can you redeem more Bing search reward points than Google's equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bing"}
{"question": "can Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paralympic_Games"}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paralympic_Games"}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh", "extracted_entity": "Paralympic_Games"}
{"question": "could Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh", "extracted_entity": "Paralympic_Games"}
{"question": "could Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paralympic_Games"}
{"question": "could Josh Blue compete in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paralympic_Games"}
{"question": "could Mohammed Ali attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Muhammad_Ali;Paralympic_Games"}
{"question": "could Stephen Hawking play chess? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stephen_Hawking"}
{"question": "could Stephen Hawking play computer games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Stephen_Hawking"}
{"question": "could Aretha Franklin vote for a president when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aretha_Franklin"}
{"question": "could Aretha Franklin vote in an election when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aretha_Franklin"}
{"question": "can you hide the state of Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Iowa;English_Channel"}
{"question": "can you hide Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Iowa;English_Channel"}
{"question": "does soy milk cause heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jainism"}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jainism"}
{"question": "wold a Jain believe in the Buddhist idea of karma?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Jainism;Buddhism;Karma"}
{"question": "are Northern European characters portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Northern_Europe;Macbeth"}
{"question": "are Northern European characters depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Northern_Europe;Macbeth"}
{"question": "did Macbeth depict Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Macbeth"}
{"question": "did Macbeth feature Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Macbeth"}
{"question": "did a German-Irish actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Macbeth"}
{"question": "did an oscar-winning actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Macbeth"}
{"question": "did an actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Macbeth"}
{"question": "did an Oscar-winning actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Academy_Award;Macbeth"}
{"question": "were any of the characters in Macbeth played by an Oscar-winning actor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Macbeth;Academy_Award"}
{"question": "was Napoleonic Wars longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Napoleonic_Wars;Great_Depression"}
{"question": "did Napoleonic Wars last longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Napoleonic_Wars;Great_Depression"}
{"question": "do we need arithmetic to launch rockets into space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "People's_Republic_of_China;Karl_Marx"}
{"question": "are the members of the communist party of China influenced by Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "People's_Republic_of_China"}
{"question": "do members of the communist party of China come from Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "People's_Republic_of_China;Asia"}
{"question": "can Uighurs become members of the communist party of China ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Uyghurs;People's_Republic_of_China"}
{"question": "are mongolians members of the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "People's_Republic_of_China"}
{"question": "do days get shorter in Seattle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Seattle"}
{"question": "do days get shorter in Seattle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Seattle"}
{"question": "is September followed by colder months in the Northern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Northern_Hemisphere"}
{"question": "can I eat Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can I hunt Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon"}
{"question": "can I hunt Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon;California"}
{"question": "can I catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon;California"}
{"question": "can you catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon;California"}
{"question": "can you fish Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon"}
{"question": "do people speak Germanic languages in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Germanic_languages;Bohemia"}
{"question": "is homeostasis associated with thermoregulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can one Ancient Greek letter represent every season of Survivor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ancient_Greek;Survivor_(American_TV_series)"}
{"question": "do traditional calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Calico"}
{"question": "has small children's painting style been profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can small children's painting style be profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a judo expert defeat a kata expert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Judo"}
{"question": "can a judo expert defeat someone that only does kata?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Judo;Kata"}
{"question": "is Jenny McCarthy in same industry as her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jenny_McCarthy"}
{"question": "do all the animals hedgehogs eat have spinal cords?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hedgehog"}
{"question": "does number of Hamlet adaptations exceed Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Hamlet's number of movie adaptations similar to Romeo and Juliet's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hamlet"}
{"question": "is Hamlet one of Shakespeare's most movie adapted plays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hamlet;William_Shakespeare"}
{"question": "did Queen Margot keep Moliere's severed head?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "La_Reine_Margot_(1994_film)"}
{"question": "would a Lord Protector hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lord_Protector;Tsar"}
{"question": "could a Tsar hypothetically boss a Lord Protector around?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tsar;Lord_Protector"}
{"question": "could a Tsar hypothetically tell a Lord Protector what to do?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tsar;Lord_Protector"}
{"question": "would a Tsar hypothetically outrank a Lord Protector?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Tsar;Lord_Protector"}
{"question": "would a Tsar hypothetically outrank a Protector of the Realm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tsar"}
{"question": "would a Tsar hypothetically outrank a US Vice President?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tsar;United_States;Vice_President_of_the_United_States"}
{"question": "can SARS-COV-2 infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "SARS-CoV-2"}
{"question": "has Leninist ideology influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "(", "extracted_entity": "Leninism;Bohemia"}
{"question": "can United States Secretary of State slap United Kingdom representative without repercussions?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "United_States_Secretary_of_State;United_Kingdom"}
{"question": "were Whirling Dervishes in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Reiki stored in a pill bottle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Reiki"}
{"question": "does Reiki require refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reiki"}
{"question": "does Reiki require frequent refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Reiki"}
{"question": "does Reiki require a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Reiki"}
{"question": "can viruses infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have politics been influenced by Marxist ideology in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Marxism;Bohemia"}
{"question": "are most Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reddit;Pledge_of_Allegiance_(United_States)"}
{"question": "is Tokyo Tower close to city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Willis_Tower;River_City"}
{"question": "is Tokyo Tower in same country as famed city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tokyo", "extracted_entity": "Willis_Tower;River_City"}
{"question": "can someone have a long life after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is shoe soup relatively harmless?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a person survive eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "can a person survive after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a", "extracted_entity": null}
{"question": "are all jokes harmless to health?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could someone in Boston watch the same episode of The Tonight Show in two days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Boston;The_Tonight_Show"}
{"question": "could someone in Houston see the opening of The Tonight Show before the day change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Houston;The_Tonight_Show"}
{"question": "could someone in Houston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Houston;The_Tonight_Show"}
{"question": "could someone in Boston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Boston;The_Tonight_Show"}
{"question": "is Beauty and the Beast (1991 film) another Disney pilfered plot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Beauty", "extracted_entity": "Beauty_and_the_Beast_(1991_film);The_Walt_Disney_Company"}
{"question": "could Eddie Murphy's children hypothetically play a full game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eddie_Murphy"}
{"question": "could Eddie Murphy's children hypothetically play a game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eddie_Murphy"}
{"question": "can Centurylink max internet plan upload 1000GB in half a day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "CenturyLink"}
{"question": "can Centurylink max internet plan upload 1000GB in a quarter day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "CenturyLink"}
{"question": "is LG Electronics located in a city with an official bird that has a blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "LG_Electronics"}
{"question": "did Holy Land belong to Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Holy_Land"}
{"question": "can Holy Land be claimed by Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Holy_Land"}
{"question": "did Adamu's people have claim to Holy Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Holy_Land"}
{"question": "did Adamu's people have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Holy_Land"}
{"question": "did Assyrians have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Assyrian_people;Holy_Land"}
{"question": "did another group have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Holy_Land"}
{"question": "did anyone have claim to Holy Land other than three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Holy_Land"}
{"question": "would US states in 1900 be enough to divide up ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;Ancient_Greece"}
{"question": "is Armageddon exclusive to the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Holy_Land"}
{"question": "were there worse vote differences than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States_presidential_election,_1980;United_States"}
{"question": "were there worse defeats than Carter's 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jimmy_Carter;United_States"}
{"question": "will sports announcing be possible with Bell's palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sports", "extracted_entity": null}
{"question": "can Bell's palsy sufferers still have productive show business careers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in three minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hobbit"}
{"question": "is it possible to finish reading The Hobbit in half an hour?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hobbit"}
{"question": "was Florence ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Florence;Theocracy"}
{"question": "was Flornce ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Theocracy"}
{"question": "can second row of QWERTY keyboard spell name of Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "QWERTY"}
{"question": "can United States Army Rangers complete a Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "United", "extracted_entity": "Canada;United_States_Army_Rangers;Mozambique"}
{"question": "can United States Army Rangers perform a successful Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Army_Rangers;Mozambique"}
{"question": "can Kelly Clarkson's singing break glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kelly_Clarkson"}
{"question": "did multiple weekdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Apollo_15"}
{"question": "did two Wednesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Apollo_15"}
{"question": "did two Tuesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Apollo_15"}
{"question": "did two Mondays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Apollo_15"}
{"question": "did Maya Angelou win a Triple Crown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Maya_Angelou"}
{"question": "are felidae represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinese_calendar"}
{"question": "does the Atlantic reach 500 battalions worth of people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Cheb Mami possibly win a Pulitzer Prize in History?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cheb_Mami;Pulitzer_Prize"}
{"question": "did Martin Luther believe in God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Martin_Luther;God"}
{"question": "is Martin Luther celebrated by many people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Martin_Luther"}
{"question": "was Martin Luther capable of logical thinking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Martin_Luther"}
{"question": "was Martin Luther susceptible to infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Martin_Luther"}
{"question": "was Martin Luther susceptible to death by infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Martin_Luther"}
{"question": "do people still fear the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can a swastika cause emotional pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Swastika"}
{"question": "can a swastika cause pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a nerd a common character in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nerd"}
{"question": "did the Great Depression break up families?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Great_Depression"}
{"question": "is dopamine snorted up the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dopamine"}
{"question": "is dopamine snorted through the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dopamine"}
{"question": "can you buy Reddit at Barnes & Noble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reddit;Barnes_&_Noble"}
{"question": "could an elephant seal reach the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Elephant_seal;Lake_Superior"}
{"question": "could an elephant seal dive to the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Elephant_seal;Lake_Superior"}
{"question": "could an elephant seal dive to the bottom of Lake Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Elephant_seal;Lake_Michigan"}
{"question": "could a nymph tick pass through a ISO 838 hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nymph_(biology)"}
{"question": "did Wilbur Scoville know about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Capsaicin"}
{"question": "did Wilbur Scoville care about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Capsaicin"}
{"question": "did George Washington cross the Potomac in a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "George_Washington;Lexus"}
{"question": "did Edward of Westminster ever spend time in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Edward_of_Westminster,_Prince_of_Wales;Gloucestershire"}
{"question": "was Edward of Westminster ever in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Edward_of_Westminster,_Prince_of_Wales;Gloucestershire"}
{"question": "is Antarctica ideal for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica;Groundhog_Day"}
{"question": "did it get dark in Tennessee during the Solar eclipse of August 21, 2017?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tennessee"}
{"question": "are polar bears indigenous to the Golan Heights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Polar_bear;Golan_Heights"}
{"question": "is it easier for a human to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Atlantic_Ocean;Arctic_Ocean"}
{"question": "is it easier to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Atlantic_Ocean;Arctic_Ocean"}
{"question": "is it easier to float in the Dead Sea than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Dead_Sea;Arctic_Ocean"}
{"question": "is Canada south of the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada;Arctic_Ocean"}
{"question": "would John the Baptist be alone at a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_the_Baptist"}
{"question": "could Jamie Lee Curtis buy a package of Marlboro in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jamie_Lee_Curtis;Marlboro_(cigarette);California"}
{"question": "could Jamie Lee Curtis buy a bottle of chardonnay in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jamie_Lee_Curtis;Chardonnay;California"}
{"question": "could Jamie Lee Curtis purchase alcohol in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Jamie_Lee_Curtis;California"}
{"question": "could 2020 Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Brazilian_Navy;Battle_of_Actium"}
{"question": "could 2020 Brazilian Navy hypothetically win Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Brazilian_Navy;Battle_of_Actium"}
{"question": "would a plate of crustaceans be appropriate for Diwali?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Diwali"}
{"question": "would an Olmec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Olmec;Great_Pyramid_of_Giza"}
{"question": "would an Incan recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Inca_Empire;Great_Pyramid_of_Giza"}
{"question": "would an Aztec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Aztec;Great_Pyramid_of_Giza"}
{"question": "would an Aztec warrior recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Aztec;Great_Pyramid_of_Giza"}
{"question": "would Mickey Mouse be camouflaged by the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mickey_Mouse;Flag_of_the_United_States"}
{"question": "can you fish Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon"}
{"question": "have the Dukes of Beaufort ever been to Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gloucestershire"}
{"question": "do people interested in numismatics care about British royals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_Kingdom"}
{"question": "do people interested in numismatics care about British royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_Kingdom"}
{"question": "do people interested in numismatics care about British kings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_Kingdom"}
{"question": "do people interested in numismatics care about Edward III?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Edward_III_of_England"}
{"question": "did Pink Floyd sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pink_Floyd;French_Riviera"}
{"question": "did Roger Waters sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Roger_Waters;French_Riviera"}
{"question": "is Bing useless for anonymous browsing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bing", "extracted_entity": null}
{"question": "is a lacrosse ball blocked by carapace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there bridges people would rather fall from than Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Golden_Gate_Bridge"}
{"question": "can someone survive a fall from Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Golden_Gate_Bridge"}
{"question": "is a fall from Golden Gate bridge likely fatal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Popeye eat an unhealthy amount of spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Popeye"}
{"question": "can you catch Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Salmon"}
{"question": "has Leninism affected politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Leninism;Bohemia"}
{"question": "did Pink Floyd play a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Pink_Floyd;French_Riviera"}
{"question": "is Return of the Jedi's Han Solo bad hypothetical United States Air Force candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Return_of_the_Jedi;Han_Solo;United_States_Air_Force"}
{"question": "did Stone Cold Steve Austin outshine Four Horsemen's tag team accomplishments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Stone_Cold_Steve_Austin;The_Four_Horsemen_(professional_wrestling)"}
{"question": "was Stone Cold Steve Austin better tag teamer than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Stone", "extracted_entity": "Stone_Cold_Steve_Austin;The_Four_Horsemen_(professional_wrestling)"}
{"question": "does AC/DC's success trump Lil Wayne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "AC", "extracted_entity": "AC/DC;Lil_Wayne"}
{"question": "do large mammals exhibit human jealousy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is human jealousy exhibited in larger mammals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does the goat zodiac hypothetically make good entertainers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 18th century judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is Hilo Hawaii an ideal location for snowboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hilo,_Hawaii"}
{"question": "could Tata Hexa get the Spice Girls to a nearby venue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spice_Girls"}
{"question": "would Fortuner be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Spice_Girls;Honda_NSX_(second_generation)"}
{"question": "would Tata Hexa be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Spice_Girls;Honda_NSX_(second_generation)"}
{"question": "would Tata Hexy be more roomy for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Spice_Girls;Honda_NSX_(second_generation)"}
{"question": "can Julia Roberts hang out with BTS?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Julia_Roberts;BTS"}
{"question": "can Julia Roberts ride Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julia_Roberts"}
{"question": "can Julia Roberts enjoy Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Julia_Roberts"}
{"question": " Is The Invisible Man its author's most adapted book to film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Pirate's Flight at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Timberlake;Six_Flags"}
{"question": "is Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Invisible_Man"}
{"question": "can Justin Timberlake ride on the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Justin", "extracted_entity": "Justin_Timberlake;Six_Flags"}
{"question": "is H.G. Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "H._G._Wells;The_Invisible_Man"}
{"question": "can Justin Timberlake ride the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Justin_Timberlake;Six_Flags"}
{"question": "is H.G. Wells' most prolific book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "H._G._Wells;The_Invisible_Man"}
{"question": "is The Invisible Man H.G. Wells most adapted film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Invisible_Man;H._G._Wells"}
{"question": "academy Awards most prolific amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Walt", "extracted_entity": "Academy_Award;Walt_Disney"}
{"question": "academy Awards most celebrated amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Academy_Award;Walt_Disney"}
{"question": "was Walt Disney most celebrated amusement park creator by Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Walt_Disney;Academy_Award"}
{"question": "would Shaun King disagree with Immanuel Kant's views?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Immanuel_Kant"}
{"question": "would Shaun King and Immanuel Kant disagree about race?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Shaun_King_(American_football);Immanuel_Kant"}
{"question": "would Shaun King and Immanuel Kant not see eye to eye about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Shaun_King_(American_football);Immanuel_Kant"}
{"question": "would Shaun King and Immanuel Kant disagree about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaun", "extracted_entity": "Shaun_King_(American_football);Immanuel_Kant"}
{"question": "is disco still influencing music in the 2000s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Disco"}
{"question": "is Bornean Orangutan higher on World Wildlife Fund priority list than Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bornean_orangutan;World_Wide_Fund_for_Nature;Leopard"}
{"question": "does World Wildlife Fund prioritize Bornean Orangutan over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "World_Wide_Fund_for_Nature"}
{"question": "is Bornian Orangutan prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "World_Wide_Fund_for_Nature;Leopard"}
{"question": "is Black Rhino prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black_rhinoceros;World_Wide_Fund_for_Nature;Leopard"}
{"question": "has Stone Cold Steve Austin had more Intercontinental success than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stone_Cold_Steve_Austin;The_Four_Horsemen_(professional_wrestling)"}
{"question": "is genre that dominated late 1970's music still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still relevant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "did one of the Beatles have similar taste in women to Eric Clapton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Beatles;Eric_Clapton"}
{"question": "are mollusks important to the history of the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "M", "extracted_entity": null}
{"question": "is Bill Gates more productive than the country of Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bill", "extracted_entity": "Bill_Gates;Dominica"}
{"question": "is Bill Gates more productive than Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bill", "extracted_entity": "Bill_Gates;Dominica"}
{"question": "did the Bible borrow from ancient myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did ancient myths inspire the Bible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Bible"}
{"question": "could a single bitcoin ever buy a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bitcoin;Volkswagen_Jetta"}
{"question": "is black pepper often combined with a substance that contains chloride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Chloride"}
{"question": "is it best to avoid chocolate with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "chocolate", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be one of Bacchus's favorite holidays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "New_Year's_Eve"}
{"question": "has Drew Carey surpassed Doug Davidson at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drew_Carey"}
{"question": "has Drew Carey surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drew_Carey;Dennis_James"}
{"question": "is music genre that dominated late 70's still alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Drew Carey one of the longest Price is Right hosts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drew_Carey"}
{"question": "is Drew Carey second longest Price is Right Host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drew_Carey"}
{"question": "does it snow in the arctic circle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Leninism influenced politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Leninism;Bohemia"}
{"question": "is music genre that dominated late 1970's still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Drew Carey's surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drew_Carey;Dennis_James"}
{"question": "are more tamarinds grown in Honduras than Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Honduras;Alaska"}
{"question": "would a tamarind tree have difficulty growing in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tamarind;Anchorage,_Alaska"}
{"question": "would a tamarind tree grow poorly in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Tamarind;Anchorage,_Alaska"}
{"question": "would a tamarind tree grow poorly in Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tamarind;Alaska"}
{"question": "is shrimp essential to Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ethiopian_cuisine"}
{"question": "does Drew Carey's Price is Right tenure surpass Dennis James's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drew_Carey;Dennis_James"}
{"question": "are more tamarinds grown in Honduras than Canada?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Honduras;Canada"}
{"question": "would someone in a sauna be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "can a student from Beder understand the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Debar;Albanian_Declaration_of_Independence"}
{"question": "can a student from Beder read the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Debar;Albanian_Declaration_of_Independence"}
{"question": "can all Suikoden characters find dates among Final Fantasy VI characters?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Suikoden;Final_Fantasy_VI"}
{"question": "would a tenth of McCain's electoral votes make John Kerry President in 2004?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_McCain;John_Kerry"}
{"question": "could Amazon defeat Iceland in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Amazon.com;Iceland"}
{"question": "did Jerry Seinfeld's mouth help him earn a lot of cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jerry_Seinfeld"}
{"question": "did Jerry Seinfeld's mouth help him earn a substantial amount of money?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jerry_Seinfeld"}
{"question": "did Jerry Seinfeld earn a lot of money using his mouth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jerry_Seinfeld"}
{"question": "does Marco Rubio have a close relationship with Vishnu?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Marco_Rubio;Vishnu"}
{"question": "does Marco Rubio have a close relationship with Brahma?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Marco_Rubio"}
{"question": "would Alan Alda fit in on the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Alan_Alda;Grey's_Anatomy"}
{"question": "does Secretariat have a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Formula_One"}
{"question": "does Secretariat hold a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Secretariat_(horse);Formula_One"}
{"question": "will a living thing survive the entirety of Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Common_Era"}
{"question": "is the Greek alphabet barred for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Greek_alphabet;K-pop"}
{"question": "is the Greek alphabet prohibited for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Greek_alphabet;K-pop"}
{"question": "is the Greek alphabet prohibited for use in Korean pop music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Greek_alphabet"}
{"question": "can Tony Blair be imprisoned in the Tower of London?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tony_Blair;Tower_of_London"}
{"question": "do travelers to Casablanca have a hard time finding a BLT?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Casablanca"}
{"question": "does Jamie Oliver care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jamie_Oliver"}
{"question": "does Gordon Ramsey know about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Gordon Ramsey care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can people in San Antonio cheer on the Memphis Grizzlies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "San_Antonio;Memphis_Grizzlies"}
{"question": "does Moon Jae-in own multiple species of pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Moon_Jae-in"}
{"question": "does Moon Jae-in support animal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Moon_Jae-in"}
{"question": "does Moon Jae-in own any pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Moon_Jae-in"}
{"question": "do humans pose a threat to Bengal foxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bengal", "extracted_entity": null}
{"question": "do Bengal foxes take care of their children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bengal_fox"}
{"question": "did a manta ray murder Steve Irwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Manta_ray;Steve_Irwin"}
{"question": "does celery contain any essential nutrients?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "celery", "extracted_entity": "Essential_nutrient"}
{"question": "has celery been used for purposes other than food?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Celery"}
{"question": "did one person cause the price of celery to spike at any time in the past?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Bill Clinton President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bill_Clinton;United_States;Dayton_Agreement"}
{"question": "can computers play Poker better than humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it possible to make a living only playing Poker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could James Brown's ex-wives play a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_Brown"}
{"question": "could James Brown's ex-wives have played a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Brown"}
{"question": "could James Brown's ex-wives have held a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "James_Brown"}
{"question": "did slaves play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "History_of_the_United_States;United_States"}
{"question": "can Evander Holyfield consume pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Evander_Holyfield"}
{"question": "can an artisan sell items to someone in another country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau require subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Coen_brothers;Guinea-Bissau"}
{"question": "did Malcolm X play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Malcolm_X;History_of_the_United_States;United_States"}
{"question": "could one Amazon stock ever buy twenty four year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon.com;Netflix"}
{"question": "can French Toast cause Salmonella poisoning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Salmonella"}
{"question": "were there any pandemics during Moon Jae-in's presidency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Moon_Jae-in"}
{"question": "is the paleo diet healthy for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Paleolithic_diet;Diabetes_mellitus"}
{"question": "does soy milk increase heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soy", "extracted_entity": null}
{"question": "are Northern Europeans depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Macbeth"}
{"question": "are Northern Europeans portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Macbeth"}
{"question": "can a one armed drummer be elite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Leninism influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Leninism;Bohemia"}
{"question": "is ABBA's 1970's music genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "ABBA"}
{"question": "is music genre that dominated late 1970's still evolving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Alan Alda fit in on Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Alan_Alda;Grey's_Anatomy"}
{"question": "does Moon Jae-in own both cats and dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Moon_Jae-in"}
{"question": "was Franjo Tudjman President of Croatia when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Franjo_Tu\u0111man;Croatia;Dayton_Agreement"}
{"question": "is the paleo diet beneficial for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Paleolithic_diet;Diabetes_mellitus"}
{"question": "was Al Gore Vice President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Al_Gore;United_States;Dayton_Agreement"}
{"question": "is Noah's Ark more imposing than Batillus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Noah's_Ark"}
{"question": "does a horse fly neigh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Brooke Shields handle University of Pennsylvania curriculum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She", "extracted_entity": "Brooke_Shields;University_of_Pennsylvania"}
{"question": "would University of Pennsylvania admit actress Brooke Shields?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_of_Pennsylvania;Brooke_Shields"}
{"question": "did actress Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Brooke_Shields;University_of_Pennsylvania"}
{"question": "did Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Brooke_Shields;University_of_Pennsylvania"}
{"question": "were many young females attracted to Justin Timberlake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Timberlake"}
{"question": "did Justin Timberlake have many young female fans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Timberlake"}
{"question": "would a physician be unlikely to prescribe Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reiki"}
{"question": "can Viper Room crowd squeeze into National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Viper_Room"}
{"question": "can National Diet building accommodate Viper Room crowd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Viper_Room"}
{"question": "would John Muir be unlikely to have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Muir"}
{"question": "would Alan Alda be comfortable on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Alan_Alda;Chicago_Hope"}
{"question": "would Alan Alda fit in on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Alda;Chicago_Hope"}
{"question": "would Alan Alda fit in on the set of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alan", "extracted_entity": "Alan_Alda;Grey's_Anatomy"}
{"question": "would Alan Alda fit in with the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Alan_Alda;Grey's_Anatomy"}
{"question": "would John Muir dislike being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Muir"}
{"question": "would John Muir hate being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Muir"}
{"question": "could Elizabeth I of England have seen Romeo and Juliet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elizabeth_I_of_England;Romeo_and_Juliet"}
{"question": "could Elizabeth I of England have seen the plays of Shakespeare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elizabeth_I_of_England;William_Shakespeare"}
{"question": "could someone have visited the Library of Alexandria and the pyramids on the same vacation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Library_of_Alexandria"}
{"question": "would Gal Gadot be hired as Victoria Secret model before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Gal_Gadot;Victoria's_Secret;Selena_Gomez"}
{"question": "do middle school students typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pythagoras"}
{"question": "does the bengal fox have hair covering it's body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bengal_fox"}
{"question": "would a librarian use a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Linus Torvald's family ride in a Maruti Suzuki Baleno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a Maruti Suzuki Baleno transport Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can Royal Albert Hall hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Royal_Albert_Hall;McDonald's"}
{"question": "can Madison Square Garden hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Madison_Square_Garden;McDonald's"}
{"question": "could Titanic hypothetically hold Spanish-American War combat casualties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "RMS_Titanic;Spanish\u2013American_War"}
{"question": "could Spanish-American War combat casualties hypothetically fit on Titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spanish\u2013American_War;RMS_Titanic"}
{"question": "did Ptolemy use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Ptolemy;Julian_calendar"}
{"question": "do camels live in Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Camel;Ontario"}
{"question": "will Victoria's Secret hire Gal Gadot before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Victoria's_Secret;Gal_Gadot;Selena_Gomez"}
{"question": "do 10-12 year olds typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pythagoras"}
{"question": "does the little penguin share space with Kiwis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_Zealand"}
{"question": "does horseradish have hooves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is paper a product of logging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "History_of_paper"}
{"question": "would Boticelli's mythological subjects fill a gallery before Salvador Dali's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "alvador", "extracted_entity": "Salvador_Dal\u00ed"}
{"question": "would ten gallons of seawater hold a six year old in the air on a seesaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are Genghis Khan's descendants more prolific than those of Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Genghis_Khan;Julius_Caesar"}
{"question": "was an event in the Balkans considered a contributing factor to World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Balkans;World_War_I"}
{"question": "could the crew members of Apollo 13 field an NHL team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Apollo_13;National_Hockey_League"}
{"question": "could the crew members of Apollo 13 field a full ice hockey team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Apollo_13;Ice_hockey"}
{"question": "could the crew members of Apollo 13 field a full basketball team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Apollo_13"}
{"question": "was Fraktur likely to have been used in the publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Fraktur"}
{"question": "was Fraktur likely to have been used in publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Fraktur"}
{"question": "does the spinal cord play a role in skateboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Skateboarding"}
{"question": "does the visual cortex play a role in regulating metabolic function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Visual_cortex"}
{"question": "does the visual cortex play a role in regulating lung function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Visual_cortex"}
{"question": "can people see the code behind OpenOffice.org?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "OpenOffice.org"}
{"question": "does a Jetta require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does a Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Toyota_Prius"}
{"question": "does a Toyota Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Toyota_Prius"}
{"question": "could the children of Greek hero Jason hypothetically field a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Greece;Jason"}
{"question": "is the artist of the album Slay Belles a drag queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind The Brig?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Lennon"}
{"question": "did John Lennon collaborate with the man who made the film Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "John_Lennon;Walden"}
{"question": "did Beethoven ever compose an EDM song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludwig_van_Beethoven"}
{"question": "did the swallow play a role in a famous film starring Michael Palin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Palin"}
{"question": "did the swallow play a role in a famous film starring John Cleese?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Cleese"}
{"question": "did John Lennon collaborate with the filmmaker behind Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Lennon"}
{"question": "was John Searle born in a state that is bordered by Wyoming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Searle;Wyoming"}
{"question": "does the Illuminati leave messages in modern society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Illuminati"}
{"question": "would King Henry V hypothetically demolish Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henry_V_of_England;Charlemagne;Battle_of_Agincourt"}
{"question": "would King Henry V hypothetically prevail against Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henry_V_of_England;Charlemagne;Battle_of_Agincourt"}
{"question": "would King Henry V hypothetically defeat Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Henry_V_of_England;Charlemagne;Battle_of_Agincourt"}
{"question": "would Constantinople repel a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Constantinople;Charlemagne"}
{"question": "would Constantinople survive a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Constantinople;Charlemagne"}
{"question": "would Constantinople withstand a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Constantinople;Charlemagne"}
{"question": "in the year 1000, did the Eastern Orthodox Church and the Byzantine Empire use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eastern_Orthodox_Church;Byzantine_Empire"}
{"question": "can Stone Cold Steve Austin apply his finisher to the largest mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stone_Cold_Steve_Austin"}
{"question": "is kaffir lime the man ingredient in key krueng?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam two White-tailed deer simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Stone_Cold_Steve_Austin;White-tailed_deer"}
{"question": "can someone from Denmark go to Sweden without their passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Denmark;Sweden"}
{"question": "can a male mule deer be body slammed by Stone Cold Steve Austin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Mule_deer;Stone_Cold_Steve_Austin"}
{"question": "can Stone Cold Steve Austin body slam a male mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stone_Cold_Steve_Austin;Mule_deer"}
{"question": "are most of the products labelled Fair Trade exported from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Fair_trade;United_States"}
{"question": "was Harry Houdini's wife bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_Houdini"}
{"question": "was Harry Houdini bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Harry_Houdini"}
{"question": "did Harry Houdini disprove the claims of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Harry_Houdini"}
{"question": "is Drew Carey in a sports Hall of Fame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Drew_Carey"}
{"question": "does Rand have valid eye exam certification in every state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Rand Paul prohibited from conducting eye exams?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rand_Paul"}
{"question": "can a Toyota Hilux transport a human heart from NYC to Raleigh NC?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Toyota_Hilux;New_York_City"}
{"question": "can a Toyota Hilux get to Havana from NYC before sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Toyota_Hilux;Havana;New_York_City"}
{"question": "can a Toyota Hilux get to Havana from NYC in sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Toyota_Hilux;Havana;New_York_City"}
{"question": "is one of the ingredients of photosynthesis found in emeralds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you pour diethyl ether?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Larry King fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Larry_King;World_War_I"}
{"question": "have any WWE champions outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kane", "extracted_entity": "WWE;Kane_(wrestler);Royal_Rumble"}
{"question": "has a Neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Neanderthal;Supreme_Court_of_the_United_States"}
{"question": "are infants excluded from joining the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Supreme_Court_of_the_United_States"}
{"question": "can a worm suffer a myocardial infarction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Myocardial_infarction"}
{"question": "can every Prime Minister of the United Kingdom since 1970 fill all the US Supreme Court chairs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Prime_Minister_of_the_United_Kingdom;Supreme_Court_of_the_United_States"}
{"question": "can every Prime Ministers of the United Kingdom since 1970 fill all the US Supreme Court positions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "List_of_Prime_Ministers_of_the_United_Kingdom;Prime_Minister_of_the_United_Kingdom;United_Kingdom;Supreme_Court_of_the_United_States"}
{"question": "will Justin Bieber replace Mike Pence in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Justin", "extracted_entity": "Justin_Bieber;Mike_Pence"}
{"question": "will Justin Bieber be Trump's running mate in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber"}
{"question": "can Justin Bieber represent New York in Congress in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;New_York_City"}
{"question": "will Justin Bieber become US President in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;United_States;President_of_the_United_States"}
{"question": "will Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber;2020_United_States_presidential_election;United_States"}
{"question": "can Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Justin_Bieber"}
{"question": "were there highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did cars travel on highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Silent_Generation"}
{"question": "did members of the Silent Generation use highways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Silent_Generation"}
{"question": "can a rabbi perform any Christian rituals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christianity"}
{"question": "can baptism be done by a rabbi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Baptism;Rabbi"}
{"question": "can a Rabbi perform a Christian baptism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rabbi;Christianity"}
{"question": "can a Rabbi administer any of the Christian Sacraments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sacrament"}
{"question": "were there typewriters in the Empire of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Empire_of_Japan"}
{"question": "did people listen to The Blues during World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Blues_Brothers;World_War_I"}
{"question": "can a person from New Brunswick understand the language of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Brunswick;Burundi"}
{"question": "can a person from New Brunswick understand the speech of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Brunswick;Burundi"}
{"question": "can a person from Madagascar order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada;Madagascar;New_Brunswick"}
{"question": "can a person from Burundi order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Burundi;New_Brunswick"}
{"question": "do lizards breed in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Brunswick"}
{"question": "can lizards survive temperatures in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_Brunswick"}
{"question": "is the Swastika a religious symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Swastika"}
{"question": "did Japanese people win the War of the Bucket?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan"}
{"question": "can you transport nine pounds of weed in a bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the husband of Elizabeth Hamilton work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elizabeth_Schuyler_Hamilton"}
{"question": "did the husband of Alexander Hamilton's granddaughter work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alexander_Hamilton"}
{"question": "did Elizabeth Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Elizabeth Margaret Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Adobe Suite and Life Is Strange video game share similar coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Life_Is_Strange_(video_game)"}
{"question": "did Sugar Ray Robinson ever experience pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sugar_Ray_Robinson"}
{"question": "did the creator of Saturday Night Live ever work with Jack Paar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Saturday_Night_Live;Jack_Paar"}
{"question": "did the creator of Saturday Night Live ever work with Johnny Carson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saturday_Night_Live;Johnny_Carson"}
{"question": "does the author of Rebecca use Cornwall in another novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rebecca_(novel);Cornwall"}
{"question": "could a participant in the American Civil War read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "American_Civil_War;Hamlet"}
{"question": "could the winner of a Nobel Peace Prize have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nobel_Peace_Prize;Hamlet"}
{"question": "could the author of The Bell Jar have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Bell_Jar;Hamlet"}
{"question": "could Saint Augustine have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Augustine_of_Hippo;Hamlet"}
{"question": "can a human safely consume Cannabis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cannabis"}
{"question": "did Metallica ever release a rap song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Metallica"}
{"question": "does the name C-SPAN refer to a form of telecommunications?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "C-SPAN"}
{"question": "could someone watch C-Span on cable in 1980?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was C-Span on the air in 1987?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Christopher Hitchens ever appear on C-Span?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Hitchens"}
{"question": "did Brian Lamb ever host a television program on C-SPAN?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Brian_Lamb;C-SPAN"}
{"question": "is Battle Hymn of the Republic considered christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Battle_Hymn_of_the_Republic;Christian_music"}
{"question": "did Fanny Crosby write Christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Fanny_Crosby;Christian_music"}
{"question": "is Christian music played in Churches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Christian_music;Church_(building)"}
{"question": "did the founders of Encyclop\u00e6dia Britannica enjoy views of the Firth of Forth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Encyclop\u00e6dia_Britannica;Firth_of_Forth"}
{"question": "was the country featured in All My Life one of the participants in the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Six-Day_War"}
{"question": "is an economist able to determine why the price of a burrito changes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is an economist able to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is an economist qualified to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Manny Pacquiao a boxing champion when Obama was first elected US president?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Manny_Pacquiao;Barack_Obama;United_States"}
{"question": "did Martin Luther know Latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Martin_Luther;Latin"}
{"question": "will Jay-Z reach the age of 60 before Kendrick Lamar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jay_Z;Kendrick_Lamar"}
{"question": "did Jay-Z know Kanye West in 2012? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jay_Z;Kanye_West"}
{"question": "did Jay-Z know Kanye West in 2011? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jay_Z;Kanye_West"}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Prussia;Wilhelm_Miklas"}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Prussia;Wilhelm_Miklas"}
{"question": "could someone from Prussia communicate easily with someone from Austria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Prussia;Austria"}
{"question": "did the lead singer of Led Zepplin ever play in a park designed by a poet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Led_Zeppelin"}
{"question": "did the lead singer of Led Zepplin ever play as a child in Leasowes Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Led_Zeppelin"}
{"question": "did Abraham Lincoln help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Abraham_Lincoln;University_of_Pittsburgh"}
{"question": "was Abraham Lincoln responsible for founding the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Abraham_Lincoln;University_of_Pittsburgh"}
{"question": "was the current U.S. speaker of the house alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "does Gavin Rossdale have a large enough net worth to purchase the most expensive Thoroughbred horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gavin_Rossdale;Thoroughbred"}
{"question": "would nickel boil if placed in the core of Neptune?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Neptune"}
{"question": "is there a state that touches several of the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "are any of the Great Lakes entirely in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "do the Great Lakes have at least 15% of the worlds freshwater fish supply?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Great_Lakes"}
{"question": "are there any fresh water sand dunes in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "has anyone drowned in the Great Lakes in 2019?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Great_Lakes"}
{"question": "has anyone ever drowned in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "do the Great Lakes have bacteria in the water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Great_Lakes"}
{"question": "do people ice fish on the Great Lakes in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "do the Great Lakes freeze in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Great_Lakes"}
{"question": "do the Great Lakes have more endangered species than other parts of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes;United_States"}
{"question": "do the Great Lakes have any endangered species?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Great_Lakes"}
{"question": "is John the Baptist still important to people today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_the_Baptist"}
{"question": "is John the Baptist still worshiped to this day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_the_Baptist"}
{"question": "was The Hobbit written before The Lord of the Rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Hobbit;The_Lord_of_the_Rings"}
{"question": "is David Cameron a Tory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "David_Cameron;Tory"}
{"question": "is Dragon Hall located in Norfolk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Norfolk"}
{"question": "is Norfolk predominantly rural?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Norfolk on the eastern coast of England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Norfolk;England"}
{"question": "are Rogue DnD characters suited for playing in cities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are DnD Ranger characters suited for play in wilderness areas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could George Washington have recorded a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "George_Washington;Compact_disc"}
{"question": "do Hindus believe Krishna exists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hindu;Krishna"}
{"question": "is Lord Hill's column in Shropshire the tallest of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shropshire"}
{"question": "are there several  towns located in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Shropshire"}
{"question": "were Charles Darwin and A.E. Housman born in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Charles_Darwin;A._E._Housman;Shropshire"}
{"question": "is there a david bowie song with the word year or years in the title?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the Hopi Indians use a symbol that was similar to the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hopi"}
