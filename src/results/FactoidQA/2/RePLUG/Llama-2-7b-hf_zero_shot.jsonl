{"question": "was Abraham Lincoln the sixteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "did Lincoln sign the National Banking Act of 1863?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did his mother die of pneumonia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many long was Lincoln's formal education?", "answer": "18 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["18 months", "18 months."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln was self-educated.", "extracted_entity": null}
{"question": "when did Lincoln begin his political career?", "answer": "1832", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1832", "1832."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1832", "extracted_entity": null}
{"question": "what did The Legal Tender Act of 1862 establish?", "answer": "the United States Note, the first paper currency in United States history", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United States Note, the first paper currency in United States history", "The United States Note, the first paper currency in United States history."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who suggested Lincoln grow a beard?", "answer": "11-year-old Grace Bedell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11-year-old Grace Bedell", "Grace Bedell."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Grace Bedell", "extracted_entity": null}
{"question": "when did the Gettysburg address argue that America was born?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776", "1776."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "November 19, 1863", "extracted_entity": null}
{"question": "did Lincoln beat John C. Breckinridge in the 1860 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln", "extracted_entity": null}
{"question": "was Abraham Lincoln the first President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was the 16th President of", "extracted_entity": null}
{"question": "did Lincoln start his political career in 1832?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abe Lincoln", "extracted_entity": null}
{"question": "did Lincoln ever represent Alton & Sangamon Railroad?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "which county was Lincoln born in?", "answer": "Hardin County", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hardin County", "Southeast Hardin County, Kentucky"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Lincoln first serve as President?", "answer": "March 4, 1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1861"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1861", "extracted_entity": null}
{"question": "who assassinated Lincoln?", "answer": "John Wilkes Booth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Wilkes Booth"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "John Wilkes Booth", "extracted_entity": null}
{"question": "did Lincoln win the election of 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln won the election of 1860", "extracted_entity": null}
{"question": "who was the general in charge at the Battle of Antietam?", "answer": "General McClellan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General McClellan", "John Alexander McClernand"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Robert E. Lee", "extracted_entity": null}
{"question": "why did Lincoln issue the Emancipation Proclamation?", "answer": "To free slaves ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To free slaves ", "The goal was to weaken the rebellion, which was led and controlled by slave owners."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do scholars rank lincoln among the top three presidents?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did lincoln have 18 months of schooling?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "was Lincoln chosen as a presidential candidate in 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln", "extracted_entity": null}
{"question": "how old was Lincoln in 1816?", "answer": "seven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["seven"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "16", "extracted_entity": null}
{"question": "when was the first photgraph of lincoln taken?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The first photograph of Lincoln was taken in 1", "extracted_entity": null}
{"question": "how long was Lincoln's legal Career?", "answer": "23 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["23 years"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1837 - 186", "extracted_entity": null}
{"question": "what trail did Lincoln use a Farmers' Almanac in? ?", "answer": "he defended William \"Duff\" Armstrong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he defended William \"Duff\" Armstrong"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The Lincoln Highway.", "extracted_entity": null}
{"question": "did Abraham Lincoln live in the Frontier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln lived in the Frontier", "extracted_entity": null}
{"question": "did Lincoln's Wife's Family support slavery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Mary Todd's family supported slavery.", "extracted_entity": null}
{"question": "who is most noted for his contributions to the theory of molarity and molecular weight?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "who graduated in ecclesiastical law at the early age of 20 and began to practice?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro", "blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he publish another memoria?", "answer": "1821", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1821"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1793", "extracted_entity": null}
{"question": "when did he become a professor?", "answer": "1820", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1820"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "is it true that he became a professor in 1820?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Lorenzo Romano Amedeo Carlo Avogadro an Italian savant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in Turin August 9th 1776 to a noble ancient family of Piedmont, Italy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in", "extracted_entity": null}
{"question": "is he most noted for his contributions to the theory of molarity and molecular weight?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was King Victor Emmanuel III there to pay homage to Avogadro ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "King Victor Emmanuel III", "extracted_entity": null}
{"question": "is Avogadro 's number commonly used to compute the results of chemical reactions ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the scientific community not reserve great attention to his theory ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, the scientific community did not reserve great attention", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", ""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The History of the Reign of the Emperor Sever", "extracted_entity": null}
{"question": "what happened in french?", "answer": "", "dataset": "factoid_qa", "split": "train", "answer_aliases": [""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1833?", "answer": "Avogadro had been recalled to Turin university", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro had been recalled to Turin university", "blah blah blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The Great Cholera Epidemic of", "extracted_entity": null}
{"question": "who determined the dependence of the boiling of water with atmospheric pressure?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is named after him?", "answer": "The Celsius crater on the Moon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Celsius crater on the Moon"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he publish a collection?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1957", "extracted_entity": null}
{"question": "is it true that he published a collection in 1738?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that thermometer had 100 for the freezing point?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, it is true that thermometer had", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Anders Celsius (November 27, 1701 April 25, 1744) a Swedish astronomer?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius", "extracted_entity": null}
{"question": "is The Celsius crater on the Moon named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Celsius crater is not named after", "extracted_entity": null}
{"question": "who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius", "extracted_entity": null}
{"question": "the Celsius crater on the Moon is what?", "answer": "named after him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["named after him"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the Celsius crater on the Moon named after him ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": null}
{"question": "had his thermometer 100 for the freezing point of water and 0 for the boiling point ?", "answer": "Yes it had", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes it had"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did he not determine the dependence of the boiling of water with atmospheric pressure -LRB- in excellent agreement with modern data -RRB- ?", "answer": "Yes he did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he did"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "boiling point of water.", "extracted_entity": null}
{"question": "what happened from 1730 to 1744?", "answer": "He was professor of astronomy at Uppsala University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was professor of astronomy at Uppsala University"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1745?", "answer": "The scale was reversed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the Jacobite rising of 1745", "extracted_entity": null}
{"question": "are beetles insects?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can beetles be found in polar regions?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do beetles antennae function primarily as organs of smell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the three sections of a beetle?", "answer": "the head, the thorax, and the abdomen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the head, the thorax, and the abdomen", "The head, the thorax, and the abdomen"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which defense mechanism uses colour or shape to deceive potential enemies?", "answer": "mimicry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mimicry", "Mimicry"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "camouflage", "extracted_entity": null}
{"question": "which type of beetle is a pest of potato plants?", "answer": "Colorado potato beetle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colorado potato beetle"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Colorado potato beetle", "extracted_entity": null}
{"question": "how can beetle larvae be differentiated from other insect larvae?", "answer": "their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "By their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do beetles eat?", "answer": "Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "beetles eat plants.", "extracted_entity": null}
{"question": "what are the similarities between beetles and grasshoppers?", "answer": "mouthparts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mouthparts", "Beetles have mouthparts similar to those of grasshoppers"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are certain species of beetles considered pests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a beetle's general anatomy uniform?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are beetles endopterygotes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, beetles are endopterygotes", "extracted_entity": null}
{"question": "how many species of beetles are there?", "answer": "350,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["350,000", "about 350,000"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when are sperm cells transferred to the female?", "answer": "during pairing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["during pairing", "During pairing"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "During the process of fertilization.", "extracted_entity": null}
{"question": "what is the study of beetles called?", "answer": "coleopterology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["coleopterology"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleopterology", "extracted_entity": null}
{"question": "is it possible that there are more than 350,000 species of beetles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is the Adephaga suborder larger than the Polyphaga suborder?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The Adephaga suborder is larger than", "extracted_entity": null}
{"question": "do carrion beetles eat dung?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are prey of various animals including birds and mammals?", "answer": "Beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "prey of various animals including birds and mamm", "extracted_entity": null}
{"question": "what was given by Aristotle for the hardened shield like forewings?", "answer": "The name \"Coleoptera\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name \"Coleoptera\"."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The shield like forewings", "extracted_entity": null}
{"question": "who or what vary greatly in form within the coleoptera?", "answer": "Antennae.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antennae."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are many beetles territorial?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are beetles endopterygotes with complete metamorphosis?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "oxygen is what?", "answer": "One kind of gas obtained via a tracheal system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One kind of gas obtained via a tracheal system."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Oxygen is an element", "extracted_entity": null}
{"question": "is there a thriving industry in the collection of beetle specimens for amateur and professional collectors ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "[beetles](https://www.google", "extracted_entity": null}
{"question": "have coleopterists formed organisations to facilitate the study of beetles ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleopterists have formed organisations to facilitate", "extracted_entity": null}
{"question": "is the study of beetles called coleopterology , and its practitioners are coleopterists ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "may a single female lay from several dozen to several thousand eggs during her lifetime ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can beetles be found in almost all habitats?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened on plants and fungi?", "answer": "They are food to beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are food to beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Calvin Coolidge the twenty-ninth vice president?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Calvin Coolidge born in Plymouth, Windsor County, Vermont?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge", "extracted_entity": null}
{"question": "did Coolidge graduate from Black River Academy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he drop John from his name?", "answer": "upon graduating from college", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon graduating from college", "Upon graduating from college"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "2010", "extracted_entity": null}
{"question": "when did Coolidge meet and marry Grace Anna Goodhue?", "answer": "1905", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1905", "In 1905"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1905", "extracted_entity": null}
{"question": "what period of rapid economic growth did the United States experience during Coolidge's presidency?", "answer": "the Roaring Twenties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Roaring Twenties", "the \"Roaring Twenties\""], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "the Roaring Twenties", "extracted_entity": null}
{"question": "what did Coolidge do after graduating from Amherst?", "answer": "Coolidge moved to Northampton, Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge moved to Northampton, Massachusetts", "Coolidge moved to Northampton, Massachusetts to take up the practice of law"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He taught at the Massachusetts Agricultural College.", "extracted_entity": null}
{"question": "of what state was Coolidge governor?", "answer": "Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Massachusetts"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who appointed Harlan Fiske Stone to the Supreme Court?", "answer": "Coolidge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Coolidge the thirteenth President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Calvin Coolidge Republican?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, Calvin Coolidge was a Republican", "extracted_entity": null}
{"question": "was Calvin Coolidge a governor of Massachusetts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge", "extracted_entity": null}
{"question": "when was Coolidge born?", "answer": "July 4 1872", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4 1872", "Plymouth, Windsor County, Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1872", "extracted_entity": null}
{"question": "where did Coolidge's grandfather had government offices?", "answer": "Plymouth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth", "The town of Plymouth Notch"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "the White House.", "extracted_entity": null}
{"question": "which state were Coolidge born in?", "answer": "Plymouth, Windsor County, Vermont", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth, Windsor County, Vermont", "Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Massachusetts", "extracted_entity": null}
{"question": "is Calvin Jr. older than John Coolidge?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "John Coolidge", "extracted_entity": null}
{"question": "did Coolidge get in Amherst College?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what caused Calvin Jr.'s death?", "answer": "heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["heart attack", "Heart attack"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin's father, Calvin Sr.", "extracted_entity": null}
{"question": "was John Calvin Coolidge Jr. was born in Las Vegas?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "John Calvin Coolidge Jr. was born", "extracted_entity": null}
{"question": "was Coolidge opposed in the Republican nomination for Governor of Massachusetts in 1918?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Charles W. Bryan", "extracted_entity": null}
{"question": "did Coolidge meet and marry Grace Anna Goodhue?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Anna Goodhue", "extracted_entity": null}
{"question": "what year did Coolidge open his own law office?", "answer": "1898", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1898"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1905", "extracted_entity": null}
{"question": "what fraternity was Coolidge a member of?", "answer": "Phi Gamma Delta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Phi Gamma Delta", "phi gamma delta"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "the Phi Beta Kappa fraternity", "extracted_entity": null}
{"question": "in 1905 Coolidge met and married whom?", "answer": "Grace Anna Goodhue", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grace Anna Goodhue"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Goodhue Coolidge", "extracted_entity": null}
{"question": "why did Coolidge not attend law school?", "answer": "It was too expensive", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was too expensive", "cost"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have there been other US presidents that have visited Cuba?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what years were Coolidge's two sons born in?", "answer": "1906 and 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1906 and 1908"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1906 and 190", "extracted_entity": null}
{"question": "did France cede nearly all of its colonies in Europe in 1763?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Canada a member of the OECD?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Canada's official language Zulu?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "what is one significant non-official language?", "answer": "Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese.", "Chinese"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "in addition to Port Royal, where else did Samuel de Champlain establish a settlement?", "answer": "Quebec City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec City"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Plaisance", "extracted_entity": null}
{"question": "what was the Consitution Act formerly called?", "answer": "The British North America Act.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British North America Act.", "British North America Act"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which countries established colonies in Canada?", "answer": "France and Britain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France and Britain.", "Fance and Britain"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "United Kingdom", "extracted_entity": null}
{"question": "how has Canada helped UN peacekeeping efforts?", "answer": "During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "Canada has played a leading role in UN peacekeeping efforts. During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what percentage of the population don't speak English or French at home?", "answer": "28%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28%", "1.5%"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "80%", "extracted_entity": null}
{"question": "is Canada bilingual?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, it is."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada is bilingual", "extracted_entity": null}
{"question": "did Canadian soldiers win the Battle of Vimy Ridge in 1917?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No, it did not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is an official language of Canada German?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, it is not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many provinces and territories does Canada have?", "answer": "A federation now comprising ten provinces and three territories", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A federation now comprising ten provinces and three territories", "Ten provinces and three territories"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada has 10 provinces and 3 territ", "extracted_entity": null}
{"question": "what is Canada's national unemployment rate?", "answer": "While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "In October 2007, Canada's national unemployment rate is 5.9%."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "unemployment rate", "extracted_entity": null}
{"question": "where is the most densely populated part of Canada?", "answer": "The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Toronto", "extracted_entity": null}
{"question": "what is the largest country in the world?", "answer": "Canada is the second largest country in the world, after Russia, and largest on the continent.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada is the second largest country in the world, after Russia, and largest on the continent.", "Russia."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": null}
{"question": "what is the largest religious group in Canada?", "answer": "According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "Christian"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Canada have one of the largest armed forces in the world?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "World War II."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1940s", "extracted_entity": null}
{"question": "what have been inhabited for millennia by aboriginal peoples?", "answer": "The lands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lands"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Australia", "extracted_entity": null}
{"question": "european books and maps began referring to this region as Canada in what year?", "answer": "1545", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1545"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1535", "extracted_entity": null}
{"question": "what was Canada's most important industry until the 1800s?", "answer": "The fur trade", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The fur trade"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that it angered many residents of the thirteen colonies?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Boston Tea Party was a political protest by", "extracted_entity": null}
{"question": "how do cuba and declining participation relate?", "answer": "Canada maintains full relations with both.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada maintains full relations with both."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that provinces have a large degree of autonomy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Canada adopt in 1965?", "answer": "The Maple Leaf Flag", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Maple Leaf Flag"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada adopted the Maple Leaf Flag.", "extracted_entity": null}
{"question": "what arrived in Britain?", "answer": "The first Canadian Army units in WW II", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first Canadian Army units in WW II"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the first large group of people from India.", "extracted_entity": null}
{"question": "are Canada 's two official languages English and French ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "English and French", "extracted_entity": null}
{"question": "are Canada 's official national sports ice hockey ( winter ) and lacrosse ( summer ) ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Canada a geographically vast and ethnically diverse country ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada is a geographically vast and ethn", "extracted_entity": null}
{"question": "did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ?", "answer": "no!", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no!"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Nova Scotia", "extracted_entity": null}
{"question": "did Cartier not use the word ` Canada ' to refer to not only that village , but the entire area subject to Donnacona , Chief at Stadacona ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada", "extracted_entity": null}
{"question": "did continental European immigrants not settle the prairies ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Prairies were settled by European immigr", "extracted_entity": null}
{"question": "is it the world 's second largest country by total area?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what happened in 1867?", "answer": "Canada was formed as a federal, semi-autonomous polity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada was formed as a federal, semi-autonomous polity"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1867 is the year the Great", "extracted_entity": null}
{"question": "are ducks in the Arctic Northern Hemisphere migratory?", "answer": "Some are", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is a drake a male?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a drake is a male", "extracted_entity": null}
{"question": "do all ducks \"quack\"?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is the name of a specialized species of duck adapted to catch large fish?", "answer": "the smew", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the smew"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what animal attracts the most humor and silliness?", "answer": "The duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The duck", "the duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "the goat", "extracted_entity": null}
{"question": "what is an economic use of a duck?", "answer": "Meat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat", "being farmed for their meat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "eggs", "extracted_entity": null}
{"question": "who is Daffy Duck?", "answer": "A silly cartoon character", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A silly cartoon character", "a silly cartoon character"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Daffy Duck is a cartoon character", "extracted_entity": null}
{"question": "what makes it more difficult for a diving duck to fly?", "answer": "They are heavier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are heavier", "they are heavier"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the reasons for hunting wild ducks?", "answer": "Meat, eggs, and feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "to provide food for people", "extracted_entity": null}
{"question": "do ducks forage underwater?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Diving ducks and sea ducks forage underwater"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is foie gras often made from ducks?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "duck", "extracted_entity": null}
{"question": "what economic uses to ducks have?", "answer": "They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "They can be farmed for meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what kind of ducks feed on land?", "answer": "Dabbling ducks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dabbling ducks", "Dabbling ducks feed on land"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Peking duck", "extracted_entity": null}
{"question": "what unrelated water birds are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Puffins", "extracted_entity": null}
{"question": "what does the word duck mean?", "answer": "It is the common name for a number of species in the Anatidae family of birds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the common name for a number of species in the Anatidae family of birds.", "to bend down low as if to get under something"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "duck is a type of bird.", "extracted_entity": null}
{"question": "what are some common predators of ducks?", "answer": "Pike, crocodilians, herons, hawks and eagles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pike, crocodilians, herons, hawks and eagles.", "pike, crocodilians, and other aquatic hunters"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does a Mallard's tongue work?", "answer": "It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "It uses short spikes to push struggling prey and other food down its throat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Mallards have a bill that is shaped like", "extracted_entity": null}
{"question": "are diving ducks heavier tha dabbling ducks?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "are adult ducks fast fliers?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are ducks an accepted presence in some populated areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Ducks are a common presence in many populated", "extracted_entity": null}
{"question": "what types of unrelated water birds with similar forms are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "seagulls", "extracted_entity": null}
{"question": "why are ducklings particularly vulnerable?", "answer": "Their inability to fly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their inability to fly.", "their inability to fly makes them easy prey"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are some economic uses for duck?", "answer": "Meat,eggs,feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat,eggs,feathers", "being farmed for their meat, eggs, feathers, (particularly their down)"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what allows a duck to filter water out of the side of their beaks and keep food inside?", "answer": "Tiny rows of plates called lamellae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tiny rows of plates called lamellae", "tiny rows of plates called lamellae like a whale's baleen"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "A duck's beak is a modified", "extracted_entity": null}
{"question": "what expression is part of a conceptual framework for testing computer systems?", "answer": "Quacks like a duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quacks like a duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The expression is a conceptual framework for testing computer", "extracted_entity": null}
{"question": "where did the expression \"a sitting duck\" come from?", "answer": "In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "In many areas, wild ducks of various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys."], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "\"a sitting duck\" came from the expression", "extracted_entity": null}
{"question": "is Egypt bordered by the Gaza Strip?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt", "extracted_entity": null}
{"question": "is Egypt the most populated country in Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt is the most populated country in Africa.", "extracted_entity": null}
{"question": "does Egypt have political influence in the Middle East?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt", "extracted_entity": null}
{"question": "when was the Six Day War?", "answer": "1967", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1967"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1967", "extracted_entity": null}
{"question": "what religions has Egypt outlawed?", "answer": "All but Christianity, Islam, and Judaism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["All but Christianity, Islam, and Judaism", "all religions and belief except Islam, Christianity and Judaism"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Christianity, Judaism, and Islam", "extracted_entity": null}
{"question": "what is the poulation of Egypt?", "answer": "more than 78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["more than 78 million", "78 million people"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "92,750,00", "extracted_entity": null}
{"question": "why does most of Egypt's population live near the Nile?", "answer": "the only arable agricultural land is found there,", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the only arable agricultural land is found there,", "The only arable agricultural land is found there"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Because of the fertile soil.", "extracted_entity": null}
{"question": "are there a large number of Jews living in Egypt today?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "during what period was Akhenaten a Pharaoh?", "answer": "The New Kingdom (c.1550\u22121070 BC) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The New Kingdom (c.1550\u22121070 BC) ", "The New Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Akhenaten was a Pharaoh from", "extracted_entity": null}
{"question": "is Egypt in Asia?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt is in Africa", "extracted_entity": null}
{"question": "does Egypt receive the least rainfall in the world?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Cairo, Egypt", "extracted_entity": null}
{"question": "does Egypt's foreign policy operates along moderate lines?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt's foreign policy operates along moderate", "extracted_entity": null}
{"question": "since when has Egypt been a republic?", "answer": "June 18 1953.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 18 1953.", "June 18 1953"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1953", "extracted_entity": null}
{"question": "when were the Great Sphinx and the Pyramids of Giza built?", "answer": "During the Old Kingdom.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Old Kingdom.", "During the Old Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The Great Sphinx and the Pyramids", "extracted_entity": null}
{"question": "what countries border Egypt?", "answer": "Libya, Sudan, the Gaza Strip and Israel.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Libya, Sudan, the Gaza Strip and Israel.", "Libya, Sudan, the Gaza Strip and Israel"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the estimated population of Egypt?", "answer": "More than 78 million.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 78 million.", "78 million"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "100 million", "extracted_entity": null}
{"question": "does snow fall in Egypt?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "when did organized agriculture appear in the Nile Valley?", "answer": "6000 BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6000 BC.", "About 6000 BC"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "10,000 BC", "extracted_entity": null}
{"question": "are elephants the largest land animals alive today?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "African bush elephant", "extracted_entity": null}
{"question": "can an elephant kill a rhinoceros?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are elephants good swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many living species of African Elephants are there?", "answer": "2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2", "The African Elephant genus contains two (or, arguably, three) living species."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how long may elephants live?", "answer": "70 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70 years", "As long as 70 years, sometimes longer."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "70 years", "extracted_entity": null}
{"question": "why does the phrase 'elephants never forget' have no metaphorical meaning?", "answer": "it refers literally to elephants supposedly having an excellent memory", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it refers literally to elephants supposedly having an excellent memory", "Because it refers literally to elephants supposedly having an excellent memory."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Elephants have a large memory capacity.", "extracted_entity": null}
{"question": "how many living species of Asian Elephants are there?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1", "one"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "how much do elephants weight at birth?", "answer": "120 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["120 kilograms", "At birth it is common for an elephant calf to weigh 120 kilograms (265 lb)."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "100 to 130 kg", "extracted_entity": null}
{"question": "what did Aristotle say about elephants?", "answer": "the beast which passeth all others in wit and mind", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the beast which passeth all others in wit and mind", "Aristotle once said the elephant was \"the beast which passeth all others in wit and mind.\""], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Aristotle's views on elephants", "extracted_entity": null}
{"question": "are elephant populations in West Africa generally small and fragmented?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Asian elephant larger than the African?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Sometimes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is an elephant's skin tough?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do elephants communicate over long distances?", "answer": "by producing and receiving low frequency sound", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by producing and receiving low frequency sound", "By producing and receiving low-frequency sound (infrasound)"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when do African elephants lie down?", "answer": "when they are sick or wounded", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when they are sick or wounded"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how long is the elephant's gestation period?", "answer": "22 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["22 months"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "22 months", "extracted_entity": null}
{"question": "how many species of African elephants have been proposed?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "two", "extracted_entity": null}
{"question": "what are the elephant's ears important for?", "answer": "temperature regulation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["temperature regulation"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "they help to cool the elephant's", "extracted_entity": null}
{"question": "what land animal has the largest brain?", "answer": "elephant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["elephant", "The elephant"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are elephants mammals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have elephants been used as working animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do they prefer forested areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what is the world population of Asian elephants?", "answer": "60000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60000", "The world population of Asian elephants \u2013 also called Indian Elephants or Elephas maximus \u2013 is estimated to be around 60,000"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what part of an elephant is very important for temperature regulation?", "answer": "Ears", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ears", "The large flapping ears of an elephant are also very important for temperature regulation."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the ear", "extracted_entity": null}
{"question": "where was the largest elephant ever recorded shot?", "answer": "Angola", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Angola", "The largest elephant ever recorded was shot in Angola in 1956."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "in 1932 in the Himal", "extracted_entity": null}
{"question": "is an Asian elephant smaller than an African elephant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what land animal is larger than an elephant?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "None, the elephant is the largest land animal."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "A blue whale", "extracted_entity": null}
{"question": "has tusklessness become a rare abnormality?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, tusklessness has become a widespread hereditary trait. "], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were the treaties signed in 1947 and 1948 with the Ukraine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The treaties were signed in 194", "extracted_entity": null}
{"question": "is Finnish a member of the Baltic-Finnic subgroup of the Uralic languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the most read newspaper in Finland Taloussanomat?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which Russian army general conquered Finland in 1809?", "answer": "Alexander I", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alexander I"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Mikhail Kutuzov", "extracted_entity": null}
{"question": "what is the life expectancy for men in Finland?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "79.8", "extracted_entity": null}
{"question": "in what year were the \"Games of the XV Olympiad\" held?", "answer": "1952", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1952"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "1956", "extracted_entity": null}
{"question": "what body of water lies to the south of Finland?", "answer": "the Gulf of Finland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Gulf of Finland", "Archipelago Sea"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The Baltic Sea", "extracted_entity": null}
{"question": "name an animal that is growing in number due to recent conservation efforts?", "answer": "Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "Golden Eagle"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "today, are there more cell phone subscriptions than people in Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, there are more cell phone subscriptions than", "extracted_entity": null}
{"question": "is Matti Vanhanen the Prime Minister of Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the capital city Oslo?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does it have a border with Norway?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what is the life expectancy for men?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "74.4 years", "extracted_entity": null}
{"question": "what is the biggest city in Finland?", "answer": "Greater Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greater Helsinki", "the cities of the Greater Helsinki metropolitan area"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki", "extracted_entity": null}
{"question": "who is the most popular rock group in Finland?", "answer": "CMX", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["CMX"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "HIM", "extracted_entity": null}
{"question": "what is Finland's economy like?", "answer": "a highly industrialised, free-market economy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a highly industrialised, free-market economy"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland's economy is characterized by a large", "extracted_entity": null}
{"question": "where is Finland located?", "answer": "Northern Europe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Europe"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland", "extracted_entity": null}
{"question": "what is the weather like in summer?", "answer": "relatively warm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["relatively warm"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "it's hot.", "extracted_entity": null}
{"question": "does salmon remain the favorite of fly rod enthusiasts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "I think it does.", "extracted_entity": null}
{"question": "are there cathedrals scattered all across Finland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki Cathedral", "extracted_entity": null}
{"question": "according to Reader's Digest, is Finland best for living?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The Nordic nation is one of the safest", "extracted_entity": null}
{"question": "when did the first verifiable written documents appear?", "answer": "Twelfth Century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twelfth Century", " The first verifiable written documents appeared in the twelfth century. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the capital city?", "answer": "Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helsinki", "The capital city is Helsinki. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the most extensively celebrated holiday?", "answer": "Christmas", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas", "Christmas is the most extensively celebrated holiday."], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a large proportion of the population speak Swedish as its mother tongue?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, Swedish is the official language of Sweden.", "extracted_entity": null}
{"question": "do women live longer than men?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "women", "extracted_entity": null}
{"question": "what is a country with which Finland is involved in an international conflict?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "Finland is not involved in international conflicts or border disputes. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": null}
{"question": "was Ford a member of the House of Representatives?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "for how long was Ford a member of the House of Representatives?", "answer": "Over eight years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over eight years.", "Ford served for over eight years as the Republican Minority Leader of the United States House of Representatives"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "25 years", "extracted_entity": null}
{"question": "was Gerald Ford a member of Delta Kappa Epsilon?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "did ford get an award called \"Congressman's congressman\"?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did ford attend the University of Michigan?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what positions Ford played in the school football team?", "answer": "Center and linebacker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Center and linebacker", "Captain"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "wide receiver, running back, and cornerback.", "extracted_entity": null}
{"question": "what did Ford say about his biological father?", "answer": "He was abusive and had a history of hitting his mother.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was abusive and had a history of hitting his mother.", "his biological father was abusive and had a history of hitting his mother"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "he was a \"mixed-up guy", "extracted_entity": null}
{"question": "who did Ford nominate for Vice President?", "answer": "Bob Dole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bob Dole", "Nelson Rockefeller"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Ford active about Vietnamese affairs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "had Ford's wife married before?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Ford related with the assassination of John F. Kennedy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford", "extracted_entity": null}
{"question": "was Gerald Ford the 38th President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Gerald Ford serve as the Republican Minority Leader of the House of Representatives?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Ford an Eagle Scout?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford was an Eagle Scout.", "extracted_entity": null}
{"question": "what district was Ford elected from?", "answer": "Michigan's 5th congressional district", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michigan's 5th congressional district"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many Eagle Scouts were involved in Ford's funeral procession?", "answer": "400", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["400", "About 400"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "27", "extracted_entity": null}
{"question": "which future Heisman Trophy winner did Ford tackle?", "answer": "Jay Berwanger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jay Berwanger"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has another US President been an Eagle Scout?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Barack Obama", "extracted_entity": null}
{"question": "did Ford need to do extra work to pay for college?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have more than five Presidents lived past the age of 90?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do kwajalein and eniwetok relate?", "answer": "The Monterey supported landings at both locations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monterey supported landings at both locations."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Ford receive on April 13, 1942?", "answer": "Ford received a commission as ensign in the US Naval Reserve.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ford received a commission as ensign in the US Naval Reserve."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "The Presidential Medal of Freedom", "extracted_entity": null}
{"question": "was Ford released from the hospital?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Ford interred at his Presidential Museum in Grand Rapids, Michigan?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Ford one of only four former Presidents to live to 90 or more years of age ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Ford the last surviving member of the Warren Commission ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford", "extracted_entity": null}
{"question": "did Grand Rapids supporters not urge him to take on Bartel J. Jonkman , the incumbent Republican congressman ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did they not claim Ford 's pardon was quid pro quo in exchange for Nixon 's resignation ?", "answer": "They did claim Ford's pardon was quid pro quo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They did claim Ford's pardon was quid pro quo."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Ghana have twice the per capita output of the poorer countries in West Africa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is English Ghana's official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the Dutch build the Elmina Castle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The Dutch built Elmina Castle", "extracted_entity": null}
{"question": "when did Ghana achieve independence from the United Kingdom?", "answer": "1957", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1957"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "6 March 1957", "extracted_entity": null}
{"question": "how many regions is Ghana divided into?", "answer": "10", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "16", "extracted_entity": null}
{"question": "who is the head of state of Ghana?", "answer": "President John Agyekum Kofuor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["President John Agyekum Kofuor", "John Agyekum Kufuor"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "John Mahama", "extracted_entity": null}
{"question": "what is the dominant religion in Ghana?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "christian"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": null}
{"question": "what are the two subfamilies of the Native Ghanaian languages?", "answer": "Kwa and Gur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kwa and Gur"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Akan and Mole-Dagbani", "extracted_entity": null}
{"question": "what is the weather like at Lake Volta?", "answer": "Warm and comparatively dry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Warm and comparatively dry", "warm and comparatively dry"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The weather at Lake Volta is sunny and", "extracted_entity": null}
{"question": "is Ghana in Asia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Ghana is in Africa", "extracted_entity": null}
{"question": "is English the official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "English is the official language of the United States.", "extracted_entity": null}
{"question": "is Ghana's head of state John Agyekum Kufuor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "John Agyekum Kufuor", "extracted_entity": null}
{"question": "what does the word Ghana mean?", "answer": "warrior king", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["warrior king", "Warrior King"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Ghana is a country in West Africa", "extracted_entity": null}
{"question": "who was Kwame Nkrumah?", "answer": "founder and first president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["founder and first president", "founder and first president of the modern Ghanaian state"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is old Ghana in relation to present Ghana?", "answer": "500 miles north", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500 miles north", "500 miles north of the present Ghana, and occupied the area between Rivers Senegal and Niger"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who makes up Ghana's Parliament?", "answer": "the New Patriotic Party and National Democratic Congress", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the New Patriotic Party and National Democratic Congress"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Members of Parliament (MPs)", "extracted_entity": null}
{"question": "what European countries established states in Ghana?", "answer": "The UK", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The UK", "Portugual, France, Britain, Netherlands"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Gray Wolf share an ancestry with the domestic dog?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do wolf pups tend to have darker fur than adults?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, They do tend to have darker furs"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do wolves leave their pack?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "They leave their birth pack when they reach sexual maturity"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "no, they don't.", "extracted_entity": null}
{"question": "what kinds of coats do wolves have?", "answer": "bulky coats with two layers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bulky coats with two layers", "Tough guard hairs that repel water and dirt, and a dense, water-resistant layer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do wolves have scent glands?", "answer": "Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "between a wolf's toes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolves have scent glands in their feet", "extracted_entity": null}
{"question": "how much do wolves weigh?", "answer": "typically varies between 32 and 62 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["typically varies between 32 and 62 kilograms", "between 32 and 68 kilograms "], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves", "extracted_entity": null}
{"question": "why do wolves howl?", "answer": "Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "To pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "because they are hungry", "extracted_entity": null}
{"question": "what is surplus killing?", "answer": "Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "the killing of several prey animals too numerous to eat at one sitting"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Gray Wolves native to North America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No. Current theory suggests that it's from Eurasia"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Gray Wolf", "extracted_entity": null}
{"question": "is the Gray Wolf a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are a wolf's teeth its main weapons?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are wolves built for stamina?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "where is the largest gray wolf population thought to be found?", "answer": "Kazakhstan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kazakhstan"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "North America", "extracted_entity": null}
{"question": "to what genus does the gray wolf belong?", "answer": "Canis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canis"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Canis lupus", "extracted_entity": null}
{"question": "when do wolves molt?", "answer": "Late Spring or Early Summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Late Spring or Early Summer", "late spring or early summer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is it beneficial for alpha males and females to forcefully prevent other wolves from mating?", "answer": "A pack can only suppport one littler per year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A pack can only suppport one littler per year", "The pack tension rises as each mature wolf feels urged to mate"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "It is beneficial for alpha males and females to", "extracted_entity": null}
{"question": "what type of tools do biologists use to capture wolves for tagging?", "answer": "Darting and Foot hold traps.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Darting and Foot hold traps.", "Darting and foot hold traps"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "collar", "extracted_entity": null}
{"question": "forward, erect ears and slightly bristle hackles are a sign of what in wolves?", "answer": "Dominance", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dominance"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "aggression", "extracted_entity": null}
{"question": "was Grover Cleveland born in New York?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "did Grover Cleveland win the 1884 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland won the 1884", "extracted_entity": null}
{"question": "did Grover Cleveland support women's suffrage?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "where was Grover Cleveland married?", "answer": "In the White House", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the White House", "in the Blue Room in the White House"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Cleveland die from?", "answer": "A heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A heart attack", "a heart attack"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years after Cleveland left office did the U.S. win the Spanish-American War?", "answer": "One Year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One Year", "one"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1898", "extracted_entity": null}
{"question": "what did Cleveland's opponents say in 1884 to counter his innocent image?", "answer": "That he had fathered an illegitimate child", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["That he had fathered an illegitimate child", "that he had fathered an illegitimate child while he was a lawyer in Buffalo"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who did Cleveland run against in 1884?", "answer": "former Senator James G. Blaine of Maine", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["former Senator James G. Blaine of Maine", "James G. Blaine"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "why did Cleveland want to hide his cancer surgery from the public?", "answer": "To avoid further market panic", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To avoid further market panic", "because of the financial depression of the country"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Grover Cleveland the twenty-seventh president of the United States?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "is Grover Cleveland honest?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "was Grover Cleveland married in the whitehouse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland married Frances Folsom in the", "extracted_entity": null}
{"question": "when was he elected sheriff of Erire County, New York?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "1870"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "2018", "extracted_entity": null}
{"question": "when did he die?", "answer": "June 24 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 24 1908", "1908 ", "1908"], "entity_annotations": ["Henri_Becquerel", "Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1922", "extracted_entity": null}
{"question": "which election did Grover Cleveland win?", "answer": "1884 and 1892 presidential elections", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1884 and 1892 presidential elections"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1884", "extracted_entity": null}
{"question": "who were Grover Cleveland's parents?", "answer": "Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "Reverend Richard Cleveland and Anne Neal."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many children did Grover Cleveland have?", "answer": "5", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5", "Six."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "who lost control of his party to the agrarians and silverites in 1896?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "William Jennings Bryan", "extracted_entity": null}
{"question": "is it true that he sent in federal troops to chicago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Cleveland born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Cleveland", "extracted_entity": null}
{"question": "was Grover Cleveland elected Sheriff of Erie County, New York?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Cleveland 's portrait on the U.S. $ 1000 bill from 1928 to 1946 ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is he buried in the Princeton Cemetery of the Nassau Presbyterian Church ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes, he is buried in the Princeton", "extracted_entity": null}
{"question": "did the Department of the Interior not charge that the rights of way for this land must be returned to the public because the railroads failed to extend their lines according to agreements ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the team , sedating Cleveland with nitrous oxide -LRB- laughing gas -RRB- , not remove his upper left jaw and portions of his hard palate ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "he became chief engineer in the Department of Bridges and Highways in what year?", "answer": "in 1894", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 1894"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1917", "extracted_entity": null}
{"question": "what led Becquerel to investigate the spontaneous emission of nuclear radiation?", "answer": "phographic plates being fully exposed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["phographic plates being fully exposed"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he married louise d\u00e9sir\u00e9e lorieux in 1890?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he shared the nobel prize in physics?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did he marry louise d\u00e9sir\u00e9e lorieux?", "answer": "1890", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1890"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1730", "extracted_entity": null}
{"question": "was Becquerel elected Permanent Secretary of the Acad\u00e9mie des Sciences?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Becquerel born in Paris into a family which, including he and his son Jean, produced four generations of scientists?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Jean-Baptiste Becquerel", "extracted_entity": null}
{"question": "who won the 1903 Nobel Prize in Physics for discovering radioactivity ?", "answer": "Henri Becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he not die at the age of 55 in Le Croisic ?", "answer": "Yes he died at the age of 55", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he died at the age of 55"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he did not die at the age of 5", "extracted_entity": null}
{"question": "did one places between the phosphorescent substance and the paper a piece of money or a metal screen not pierce with a cut-out design ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1896?", "answer": " Becquerel accidentally discovered radioactivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Becquerel accidentally discovered radioactivity."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1896 was a leap year", "extracted_entity": null}
{"question": "what happened with a bromide emulsion in two sheets of very thick black paper?", "answer": "the plate does not become clouded upon being exposed to the sun for a day", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the plate does not become clouded upon being exposed to the sun for a day"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "The paper is a bromide emulsion in", "extracted_entity": null}
{"question": "is there a becquerel crater on the moon and a becquerel crater on mars?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the Javanese the largest and politically dominant ethnic group in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Javanese", "extracted_entity": null}
{"question": "is Indonesia a monarchy with a presidential system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is a republic with a presidential system", "extracted_entity": null}
{"question": "did the Japanese invasion and subsequent occupation during WWII end Dutch rule?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "in what year did East Timor secede from Indonesia?", "answer": "1999", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1999"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "2002", "extracted_entity": null}
{"question": "when did Islam become the dominant religion in Java and Sumatra?", "answer": "the end of the 16th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the end of the 16th century", "by the end of the 16th century"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "15th century", "extracted_entity": null}
{"question": "does Indonesia have the world's hightest level of biodiversity?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia has the world's hightest", "extracted_entity": null}
{"question": "was Indonesia named after it became an independent country?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did Indonesia break off relations with the People's Republic of China?", "answer": "because of anti-communist purges early in the Suharto era", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["because of anti-communist purges early in the Suharto era", "anti-communist purges early in the Suharto era"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which spice originally attracted Europeans to Indonesia?", "answer": "nutmeg, cloves, or cubeb pepper", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nutmeg, cloves, or cubeb pepper", "nutmeg, cloves, and cubeb pepper"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "pepper.", "extracted_entity": null}
{"question": "what is Jakarta?", "answer": "Indonesia's special capital region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia's special capital region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "what shares land borders with Papua New Guinea, East Timor and Malaysia?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "what are the largest and politically dominant ethnic group?", "answer": "the Javanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Javanese"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happens when the srivijaya kingdom formed trade links?", "answer": "The indonesian archipelago became an important trade region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The indonesian archipelago became an important trade region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that indonesia has vast areas of wilderness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Wilderness is defined as an area that is", "extracted_entity": null}
{"question": "is Indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is a republic", "extracted_entity": null}
{"question": "are Sports in Indonesia generally male-orientated?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Football", "extracted_entity": null}
{"question": "is Indonesia the world's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is the world's largest archip", "extracted_entity": null}
{"question": "are sports in Indonesia generally male-orientated and spectator sports are often associated with illegal gambling ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Football", "extracted_entity": null}
{"question": "is Pencak Silat an Indonesian martial art ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has Indonesia the world 's largest Muslim population ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia has the world's largest Muslim population", "extracted_entity": null}
{"question": "who has its own political legislature and governor ?", "answer": "each province", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["each province"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "[Minnesota](https://en.wikipedia", "extracted_entity": null}
{"question": "is it the world 's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "The Philippines", "extracted_entity": null}
{"question": "is it the world 's fourth most populous country and the most populous muslim-majority nation?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "India", "extracted_entity": null}
{"question": "is indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is a republic.", "extracted_entity": null}
{"question": "is James Monrow the fifth president of US?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monrow is the fifth president of US.", "extracted_entity": null}
{"question": "what is the first word on the page?", "answer": "James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James", "James_Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": null}
{"question": "what is the first number on the page?", "answer": "28", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "when did Monroe's presidency expired?", "answer": "March 4, 1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1825", "March 4, 1825."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1825", "extracted_entity": null}
{"question": "what is Monroe's father's name?", "answer": "Spence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spence", "Spence Monroe."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Monroe's daughter's name?", "answer": "Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does \"Era of Good Feelings\" refers to?", "answer": "Reduced tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Reduced tension", "Monroe allowed his political base to decay, which reduced tensions and led to the naming of his era as the \"Era of Good Feelings\"."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Era of Good Feelings was the period", "extracted_entity": null}
{"question": "did Monroe' wedding happen at the Trinity Church in New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what is Monroe Doctrine?", "answer": "A doctrine declaring U.S. opposition to European interference in the Americas.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A doctrine declaring U.S. opposition to European interference in the Americas.", "In it, Monroe proclaimed the Americas should be free from future European colonization and free from European interference in sovereign countries' affairs. It further stated the United States' intention to stay neutral in European wars and wars between European powers and their colonies, but to consider any new colonies or interference with independent countries in the Americas as hostile acts toward the United States. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe Doctrine", "extracted_entity": null}
{"question": "was James Monroe President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe", "extracted_entity": null}
{"question": "did James Monroe attend the College of William and Mary?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did James Monroe fight in the Continental Army?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe did fight in the Continental Army", "extracted_entity": null}
{"question": "where was James Monroe born?", "answer": "Westmoreland County, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westmoreland County, Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe was born in Monroe Hall, Virginia", "extracted_entity": null}
{"question": "where was James Monroe shot?", "answer": "at the Battle of Trenton, in his left shoulder", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the Battle of Trenton, in his left shoulder", "The Battle of Trenton"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Battle of Monroe's Farm", "extracted_entity": null}
{"question": "who did James Monroe marry?", "answer": "Elizabeth Kortright", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Elizabeth Kortright"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Elizabeth Monroe", "extracted_entity": null}
{"question": "what did James Monroe do before being elected governor?", "answer": "He practiced law in Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He practiced law in Virginia.", "He practiced law"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "he was a lawyer", "extracted_entity": null}
{"question": "in which years were John Monroe elected as President?", "answer": "1817-1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1817-1825"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1817, 182", "extracted_entity": null}
{"question": "who was John Monroe standing behind in the painting of  Washington Crossing the Delaware?", "answer": "George Washington", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["George Washington"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "John Monroe was standing behind George Washington.", "extracted_entity": null}
{"question": "when was James Monroe appointed to Secretary of War?", "answer": "1814", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1814"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1811", "extracted_entity": null}
{"question": "when did James Monroe die?", "answer": "July 4, 1831", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4, 1831"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1831", "extracted_entity": null}
{"question": "when did James Monroe graduate from William and Mary?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": null}
{"question": "when was James Monroe elected president?", "answer": "Monroe was elected president in the election of 1816, and re-elected in 1820.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president in the election of 1816, and re-elected in 1820."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1816", "extracted_entity": null}
{"question": "which property did James Monroe sell in 1817?", "answer": "Monroe Hill on the grounds of the University of Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe Hill on the grounds of the University of Virginia."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did James Monroe introduce the Monroe Doctrine?", "answer": "December 2, 1823", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 2, 1823"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1823", "extracted_entity": null}
{"question": "who did James Monroe live with in New York City?", "answer": "His daughter Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His daughter Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe lived with his wife, Elizabeth Mon", "extracted_entity": null}
{"question": "what did James Monroe's letters not contain?", "answer": "No letters survive in which he might have discussed his religious beliefs.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No letters survive in which he might have discussed his religious beliefs."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the letters did not contain the name of the sp", "extracted_entity": null}
{"question": "what was the result of the rejection of the Jay Treaty?", "answer": "As a result, the two nations moved closer toward the War of 1812. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As a result, the two nations moved closer toward the War of 1812. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The result of the rejection of the Jay Tre", "extracted_entity": null}
{"question": "who is depicted holding the flag in the famous painting of Washington Crossing the Delaware?", "answer": "Monroe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "George Washington", "extracted_entity": null}
{"question": "what dwindled and eventually died out, starting with the Hartford Convention?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Federalist Party", "extracted_entity": null}
{"question": "is it true that he practiced law in fredericksburg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did James Monroe make in 1817?", "answer": "two long tours", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two long tours"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Secretary of State", "extracted_entity": null}
{"question": "was Monroe anticlerical?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Monroe appointed Minister to France from 1794 to 1796?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson", "extracted_entity": null}
{"question": "what expired on March?", "answer": "Monroe's presidency", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe's presidency"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The 11th", "extracted_entity": null}
{"question": "had Monroe racked up many debts during his years of public life ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe had racked up many debts", "extracted_entity": null}
{"question": "who dismantled partisan and sectional coalitions ?", "answer": "many congressmen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["many congressmen"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "FDR", "extracted_entity": null}
{"question": "what happened in 1811?", "answer": "Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Battle of Tippecanoe", "extracted_entity": null}
{"question": "what happened in the election of 1816?", "answer": "Monroe was elected president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe was elected president.", "extracted_entity": null}
{"question": "who died in childbirth in 1772?", "answer": "Margaret Miller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who died in 1832?", "answer": "Margaret Miller.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is watt commemorated by statuary in George Square, Glasgow and Princes Street, Edinburgh?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was watt a gentleman, greatly respected by other prominent men of the Industrial Revolution?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was watt a fellow of the Royal Society of Edinburgh and the Royal Society of London?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "four years after opening his shop , Watt began what?", "answer": "Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "selling", "extracted_entity": null}
{"question": "was Watt ranked 22nd in Michael H. Hart 's list of the most influential figures in history ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Watt", "extracted_entity": null}
{"question": "did John Adams represent the Continental Congress in Europe?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Adams raised Congregationalist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Adams an opponent of the Stamp Act?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "when did Adams graduate from college?", "answer": "1755.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1755.", "1755"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1976", "extracted_entity": null}
{"question": "who was on the committee with Adams to draft  a Declaration of Independence?", "answer": "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Jefferson, Franklin, Adams, and Sherman", "extracted_entity": null}
{"question": "what did Jefferson call John Adams?", "answer": "The \"Colossus of Independence\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The \"Colossus of Independence\".", "Colossus of Independence"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams was a Founding Father of the United", "extracted_entity": null}
{"question": "what was Adams' political party?", "answer": "The Federalist Party.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party.", "Federalist"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "National Republican Party", "extracted_entity": null}
{"question": "was Adams the first to introduce a bicameral legislature?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did John Adams get along with Alexander Hamilton?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams did not get along with Alexander Hamilton.", "extracted_entity": null}
{"question": "did John Adams go to Harvard? ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "did John Adams support the Stamp Act of 1765?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No, he opposed it.", "extracted_entity": null}
{"question": "is Adams' birthplace part of a national park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams' birthplace is part of the National Park", "extracted_entity": null}
{"question": "when did John Adams serve as Vice President?", "answer": "1789-1797", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1789-1797"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "with what party did Adams run for presidency?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Democratic-Republican", "extracted_entity": null}
{"question": "where is Adams buried?", "answer": "United First Parish Church", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["United First Parish Church"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who were the midnight judges?", "answer": "They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "They were judges formally appointed days before Adams term expired"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The midnight judges were John Adams, John", "extracted_entity": null}
{"question": "in what ways was Adams opposed by Anderw Hamilton?", "answer": "Hamilton wanted to control the army differently than Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hamilton wanted to control the army differently than Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "in what ways was Adams opposed by Anderw", "extracted_entity": null}
{"question": "what information did he record in his diary?", "answer": "Descriptions of events and ompressions of men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Descriptions of events and ompressions of men", "He wrote descriptions of events and impressions of men"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "He recorded the date and time of the events,", "extracted_entity": null}
{"question": "who was defeated for re-election in the`` Revolution of 1800'' by Thomas Jefferson?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "who represented the Continental Congress in Europe?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "what is now part of Adams National Historical Park?", "answer": "John Adams' birthplace", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams' birthplace"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The Adams National Historical Park", "extracted_entity": null}
{"question": "is it true that adams had spent some time as the ambassador?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it true that massachusetts sent him in 1774?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "who did Massachusetts send in 1774?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1774", "extracted_entity": null}
{"question": "are his last words often quoted as \" Thomas Jefferson survives \" . ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "the John Adams Library , housed at the Boston Public Library , contains what?", "answer": "Adams's personal collection of more than 3,500 volumes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams's personal collection of more than 3,500 volumes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "A collection of books.", "extracted_entity": null}
{"question": "adams ' opponents were what?", "answer": "Democratic Republicans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Democratic Republicans"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Jackson's opponents", "extracted_entity": null}
{"question": "did the election of 1800 not become a bitter and volatile battle , with each side expressing extraordinary fear of the other party and its policies ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1764?", "answer": "Adams married Abigail Smith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams married Abigail Smith"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the first recorded case of the Black Death", "extracted_entity": null}
{"question": "is a kangaroo a reptile?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a kangaroo on the Australian coat of arms?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was James Cook the first to record the name \"Kangooroo?\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is a collective noun for kangaroos?", "answer": "mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mob, troop, or court", "Mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "mob", "extracted_entity": null}
{"question": "where do joeys complete postnatal development?", "answer": "marsupium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["marsupium", "A pouch called a marsupium"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "in the pouch", "extracted_entity": null}
{"question": "what do kangaroos use for \"crawl-walking?\"?", "answer": "its hind feet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["its hind feet", "Their hind feet and their tails"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "why do kangaroos have a wide bite?", "answer": "The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "Because of grazing"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroos have a wide bite because", "extracted_entity": null}
{"question": "what is responsible for converting the hydrogen byproduct of fermentation into acetate?", "answer": "bacteria", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bacteria", "The digestive system of a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "acetate kinase", "extracted_entity": null}
{"question": "are wild kangaroos shot for meat?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroo", "extracted_entity": null}
{"question": "have kangaroos fared well since European settlement?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do kangaroos have many natural predators?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos are preyed upon by d", "extracted_entity": null}
{"question": "what is the average life expectancy of a kangaroo?", "answer": "4.6 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["4.6 years", "about 4.6 years", "4 to 6 years"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "12 years", "extracted_entity": null}
{"question": "when did the first official report of kangaroo blindness take place?", "answer": "1994", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1994"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "1970", "extracted_entity": null}
{"question": "what are vehicles that frequent isolated roads often fitted with?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do kangaroos eat plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are some extinct predators of the kangaroo?", "answer": "Thylacine, Marsupial Lion, Maagalania, and Wonambi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thylacine, Marsupial Lion, Maagalania, and Wonambi", "The Thylacine"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Dingo", "extracted_entity": null}
{"question": "is a Kangaroo a Marsupial?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the kangaroo an herbivour?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Kangaroos Shy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what method of locomotion do Kangaroos Use?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "what is a collective noun for a kangaroo?", "answer": "a mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A mob of kangaroos.", "extracted_entity": null}
{"question": "what is a roo?", "answer": "a kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A roo is a marsupial from the", "extracted_entity": null}
{"question": "what is used to protect a vehicale from a Kangaroo?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what method is used by Kangaroos to travel?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "who asked a nearby local what the creatures were called?", "answer": "Captain James Cook and naturalist Sir Joseph Banks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Captain James Cook and naturalist Sir Joseph Banks"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are four species that are commonly referred to as kangaroos?", "answer": "The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Red Kangaroo, Eastern Grey Kang", "extracted_entity": null}
{"question": "are kangaroos and wallabies adept swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "wallabies.", "extracted_entity": null}
{"question": "are kangaroos farmed to any extent?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos are not farmed, however", "extracted_entity": null}
{"question": "who also discovered that less than three percent of kangaroos exposed to the virus developed blindness ?", "answer": "Veterinarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Veterinarians"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos", "extracted_entity": null}
{"question": "different species of kangaroos eat what?", "answer": "different diets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different diets"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a collision with a vehicle capable of killing a kangaroo ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was the game of Marn grook played using a ball made from kangaroo by the Kurnai people ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "have kangaroos large , powerful hind legs , large feet adapted for leaping , a long muscular tail for balance , and a small head ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a kangaroo a marsupial from the family macropodidae -LRB- macropods , meaning (`` ` large foot ('' ' -RRB-?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Macropus", "extracted_entity": null}
{"question": "what kinds of changes have larger kangaroos adapted much better to?", "answer": "Changes wrought to the Australian landscape by humans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Changes wrought to the Australian landscape by humans."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "adaptive changes", "extracted_entity": null}
{"question": "have larger kangaroos adapted much better to changes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the leopard smaller than the other members of Panthera?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The leopard is the smallest of the four", "extracted_entity": null}
{"question": "is a leopard larger and less lanky than a cheetah?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are large pythons potential prey for leopards?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what may a leopard be mistaken for?", "answer": "A cheetah or a jaguar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cheetah or a jaguar"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a jaguar", "extracted_entity": null}
{"question": "what is a hybrid animal resulting from a union between a leopard and a puma?", "answer": "a pumapard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pumapard", "A pumapard"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a leopon", "extracted_entity": null}
{"question": "where do leopards often hide their kills?", "answer": "in dense vegetation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in dense vegetation", "in dense vegetation or in trees"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how did early naturalists distinguish between leopards and panthers?", "answer": "by the length of their tails", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by the length of their tails", "By the length of the tail, panthers having longer tails than leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are dark leopards known as colloquially?", "answer": "Black Panthers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Black Panthers", "black panthers"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "black panthers", "extracted_entity": null}
{"question": "what shape are a leopard's black rosettes in East Africa?", "answer": "circular", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["circular"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Round", "extracted_entity": null}
{"question": "is the leopard an Old World mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is the leopard of the Felidae family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The leopard is a member of the Fel", "extracted_entity": null}
{"question": "is the leopard one of the four 'big cats'?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "where is the leopard distributed?", "answer": "southern Eurasia and Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["southern Eurasia and Africa", "southern Eurasia and Africa, from Korea to South Africa and Spain"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how long is a leopard's tail?", "answer": "60 to 110cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 to 110cm", "60-110cm"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how much does a leopard weigh?", "answer": "Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "(because of ambiguity) - 28-90kg "], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a male leopard weigh more than a female leopard?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the difference between leopards and cheetahs?", "answer": "The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "The leopard has rosettes rather than cheetah's simple spots, but they lack internal spots, unlike the jaguar. The leopard is larger and less lanky than the cheetah but smaller than the jaguar."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does the distribution size of the leopard compare to the distribution of other wild cats?", "answer": "As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "the leopard had the largest distribution of any wild cat."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard has a larger distribution than any", "extracted_entity": null}
{"question": "what resembles that of the similarly-sized cougar in the Americas?", "answer": "The leopard's ecological role", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard's ecological role"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Puma concolor", "extracted_entity": null}
{"question": "what was one of the many species described in Linnaeus's 18th-century work, Systema Naturae?", "answer": "Felis pardus", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felis pardus"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what sort of cats are solitary?", "answer": "Leopards", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Lions", "extracted_entity": null}
{"question": "are leopards circular?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the leopard solitary?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard is a solitary animal", "extracted_entity": null}
{"question": "what centred in Sierra?", "answer": "The leopard men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard men"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Sierra", "extracted_entity": null}
{"question": "felis pardus was what?", "answer": "One of the many species described in Linnaeus's 18th-century work, Systema Naturae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of the many species described in Linnaeus's 18th-century work, Systema Naturae"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has the New Zealand Rugby League featured the Otahuhu Leopards and then the Tamaki Leopards ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were the Leopard men a West African secret society who practised cannibalism ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the leopard tank a German designed tank which entered service in 1965 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopard 1", "extracted_entity": null}
{"question": "is the black color heritable and caused by only one recessive gene locus ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the leopard -LRB- panthera pardus -RRB- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can a panther be any of several species of large felid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the leopard an agile and graceful predator?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Liechtenstein bordered by Switzerland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is bordered by Switzerland", "extracted_entity": null}
{"question": "is Liechtenstein the smallest German-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "was Liechtenstein part of the ancient Roman province of Raetia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "when was the first factory opened?", "answer": "1836", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1836"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1771", "extracted_entity": null}
{"question": "how many municipalities is Liechtenstein divided into?", "answer": "11", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "11", "extracted_entity": null}
{"question": "what is the national currency of Liechtenstein?", "answer": "Swiss franc", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swiss franc"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Swiss Franc", "extracted_entity": null}
{"question": "has Leichtenstein worked to promote the county's image by prosecuting international money-laundering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what company administers Leichtenstein's railways?", "answer": "Austrian Federal Railways", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austrian Federal Railways"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Deutsche Bahn", "extracted_entity": null}
{"question": "what percentage of Liechtenstein's population is foreign-born?", "answer": "two-thirds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two-thirds", "67%"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "27.4%", "extracted_entity": null}
{"question": "is there an airport in Liechtenstein?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein Airport (LXK)", "extracted_entity": null}
{"question": "is Liechtenstein heavily urbanized?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Liechtenstein doubly landlocked?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is doubly landlocked.", "extracted_entity": null}
{"question": "does Liechtenstein have an army?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when was Liechtenstein's current constitution adopted?", "answer": "October 1921.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["October 1921.", "October 1921"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1921", "extracted_entity": null}
{"question": "what is the official language of Liechtenstein?", "answer": "German.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German.", "German"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "German", "extracted_entity": null}
{"question": "what countries border Liechtenstein?", "answer": "Switzerland and Austria.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Switzerland and Austria.", "Switzerland and Austria"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Austria, Switzerland, Germany", "extracted_entity": null}
{"question": "what Roman province was Liechtenstein part of?", "answer": "Raetia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Raetia.", "Raetia"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Raetia", "extracted_entity": null}
{"question": "how many municipalities are within Oberland?", "answer": "6.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.", "6"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the smallest German-speaking country in the world?", "answer": "Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "the Savings and Loans Bank was founded, as was the first cotton-weaving mill in what year?", "answer": "1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1861"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1831", "extracted_entity": null}
{"question": "what enjoys one of the world's highest standards of living?", "answer": "Liechtenstein's population", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein's population"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The United States", "extracted_entity": null}
{"question": "does the state court rule on the conformity of laws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the court does not rule on the conformity of", "extracted_entity": null}
{"question": "what do most recognizable international company and largest employer have in common?", "answer": "They are Hilti.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are Hilti."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "they are all from the same country.", "extracted_entity": null}
{"question": "was Liechtenstein completed in November 2000?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "what completed in November?", "answer": "the Kunstmuseum Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Kunstmuseum Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The book was published in November 196", "extracted_entity": null}
{"question": "the State Court rules what?", "answer": "on the conformity of laws with the constitution", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["on the conformity of laws with the constitution"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The State Court rules on the case.", "extracted_entity": null}
{"question": "the Historical Society of the Principality of Liechtenstein plays what?", "answer": "a role in preserving the culture and history of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a role in preserving the culture and history of the country."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The Museum of the Principality of Liechten", "extracted_entity": null}
{"question": "is the museum collection also the national art collection of Liechtenstein ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a small heliport at Balzers in Liechtenstein available for charter helicopter flights ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Heliport Balzers", "extracted_entity": null}
{"question": "are nationals referred to by the plural : Liechtensteiners ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtensteinians", "extracted_entity": null}
{"question": "is it a winter sports resort , although it is perhaps best known as a tax haven ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it a winter sports resort?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "It is a winter sports resort.", "extracted_entity": null}
{"question": "is it the smallest german-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "what happened in moravia , lower austria , silesia , and styria , though in all cases , these territories were held in fief under other more senior feudal lords?", "answer": "The Liechtenstein Dynasty acquired vast swaths of land", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Liechtenstein Dynasty acquired vast swaths of land"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Webster write, \"I can now sleep of nights\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the arrival of Louis Kossuth (exiled leader of a failed Hungarian revolution) another issue that presented itself during Fillmore's presidency?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, the arrival of Louis Kossuth was", "extracted_entity": null}
{"question": "did he die at 11:10 p.m. on March 8, 1874, of the after-effects of a stroke?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who got Seward elected to the senate?", "answer": "Weed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Weed", "Weed ultimately got Seward elected to the senate"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Thurlow Weed", "extracted_entity": null}
{"question": "where was Commodore Matthew C. Perry sent to open Japan to Western trade?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "Commodore Matthew C. Perry send to Japan"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is The Remarkable Millard Fillmore?", "answer": "A fake biography based on real events that happened in Fillmore's life.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A fake biography based on real events that happened in Fillmore's life.", "In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events that happened in Fillmore's life"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The Remarkable Millard Fillmore is", "extracted_entity": null}
{"question": "did Fillmore form a law partnership before or after he founded the private University of Buffalo?", "answer": "Before.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Before.", "Law partnership founded before University of Buffalo"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore formed the law partnership before he", "extracted_entity": null}
{"question": "did Fillmore run for President a second time?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "I don't know (semantic ambiguity)"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore", "extracted_entity": null}
{"question": "how long after the death of his first wife did Fillmore marry Caroline McIntosh?", "answer": "where is the death date of his first wife?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where is the death date of his first wife?"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "1858", "extracted_entity": null}
{"question": "was Millard Fillmore the thirteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Millard Fillmore born on January 7, 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Millard Fillmore die on March 8, 1974?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore", "extracted_entity": null}
{"question": "where was Millard Fillmore born?", "answer": "in a log cabin in Summerhill, New York", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in a log cabin in Summerhill, New York"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Buffalo, New York", "extracted_entity": null}
{"question": "to whom was Millard Fillmore born to?", "answer": "to Nathaniel and Phoebe Millard Fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to Nathaniel and Phoebe Millard Fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore was born to Nathaniel", "extracted_entity": null}
{"question": "who did Millard Fillmore fall in love with?", "answer": "He fell in love with Abigail Powers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He fell in love with Abigail Powers", "Abigail Powers"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Taylor and Fillmore disagree upon?", "answer": "slavory issues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slavory issues", "Taylor and Fillmore disagreed on the slavery issue in the new western territories taken from Mexico in the Mexican-American War"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Taylor was a Democrat and Fillmore was", "extracted_entity": null}
{"question": "how did Fillmore ascend to the presidency?", "answer": "upon the death of the sitting president, Taylor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon the death of the sitting president, Taylor", "Official White House portrait of Millard FillmoreFillmore ascended to the presidency upon the sudden and unexpected death of President Taylor in July 1850."], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The 1852 Democratic National Convention,", "extracted_entity": null}
{"question": "how did the supporters of Henry Clay feel about Fillmore in 1848?", "answer": "they were ok with him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they were ok with him", "angry"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "they were disappointed in him", "extracted_entity": null}
{"question": "who was first apprenticed to a fuller to learn the cloth-making trade?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "John Smith", "extracted_entity": null}
{"question": "he founded the private university of buffalo on what date?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "1846", "extracted_entity": null}
{"question": "who or what fell in love with abigail powers?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Fillmore one of the founders of the University of Buffalo?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Another primary objective of Fillmore to preserve the Union from the intensifying slavery debate?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Fillmore the second Chancellor, a position he maintained while both Vice President and President?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore was the second Vice President to assume", "extracted_entity": null}
{"question": "is Fillmore the first of two presidents to have been an indentured servant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Andrew Johnson", "extracted_entity": null}
{"question": "is the comic strip Mallard Fillmore named after the president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Fillmore the first U.S. President born after the death of a former president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did Fillmore not turn down the honor , explaining that he had neither the `` literary nor scientific attainment '' to justify the degree ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in a log cabin in summerhill , new york?", "answer": "Fillmore was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fillmore was born"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "A man died in a log cabin in Summerhill", "extracted_entity": null}
{"question": "who was born precisely at midnight during an electrical storm , to a Serbian family in the village of Smiljan near Gospi\u0107 , in the Lika region of the Croatian Krajina in Military Frontier ( part of the Austrian Empire ) , in the present-day Croatia . ?", "answer": "Nikola Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla", "extracted_entity": null}
{"question": "are there at least two films describing Tesla 's life ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "_The Prestige_", "extracted_entity": null}
{"question": "have a number of live theatrical plays based on Tesla 's life been produced and staged worldwide ?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "The answer is: _______________", "extracted_entity": null}
{"question": "do sea otters have long muscular tails?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "can otters survive in cold water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does the giant otter inhabit South Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many species of otter are there?", "answer": "13", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "There are 13 species of otter.", "extracted_entity": null}
{"question": "what do river otters eat?", "answer": "a variety of fish and shellfish, as well as small land mammals and birds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a variety of fish and shellfish, as well as small land mammals and birds", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds. ", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish, crayfish, and frogs", "extracted_entity": null}
{"question": "how much do sea otters weigh?", "answer": "30 kg (about 65 pounds)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["30 kg (about 65 pounds)"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do sea otters insulate themselves?", "answer": "a layer of air trapped in their fur(!)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a layer of air trapped in their fur(!)", "As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why are sea otters nearly extinct?", "answer": "They've been hunted for their fur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They've been hunted for their fur", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is one of the challenges of re-establishing a population of Eurasian otters in the UK?", "answer": "roadkill deaths", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roadkill deaths", "Roadkill deaths have become one of the significant threats to the success of their re-establishment. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "habitat loss", "extracted_entity": null}
{"question": "is otter a kind of mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many species and genera does otter have?", "answer": "13 species and 7 genera", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13 species and 7 genera", "13 species in 7 genera."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "14 species and 13 genera", "extracted_entity": null}
{"question": "do otters live in water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes, otters live in water.", "extracted_entity": null}
{"question": "where do sea otters live?", "answer": "Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "The sea otter lives actually in the sea. ", "Beside water", "Pacific coast of North America"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "sea otters live in the Pacific Ocean.", "extracted_entity": null}
{"question": "where does the word \"otter\" derive from?", "answer": "The word \"otter\" derives from the Old English word otr, otor or oter. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The word \"otter\" derives from the Old English word otr, otor or oter. ", "The word \"otter\" derives from the Old English word otr, otor or oter. This and cognate words in other Indo-European languages ultimately stem from a root which also gave rise to the English words \"water\", \"wet\" and \"winter\". "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does otter give birth or lay egg?", "answer": "give birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["give birth"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otter give birth", "extracted_entity": null}
{"question": "what drives sea otter almost to extinction?", "answer": "Humans hunted them almost to extinction. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Humans hunted them almost to extinction. ", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them almost to extinction. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why otters are considered as totem animals?", "answer": "The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are considered to be totem animals because", "extracted_entity": null}
{"question": "do sea otters live along the Pacific coast?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea otters live along the Pacific coast", "extracted_entity": null}
{"question": "are otters totem animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are otters herbivores?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are carnivores.", "extracted_entity": null}
{"question": "what is the primary item in an otter's diet?", "answer": "Fish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fish.", "fish"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Fish", "extracted_entity": null}
{"question": "what is an otter's den called?", "answer": "Holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holt", "holt", "A holt"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "A den is a shelter for animals, and an", "extracted_entity": null}
{"question": "why is the giant otter becoming increasingly rare?", "answer": "Poaching, habitat loss, and toxins in gold mining.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Poaching, habitat loss, and toxins in gold mining.  ", "poaching, habitat loss, and the use of mercury in illegal alluvial gold mining"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Giant otters are becoming increasingly rare due", "extracted_entity": null}
{"question": "how do otters keep themselves warm without blubber?", "answer": "A layer of air trapped in their fur.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A layer of air trapped in their fur.  ", "a layer of air trapped in their fur"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they have a thick fur coat that keeps them warm", "extracted_entity": null}
{"question": "how are otters playful animals?", "answer": "The slide down snowy slopes, apparently for sheer enjoyment.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The slide down snowy slopes, apparently for sheer enjoyment.  ", "They slide repeatedly down snowy slopes for sheer enjoyment."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are playful animals.", "extracted_entity": null}
{"question": "what animals are related to otters?", "answer": "weasels, polecats, and badgers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["weasels, polecats, and badgers"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "The otter is a member of the weas", "extracted_entity": null}
{"question": "what traps a layer of air, and keeps them dry and warm under water?", "answer": "Long guard hair", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Long guard hair"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "A wet suit", "extracted_entity": null}
{"question": "the collective noun romp is sometimes used for a group of what?", "answer": "Otters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Otters"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that otters eat a variety of fish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are otters playful animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are playful animals.", "extracted_entity": null}
{"question": "are otters very active?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are very active", "extracted_entity": null}
{"question": "are male otters dog-otters?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "an otter 's den is what?", "answer": "A holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A holt"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "a den", "extracted_entity": null}
{"question": "sea otters eat what?", "answer": "Shellfish and other invertebrates", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shellfish and other invertebrates"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea otters eat sea urchins", "extracted_entity": null}
{"question": "is the myth of Otter 's Ransom the starting point of the Volsunga saga ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Male otters dog-otters , females are bitches and babies are cubs or pups ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "bitches, pups", "extracted_entity": null}
{"question": "is an otter 's den called a holt ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "A holt is a den of an otter", "extracted_entity": null}
{"question": "have most otters fish as the primary item in their diet , supplemented by frogs , crayfish and crabs ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do penguins feed on krill?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what is the largest living species of penguin?", "answer": "Emperor Penguin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Emperor Penguin", "the Emperor Penguin (Aptenodytes forsteri)", "Emperor"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "emperor penguin", "extracted_entity": null}
{"question": "do penguins live almost exclusively in the Southern Hemisphere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are birds that live almost exclusively", "extracted_entity": null}
{"question": "what is \"tobogganing\"?", "answer": "when penguins slide on their bellies across the snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when penguins slide on their bellies across the snow", "Tobogganing is when penguins slide on their bellies across the snow."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "\"Tobogganing\" is the act", "extracted_entity": null}
{"question": "why are penguins countershaded?", "answer": "for camouflage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["for camouflage"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are penguins afraid of humans?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguins are not afraid of humans", "extracted_entity": null}
{"question": "how much time to penguins spend on land?", "answer": "half of their life", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["half of their life", "They spend half of their life on land."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "12 hours", "extracted_entity": null}
{"question": "how many species of penguins are there?", "answer": "between 17 and 20 living species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["between 17 and 20 living species", "Penguin biodiversity varies between 17 and 20 living species, all in the subfamily Spheniscinae."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "18", "extracted_entity": null}
{"question": "how are Isabelline penguins different from most penguins?", "answer": "they have brown rather than black plumage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they have brown rather than black plumage", "Because they are born with brown rather than black plumage."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they have a yellow patch on their cheek.", "extracted_entity": null}
{"question": "are penguins birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, penguins are birds.", "extracted_entity": null}
{"question": "do penguins have a better than average sense of hearing for birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are penguins considered \"higher waterbirds\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what order do penguins belong to?", "answer": "Sphenisciformes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sphenisciformes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Sphenisciformes", "extracted_entity": null}
{"question": "how tall were the tallest prehistoric penguins?", "answer": "1.80 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.80 meters", "as tall as an adult human"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what makes penguins so agile in the water?", "answer": "Their wings have become flippers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their wings have become flippers", "Smooth plumage preserves a layer of air, ensuring buoyancy, wings are flippers"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why do penguins \"tobaggan\"?", "answer": "It conserves energy while moving quickly", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It conserves energy while moving quickly", "Conserves energy while moving quickly"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "\"To baggon\" is a slang", "extracted_entity": null}
{"question": "who spend half of their life on land and half in the oceans?", "answer": "Penguins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is also the distance that Antarctic tourists are told to keep from penguins?", "answer": "3 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 meters"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "500m", "extracted_entity": null}
{"question": "what have become flippers, useless for flight in the air?", "answer": "Penguins' wings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins' wings"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "wings", "extracted_entity": null}
{"question": "what happens when mothers lose a chick?", "answer": "They sometimes attempt to \"steal\" another chick.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes attempt to \"steal\" another chick."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that each penguin gets a turn in the center?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "the penguins have a rotation schedule.", "extracted_entity": null}
{"question": "are all penguins countershaded?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, all penguins are countershaded", "extracted_entity": null}
{"question": "are penguins astonishingly agile?", "answer": "In the water they are.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the water they are."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are astonishingly agile.", "extracted_entity": null}
{"question": "are penguins at risk?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it also possible that penguin comes from the Latin pinguis , fat ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have penguins an average sense of hearing for birds ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguins have an average sense of hearing for", "extracted_entity": null}
{"question": "can larger penguins dive deep in case of need ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the smallest penguin species the Little Blue Penguin -LRB- also known as the Fairy Penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2.2 lb -RRB- ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Little Blue Penguin", "extracted_entity": null}
{"question": "is it not even known whether the gigantic palaeeudyptines constitute a monophyletic lineage , or whether gigantism was evolved independently in a much restricted Palaeeudyptinae and the Anthropornithinae ?", "answer": "It is not even known.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not even known."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Anthropornithinae", "extracted_entity": null}
{"question": "is the largest living species the emperor penguin -LRB- aptenodytes forsteri -RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "emperor penguin", "extracted_entity": null}
{"question": "is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Little Penguin", "extracted_entity": null}
{"question": "what happened in a region not quite 2000 km south of the equator 35 mya?", "answer": "At least one giant penguin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At least one giant penguin."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the last word on the page?", "answer": "Connecticut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Connecticut", "(SSN-22)"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is polar bear a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is polar bear a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, polar bear is a carnivore", "extracted_entity": null}
{"question": "what is largest polar bear on record?", "answer": "2200 lb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2200 lb", "A huge male, allegedly weighing 1002 kg (2200 lb) shot at Kotzebue Sound in northwestern Alaska in 1960."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is polar bear's skin color?", "answer": "white or cream", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["white or cream", "Black"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "White", "extracted_entity": null}
{"question": "how long is polar bear's guard hair?", "answer": "5-15 cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5-15 cm"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The guard hair is 10 cm long.", "extracted_entity": null}
{"question": "what is cause of polar bear's skin diseases?", "answer": "mites or other parasites", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mites or other parasites", "Mites or other parasites"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "skin diseases caused by bacterial and fung", "extracted_entity": null}
{"question": "why polar bear is a special species?", "answer": "The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bear is a special species because it is", "extracted_entity": null}
{"question": "what do fossil and DNA evidence tell us?", "answer": "The polar bear diverged from the brown bear about 200 thousand years ago.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The polar bear diverged from the brown bear about 200 thousand years ago.", "The polar bear diverged from the brown bear roughly 200 thousand years ago."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The fossil record shows that modern humans evolved", "extracted_entity": null}
{"question": "does a polar bear live in the Arctic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, a polar bear lives in the Arctic", "extracted_entity": null}
{"question": "is a polar bear white in color?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The polar bear is white in color.", "extracted_entity": null}
{"question": "is a polar bear at high risk of extinction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bear", "extracted_entity": null}
{"question": "how heavy is a male polar bear?", "answer": "300-600 kg (660-1320 lb)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300-600 kg (660-1320 lb)", "Most adult males weigh 350-650 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how heavy was the largest polar bear on record?", "answer": "1002 kg (2200 lb) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1002 kg (2200 lb) ", "He allegedly weighed 1002 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does a polar bear's fur provide?", "answer": "It provides the animal with effective camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It provides the animal with effective camouflage.", "A polar bear's fur provides camouflage and insulation"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do female polar bears weight more than the male?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "female polar bear weigh more than the male.", "extracted_entity": null}
{"question": "how much weight do female polar bears gain during pregnancy?", "answer": "They gain double their weight.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They gain double their weight.", "They double their weight during pregnancy"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "100-150 kg", "extracted_entity": null}
{"question": "can polar bears be seen under infrared photography?", "answer": "Polar bears are nearly invisible under infrared photography.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Polar bears are nearly invisible under infrared photography.", "Only their breath and muzzles can be easily seen"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears are not visible under infra", "extracted_entity": null}
{"question": "what is actually black in color?", "answer": "A polar bear's skin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A polar bear's skin."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what insulate it against the cold?", "answer": "Its think blubber and fur.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its think blubber and fur."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a", "extracted_entity": null}
{"question": "what includes a lengthy justification of why this species is listed as vulnerable?", "answer": "Database entry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Database entry."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "This includes a lengthy justification of why this", "extracted_entity": null}
{"question": "what do with greenpeace and the natural resources defense council have in common?", "answer": "They filed lawsuits in California.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They filed lawsuits in California."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they are non-profit organizations.", "extracted_entity": null}
{"question": "are studies insufficient evidence for global protection?", "answer": "It is arguable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is arguable."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the global protection of the environment is a necessity,", "extracted_entity": null}
{"question": "are polar bears excellent swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears are excellent swimmers.", "extracted_entity": null}
{"question": "a semi-aquatic marine mammal , the polar bear has what?", "answer": "It has adapted for life on a combination of land, sea, and ice.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It has adapted for life on a combination of land, sea, and ice."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Ursus maritimus", "extracted_entity": null}
{"question": "garbage is what?", "answer": "Garbage is now recycled or transported to Thompson, Manitoba.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Garbage is now recycled or transported to Thompson, Manitoba."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "trash", "extracted_entity": null}
{"question": "the bears sometimes what?", "answer": "They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The bears sometimes eat the sheep.", "extracted_entity": null}
{"question": "is the Polar Bear the mascot of Bowdoin college ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Bowdoin Polar Bear", "extracted_entity": null}
{"question": "have thumbpolar bears been made both controversial and famous for their distinctive white fur and their habitat ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "polar bear", "extracted_entity": null}
{"question": "did Mitchell Taylor , the Nunavut Government Manager of Wildlife Research , not write a letter to the U.S. Fish and Wildlife Service arguing that local studies are insufficient evidence for global protection at this time ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Cubs born in December without awakening the mother ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Qatar rank as the eighth richest country in the world per capita?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar", "extracted_entity": null}
{"question": "do nearly all Qataris profess Islam?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can women legally drive in Qatar?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "when did Qatar become an independent state?", "answer": "September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 3, 1971.", "September 3, 1971"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "3 September 1971", "extracted_entity": null}
{"question": "where is Qatar's telecommunication system centered?", "answer": "Doha.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Doha.", "Doha"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar Telecom", "extracted_entity": null}
{"question": "where did a suicide-bombing kill a teacher in 2005?", "answer": "At the Doha Players Theatre.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At the Doha Players Theatre.", "Doha Players Theatre"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the native pronunciation of Qatar sound like 'cutter'?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar", "extracted_entity": null}
{"question": "what is the Arabic word for municipalities?", "answer": "baladiyah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["baladiyah.", "baladiyah"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "\u0645\u062d\u0627\u0641\u0638\u0629", "extracted_entity": null}
{"question": "what universities are in Education City?", "answer": "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar University, Weill Cornell Medical College", "extracted_entity": null}
{"question": "what carried with it a tacit recognition of Qatar's status as distinct from Bahrain?", "answer": "The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar", "extracted_entity": null}
{"question": "qatar became an independent sovereign state in what year?", "answer": "On September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On September 3, 1971."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": null}
{"question": "what consists of a low, barren plain, covered with sand?", "answer": "Much of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much of the country."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "A desert is a barren area of landscape where", "extracted_entity": null}
{"question": "how do qatar and the wider region relate?", "answer": "Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "qatar is a country in the middle east.", "extracted_entity": null}
{"question": "was Qatar University founded in 1973?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1973", "extracted_entity": null}
{"question": "is Qatar bordered by Saudi Arabia to the south?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar is bordered by Saudi Arabia", "extracted_entity": null}
{"question": "what bordered by Saudi?", "answer": "Qatar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who served as the headquarters and one of the main launching sites of the US invasion of Iraq in 2003 ?", "answer": "Qatar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Camp ________________", "extracted_entity": null}
{"question": "has Qatar a modern Telecommunication system centered in Doha ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Qatar University founded in 1973 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did the Qataris not choose as their negotiator the respected entrepreneur and long-time resident of Doha , Muhammed bin Thani ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Muhammed bin Thani", "extracted_entity": null}
{"question": "will oil and gas probably remain the backbone of Qatar 's economy for some time to come ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Oil and gas will probably remain the backbone", "extracted_entity": null}
{"question": "has it been created with a long term perspective to support the development of Qatar and the wider region , develop local and regional markets , and strengthen the links between the energy based economies and global financial markets ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar Investment Authority", "extracted_entity": null}
{"question": "what happened in these positions in english?", "answer": "These allophones cannot occur there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["These allophones cannot occur there."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1. The first person 2. The", "extracted_entity": null}
{"question": "can these allophones not occur in these positions in english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is -RRB- , officially the state of qatar -LRB- arabic (: : transliterated as dawlat qatar -RRB- , an arab emirate in southwest asia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar", "extracted_entity": null}
{"question": "does Romania border Hungary?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania does not border Hungary.", "extracted_entity": null}
{"question": "is Romania a secular state?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania is a secular state", "extracted_entity": null}
{"question": "is the president elected by popular vote?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The President of the United States is elected indirectly", "extracted_entity": null}
{"question": "how many counties is Romania divided into?", "answer": "41.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["41.", "forty-one"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "41", "extracted_entity": null}
{"question": "what is the highest mountain in Romania?", "answer": "Moldoveanu Peak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moldoveanu Peak.", "Moldoveanu Peak"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the official language of Romania?", "answer": "Romanian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romanian.", "Romanian"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanian", "extracted_entity": null}
{"question": "is the Romanian economy doing well?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The Romanian economy is doing well.", "extracted_entity": null}
{"question": "are there many Roma in Romania?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many territories joined to form Romania?", "answer": "2.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2.", "2"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Romania share a border with Ukraine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Romania declare neutrality during World War I?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania", "extracted_entity": null}
{"question": "does Romania share the same language with Moldova?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Practically"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who was the first gymnast to score a perfect \"ten\"?", "answer": "Nadia Com\u0103neci", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nadia Com\u0103neci"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Nadia Com\u0103neci", "extracted_entity": null}
{"question": "what is the largest city in Romania?", "answer": "Bucharest", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bucharest"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest", "extracted_entity": null}
{"question": "what is the largest ethnic minority in Romania?", "answer": "Hungarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hungarians"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanians", "extracted_entity": null}
{"question": "how many countries in Europe are bigger than Romania?  ?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven", "Eleven"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "why doesn't Romania have a state religion?  ?", "answer": "Romania is a secular state", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romania is a secular state"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania is a secular state.", "extracted_entity": null}
{"question": "why wasn't Romania neutral during World War II?", "answer": "it received a Soviet ultimatum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it received a Soviet ultimatum", "The Soviets threatened invasion."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania was not neutral during World War II because", "extracted_entity": null}
{"question": "sibiu, a large city in Transylvania, was chosen as European Capital of Culture in what year?", "answer": "2007", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2007"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "2007", "extracted_entity": null}
{"question": "what are approximately 42,000 years old and have been nicknamed`` John of Anina''?", "answer": "the remains (the lower jaw) of the oldest modern human", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the remains (the lower jaw) of the oldest modern human"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The \"John of Anina\" are a group", "extracted_entity": null}
{"question": "is it true that romania has a population of 21,698,181?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do economy and law have in common?", "answer": "(not sure how to answer this)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(not sure how to answer this)"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Tourism in Romania attract in 2005?", "answer": "investments worth 400 million euros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["investments worth 400 million euros"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Tourism in Romania attracted 2.", "extracted_entity": null}
{"question": "is Romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Romania a semi-presidential democratic republic where executive functions are shared between the president and the prime minister ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Gellu Naum the leader of the surrealist movement in Romania ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Gellu Naum", "extracted_entity": null}
{"question": "was the Bucharest Metro only opened in 1979 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest Metro", "extracted_entity": null}
{"question": "is romania -LRB- , -RRB- a country in southeastern europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, Romania is a semi-president", "extracted_entity": null}
{"question": "do singaporeans enjoy squid?", "answer": "Yes, they do.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they do.", "yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "squid is a type of seafood that", "extracted_entity": null}
{"question": "did Singapore's standard of living decrease since independence?", "answer": "No, the standard living was raised.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the standard living was raised.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the standard of living in Singapore has increased.", "extracted_entity": null}
{"question": "did the media contribute 15.6% to Singapore's GDP in 2001?", "answer": "No, it contributed 1.56% to GDP in 2001.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, it contributed 1.56% to GDP in 2001.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "MediaCorp", "extracted_entity": null}
{"question": "how many people use the bus network daily?", "answer": "More than 2.78 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 2.78 million people.", "2.78 million people"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "50,000", "extracted_entity": null}
{"question": "where did Orang Laut people live?", "answer": "Orang Laut people lived around the coast, rivers and smaller islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Orang Laut people lived around the coast, rivers and smaller islands.", "Several hundred indigenous Orang Laut people also lived around the coast, rivers and smaller islands."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The Orang Laut people live in the coast", "extracted_entity": null}
{"question": "what is the MRT?", "answer": "One of three heavy rail passenger transport lines in Singapore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of three heavy rail passenger transport lines in Singapore.", "Mass Rapid Transit system"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how is Singapore's climate characterized?", "answer": "Singapore has a tropical rainforest climate with no distinctive seasons.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore has a tropical rainforest climate with no distinctive seasons.", "Its climate is characterized by uniform temperature and pressure, high humidity, and abundant rainfall"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "tropical rainforest climate.", "extracted_entity": null}
{"question": "how much increase in the media's contribution to GDP is the government seeking?", "answer": "The government seeks to increase its GDP contribution to 3% by 2012. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government seeks to increase its GDP contribution to 3% by 2012. ", "the government seeks to increase its GDP contribution to 3% by 2012. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "10%", "extracted_entity": null}
{"question": "how many days did it take the Imperial Japanese Army to win the Battle of Singapore?", "answer": "Six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six days", "6 days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "7 days", "extracted_entity": null}
{"question": "is Singapore located at the southern tip of the Korean Penisula?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "is Singapore a city-state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "has Sang Nila Utama visited Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does \"singa\" mean?", "answer": "lion", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lion"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "\"singa\" means \"lion\" in", "extracted_entity": null}
{"question": "when is the first record of settlement in Singapore?", "answer": "second century AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["second century AD", "The first records of settlement in Singapore are from the second century AD."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1819", "extracted_entity": null}
{"question": "which nation invaded Singapore during World War II?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "During World War II, the Imperial Japanese Army invaded Malaya, culminating in the Battle of Singapore."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": null}
{"question": "why Churchill called the occupation of Singapor by Janpan during WWII \"Britain's greatest defeat\"?", "answer": "The British were defeated in six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British were defeated in six days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Lee Kuan Yew a successful leader of Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Goh Chok Tong succeed Lee as Prime Minister?", "answer": "In 1990, Goh Chok Tong succeeded Lee as Prime Minister.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1990, Goh Chok Tong succeeded Lee as Prime Minister."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "2004", "extracted_entity": null}
{"question": "what is the primary domestic source of water supply?", "answer": "The primary domestic source of water supply in Singapore is rainfall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The primary domestic source of water supply in Singapore is rainfall."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the national language of Singapore?", "answer": "The national language of Singapore is Malay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The national language of Singapore is Malay."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "where is the National Orchid Garden?", "answer": "Singapore Botanic Gardens.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore Botanic Gardens."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "National Orchid Garden is located at the Singapore", "extracted_entity": null}
{"question": "why are relations with Malaysia and Indonesia important?", "answer": "Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Indonesia and Malaysia are both important in terms", "extracted_entity": null}
{"question": "what is the most densely populated independent country in the world?", "answer": "Monaco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monaco"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who did Sir Thomas Stamford Raffles work for?", "answer": "British East India Company", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was Lee Kuan Yew prime minister of Singapore?", "answer": "1959", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1959"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1959-1990", "extracted_entity": null}
{"question": "what is the punishment for first-degree murder?", "answer": "There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "death penalty", "extracted_entity": null}
{"question": "what established a trading post on the island in 1819?", "answer": "British East India Company ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what used Singapore as a strategic trading post along the spice route?", "answer": "British", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Malacca", "extracted_entity": null}
{"question": "what became one of the most important commercial and military centres of the British Empire?", "answer": "Singapore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "who boasted a sizeable community of 100,000 by 1869?", "answer": "British India office", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British India office"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do reservoirs and water relate?", "answer": "By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Reservoirs and water relate in that they", "extracted_entity": null}
{"question": "is it true that singapore has several long - standing disputes with malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Singapore Changi Airport a network of 81 airlines connecting Singapore to 185 cities in 58 countries ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore Changi Airport", "extracted_entity": null}
{"question": "has Singapore a network of reservoirs and water catchment areas ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has Urbanization eliminated many areas of former primary rainforest , with the only remaining area of primary rainforest being Bukit Timah Nature Reserve ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Bukit Timah Nature Reserve", "extracted_entity": null}
{"question": "does the economy not depend heavily on exports refining imported goods , especially in manufacturing ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Exports depend on imports.", "extracted_entity": null}
{"question": "is singapore -LRB- ; (: ; , -RRB- , officially the republic of singapore -LRB- ; (: ; , -RRB- , an island nation located at the southern tip of the malay peninsula?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "singapore -LRB- ; (: ;", "extracted_entity": null}
{"question": "is singapore the 17th wealthiest country in the world in terms of gdp per capita?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "was Thedore Roosevelt  a member of the Republican Party?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt was a member of", "extracted_entity": null}
{"question": "does Theodore Roosevelt have a brother?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes, he has a brother named Theodore Ro", "extracted_entity": null}
{"question": "did Theodore Roosevelt go to Harvard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where was Theodore Roosevelt born?", "answer": "28 East 20th Street in the modern-day Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28 East 20th Street in the modern-day Gramercy section of New York City", "a four-story brownstone at 28 East 20th Street, in the modern-day Gramercy section of New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York", "extracted_entity": null}
{"question": "what did Roosevelt do to improve his physical condition?", "answer": "Roosevelt took up exercise", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roosevelt took up exercise", "He started boxing"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "he was given a \"diet of eggs,", "extracted_entity": null}
{"question": "why did Roosevelt start Boxing lessons?", "answer": "To deal with bullies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To deal with bullies"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt wanted to start boxing less", "extracted_entity": null}
{"question": "what was the nickname of Theodore Roosevelt's sister Anna?", "answer": "Bamie as a child and Bye as an adult", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bamie as a child and Bye as an adult", "Bamie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Anna Roosevelt Cowles", "extracted_entity": null}
{"question": "what was Roosevelt's justification for supporting desegregation of schools in New York?", "answer": "Noting that his children had been educated with other races and there was nothing wrong with it", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Noting that his children had been educated with other races and there was nothing wrong with it", "that his children had been educated with other races and there was nothing wrong with it"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how did newspapers respond to Roosevelt inviting Brooker T. Washington to dinner at the White House?", "answer": "mixing of the races on social occassions", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mixing of the races on social occassions", "They outraged."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Booker T. Washington", "extracted_entity": null}
{"question": "did Theodore Roosevelt study Judo?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No clue"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Judo", "extracted_entity": null}
{"question": "did Roosevelt receive a Medal of Honor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "nein"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt", "extracted_entity": null}
{"question": "did Roosevelt study biology?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He was strong in biology."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Theodore Roosevelt from?", "answer": "Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gramercy section of New York City", "New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York", "extracted_entity": null}
{"question": "who was Roosevelt's presidential hero?", "answer": "Lincoln was Roosevelt's presidential hero.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lincoln was Roosevelt's presidential hero.", "Lincoln was Roosevelt's presidential hero"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "when did Roosevelt die?", "answer": "On January 6, 1919, Roosevelt died in his sleep.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 6, 1919, Roosevelt died in his sleep."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": null}
{"question": "who helped to fund Roosevelt's African safari?", "answer": "Financed by Andrew Carnegie and his own proposed writings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Financed by Andrew Carnegie and his own proposed writings", "Andrew Carnegie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Roosevelt support racial integration in schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He supported a bill allowing school desegregation in New York State."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Roosevelt's family rich?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt's family was rich.", "extracted_entity": null}
{"question": "are turtles ectothermic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does the mother care for the young?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "(Mother)", "extracted_entity": null}
{"question": "are turtles pets?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what shape are the eggs of the larest species of turtle?", "answer": "Spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spherical", "spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how often do turtles breed?", "answer": "Every few years or more", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Every few years or more"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "every year", "extracted_entity": null}
{"question": "what do all turtles and tortoises breathe?", "answer": "air", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["air"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "air", "extracted_entity": null}
{"question": "how do turtles reproduce?", "answer": "They lay eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They lay eggs", "they lay eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles reproduce by laying eggs.", "extracted_entity": null}
{"question": "what has been discovered about turtles organs?", "answer": "They do not gradually break down or become less efficient over time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They do not gradually break down or become less efficient over time", "they do not break down"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the shell of the turtle is a hard", "extracted_entity": null}
{"question": "what suborder of turtle draws its head into its shell?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do sea turtles lay eggs on dry sandy beaches?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Pleurodira known as side-necked turtles?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Chelonia", "extracted_entity": null}
{"question": "are turtles a part of the Flying Spaghetti Monster?", "answer": "*shrug*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["*shrug*"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many suborders are turtles divided into?", "answer": "Three.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three.", "three"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "name a turtle popularly kept as a pet.?", "answer": "Russian Tortoises", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Russian Tortoises"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "name four related articles for the article on turtles.?", "answer": "Triassic, Ernst Haeckel, Kunstformen der Natur, Animal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Triassic, Ernst Haeckel, Kunstformen der Natur, Animal"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "if a turtle egg was kept warm, what would likely hatch?", "answer": "A female turtle.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A female turtle.", "A female turtle"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "female turtle", "extracted_entity": null}
{"question": "what do turles use to breathe in the water?", "answer": "Papillae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Papillae"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why are side-necked turtles called side-necked turtles?", "answer": "Because of the way they withdraw their heads into their shells.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because of the way they withdraw their heads into their shells.", "The way they withdraw their heads into their shells."], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do turtles lay eggs underwater?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are turtle eggs leathery and soft?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are turtles being examined for longevity genes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The turtle is the oldest known living animal", "extracted_entity": null}
{"question": "what suborder of turtles is extinct?", "answer": "Paracryptodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paracryptodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Testudines", "extracted_entity": null}
{"question": "what do turtle eggs preared to eat consist mainly of?", "answer": "Yolk", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yolk"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Eggs are a good source of protein,", "extracted_entity": null}
{"question": "where do sea turtles lay their eggs?", "answer": "Holes Dug into the Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holes Dug into the Mud or Sand", "holes dug into mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "beach", "extracted_entity": null}
{"question": "what is the smallest suborder of turtles?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Testudines", "extracted_entity": null}
{"question": "what are turtle eggs covered in when they incubate?", "answer": "Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Mud or Sand", "mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "shell", "extracted_entity": null}
{"question": "what type of creatures breathe air and don't lay eggs underwater?", "answer": "Amniotes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amniotes", "amnoites"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Fish", "extracted_entity": null}
{"question": "what are deposited in holes dug into mud or sand?", "answer": "eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Bones", "extracted_entity": null}
{"question": "what sort of turtles are ectothermic?", "answer": "all of them", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["all of them"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles", "extracted_entity": null}
{"question": "are the largest turtles aquatic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is aquatic respiration in Australian freshwater turtles being studied?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "these projections , called papillae , have what?", "answer": "a rich blood supply", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a rich blood supply"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "papillae", "extracted_entity": null}
{"question": "the eggs of the largest species are what?", "answer": "spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the largest species of eggs", "extracted_entity": null}
{"question": "has this inspired genetic researchers to begin examining the turtle genome for longevity genes ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The Turtle Genome", "extracted_entity": null}
{"question": "can turtles take many years to reach breeding age ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Immature sea turtles not cared for by the adults ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the Pleurodira sometimes known as the side-necked turtles , a reference to the way they withdraw their heads into their shells ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Order Testudines not include both extant -LRB- living -RRB- and extinct species , the earliest known turtles being from around 215 million years ago ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Testudines", "extracted_entity": null}
{"question": "was Ulysses Grant a general in the American Civil War?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "General Grant", "extracted_entity": null}
{"question": "was Grant's father-in-law a Democrat?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yup"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "did Ulysses win the Battle of Champion Hill?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "you betcha"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was Grant's brother in law?", "answer": "Fred Dent", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fred Dent"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Grant born?", "answer": "Point Pleasant, Ohio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Point Pleasant, Ohio", "A log cabin in Point Pleasant, Clermont County, Ohio"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was Grant's political affiliation?", "answer": "Republican", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Republican"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Republican", "extracted_entity": null}
{"question": "why did Grant say \"Damn, I had nothing to do with this batte.\"?", "answer": "It went well.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It went well.", "Because the generals under him acted on their own."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Horace Greenley lose in the presidential elections of 1872?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "si"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Horace Greeley", "extracted_entity": null}
{"question": "why did Grant think that war was unjust?", "answer": "He thought it was designed to gain land open to slavery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He thought it was designed to gain land open to slavery.", "He accepted the theory that it was designed to gain land open to slavery."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant believed that war was unjust because it was", "extracted_entity": null}
{"question": "who achieved international fame as the leading Union general in the American Civil War?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "grant was elected president as a Republican in what year?", "answer": "1868", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1868"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1868", "extracted_entity": null}
{"question": "who took a hard line that reduced violence by groups like the Ku Klux Klan?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "[1]", "extracted_entity": null}
{"question": "when did he vote for democrat james buchanan?", "answer": "1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1856"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1856", "extracted_entity": null}
{"question": "who did President Lincoln promote of major general in the regular army, effective July 4?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Grant elected president as a Republican?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was from Pennsylvania?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Iroquois", "extracted_entity": null}
{"question": "was it a two-sentence description that completely caught the essence of Ulysses S. Grant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Grant 's favorite brand of bourbon whiskey Old Crow ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "had he smoked only sporadically ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Grant & Perkins not sell harnesses , saddles , and other leather goods and purchase hides from farmers in the prosperous Galena area ?", "answer": "they did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they did"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the famous dragon gate at the entrance to the district at the corner of Grant and Bush Street ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1865?", "answer": "he accepted the surrender of Robert E. Lee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he accepted the surrender of Robert E. Lee"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "The Confederate States of America ceased to exist.", "extracted_entity": null}
{"question": "what happened in recent years?", "answer": "his reputation as president has improved", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["his reputation as president has improved"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "2016, 201", "extracted_entity": null}
{"question": "is Uruguay located in the northwesten part of Africa?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "is Uruguay's capital Montevideo?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Montevideo", "extracted_entity": null}
{"question": "does Uruguay have cold summers?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who founded Montevideo?", "answer": "The Spanish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Spanish.", "the Spanish", "By the Spanish, in the early 18th century", "Montevideo was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Uruguay's oldest church?", "answer": "San Carlos, Maldonado.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Carlos, Maldonado.", "San Carlos, Maldonado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the Catedral de San Jos\u00e9", "extracted_entity": null}
{"question": "who heavily influenced the architecture and culture of Montevideo?", "answer": "European immigrants.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["European immigrants.", "European immigrants"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are poor neighborhoods called informally?", "answer": "Cantegriles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cantegriles.", "Cantegriles"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "poor neighborhoods", "extracted_entity": null}
{"question": "is uruguay's landscape mountainous?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "not really?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what religion do most Uruguayans profess?", "answer": "None.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None.", "Roman Catholic"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic", "extracted_entity": null}
{"question": "is Uruguay located in South America?", "answer": "Yes. It's located in the southeastern part", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. It's located in the southeastern part", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay is a country located in South America.", "extracted_entity": null}
{"question": "can citizens propose changes to the Constitution?", "answer": "Yes. People are allowed to challenge laws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. People are allowed to challenge laws.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, citizens can propose changes to the Constitution.", "extracted_entity": null}
{"question": "did Uruguay host the first ever World Cup?", "answer": "Yes, in 1930.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, in 1930.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "how many square kilometres of continental land is Uruguay?", "answer": "176.215 km\u00b2 ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["176.215 km\u00b2 ", "176,214 square kilometres"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "176,220 km\u00b2", "extracted_entity": null}
{"question": "how much of the population is of white European descent?", "answer": "88%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88%", "Approximately 88% of its population are of prevalently white European descent."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "100%", "extracted_entity": null}
{"question": "why was the capital of Uruguay founded?", "answer": "For a military stronghold.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For a military stronghold.", "Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Montevideo", "extracted_entity": null}
{"question": "how many times has Uruguay won the World Cup?", "answer": "Twice. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twice. ", "On two or more occasions."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "how much is the average income of an African woman compared to a European man?", "answer": "African women earns 0.65 * 0.718  = 46.67% of a European man earns in average", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["African women earns 0.65 * 0.718  = 46.67% of a European man earns in average"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "European man", "extracted_entity": null}
{"question": "does Uruguay recognize same-sex civil unions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Uruguay border French Guiana?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Uruguay's warmest month June?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does a citizen use to propose changes to the Constitution?", "answer": "Referendum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Referendum", "Plebiscite"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what religions are found in Uruguay?", "answer": "Roman Catholic, Protestant, Jewish, and nonprofessing.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roman Catholic, Protestant, Jewish, and nonprofessing.", "Roman Catholic, Protestant, Jewish"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic, Protestant, Jewish, Muslim,", "extracted_entity": null}
{"question": "what has Uruguay done to be competitive in agriculture?", "answer": "Labeling as \"Natural\" or \"Ecological\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Labeling as \"Natural\" or \"Ecological\"", "Use low inputs of labour, technology, and capital, which results in lower yields but also opens the door for Uruguay to market its products as \"natural\" or \"ecological\""], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay is a country with a lot of agriculture", "extracted_entity": null}
{"question": "what are the names of Uruguay's political parties?", "answer": "Partido Colorado and Partido Blanco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Partido Colorado and Partido Blanco", "Partido Blanco and Partido Colorado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Uruguay the smallest soverign nation in South America?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "what was founded by the Spanish in the early 18th century as a military stronghold?", "answer": "Montevideo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montevideo"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Fort San Juan Bautista", "extracted_entity": null}
{"question": "what has many possible meanings?", "answer": "(What?)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(What?)"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "life", "extracted_entity": null}
{"question": "how do india and latin america relate?", "answer": "Uruguay exports architectural services to India and Latin America", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uruguay exports architectural services to India and Latin America"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Uruguay win in 1828?", "answer": "Its independence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its independence"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay won the Battle of Ituzaing\u00f3", "extracted_entity": null}
{"question": "is Uruguay very common?", "answer": "No?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "is The climate in Uruguay temperate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the main sport in Uruguay football ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Uruguay 's oldest church in San Carlos , Maldonado ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "San Carlos Church", "extracted_entity": null}
{"question": "is Uruguay a member of the Cairns Group of exporters of agricultural products ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "is it widely considered the most secular nation in Latin America ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it the second smallest independent country in south america , larger only than suriname and the french overseas department of french guiana?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "French Guiana", "extracted_entity": null}
{"question": "is it a constitutional democracy , where the president fulfills the roles of both head of state and head of government?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "was Woodrow Wilson the thirtieth President of the United States?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "did Woodrow Wilson create the League of Nations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson did not create the League of Nations", "extracted_entity": null}
{"question": "did the U.S. join the League of Nations?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "League of Nations", "extracted_entity": null}
{"question": "where was the League of Nations created?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The League of Nations was created in 19", "extracted_entity": null}
{"question": "when was Woodrow Wilson born?", "answer": "December 28, 1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 28, 1856"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson was born in 185", "extracted_entity": null}
{"question": "who was President when Wilson finished Congressional Government?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "what field did Woodrow Wilson leave law practice to study?", "answer": "history and political science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["history and political science", "Government"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Political Science", "extracted_entity": null}
{"question": "what caused Wilson to ask Congress to declare war on the Central Powers?", "answer": "German began unrestricted submarine warfare", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German began unrestricted submarine warfare", "World War I"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was more damaging than moving students into colleges?", "answer": "His confrontation with Andrew Fleming West", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His confrontation with Andrew Fleming West"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the idea of the \"noble lie\"", "extracted_entity": null}
{"question": "was Wilson a member of the Phi Kappa Psi fraternity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Totally"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson an automobile enthusiast?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "ouai"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Wilson's father own slaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yeah"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where is Wilson buried?", "answer": "He was buried in Washington National Cathedral", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was buried in Washington National Cathedral", "Washington National Cathedral"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did Wilson attend law school?", "answer": "Wilson attended law school at University of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson attended law school at University of Virginia", "University of Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Columbia Law School", "extracted_entity": null}
{"question": "where was Woodrow Wilson born?", "answer": "Woodrow Wilson was born in Staunton, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Woodrow Wilson was born in Staunton, Virginia", "Staunton, Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Thomas Woodrow Wilson", "extracted_entity": null}
{"question": "did Wilson support desegregation?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Not in the slightest."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Wilson support the committee system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The House of Representatives Committee System.", "extracted_entity": null}
{"question": "did Wilson have any siblings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he had four siblings.", "extracted_entity": null}
{"question": "what was Scots-Irish and Scottish?", "answer": "His ancestry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His ancestry"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The Scots-Irish were Irish people", "extracted_entity": null}
{"question": "what defended slavery, owned slaves and set up a Sunday school for them?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Theodore Dwight Weld", "extracted_entity": null}
{"question": "who did Wilson win in 1917?", "answer": "Irish Americans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish Americans"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "California", "extracted_entity": null}
{"question": "was Wilson awarded the 1919 Nobel Peace Prize?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson a remarkably effective writer and thinker?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what lived in Columbia?", "answer": "Wilson", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The Columbia was a ship that sailed from the", "extracted_entity": null}
{"question": "was Wilson president of the American Political Science Association in 1910 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "did he not cast his ballot for John M. Palmer , the presidential candidate of the National Democratic Party , or Gold Democrats , a short-lived party that supported a gold standard , low tariffs , and limited government ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "John M. Palmer", "extracted_entity": null}
{"question": "did Wilson not spend 1914 through the beginning of 1917 trying to keep America out of the war in Europe ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson , a staunch opponent of antisemitism , sympathetic to the plight of Jews , especially in Poland and in France ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1917?", "answer": "raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The French Army mutinied.", "extracted_entity": null}
{"question": "was Volta an Italian physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Volta buried in the city of Pittsburgh?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta buried in the city of Pittsburgh.", "extracted_entity": null}
{"question": "did Volta have a passion for the study of electricity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The Volt is the SI unit of electric potential", "extracted_entity": null}
{"question": "what is the battery made by Volta credited to be?", "answer": "the first cell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the first cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the Voltaic Pile", "extracted_entity": null}
{"question": "what important electrical unit was named in honor of Volta?", "answer": "the volt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the volt", "volt"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "volt", "extracted_entity": null}
{"question": "where did Volta enter retirement?", "answer": "Spain", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spain"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the 1990s", "extracted_entity": null}
{"question": "is it a disadvantage for something to be unsafe to handle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Lombardy under Napoleon's rule in 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the Italian 10.000 lira banknote created before the euro?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "10.000 lira bank", "extracted_entity": null}
{"question": "for how many years did Alessandro Volta live?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "81 years", "extracted_entity": null}
{"question": "did Alessandro Volta live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta lived to be 80", "extracted_entity": null}
{"question": "what was Alessandro Volta`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was a physicist and", "extracted_entity": null}
{"question": "how old was Alessandro Volta when he died?", "answer": " 82", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" 82"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82", "extracted_entity": null}
{"question": "how many years ago was it when Volta married the daughter of Count Ludovico Peregrini , Teresa , with whom he raised three sons?", "answer": "215", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["215"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the electrolyte sulphuric acid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "sulfuric acid", "extracted_entity": null}
{"question": "is volta buried in the city  of Como?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta is buried in the city of Como.", "extracted_entity": null}
{"question": "was his 1800  paper written in French?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "[Fran\u00e7ais]", "extracted_entity": null}
{"question": "before 1796, was Lombardy ruled by Austria?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Lombardy", "extracted_entity": null}
{"question": "did he receive the Society`s 1794 Copley Medal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he experiment with individual cells?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did lombardy come under Napoleon`s rule?", "answer": "From 1796 to 1815", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1796 to 1815"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1805", "extracted_entity": null}
{"question": "where did he publish his invention of the Voltaic pile battery?", "answer": "the Philosophical Transactions of the Royal Society", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Philosophical Transactions of the Royal Society"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1799", "extracted_entity": null}
{"question": "did he become professor of experimental physics at the University of Pavia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that his passion been always the study of electricity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "his passion been always the study of what?", "answer": "Electricity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Electricity"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the nature of things", "extracted_entity": null}
{"question": "is it true that Volta married the daughter of Count Ludovico Peregrini?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he published his invention of the Voltaic pile battery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Amedeo Avogadro graduate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Avogadro live in England?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where was Avogadro a professor of physics?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "University of Turin", "extracted_entity": null}
{"question": "how many children did Avogadro have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Avogadro most noted for?", "answer": "contributions to the theory of molarity and molecular weight", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["contributions to the theory of molarity and molecular weight"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's law", "extracted_entity": null}
{"question": "what is Avogadro's number?", "answer": "6.02214199x10 23", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.02214199x10 23"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number is 6", "extracted_entity": null}
{"question": "in what year did Avogadro stop teaching at Turin University?", "answer": "1853", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1853", "1823"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1850", "extracted_entity": null}
{"question": "what subject did Avogadro study first?", "answer": "ecclesiastical law", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ecclesiastical law"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "physics", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s birthplace?", "answer": "Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s profession?", "answer": "professor of physics", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of physics"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years ago was it when he became a professor of physics at the University of Turin?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1914", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s first name?", "answer": "Amedeo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s last name?", "answer": "Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in North America?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "was King Victor  Emmanuel  III there to pay  homage  to Avogadro?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in fact, was Avogadro `s  famous  1811  paper written in French . )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "is avogadro `s  number commonly used to compute  the results  of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "hence, can relative molecular  masses be calculated from the masses  of gas samples?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he publish his work?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Avogadro submit his poem?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, Avogadro submitted his poem.", "extracted_entity": null}
{"question": "is it true that loschmidt calculated first the value of Avogadro number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, it is true that loschmidt calculated", "extracted_entity": null}
{"question": "loschmidt calculated first the value of what?", "answer": "Avogadro's number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro's number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The value of the density of the gas", "extracted_entity": null}
{"question": "hence , can relative molecular masses be calculated from the masses of gas samples ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into English as \"essay on determining the Relative Masses of the Elementary Molecules of Bodies\" ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is Amedeo Avogadro?", "answer": "A person.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A person."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was an Italian", "extracted_entity": null}
{"question": "where is Amedeo Avogadro from?", "answer": "Italy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Italy."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Italy", "extracted_entity": null}
{"question": "when did Anders Celcius publish his observations on the aurora borealis?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733", "1733 "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1742", "extracted_entity": null}
{"question": "what did Anders Celsius determine about the boiling of water?", "answer": "He determined the dependence of the boiling of water with atmospheric pressure.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He determined the dependence of the boiling of water with atmospheric pressure.", "dependence with atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which temperature scale did Celsius propose?", "answer": "Celcius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celcius", " the Celsius temperature scale "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius scale", "extracted_entity": null}
{"question": "what is the boiling point of water dependent on?", "answer": "atmospheric pressure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the boiling point of water is dependent on pressure", "extracted_entity": null}
{"question": "where was Celsius born?", "answer": "Uppsala in Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uppsala in Sweden", "Uppsala, Sweden"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius was born in Uppsala", "extracted_entity": null}
{"question": "whad did Celsius report in his paper \"Observations of two persistent degrees on a thermometer\"?", "answer": "the freezing point is independent of latitude", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the freezing point is independent of latitude"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the boiling point of water", "extracted_entity": null}
{"question": "how old was Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "42 years old.", "extracted_entity": null}
{"question": "what is temperature would water have to be to be halfway between its standard boiling and freezing point?", "answer": "50", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["50"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "100 degrees Celsius", "extracted_entity": null}
{"question": "what is the difference between Celsius' original temperature scale and the one we use today?", "answer": "The scale was reversed by Carolus Linnaeus. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed by Carolus Linnaeus. ", "scale was reversed "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The original temperature scale was based on the boiling", "extracted_entity": null}
{"question": "what was Anders Celsius`s profession?", "answer": "professor of astronomy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of astronomy"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Physicist", "extracted_entity": null}
{"question": "how old was Anders Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "71 years old", "extracted_entity": null}
{"question": "what is Anders Celsius`s last name?", "answer": "Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": null}
{"question": "was celsius born in Uppsala  in Sweden?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Uppsala, Sweden", "extracted_entity": null}
{"question": "is The Celsius  crater  on the Moon named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "was Anders Celsius  ( November 27 ,  1701  April 25 ,  1744  ) a Swedish  astronomer?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "was Anders Celsius the first  to perform  and  publish  careful experiments  aiming at the definition  of an international  temperature  scale  on scientific grounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was he professor at Uppsala University?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he published a collection of 316 observations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "he published a collection of what?", "answer": "observations", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["observations"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "A collection of short stories.", "extracted_entity": null}
{"question": "can syllables begin with a vowel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are calligraphers held in great esteem?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "have many European languages borrowed numerous words from it?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes, many European languages have borrowed numerous words", "extracted_entity": null}
{"question": "are the most active in Damascus and Cairo?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do arabs consider literary Arabic?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "the sociolinguistic situation of Arabic provides a prime example of what?", "answer": "the linguistic phenomenon of diglossia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the linguistic phenomenon of diglossia"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is a diglossic language, with", "extracted_entity": null}
{"question": "are pronouns in Literary Arabic marked for person , number and gender ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "however , are non-human plural nouns grammatically considered to be feminine singular ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a bee an insect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are bees related to ants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "have managed populations of European honey bees experienced substantial declines?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where do bees live?", "answer": "in colonies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in colonies"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "in the nest.", "extracted_entity": null}
{"question": "when do bumblebee colonies reach peak population?", "answer": "mid to late summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mid to late summer"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do people commonly call cleptoparasitic bees?", "answer": "cuckoo bees", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cuckoo bees"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "cleptoparasitic bees", "extracted_entity": null}
{"question": "how did vespoid wasps evolve?", "answer": "from predatory ancestors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from predatory ancestors"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "The wasps are an insect order, Hymen", "extracted_entity": null}
{"question": "what do humans use bees for?", "answer": "honey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["honey"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Honey, pollination, beesw", "extracted_entity": null}
{"question": "why do honey bee queens continue to lay eggs during the foraging season?", "answer": "to replace daily casualties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to replace daily casualties"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "The queen is the only bee that can lay", "extracted_entity": null}
{"question": "does Beijing mean \"northern capital\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "No. Beijing means \"northern capital", "extracted_entity": null}
{"question": "are the caves of Dragon Bone Hill near the village of Zhoukoudian in Fangshan District?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are hutongs disappearing?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "hutongs are disappearing.", "extracted_entity": null}
{"question": "when did Beijing host the Olympic Games?", "answer": "2008", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2008"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "2008", "extracted_entity": null}
{"question": "when was the An Shi Rebellion launched?", "answer": "in 755 AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 755 AD"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "17 February 755", "extracted_entity": null}
{"question": "in 1949, where did Communist forces enter without a fight?", "answer": "Beiping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beiping"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "The Soviets.", "extracted_entity": null}
{"question": "when did Yuan Shikai die?", "answer": "1916", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1916"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": null}
{"question": "where does air pollution in Beijing come from?", "answer": "surrounding cities and provinces", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["surrounding cities and provinces"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "The air pollution in Beijing is caused", "extracted_entity": null}
{"question": "as of August 1st, 2006, how many trains stop daily at the Beijing Railway Station or the Beijing West Railway Station?", "answer": "167", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["167"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing Railway Station or the Beijing", "extracted_entity": null}
{"question": "what is the name of a university (or similar institution for imparting higher education) in Beijing?", "answer": "Tsinghua University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tsinghua University"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing Normal University", "extracted_entity": null}
{"question": "are famous middle  schools  in Beijing :?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Blaise Pascal a mathematician of the first order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal's triangle", "extracted_entity": null}
{"question": "could Blaise Pascal move without crutches?", "answer": "He could move without crutches until a paralytic attack in 1647.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He could move without crutches until a paralytic attack in 1647.", "no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has the name Pascal been given to the SI unit of pressure?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal", "extracted_entity": null}
{"question": "from what did Pascal suffer throughout his life?", "answer": "ill health", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ill health", "poor health"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Pascal argue was as perfect as possible?", "answer": "the procedure used in geometry ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the procedure used in geometry ", "the procedure used in geometry"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the best physician?", "answer": "Time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Time"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Doctor", "extracted_entity": null}
{"question": "who was the eldest sibling?", "answer": "Gilberte", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gilberte"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "aside from the Provincial Letters' religious influence, were they popular as a literary work?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, they were popular", "extracted_entity": null}
{"question": "of whose continual poor health was the cause never precisely determined?", "answer": "Blaise Pascal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Blaise Pascal"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the violin cello a bowed string instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Cello", "extracted_entity": null}
{"question": "is the purfling just for decoration?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the z-holes used as access points to the interior of the cello? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what does violincello mean?", "answer": "Violincello means \"little violene\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Violincello means \"little violene\"."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Cello", "extracted_entity": null}
{"question": "what are the two materials that bows are made of traditionally? ?", "answer": "Traditionally, bows are made from pernambuco or brazilwood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Traditionally, bows are made from pernambuco or brazilwood."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "wood and horn.", "extracted_entity": null}
{"question": "what is used to tune a cello?", "answer": "Pegs are used to tune a cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pegs are used to tune a cello.", "the pegs"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a tuning fork", "extracted_entity": null}
{"question": "which harmonics can produce any notes above middle C? ?", "answer": "Artificial harmonics can produce any notes above middle C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Artificial harmonics can produce any notes above middle C.", "Artificial harmonics"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "123456789", "extracted_entity": null}
{"question": "who referred to the cello as \"basso de viola da braccio\"?", "answer": "Monteverdi referred to the cello as \"basso de viola da braccio\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monteverdi referred to the cello as \"basso de viola da braccio\".", "Monteverdi"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Giovanni Gabrieli", "extracted_entity": null}
{"question": "where is the violincello held?", "answer": "The violincello is held on the shoulder.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The violincello is held on the shoulder.", "on the shoulder"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "It is held in the upper part of the body", "extracted_entity": null}
{"question": "is there cello in performances by Rihanna?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "\"Rihanna\" is a name of a", "extracted_entity": null}
{"question": "are cellos constructed with glue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the cello a stringed instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is cello an abbreviation of?", "answer": "violoncello", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["violoncello"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Cello is an abbreviation of \"Cello", "extracted_entity": null}
{"question": "what position is used to play the cello?", "answer": "a gamba", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a gamba", "seated"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "\"The Cello is played in the Cello", "extracted_entity": null}
{"question": "what is a person who plays the cello called?", "answer": "cellist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cellist"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "cellist", "extracted_entity": null}
{"question": "why is there purling on a cello?", "answer": "to stop cracks from forming ****misspelled****", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to stop cracks from forming ****misspelled****"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The cello is an instrument that produces sound by", "extracted_entity": null}
{"question": "when did the first educational works appear for the cello?", "answer": "the 18th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 18th century", "18th century"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "1756", "extracted_entity": null}
{"question": "what cello manufacturer should I buy from if I want to play outside?", "answer": "Luis & Clark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luis & Clark"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a member of the National Institute?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb was a", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb find any relationship between electric charges and magnetic poles? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb's father's family in Montpellier? ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb", "extracted_entity": null}
{"question": "where did Charles-Augustin de Coulomb die?", "answer": "Charles-Augustin de Coulomb died in Paris.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb died in Paris.", "Paris, France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb permanently stationed in Paris?", "answer": "Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1785", "extracted_entity": null}
{"question": "what contribution did Charles-Augustin de Coulomb make to the field of geotechnical engineering?", "answer": "Retaining wall design", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Retaining wall design"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb made the", "extracted_entity": null}
{"question": "where did the construction of Fort Bourbon take place?", "answer": "The construction of Fort Bourbon took place in Martinique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The construction of Fort Bourbon took place in Martinique.", "Martinique "], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Fort Bourbon was built on the north side of", "extracted_entity": null}
{"question": "what is the definition of the electrostatic force of attraction and repulsion? ?", "answer": "Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "Coulomb's Law"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb retire to a small estate he possessed at Blois?", "answer": "Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "1789"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1704", "extracted_entity": null}
{"question": "if Charles-Augustin de Coulomb was alive today, how old would he have been?", "answer": "273", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["273"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb (1", "extracted_entity": null}
{"question": "for how many years did Charles-Augustin de Coulomb live?", "answer": "70", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736-1806", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how old was Charles-Augustin de Coulomb when he died?", "answer": "1806", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1806"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "82", "extracted_entity": null}
{"question": "how many years ago did he resign his appointment as intendant de eaux et fontaine ?", "answer": "220", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["220"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years ago did Charles-Augustin de Coulomb die?", "answer": "203", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["203"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "11 years ago", "extracted_entity": null}
{"question": "was The SI  unit  of charge , the  coulomb , named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was coulomb born in Angoul\u00eame,  France ,  to a well  to do  family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was his father ,  Henri  Coulomb , inspector of the Royal  Fields  in Montpellier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Henri Coulomb", "extracted_entity": null}
{"question": "is coulomb distinguished in the history  of mechanics  and  of electricity and  magnetism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb is distinguished in the history of mechan", "extracted_entity": null}
{"question": "was Charles Augustin  de Coulomb  ( born  June  14 ,  1736 ,  Angoul\u00eame ,  France  -  died August 23 ,  1806 ,  Paris ,  France  ) a French  physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Coulomb leave a legacy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb born?", "answer": "June 14, 1736", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 14, 1736"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736", "extracted_entity": null}
{"question": "did he publish an important investigation of the laws of friction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Coulomb born in Angoul\u00eame, France, to a well to do family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was his father inspector of the Royal Fields?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "he discovered an inverse relationship of what?", "answer": "distance and electric force", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["distance and electric force"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that coulomb leaves a legacy as a hero?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Charles-Augustin de Coulomb from?", "answer": "France", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "France", "extracted_entity": null}
{"question": "was Old Chinese wholly uninflected?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are The modern Chinese dialects more like a family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that one-fifth of world population speak some form of Chinese?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "one-fifth of world population speak some form of what?", "answer": "Chinese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is mandarin spoken Chinese distinguished by its high level?", "answer": "no, it's distinguished by diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no, it's distinguished by diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "copenhagen is the capital of what country?", "answer": "Denmark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Denmark"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Denmark", "extracted_entity": null}
{"question": "what is the population of Copenhagen?", "answer": "1,161,063", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,161,063", "1,153,615"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "777,218", "extracted_entity": null}
{"question": "what transnational bridge was completed in 2000?", "answer": "Oresund Bridge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oresund Bridge"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "the Chesapeake Bay Bridge-Tunnel", "extracted_entity": null}
{"question": "why is Copenhagen a regional hub?", "answer": "Its strategic location and excellent infrastructure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its strategic location and excellent infrastructure", "Its strategic location and excellent infrastructure with the largest airport in Scandinavia[6] located 14 minutes by train from the city centre"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened to Copenhagen between 1251 and 1255?", "answer": "a bunch of things", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a bunch of things"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Copenhagen completely surrounded with water? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "The Lakes", "extracted_entity": null}
{"question": "copenhagen is ranked number one worldwide for which things?", "answer": "Most Livable City in the World, ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most Livable City in the World, ", "Most Livable City, Location Ranking Survey"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did German troops occupy Copenhagen?", "answer": "Because it was WW2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it was WW2"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have cymbals been used historically to suggest bacchanal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can the origins of cymbals be traced to prehistoric times?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The cymbal was used in the ancient cult", "extracted_entity": null}
{"question": "are cymbals used in moden orchestras?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the most common Cymbals?", "answer": " The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "Hi-Hats, Crash, Splash, Ride, and China"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals are made of brass.", "extracted_entity": null}
{"question": "what is the second main orchestral use of cymbals?", "answer": "The suspended cymbal is the second main orchestral use of symbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The suspended cymbal is the second main orchestral use of symbals.", "The suspended cymbal", "the suspended cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what have clash cymbals traditionally been accompanied by?", "answer": "Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "The bass drum playing an identical part"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happens when the center of a Cymbal is hit?", "answer": "It causes a sound which is in a higher register than the rest of the symbal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It causes a sound which is in a higher register than the rest of the symbal.", "It causes a sound which is in a higher register than the rest of the cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the center of the Cymbal is hit.", "extracted_entity": null}
{"question": "what is an effective way to accentuate a note?", "answer": "Playing a bass drum playing an identical part is an effective way to accentuate a note.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Playing a bass drum playing an identical part is an effective way to accentuate a note.", "The combination of a clash cymbol being accompanied by a bass drum playing an identical part, played loudly"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "from what type of Cymbals can a expert player obtain an enormous dynamic range?", "answer": "An expert player can obtain an enormous dynamic range from crash cymbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An expert player can obtain an enormous dynamic range from crash cymbals.", "Crash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "from a set of Cymbals.", "extracted_entity": null}
{"question": "can the origins of cymbals be traced back to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the modern hi-hat evolve from clash cymbals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the modern hi-hat evolved from clash", "extracted_entity": null}
{"question": "is the word cymbal derived from a Germanic word?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbal", "extracted_entity": null}
{"question": "what is the center of a cymbal called?", "answer": "Bell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bell", "bell"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "bell", "extracted_entity": null}
{"question": "what part of the cymbal gets the best \"crash\"?", "answer": "Edge or rim", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Edge or rim", "edge or rim"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the bell", "extracted_entity": null}
{"question": "what two instruments form the modern drum kit?", "answer": "the cymbals and the drum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cymbals and the drum"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "bass drum and snare drum", "extracted_entity": null}
{"question": "what Greek mythical creatures were depicted holding cymbals?", "answer": "fauns and satyrs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["fauns and satyrs"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are orchestral crash cymbals technically called?", "answer": "a pair of clash cymbals", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pair of clash cymbals", "clash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "orchestral crash cymbals", "extracted_entity": null}
{"question": "is Dhaka the capital of Bangladesh?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Dhaka is the capital of Bangladesh", "extracted_entity": null}
{"question": "is Dhaka located on the eastern banks of the Buriganga River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is University of Dhaka the largest public university in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "University of Dhaka", "extracted_entity": null}
{"question": "how much area does Dhaka cover?", "answer": "1462.60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1462.60 square kilometers"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Dhaka covers 2,500", "extracted_entity": null}
{"question": "how many people did the 1970 Bhola cyclone kill?", "answer": "500,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500,000"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "300,000", "extracted_entity": null}
{"question": "when is the end of the independence war of Bangladesh?", "answer": "December 16, 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 16, 1971"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "16th December, 197", "extracted_entity": null}
{"question": "did the Mughals arrive in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is University of Dhaka older than the Dhaka College?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "University of Dhaka", "extracted_entity": null}
{"question": "what is given for the number of native speakers?", "answer": "No figure is given for the number of native speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No figure is given for the number of native speakers."], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "is english an intonation  language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is english a de  jure  official  language  of Israel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the stressed  syllable called the nuclear  syllable?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does english grammar have minimal inflection?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "did old English develop into Middle English?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "are books, magazines, and newspapers written in English available?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is the syntax of German different with different rules?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes, the syntax of German is different with different", "extracted_entity": null}
{"question": "is it true that semantics causes a number of false friends?", "answer": "true", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["true"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "here , are all syllables unstressed , except the syllables/words best and done , which are stressed ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "The syllables \"best\" and \"done", "extracted_entity": null}
{"question": "are many words describing the navy , types of ships , and other objects or activities on the water of dutch origin ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are local variations in the formal written version of the language quite limited , being restricted largely to the spelling differences between British and American English ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the flute a musical instrument?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is it possible to open flutes at one or both ends?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Indian concert flutes available in standard pitches?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do we refer musicians who play flute?", "answer": "A flute player, a flautist or a flutist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute player, a flautist or a flutist. ", "A flute player, a flautist, or a flutist"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Flute Player", "extracted_entity": null}
{"question": "when was a three-holed flute made from a mammoth tusk discovered?", "answer": "2004.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2004.", "In 2004"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did the tin whistle first appear?", "answer": "12th century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["12th century.", "In the 12th century"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1750s", "extracted_entity": null}
{"question": "what are the oldest known musical instruments?", "answer": "A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "A three-holed flute made from a mammoth tusk and two flutes made from swans' bones are among the oldest known musical instruments"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was the pan flute spread to other parts of Europe?", "answer": "7th century BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7th century BC.", "After the 7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "in 1460.", "extracted_entity": null}
{"question": "what does the air stream across this hole create?", "answer": "A Bernoulli, or siphon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A Bernoulli, or siphon."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "a suction", "extracted_entity": null}
{"question": "are foxes wary of humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are fennec foxes endangered?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the diet of foxes include reptiles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how long do most foxes live?", "answer": "Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "2 to 3 years"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in what country did fox hunting originate?", "answer": "the United Kingdom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United Kingdom", "in the United Kingdom"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the smallest species of fox?", "answer": "The Fennec Fox is the smallest species of fox. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Fennec Fox is the smallest species of fox. ", "the Fennec Fox"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "The smallest species of fox is the Fenne", "extracted_entity": null}
{"question": "what are female foxes called?", "answer": "vixens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["vixens"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is caching food?", "answer": "Caching is burying it for later consumption, usually under leaves, snow, or soil.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Caching is burying it for later consumption, usually under leaves, snow, or soil.", "burying it for later consumption"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Food is stored for future use.", "extracted_entity": null}
{"question": "do foxes damage fruit on farms?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many years ago did African people spread across 31 francophone African countries can speak French either as a first or second language?", "answer": "2 years ago", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2 years ago"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in Old French, was the plural  for animal animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is catalan the only  official  language  of Andorra?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Catalan", "extracted_entity": null}
{"question": "in Belgium, however, is quatre-vingts universally used?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Four-eighty", "extracted_entity": null}
{"question": "is french taught in many schools as a primary language along with Arabic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is widely used by the Lebanese, especially for administrative purposes?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is french descendant of the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there also a variety of regional languages In addition?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it true that france signed the European Charter for Regional Languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "French", "extracted_entity": null}
{"question": "are the prefixes en- and em- always nasalized ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "em-", "extracted_entity": null}
{"question": "how many years ago was the Luther Bible by Martin Luther printed?", "answer": "475", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["475"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "1534", "extracted_entity": null}
{"question": "is german an inflected  language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in Switzerland, is \u00df used at all?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "No, it is not used.", "extracted_entity": null}
{"question": "are there two common  word  orders?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are feminines declined in the singular?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is 9 %  of the Internet  population German?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is german written using the Latin  alphabet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was the Luther Bible by Martin Luther printed?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does a  tz indicate that the preceding vowel is short?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there 622,650 speakers of German In Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there also large populations of German ancestry In Mexico?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "German", "extracted_entity": null}
{"question": "would Bundesl\u00e4nder not accept North Rhine Westphalia and Bavaria ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Hesse", "extracted_entity": null}
{"question": "have adverbs of time to appear in the third place in the sentence , just after the predicate ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do both sexes of giraffe have horns?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do giraffes give birth standing up?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Giraffes give birth standing up.", "extracted_entity": null}
{"question": "are giraffes hunted for their hides?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "giraffes are not hunted for their", "extracted_entity": null}
{"question": "what areas can giraffes inhabit?", "answer": "savannas, grasslands, or open woodlands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["savannas, grasslands, or open woodlands", "inhabit savannas, grasslands, or open woodlands"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Africa", "extracted_entity": null}
{"question": "what will a giraffe use to clean off any bugs that appear on its face?", "answer": "extremely long tongue ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["extremely long tongue ", "It's extremely long tongue"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "A giraffe's skin is thick and", "extracted_entity": null}
{"question": "how do giraffes defend themselves?", "answer": "with a powerful kick", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["with a powerful kick", "With powerful kicks"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Giraffes defend themselves by head butting", "extracted_entity": null}
{"question": "do male giraffes weigh more than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Male giraffes weigh more than female", "extracted_entity": null}
{"question": "do male giraffes have larger horns than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "male giraffes have larger horns than", "extracted_entity": null}
{"question": "are male females generally taller than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Henri Becquerel one of the discoverers of radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the SI unit for radioactivity named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "becquerel", "extracted_entity": null}
{"question": "was Henri Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in what year did Henri Becquerel die?", "answer": "1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1908", "August 24, 1908"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": null}
{"question": "where was Henri Becquerel born?", "answer": " Paris ", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Paris ", "Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel, Henri (185", "extracted_entity": null}
{"question": "in what year did Henri Becquerel win the Nobel Prize in Physics?", "answer": "1903", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1903"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": null}
{"question": "was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Henri Becquerel the sole winner of the 1903 Nobel Prize in Physics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Henri Becquerel intentionally discover radioactivity?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "He discovered it accidentally"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "if Henri Becquerel was alive today, how old would he have been?", "answer": "157", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["157"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "128 years old", "extracted_entity": null}
{"question": "for how many years did Henri Becquerel live?", "answer": "56", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["56"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1852-1908", "extracted_entity": null}
{"question": "did Henri Becquerel live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel", "extracted_entity": null}
{"question": "what was Henri Becquerel`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "French physicist", "extracted_entity": null}
{"question": "how old was Henri Becquerel when he died?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "59", "extracted_entity": null}
{"question": "how many years ago was it when he became the third in his family to occupy the physics chair at the Musum National d`Histoire Naturelle?", "answer": "117", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["117"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1911", "extracted_entity": null}
{"question": "in 1908, the year  of his death, was Becquerel elected Permanent Secretary  of the Acad\u00e9mie  des  Sciences?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was Henri Becquerel born?", "answer": "December 15, 1852", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 15, 1852"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he became the chief engineer in the Department of Br", "extracted_entity": null}
{"question": "did he share the Nobel Prize in Physics?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that Becquerel wrapped a fluorescent substance in photographic plates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel wrapped a fluorescent subst", "extracted_entity": null}
{"question": "becquerel wrapped a fluorescent substance in what?", "answer": "photographic plates and black material", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["photographic plates and black material"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "film", "extracted_entity": null}
{"question": "is there a Becquerel crater on the Moon for radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel crater is a lunar", "extracted_entity": null}
{"question": "must \u2026 One conclude from these experiments that the phosphorescent substance in question emits rays which pass through the opaque paper and reduces silver salts ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "silver nitrate", "extracted_entity": null}
{"question": "what is Henri Becquerel?", "answer": "Henri Becquerel was a famous physicist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel was a famous physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Henri Becquerel from?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": null}
{"question": "what does Henri Becquerel do?", "answer": "was a physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["was a physisist", "Henri Becquerel was a physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Newton a English physicist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton", "extracted_entity": null}
{"question": "did Newton reject the church's doctrine of the Trinity?", "answer": "Newton may have rejected the church's doctrine of the Trinity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newton may have rejected the church's doctrine of the Trinity.", "Maybe"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Newton admitted into Havard College?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "Maybe, but it doesn't say so in the article"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "when was Newton's date of birth recorded?", "answer": "Christmas Day, Decembeer 25, 1642.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas Day, Decembeer 25, 1642.", "his date of birth was recorded as Christmas Day, December 25, 1642"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "4 January 1643", "extracted_entity": null}
{"question": "how many laws of motion did Netwon have?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who did Newton see as the master creator?", "answer": "God", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["God", "Newton saw God as the master creator whose existence could not be denied in the face of the grandeur of all creation"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "the man", "extracted_entity": null}
{"question": "when did Netwon investigate the refraction of light?", "answer": "From 1670 to 1672", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1670 to 1672", "1670-1672"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1665", "extracted_entity": null}
{"question": "what principles did Newton explain for mechanics?", "answer": "The principles of conservation of momentum and angular momentum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The principles of conservation of momentum and angular momentum", "In mechanics, Newton enunciated the principles of conservation of momentum and angular momentum"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton's laws of motion", "extracted_entity": null}
{"question": "interaction with what man stirred up Newton's interest in alchemy?", "answer": "Henry More", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henry More", "The contact with the theosophist Henry More, revived his interest in alchemy"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "james Watt's improvements of what were fundamental to the changes wrought by the Industrial Revolution?", "answer": "The steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The steam engine.", "steam engine"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt's improvements to the steam engine", "extracted_entity": null}
{"question": "james Watt was born where?", "answer": "Greenock, a seaport on the Firth of Clyde", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greenock, a seaport on the Firth of Clyde"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Glasgow", "extracted_entity": null}
{"question": "what was the name of James Watt's mother?", "answer": "Agnus Muirhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Agnus Muirhead", "Agnes Muirhead"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Mary McMillan", "extracted_entity": null}
{"question": "how many of James Watt's children did not live to adulthood?", "answer": "3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "james Watt ranked first among how many people in Charles Murray's survey of historiometry?", "answer": "229", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["229"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "25", "extracted_entity": null}
{"question": "in what year did James travel to Lodon to study instrument-making?", "answer": "1753", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1753"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1803", "extracted_entity": null}
{"question": "for how many years of James Watt's life was his mother alive?", "answer": "17 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["17 years"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is James Watt most famous for?", "answer": "Steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do most Japanese people employ politeness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that Japanese borrowed a considerable number of words?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are the several dialects of Kagoshima famous to speakers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "Kagoshima dialect is famous to speakers", "extracted_entity": null}
{"question": "is Lima the capital of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima", "extracted_entity": null}
{"question": "is the Museum of Gold in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Gold Museum", "extracted_entity": null}
{"question": "does Lima have volleyball venues?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "when was Lima founded?", "answer": "January 18,1535.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["January 18,1535."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima was founded on January 18,", "extracted_entity": null}
{"question": "where is the Park of the Reserve located?", "answer": "Near the downtown area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Near the downtown area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Park of the Reserve is located in the city", "extracted_entity": null}
{"question": "what is a colectivo?", "answer": "Automobiles that renders express service on some major roads of the Lima Metropolitan Area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Automobiles that renders express service on some major roads of the Lima Metropolitan Area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "A collectivo is a shared taxi in Latin", "extracted_entity": null}
{"question": "is the National University of San Marcos in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has the San Lorenzo Megaport Project been completed?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years ago was the Lima Stock Exchange the most profitable in the world?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "2006", "extracted_entity": null}
{"question": "is rainfall very low?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "dry", "extracted_entity": null}
{"question": "is lima the capital  and  largest  city  of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima", "extracted_entity": null}
{"question": "is The GDP  per capita in Lima $ 7,600  /  ref )?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the Spanish Crown named Pizarro governor of what?", "answer": "Hemming.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hemming."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Peru", "extracted_entity": null}
{"question": "is it true that the ideas of the Enlightenment shaped the development of the city?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the ideas of the Enlightenment shaped the development of what?", "answer": "the city, Lima", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the city, Lima"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is London the capital of the United Kingdom?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "does London's population draw from a wide range of religions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Caunte take control of the English throne in 101?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "The English throne was taken by Canute.", "extracted_entity": null}
{"question": "over how many languages are spoken in London?", "answer": "Over 300 languages are spoken in london", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over 300 languages are spoken in london", "Over 300"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "300", "extracted_entity": null}
{"question": "what had the Anglo-Saxons created by the 600s?", "answer": "By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "a new settlement called Lundenwic"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "The Anglo-Saxons had created a", "extracted_entity": null}
{"question": "what city in the UK has been subjected to bouts of terrorism?", "answer": "London has been subjected to bouts of terrorism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London has been subjected to bouts of terrorism.", "London"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what countries did James VI of Scotland unite?", "answer": "James VI of Scotland united Scotland and England.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James VI of Scotland united Scotland and England.", "England and Scotland"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Scotland, England and Ireland", "extracted_entity": null}
{"question": "how did civil wars affect England during the Middle Ages?", "answer": "London remained relatively untouched", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London remained relatively untouched"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the strings of a classical lyre made of gut?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the strings of a classical lyre are made of", "extracted_entity": null}
{"question": "does a classical lyre have a sound-chest?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "lyre", "extracted_entity": null}
{"question": "is lyre played in mordern Greece?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Lyra is played in modern Greece.", "extracted_entity": null}
{"question": "which part of the strings does the left hand touch?", "answer": "The lower strings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lower strings"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the fingertips", "extracted_entity": null}
{"question": "in which place is lyre still played?", "answer": "North-eastern parts of Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North-eastern parts of Africa"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The lyre is still played in Greece.", "extracted_entity": null}
{"question": "does a classical lyre have a fingerboard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a guitar played with a plectrum like a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Michael Faraday an English chemist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was an English chemist.", "extracted_entity": null}
{"question": "was Faraday a devout Christian?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Faraday considered a gentleman?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who was a member of the Sandemanian sect of Christianity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "James", "extracted_entity": null}
{"question": "what did Joseph Henry likely discover?", "answer": "self-induction", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["self-induction"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the relationship between magnetism and electricity.", "extracted_entity": null}
{"question": "what did the University of Oxford grant Faraday?", "answer": "a Doctor of Civil Law degree (honorary)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Doctor of Civil Law degree (honorary)", "Doctor of Civil Law degree (honorary)"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was made to travel outside the coach?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The coachman", "extracted_entity": null}
{"question": "did Faraday construct the ancestor of modern power generators?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Faraday lecture on education in 1854?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday did not lecture on education in", "extracted_entity": null}
{"question": "did Faraday receive an extensive formal education?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did Faraday discover the chemical substance benzene?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "benzene", "extracted_entity": null}
{"question": "was Faraday born in England?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was born in England.", "extracted_entity": null}
{"question": "what do some historians of science refer to Faraday as?", "answer": "the best experimentalist in the history of science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the best experimentalist in the history of science", "best experimentalist in the history of science"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was the sponsor and mentor of Faraday?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sir Humphry Davy", "extracted_entity": null}
{"question": "what did Faraday invent that is used almost universally in science laboratories?", "answer": "the Bunsen burner", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Bunsen burner", "Bunsen burner"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the production of chemical weapons raise an ethical issue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, because the use of chemical weapons is just", "extracted_entity": null}
{"question": "what was the Faraday effect first called?", "answer": "diamagnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["diamagnetism"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday effect", "extracted_entity": null}
{"question": "can you meet someone through attending a church?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was Michael Faraday`s birthplace?", "answer": "Newington Butts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what was Michael Faraday`s profession?", "answer": "chemist and physicist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["chemist and physicist"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "a chemist", "extracted_entity": null}
{"question": "how many years ago did Faraday report on the first synthesis of compound make from carbon and chlorine , c 2 h 6 and c 2 h 4 ?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years ago was it when he discovered the phenomenon that he named diamagnetism , and what is now called the Faraday effect?", "answer": "164", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["164"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1845", "extracted_entity": null}
{"question": "how many years ago was it when Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames , which resulted in an oft-reprinted cartoon in Punch?", "answer": "154", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["154"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1855", "extracted_entity": null}
{"question": "how many years ago was it when he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal?", "answer": "162", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["162"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1856", "extracted_entity": null}
{"question": "was education another area  of service  for Faraday?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was faraday `s  earliest  chemical  work as an assistant  to Davy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday", "extracted_entity": null}
{"question": "who died at his house at Hampton Court on August 25, 1867?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was Michael Faraday born?", "answer": "September 22, 1791", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 22, 1791"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1791", "extracted_entity": null}
{"question": "where did he serve two terms?", "answer": "the Church of Scotland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Church of Scotland"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1841-1845", "extracted_entity": null}
{"question": "who was born in Newington Butts, near present-day South London, England?", "answer": "Michael Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "_________________", "extracted_entity": null}
{"question": "is it true that institution formed the foundation of electric motor technology?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "institution formed the foundation of what?", "answer": "modern electromagnetic technology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern electromagnetic technology"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The institution of the modern university.", "extracted_entity": null}
{"question": "was his father member of the Sandemanian sect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was faraday Christian of the small Sandemanian denomination?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Nassau Sound a body of water in Maine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Sound is a body of water in", "extracted_entity": null}
{"question": "was Nassau placed in operation by Robert Fulton?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was Fort Nassau built by?", "answer": "the Dutch", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Dutch"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Nassau Coliseum?", "answer": "an arena in Uniondale, New York, USA", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["an arena in Uniondale, New York, USA"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Veterans Memorial Coliseum", "extracted_entity": null}
{"question": "what was the Dead or Alive 4 fighting arena modeled after?", "answer": "a Magnetic Accelerator Cannon station from Halo 2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Magnetic Accelerator Cannon station from Halo 2"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The fighting arena is modeled after the Battle", "extracted_entity": null}
{"question": "is Nassau Range the highest mountain range in the world?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Mount Everest", "extracted_entity": null}
{"question": "is Nassau County named after a German town?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau County is named after a German town", "extracted_entity": null}
{"question": "does the United States have a base near Glasgow?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Yes, the USAF has a base at R", "extracted_entity": null}
{"question": "did Tesla study electrical engineering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla studied electrical engineering", "extracted_entity": null}
{"question": "was Tesla born in the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was born in the United States.", "extracted_entity": null}
{"question": "was Tesla hired by Edison?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Edison Electric Illuminating Company of New York", "extracted_entity": null}
{"question": "when did Tesla demonstrate wireless communication (radio)?", "answer": "1893", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1893"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1893", "extracted_entity": null}
{"question": "what is the SI unit measuring magnetic flux density or magnetic induction?", "answer": "the tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1 Tesla (T)", "extracted_entity": null}
{"question": "was Tesla regarded as a mad scientist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what type of current did Tesla invent?", "answer": "AC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["AC"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Alternating Current", "extracted_entity": null}
{"question": "who was the victor of the \"War of Currents\"?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla", "Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "The Tesla.", "extracted_entity": null}
{"question": "where did Tesla study electrical engineering?", "answer": "the Austrian Polytechnic in Graz ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Austrian Polytechnic in Graz ", "Austrian Polytechnic in Graz", "The Austrian Polytechnic in Graz"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "The University of Prague", "extracted_entity": null}
{"question": "is Ottawa the capital of Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Ottawa sit on the Ottowa River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do small tremors occur in Ottawa?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Ottawa's population?", "answer": "1,190,982", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,190,982"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "900,000", "extracted_entity": null}
{"question": "what are Ottawa's primary industries?", "answer": "Logging and lumber", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Logging and lumber"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa's primary industries include the public", "extracted_entity": null}
{"question": "what is Ottawa's junior ice hockey team?", "answer": "the Ottawa 67's", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa 67's"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa 67's", "extracted_entity": null}
{"question": "what is Ottawa's major league hockey team?", "answer": "the Ottawa Senators", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa Senators"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa Senators", "extracted_entity": null}
{"question": "what are Carleton University's athletic teams called?", "answer": "Carleton Ravens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carleton Ravens"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The Ravens", "extracted_entity": null}
{"question": "is Ottawa colder than Moscow in January?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa is colder than Moscow in January", "extracted_entity": null}
{"question": "is Ottawa on a river?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa River", "extracted_entity": null}
{"question": "does it snow in Ottawa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was there a cholera outbreak in 1832?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The Great Cholera Pandemic", "extracted_entity": null}
{"question": "who is the mayor of Ottawa?", "answer": "Larry O'Brien", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Larry O'Brien"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Jim Watson", "extracted_entity": null}
{"question": "how many townships are in Ottawa?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what's the main highway in Ottawa?", "answer": "Highway 417, The Queensway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Highway 417, The Queensway", "provinical Highway 417"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Highway 417", "extracted_entity": null}
{"question": "what was Ottawa's name in 1850?", "answer": "Bytown", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bytown"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa", "extracted_entity": null}
{"question": "based on the average, in what year will the next small tremor occur in Ottawa?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "2012", "extracted_entity": null}
{"question": "where were immigrants from in the 1800s?", "answer": "Irish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish", "Ireland"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Scotland", "extracted_entity": null}
{"question": "are there also discussions of having OC Transpo Light Rail and STO Transitway networks linking on the Gatineau side of the Ottawa River ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, there are discussions of having OC", "extracted_entity": null}
{"question": "does the giant otter live in South America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do otters have claws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "the otter is a member of the family Must", "extracted_entity": null}
{"question": "do otters eat fish?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "why are otters vulnerable to prey depletion?", "answer": "Prey-dependence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Prey-dependence", "prey-dependency"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "because they are apex predators.", "extracted_entity": null}
{"question": "do otters enjoy playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do sea otters have a layer of fat like whales?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "sea otters have a layer of fat like wh", "extracted_entity": null}
{"question": "is the otter in the same family as the badger?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there any cities named Santiago in the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago, California", "extracted_entity": null}
{"question": "was Santiago the name of an indie/punk band?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Santiago a name in Spanish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the origin of the name Santiago?", "answer": "Spanish for St. James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spanish for St. James", "sanit + iago, James"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the Spanish form of the Greek name", "extracted_entity": null}
{"question": "who has released an album or song named Santiago?", "answer": "The Chieftains; Loreena McKennitt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Chieftains; Loreena McKennitt", "The Chieftains and Loreena McKennitt"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "(1)", "extracted_entity": null}
{"question": "what is the Order of Santiago?", "answer": "a Spanish knightly order", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Spanish knightly order", "a Spanish Knightly order"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Order of Santiago is a Catholic religious order founded", "extracted_entity": null}
{"question": "what notable people have been named Santiago?", "answer": "Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "Benito, Hugo, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what fictional stories include a main character named Santiago?", "answer": "Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "Interview with the Vampire, The Alchemist, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Alchemist", "extracted_entity": null}
{"question": "is Santiago the national capital of a country?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the capital of Chile", "extracted_entity": null}
{"question": "is santiago Spanish for St. James  ( from santo ,  saint  +  iago ,  James  )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is santiago Spanish for St. James?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Santiago?", "answer": "Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry."], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago", "extracted_entity": null}
{"question": "has swahili no diphthongs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a third  prefix the object  prefix?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is      / ref Swahili unusual among sub-Saharan languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are vowels never reduced , regardless of stress ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the Australian Black Swan have white feathers on its wings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Australian Black Swan", "extracted_entity": null}
{"question": "do swans belong to the family Anatidae?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can black swans swim with only one leg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are young swans known as?", "answer": "cygnets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cygnets", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "what is the Irish legend of the Children of Lir about?", "answer": "a stepmother transforming her children into swans for 900 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a stepmother transforming her children into swans for 900 years", "A stepmother transforming her children into swans for 900 years"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The story is about the Children of Lir,", "extracted_entity": null}
{"question": "what is the Sanskrit word for swan?", "answer": "hamsa or hansa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hamsa or hansa", "Hamsa or hansa"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "hamsa", "extracted_entity": null}
{"question": "which album was the song \"The Bonny Swans\" from?", "answer": "The Mask and Mirror", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Mask and Mirror"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Bonny Swans is a song by the", "extracted_entity": null}
{"question": "what is the Polish word for swan?", "answer": "\u0141ab\u0119d\u017a", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["\u0141ab\u0119d\u017a"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cyganie", "extracted_entity": null}
{"question": "which ballet by Pyotr Tchaikovsky is partially based on an Ancient German legend about a princess who was turned into a swan?", "answer": "Swan Lake", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swan Lake"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swan Lake", "extracted_entity": null}
{"question": "are swan birds of the family Anatidae?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do swans feature strongly in mythology?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes, they are the symbol of the sun in", "extracted_entity": null}
{"question": "is the Trumpeter Swan the largest bird?", "answer": "It is the largest bird of North America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the largest bird of North America.", "The Trumpeter Swan is the largest bird of North America"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No, the Andean Condor is the", "extracted_entity": null}
{"question": "what is the name of young swans?", "answer": "Cygnets.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cygnets.", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "where is the word \"swan\" derived from?", "answer": "Old English swan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old English swan.", "The Old English word \"swan,\" derived from the Indo-European root \"*swen\", meaning to sound or to sing"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "from Old English swan, from Proto-", "extracted_entity": null}
{"question": "what is the best known story about a swan?", "answer": "The Ugly Duckling fable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Ugly Duckling fable.", "The Ugly Duckling"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are swans and ducks birds of the same family?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Anatidae are a family of waterf", "extracted_entity": null}
{"question": "what is the name of an adult female?", "answer": "Pen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pen.", "An adult female swan is called a pen."], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Woman", "extracted_entity": null}
{"question": "what is the color of the Australian Black Swan?", "answer": "Completely black except for the white flight feathers on its wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Completely black except for the white flight feathers on its wings.", "Completely black except for the white feathers on its wings"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Black Swan", "extracted_entity": null}
{"question": "is the Tiger a member of the Felidar family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "are tigers solitary animals?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are panthers a type of tiger?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what do tigers' powerfully built legs and shoulders allow tigers to do?", "answer": "They have the ability to pull down prey substantially heavier than themselves.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They have the ability to pull down prey substantially heavier than themselves.", "have the ability to pull down prey substantially heavier than themselves"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Tigers' powerfully built legs and shoulders allow", "extracted_entity": null}
{"question": "what would a tiger do when seized by a crocodile?", "answer": "A tiger will strike at the reptile's eyes with its paws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A tiger will strike at the reptile's eyes with its paws.", "strike at the reptile's eyes with its paws"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "The tiger would try to escape by using its", "extracted_entity": null}
{"question": "what are the typical features of a tiger country?", "answer": "It will always have good cover, it will always be close to water and plenty of prey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It will always have good cover, it will always be close to water and plenty of prey.", "Good cover, close to water, and plenty of prey "], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "would it be common for tigers to mate in January?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "tiger", "extracted_entity": null}
{"question": "about how many square kilometers would be needed to house 3 female tigers?", "answer": "60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 square kilometers", "60"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "would a tiger be likely to feedon smaller animals, such as mice?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the predecessors to trumpets have valves?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The predecessors to trumpets had val", "extracted_entity": null}
{"question": "are trumpets constructed of brass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is the bass trumpet the smallest trumpet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what musical styles is the trumpet used in?", "answer": "Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Jazz, Classical, and Military", "extracted_entity": null}
{"question": "what is the most common type of trumpet?", "answer": "The B trumpet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The B trumpet."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "B-flat trumpet", "extracted_entity": null}
{"question": "how does a trumpet produce sound?", "answer": "By blowing air through closed lips.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By blowing air through closed lips."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The trumpet produces sound by the vibration", "extracted_entity": null}
{"question": "how do trumpets compensate for wolf tones?", "answer": "Somce trumpetes have a slide mechanism built in to compensate.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Somce trumpetes have a slide mechanism built in to compensate."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Miles Davis play the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "does a trumpet have a mellower tone than a cornet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Trumpet", "extracted_entity": null}
{"question": "is the name of the upper shell of a turtle called the plastron?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do scutes fall away from the turtle's shell?", "answer": "Some of the scutes eventually fall away from the shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some of the scutes eventually fall away from the shell.", "yes, eventually"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "the turtle's shell", "extracted_entity": null}
{"question": "do Terrestrial tortoises have short feet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many groups are turtles broken down into?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two", "two"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where are the only surviving giant tortoises?", "answer": "They are on the Seychelles and Gal\u00e1pagos Islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are on the Seychelles and Gal\u00e1pagos Islands.", "Seychelles and Gal\u00e1pagos Islands"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Aldabra Atoll", "extracted_entity": null}
{"question": "how do turtles chew food?", "answer": "Turtles use their jaws to cut and chew food.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles use their jaws to cut and chew food.", "Turtles use their jaws to cut and chew food"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "they have teeth", "extracted_entity": null}
{"question": "are tortoises land based?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where are turtle eggs layed?", "answer": "Turtles lay eggs on land.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles lay eggs on land.", "on land"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtle eggs are laid in nests that", "extracted_entity": null}
{"question": "is turtle soup considered a delicacy?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "turtle soup is considered a delicacy", "extracted_entity": null}
{"question": "are Testudines the crown group of the superorder Chelonia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do turtles breathe air?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are harvesting wild turtles legal anywhere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtle farms in the United States", "extracted_entity": null}
{"question": "approximately how many species of Testudines are alive today?", "answer": "300", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is harvesting wild turtles legal?", "answer": "Florida", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Florida"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was the largest ever chelonian?", "answer": "Archelon ischyros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Archelon ischyros", "The great letherback sea tutrtle"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "The largest ever chelonian is the giant tort", "extracted_entity": null}
{"question": "is there a way to approximate the age of a turtle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, using the growth rings in the shell", "extracted_entity": null}
{"question": "can turtles spend all their time underwater?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are violinists and fiddlers the same thing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are violins a single size?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "are violas and cellos in the same family of instruments as violins?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "where does the word \"violin\" come from?", "answer": "the Middle Latin word vitula, meaning \"stringed instrument\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Middle Latin word vitula, meaning \"stringed instrument\"", "the Middle Latin word vitula"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violin", "extracted_entity": null}
{"question": "what is someone who makes violins called?", "answer": "a luthier, or simply a violin maker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a luthier, or simply a violin maker", "a luthier"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many strings does a violin usually have?", "answer": "four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["four", "4"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "4 strings", "extracted_entity": null}
{"question": "what are violins made of?", "answer": "different types of wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different types of wood", "maple, ebony, sheep gut"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The violin is a stringed musical instrument,", "extracted_entity": null}
{"question": "how long have people been making instruments like violins?", "answer": "since ancient times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["since ancient times", "since 1555"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1700", "extracted_entity": null}
{"question": "whate is the usual pitch range of a violin?", "answer": "from G3 to C8", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from G3 to C8"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "400 Hz to 42", "extracted_entity": null}
{"question": "are xylophone bars made of rosewood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone bars are made of rosewood", "extracted_entity": null}
{"question": "is the xylophone a precursor to the vibraphone?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone", "extracted_entity": null}
{"question": "were ancient mallets made of copper?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did the xylophone originate?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the xylophone originated in Africa", "extracted_entity": null}
{"question": "what is the earliest historical reference in Europe?", "answer": "Arnold Schlick's Spiegel der Orgelmacher und Organisten", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arnold Schlick's Spiegel der Orgelmacher und Organisten"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how are western-style xylophones characterised?", "answer": "by a bright, sharp tone and high register", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by a bright, sharp tone and high register"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can a xylophone be 3 octaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a short bar follow a long bar?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did vibraphones exist in 1930?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Alessandro Volta a professor of chemistry?", "answer": "Alessandro Volta was not a professor of chemistry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was not a professor of chemistry.", "No"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Alessandro Volta invent the remotely operated pistol?", "answer": "Alessandro Volta did invent the remotely operated pistol.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta did invent the remotely operated pistol.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The electric gun", "extracted_entity": null}
{"question": "was Alessandro Volta taught in public schools?", "answer": "Volta was taught in public schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was taught in public schools.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was not taught in public schools", "extracted_entity": null}
{"question": "who did Alessandro Volta marry?", "answer": "Alessandro Volta married Teresa Peregrini.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta married Teresa Peregrini.", "Teresa Peregrini"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Alessandro Volta invent in 1800?", "answer": "In 1800, Alessandro Volta invented the voltaic pile.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800, Alessandro Volta invented the voltaic pile.", "voltaic pile"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The voltaic pile", "extracted_entity": null}
{"question": "what is the battery made by Alessandro Volta credited as?", "answer": "The battery made by Volta is credited as the first electrochemical cell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The battery made by Volta is credited as the first electrochemical cell.", "the first electrochemical cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Voltaic pile", "extracted_entity": null}
{"question": "did Alessandro Volta die and retire in the same place?", "answer": "Alessandro Volta retired and died in the same place.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta retired and died in the same place.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta died in Como, Italy.", "extracted_entity": null}
{"question": "when did Alessandro Volta improve  and popularize the electrophorus?", "answer": "Alessandro Volta improved and popularized the electrophorus in 1775.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta improved and popularized the electrophorus in 1775.", "1775"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1775", "extracted_entity": null}
{"question": "how long was Alessandro Volta a professor at the University of Pavia?", "answer": "Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "almost 25 years"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1779-1794", "extracted_entity": null}
{"question": "was Alessandro Volta an Egyptian?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was an Italian physicist", "extracted_entity": null}
{"question": "was Volta taught in public schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Volta made a count in 1810?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Count of Volta", "extracted_entity": null}
{"question": "who made Volta a count?", "answer": "Napoleon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Napoleon"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the king", "extracted_entity": null}
{"question": "where was Volta born?", "answer": "Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Como", "Como, Italy"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta, Ghana", "extracted_entity": null}
{"question": "when did Volta retire?", "answer": "1819", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1819", "In 1819."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "2013", "extracted_entity": null}
{"question": "a year before improving and popularizing the electrophorus, what did Volta become?", "answer": "A professor of physics at the Royal School in Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A professor of physics at the Royal School in Como"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "a professor of physics at the Royal School in Como", "extracted_entity": null}
{"question": "was Volta buried where he died or was he buried someplace else?", "answer": "where he died", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where he died", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was buried in the Cimeti\u00e8re de", "extracted_entity": null}
{"question": "did Volta marry before he became professor of experimental physics at the University of Pavia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1810?", "answer": "Volta was made a count by Napoleon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The War of 1812", "extracted_entity": null}
{"question": "wasn't Alessandro Volta born in Como?", "answer": "Yes, Volta was born in Como, Italy and was taught in the public schools there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was born in Como, Italy and was taught in the public schools there."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was born in Como.", "extracted_entity": null}
{"question": "was Alessandro Volta born in Como?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who became a professor of physics at the Royal School in Como?", "answer": "Volta.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Galileo Galilei", "extracted_entity": null}
{"question": "when was Volta made a count by Napoleon?", "answer": "Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1808", "extracted_entity": null}
{"question": "is Volta's legacy celebrated by a Temple on the shore of Lake Como in the center of the town?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the battery made by Volta credited as the first electrochemical cell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Volta made a count by Napoleon in 1810?", "answer": "Yes, Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Count Volta", "extracted_entity": null}
{"question": "in what year did he become a professor of physics at the Royal School in Como?", "answer": "1774", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1774"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1700", "extracted_entity": null}
{"question": "was Avogadro a  professor at the University of Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avogadro was a professor at the University of Turin."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was he a member of the Royal Superior Council on Public Instruction?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes,  Avogadro was a member of the Royal Superior Council on Public Instruction."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The Royal Superior Council on Public Instruction", "extracted_entity": null}
{"question": "is Avogadro's number used to compute the results of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avagadro's number is used to compute the results of chemical reactions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who first calculated the value of Avogadro's number?", "answer": "Johann Josef Loschmidt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Johann Josef Loschmidt", "Johann Josef Loschmidt first calculated the value of Avogadro's number."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Johann Josef Loschmidt", "extracted_entity": null}
{"question": "what does Avogadro's Law state?", "answer": "The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "Avogadro's Law states that the relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's law states that the", "extracted_entity": null}
{"question": "who showed that Avogadro's theory held in dilute solutions?", "answer": "Jacobus Henricus van Hoff", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacobus Henricus van Hoff", "Jacobus Henricus van 't Hoff showed that Avogadro's theory holds in dilute solutions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in what language was his 1811 paper published?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French", "Avogadro's 1811 paper was published in French."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "German", "extracted_entity": null}
{"question": "who was Avogadro's wife?", "answer": "Felicita Mazz", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felicita Mazz", "Felicita Mazz\u00e9 was Avogadro's wife."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Lina Avogadro", "extracted_entity": null}
{"question": "why did Avogadro lose his chair at the University of Turin?", "answer": "He was active in the revolutionary movements of 1821 against the king of Sardinia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was active in the revolutionary movements of 1821 against the king of Sardinia", "Avogadro lost his chair at the University of Turin because he was active in the revolutionary movements of 1821 against the king of Sardinia."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Amedeo Avogadro was born in Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Avogadro hailed as a founder of the atomic-molecular theory?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "did Johann Josef Loschmidt first calculate the value of Avogadro's number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "in 1820, Avogadro became a professor of physics where?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "in Turin", "extracted_entity": null}
{"question": "avogadro did not actually use what word?", "answer": "atom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atom"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the number of elementary entities in 1 mole of a substance is known as what?", "answer": "Avogadro constant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro constant", "Avogadro's constant"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Atomic number", "extracted_entity": null}
{"question": "what would a German call Avogadro's number?", "answer": "Loschmidt number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loschmidt number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number.", "extracted_entity": null}
{"question": "is Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he become a professor before the revolutionary movements against the king of Sardinia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "he was a professor at the University of Turin", "extracted_entity": null}
{"question": "do ants belong to the Hymenoptera order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are ants used in cuisine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "[cuisine]", "extracted_entity": null}
{"question": "does an ant's head contain sensory organs?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how do most ants travel?", "answer": "most ants travel by walking", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["most ants travel by walking", "by walking"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Formicidae", "extracted_entity": null}
{"question": "in ant colonies, what are the fertile female ants called?", "answer": "queens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["queens"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "queen", "extracted_entity": null}
{"question": "who wrote about ants in A Tramp Abroad?", "answer": "mark twain wrote about ants", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mark twain wrote about ants", "Mark Twain"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Mark Twain", "extracted_entity": null}
{"question": "do the ants eat plants, meats, or both?", "answer": "both", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["both"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "ants eat plants", "extracted_entity": null}
{"question": "what organs gives a bull ant its good sight?", "answer": "compound eyes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["compound eyes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "the eye", "extracted_entity": null}
{"question": "what may happen to red fire ants if we use boiling water on the queen?", "answer": "nests of red fire ants may be destroyed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nests of red fire ants may be destroyed", "die"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "the queen will die and the colony will die", "extracted_entity": null}
{"question": "are ants found in Antartica?", "answer": "No, there are no ants in antarctica.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, there are no ants in antarctica."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do male ants take flight before females?", "answer": "Yes, male ants take flight before females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, male ants take flight before females."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do worker ants have wings?", "answer": "No, worker ants do not have wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, worker ants do not have wings."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the three segments of an ant?", "answer": "The head, mesosoma and metasoma are the three distinct body segments.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The head, mesosoma and metasoma are the three distinct body segments."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "head, thorax and abdomen", "extracted_entity": null}
{"question": "what are the ant colonies that lack queens called?", "answer": "Colonies that lack queens are called gamergate colonies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colonies that lack queens are called gamergate colonies."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where are bullet ants located?", "answer": "Bullet ants are located in Central and South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bullet ants are located in Central and South America."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do ants belong to the same order as bees?", "answer": "Yes, ants belong to the same order as bees.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, ants belong to the same order as bees."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what is the most common color of ants?", "answer": "Most ants are red or black.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most ants are red or black."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Red", "extracted_entity": null}
{"question": "do all ants build nests?", "answer": "No, not all ants build nests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, not all ants build nests."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, all ants build nests.", "extracted_entity": null}
{"question": "ultraviolet vision was first discovered in ants by Sir who in 1881?", "answer": "John Lubbok", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Lubbok"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do ants thrive in most ecosystems?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Ants thrive in most ecosystems", "extracted_entity": null}
{"question": "are several species of ants able to use the Earth's magnetic field?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are termites actually more closely related to cockroaches as well as mantids?", "answer": "Yes, termites are actually more closely related to cockroaches as well as mantids.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, termites are actually more closely related to cockroaches as well as mantids."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "termites", "extracted_entity": null}
{"question": "do some caterpillars produce vibrations as well as sounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The sound of a caterpillar is", "extracted_entity": null}
{"question": "did ants evolve from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the ant a marsupial?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are they easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist?", "answer": "Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Antwerp a city?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp", "extracted_entity": null}
{"question": "is Antwerp a municipality?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a municipality.", "extracted_entity": null}
{"question": "is Antwerp in Belgium?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is in Belgium.", "extracted_entity": null}
{"question": "what is Antwerp?", "answer": "Antwerp is a city and municipality in Belgium.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is a city and municipality in Belgium."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a city in Belgium, the", "extracted_entity": null}
{"question": "what is the population of the city of Antwerp?", "answer": "Antwerp's population is 472,071.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp's population is 472,071."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "520,504", "extracted_entity": null}
{"question": "where is the city of Antwerp?", "answer": "Antwerp is in Belgium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is in Belgium"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp", "extracted_entity": null}
{"question": "why is Antwerp important to fashion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "because it is the birthplace of fashion", "extracted_entity": null}
{"question": "antwerp is to the east of what river?", "answer": "Antwerp is to the east of the Scheldt river", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is to the east of the Scheldt river"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Scheldt", "extracted_entity": null}
{"question": "how many municipalities does Antwerp have?", "answer": "None, Antwerp is a municipality.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None, Antwerp is a municipality."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "19", "extracted_entity": null}
{"question": "is Arabic a Central Semitic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Arabic a major vehicle of culture in Europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "The Arabic language is the language of the Q", "extracted_entity": null}
{"question": "is Arabic classified alongside Semitic languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is a Semitic language.", "extracted_entity": null}
{"question": "how many people speak the Arabic language?", "answer": "280 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["280 million people."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "280 million people speak the Arabic", "extracted_entity": null}
{"question": "when was Arabic calligraphy invented?", "answer": "Many styles were developed after 786.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many styles were developed after 786."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Arabic spoken?", "answer": "The Middle East and North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Middle East and North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is Arabic related to Islam?", "answer": "Arabic is the liturgical language of Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic is the liturgical language of Islam"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is a language spoken by Muslims and", "extracted_entity": null}
{"question": "why does Arabic heavily influence European languages?", "answer": "Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic heavily influences European languages because it was", "extracted_entity": null}
{"question": "is Arabic the largest member of the Semitic language family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Modern Standard Arabic continue to evolve like other languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Hassaniya Arabic spoken in Mauritania?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is the only variety of modern Arabic that has acquired official language status?", "answer": "maltese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["maltese", "Maltese"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Modern Standard Arabic (MSA)", "extracted_entity": null}
{"question": "egyptian Arabic is spoken by how many in Egypt?", "answer": "76 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["76 million"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "45 million", "extracted_entity": null}
{"question": "where are the Western Arabic numerals used?", "answer": "North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North Africa", "present-day North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Middle East", "extracted_entity": null}
{"question": "the most active Academies of Arabic Language are found where?", "answer": "damascus and cairo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["damascus and cairo"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "hassan Massoudy is a master of what genre?", "answer": "Hassan Massoudy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hassan Massoudy", "Arabic calligraphy"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Berlin the capital city of Germany?", "answer": "Berlin is the capital city of Germany.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is the capital city of Germany.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Berlin the largest city in Germany?", "answer": "Berlin is Germany's largest city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is Germany's largest city.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Schloss Charlottenburg the largest existing palace in Berlin?", "answer": "Schloss Charlottenburg is the largest existing palace in Berlin. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Schloss Charlottenburg is the largest existing palace in Berlin. ", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Schloss Charlottenburg", "extracted_entity": null}
{"question": "which building is the site of the German parliament?", "answer": "The Reichstag building is the site of the German parliament. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building is the site of the German parliament. ", "The Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Reichstag", "extracted_entity": null}
{"question": "which two sports events did the Olympiastadion host?", "answer": "The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "the 1936 Summer Olympics and the 2006 FIFA World Cup final"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is the Berliner Dom located?", "answer": "The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "on the Spree Island"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berliner Dom", "extracted_entity": null}
{"question": "which Berlin building did architect Norman Foster remodel in the 1990s?", "answer": "The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Reichstag", "extracted_entity": null}
{"question": "why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?", "answer": "Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "Because burning lignite produces harmful emissions"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Vattenfall shifting towards reliance on cleaner", "extracted_entity": null}
{"question": "what are the names of the two zoos in Berlin?", "answer": "The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "Zoologischer Garten Berlin and Tierpark Friedrichsfelde"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Tierpark Berlin and Berlin Zoological Garden", "extracted_entity": null}
{"question": "is Berlin the headquarters of Springer?", "answer": "Yes, Berlin is the headquarters of Springer.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Berlin is the headquarters of Springer."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Gendarmenmarkt border the French Cathedral?", "answer": "Yes, the Gendarmenmarkt borders the French Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the Gendarmenmarkt borders the French Cathedral."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the Nauen plain north of Berlin?", "answer": "No, the Nauen plain stretches to the west of Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the Nauen plain stretches to the west of Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Nauen Plateau", "extracted_entity": null}
{"question": "where is the bust of Queen Nefertiti?", "answer": "The bust of Queen Nefertiti is in the Altes Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The bust of Queen Nefertiti is in the Altes Museum."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "in the Egyptian Museum in Berlin.", "extracted_entity": null}
{"question": "where does the German President live?", "answer": "The German President lives west of the center, Schloss Bellevue.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The German President lives west of the center, Schloss Bellevue."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Berlin give up its status as a free Hanseatic city?", "answer": "In 1451 Berlin gave up its status as a free Hanseatic city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1451 Berlin gave up its status as a free Hanseatic city."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "1709", "extracted_entity": null}
{"question": "which is the busiest airport in Berlin?", "answer": "Tegel International Airport is the busiest airport in Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tegel International Airport is the busiest airport in Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Tegel Airport", "extracted_entity": null}
{"question": "how many occupation zones was Germany divided into after the war?", "answer": "Germany was divided into four occupation zones after the war.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany was divided into four occupation zones after the war."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "four", "extracted_entity": null}
{"question": "where was Popkomm held before moving to Berlin?", "answer": "Popkomm was held in Cologne before moving to Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Popkomm was held in Cologne before moving to Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin", "extracted_entity": null}
{"question": "who was Blaise Pascal's father?", "answer": "His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "\u00c3\u2030tienne Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "\u00c9tienne Pascal", "extracted_entity": null}
{"question": "was Pascal a French mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal was a French mathematician.", "extracted_entity": null}
{"question": "did Pascal have poor health throughout his life?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how old was Pascal when he lost his mother?", "answer": "at the age of three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the age of three", "3."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "19 years old.", "extracted_entity": null}
{"question": "who was Pascal's younger sister?", "answer": "Jacqueline", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacqueline", "Jacqueline."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Marie", "extracted_entity": null}
{"question": "what led Pascal to his religious conversion?", "answer": "Two basic influences led him to his conversion: sickness and Jansenism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two basic influences led him to his conversion: sickness and Jansenism", "sickness and Jansenism"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the \"Jesus Prayer\"", "extracted_entity": null}
{"question": "how old was Pascal when he died?", "answer": "39", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["39", "1662-08-19."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "39", "extracted_entity": null}
{"question": "did Pascal write about cycloid before 1658?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The answer is:", "extracted_entity": null}
{"question": "is there a programming language called Pascal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "crowds of believers came to see and kiss the thorn; all of Catholic where acclaimed a miracle?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The Holy Thorn is the relic of the", "extracted_entity": null}
{"question": "wasn't Blaise Pascal a work of Desargues on conic sections?", "answer": "Yes, it was", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it was"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The answer is a conic section.", "extracted_entity": null}
{"question": "who died the next morning?", "answer": "Pascal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did his father die?", "answer": "1651", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1651", "His father died in 1651."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "1911", "extracted_entity": null}
{"question": "was he a child prodigy who was educated by his father, a civil servant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Pascal's earliest work in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal's Arithmetical Triangle.", "extracted_entity": null}
{"question": "did Pascal also write in defense of the scientific method?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "do butterflies make sounds?", "answer": "Some butterflies make sounds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some butterflies make sounds."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do butterflies have two eyes?", "answer": "Yes, butterflies have two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, butterflies have two eyes."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does a black moth mean that someone has died?", "answer": "In the Philippines a black butterfly or moth mean that someone has died.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Philippines a black butterfly or moth mean that someone has died."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what is the outer layer of the cuticle made of?", "answer": "The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "The outer layer of the cuticle is made of", "extracted_entity": null}
{"question": "where was there a vast swarm of butterflies?", "answer": "In Kyoto there was a vast swarm of butterflies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Kyoto there was a vast swarm of butterflies."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "In the 19th century, a vast", "extracted_entity": null}
{"question": "what butterfly is migratory?", "answer": "The Monarch butterfly is migratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monarch butterfly is migratory."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Monarch", "extracted_entity": null}
{"question": "do butterflies carry more pollen than Hymenoptera?", "answer": "No, butterflies carry less pollen than Hymenoptera.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, butterflies carry less pollen than Hymenoptera."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what allows the butterfly to evade predators?", "answer": "A butterfly's hind wings allow the butterfly to evade predators.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A butterfly's hind wings allow the butterfly to evade predators."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "the butterfly's wings are covered with", "extracted_entity": null}
{"question": "what happens if a wing is removed?", "answer": "If a ring is removed the other three will grow to a larger size.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["If a ring is removed the other three will grow to a larger size."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "the wing is removed", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a Spanish Biologist?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, he was a French physicist."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the SI unit of charge named after Charles-Augustin de Coulomb?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, the SI unit of charge, the coulomb, was named after him."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb ever employed at La Rochelle?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, upon his return to France, with the rank of Captain, he was employed at La Rochelle."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb was never", "extracted_entity": null}
{"question": "what is Charles-Augustin de Coulomb best known for?", "answer": "He is best known for developing Coulomb's law.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He is best known for developing Coulomb's law."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Coulomb force, which he discovered.", "extracted_entity": null}
{"question": "whose ideas inspired Charles-Augustin de Coulomb's experiments on the resistance of masonries?", "answer": "Pieter van Musschenbroek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pieter van Musschenbroek.", "He carried out several experiments on the resistance of masonries which were inspired by the ideas of Pieter van Musschenbroek on friction."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Jacques Aymar-Vernay", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb join his father's family in Montpeillier?", "answer": "From 1757 to 1759 he joined his father's family in Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1757 to 1759 he joined his father's family in Montpellier."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1732", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb come from a poor family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, his mother came from a wealthy family in the wool trade."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb came from", "extracted_entity": null}
{"question": "what is Coulomb's law?", "answer": "The definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The definition of the electrostatic force of attraction and repulsion."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb's law states that the magnitude", "extracted_entity": null}
{"question": "where is Coll\u00e8ge des Quatre-Nations located?", "answer": "In Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Montpellier.", "In Paris."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coll\u00e8ge des Quatre-Nations", "extracted_entity": null}
{"question": "what happened in 1785?", "answer": "Coulomb presented his three reports on Electricity and Magnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb presented his three reports on Electricity and Magnetism"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1757 to 1759?", "answer": "Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "regarding this topic, what did the Revolution do?", "answer": "The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The Revolution did not do anything.", "extracted_entity": null}
{"question": "who determined to pursue mathematics?", "answer": "Charles-Augustin de Coulomb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Coulomb return to Paris?", "answer": "In 1759.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1759."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1794", "extracted_entity": null}
{"question": "does Coulomb leave a legacy as a pioneer in the field of geotechnical engineering for his contribution to retaining wall design?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb left a legacy as a pioneer", "extracted_entity": null}
{"question": "did the family move to Paris as well as there Coulomb studied at the prestigious Collge des Quatre-Nations?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Coulomb born in Angoulme, France to a well-to-do family?", "answer": "Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb was born in Angoul\u00eame,", "extracted_entity": null}
{"question": "is he best known for developing Coulomb's law, the definition of the electrostatic force of attraction and repulsion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the SI unit of charge, the coulomb, was named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb", "extracted_entity": null}
{"question": "did Coulomb die in Angoul\u00c3\u00aame, France, to a well-to-do family?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "did Coulomb die in Angoul\u00c3\u00aa", "extracted_entity": null}
{"question": "are all spoken varieties of Chinese tonal and analytical?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is fifty percent or more of Korean vocabulary of Chinese origin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does \"ketchup\" come from the Minnan pronunciation of \u9bad\u6c41 (koe-tsiap)?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "ketchup", "extracted_entity": null}
{"question": "what is the most common romanization standard for Standard Mandarin today?", "answer": "Hanyu Pinyin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hanyu Pinyin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "pinyin", "extracted_entity": null}
{"question": "what is the official standard language used by the People's Republic of China, the Republic of China, and Singapore (where it is called \"Huayu\")?", "answer": "Putonghua / Guoyu, often called \"Mandarin\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Putonghua / Guoyu, often called \"Mandarin\"", "Standard Mandarin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Mandarin", "extracted_entity": null}
{"question": "how many head entries for character definitions does the Zhonghua Zihai \u4e2d\u534e\u5b57\u6d77 (1994) contain?", "answer": "85,568 head entries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["85,568 head entries", "54,678"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "143,587", "extracted_entity": null}
{"question": "what does the PRC government classify as literacy amongst workers?", "answer": "knowledge of 2,000 characters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["knowledge of 2,000 characters", "a knowledge of 2000 characters"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The PRC government classifies literacy amongst workers", "extracted_entity": null}
{"question": "how is South China linguistically different from North China?", "answer": "South China displays more linguistic diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["South China displays more linguistic diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "\u5348\u9910\u76d2 \u201clunchbox or boxed lunch\u201d (from bento) and \u6599\u7406 \u201cprepared cuisine\u201d are loan words from which language?", "answer": "Japanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japanese", "Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "\u65e5\u672c \u201cJapan\u201d", "extracted_entity": null}
{"question": "what is the standardized form of spoken chinese?", "answer": "The standardized form of spoken Chinese is Standard Mandarin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The standardized form of spoken Chinese is Standard Mandarin.", "The standardized from of spoken Chinese is Standard Mandarin."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "\u666e\u901a\u8bdd", "extracted_entity": null}
{"question": "is diglossia a common feature in mainland China and Taiwan?", "answer": "Yes, diglossia is a common feature in mainland China and Taiwan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, diglossia is a common feature in mainland China and Taiwan."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do linguists often view Chinese as a language family?", "answer": "Yes, linguists often view Chinese as a language family.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, linguists often view Chinese as a language family."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Chinese is considered as a language family,", "extracted_entity": null}
{"question": "when did classical Chinese literature begin?", "answer": "Classical Chinese literature began in the Spring and Autumn period.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Classical Chinese literature began in the Spring and Autumn period.", "Classical Chinese literature began in the 14th to 11th centuires BCE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does the government of the People's Republic of China often refer to all divisions of the Sinitic language(s) besides Standard Mandarin?", "answer": "The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "The PRC commonly refers to all divisons of the Sinitic language(s) beside Standard Mandarin as fangyan, or \"regional tongues.\""], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Hanyu", "extracted_entity": null}
{"question": "when was Middle Chinese used?", "answer": "Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "Middle Chinese was the language used in the 6th through 1-th centuries CE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how is the pronunciation of foreign words determined?", "answer": "The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "Foreign words are transliterated into close-match existing Chinese morphemes."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "the pronunciation of foreign words is determined by", "extracted_entity": null}
{"question": "what is the total number of Chinese words?", "answer": "The Hanyu Da Zidian includes 54,678 head entries for characters.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Hanyu Da Zidian includes 54,678 head entries for characters."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "49936", "extracted_entity": null}
{"question": "what language was used in the Zhou and Southern Dynasties?", "answer": "Old Chinese was the language common during the early and middle Zhou Dynasty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old Chinese was the language common during the early and middle Zhou Dynasty."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Classical Chinese", "extracted_entity": null}
{"question": "is a cougar also known as a mountain lion?", "answer": "A cougar is also known as a mountain lion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cougar is also known as a mountain lion.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "are cougars larger than jaguars?", "answer": "Cougars are not larger than jaguars.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cougars are not larger than jaguars.", "no"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "cougars are larger than jaguars.", "extracted_entity": null}
{"question": "does the World Conservation Union consider the cougar a \"least concern\" species?", "answer": "The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "cougar", "extracted_entity": null}
{"question": "what are some of the cougar's primary food sources?", "answer": "The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses, and sheep"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "deer, elk, moose, and", "extracted_entity": null}
{"question": "how long are cougar adult males (from nose to tail)?", "answer": "The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "2.4 meters"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "cougar", "extracted_entity": null}
{"question": "how long is an adult cougar's paw print?", "answer": "An adult cougar's paw print is approximately 10 cm (4 inches) long.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An adult cougar's paw print is approximately 10 cm (4 inches) long.", "4 inches"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "a cougar's paw print is", "extracted_entity": null}
{"question": "what are the three heaviest cats in the world?", "answer": "The three heaviest cats in the world are the tiger, lion and jaguar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three heaviest cats in the world are the tiger, lion and jaguar.", "tiger, lion, and jaguar"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "liger, tigon, and leop", "extracted_entity": null}
{"question": "on average, are cougar males heavier than females?", "answer": "On average, cougar males are heavier than females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On average, cougar males are heavier than females.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Cougar males are heavier than females.", "extracted_entity": null}
{"question": "what are the three cat species that are native to Canada?", "answer": "The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "cougar, bobcat, Canadian lynx"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Canada lynx, bobcat, and", "extracted_entity": null}
{"question": "is one of the parts of a cymbal called a disc?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbal", "extracted_entity": null}
{"question": "are ride cymbals very common?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Fender make cymbals?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, Fender does not make cymb", "extracted_entity": null}
{"question": "what part of the cymbal is the bell?", "answer": "The center of a Cymbal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The center of a Cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "bell", "extracted_entity": null}
{"question": "what's the timber of ancient cimbals like? disfluent?", "answer": "like that of small hand-bells or of the notes of the keyed harmonica", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["like that of small hand-bells or of the notes of the keyed harmonica"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "oak", "extracted_entity": null}
{"question": "what does it sound like to play cymbals with a cello bow?", "answer": "squealing car brakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["squealing car brakes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals and cello are two different", "extracted_entity": null}
{"question": "what are cymbals made of?", "answer": "brass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many cymbals typically go in a drum kit?", "answer": "at least 3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at least 3"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what kinds of musical groups use cymbals?", "answer": "Cymbals are used in modern orchestras and many military, marching, concert and other bands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cymbals are used in modern orchestras and many military, marching, concert and other bands"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the dragonfly associated with late summer as well as early autumn?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Exposure to air cause the larva to begin breathing?", "answer": "Yes (capitalization).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (capitalization)."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "Exposure to air causes the larva to", "extracted_entity": null}
{"question": "is it characterized by large multifaceted eyes, two pairs of strong transparent wings, and an elongated body?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "Araneae", "extracted_entity": null}
{"question": "is the drum a member of the percussion group?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does each type of drum head serve its own musical purpose?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the Aburukuwa a type of drum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "how long has the basic design of the drum remained unchanged?", "answer": "remained virtually unchanged for thousands of years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["remained virtually unchanged for thousands of years", "thousands of years"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "1000 years", "extracted_entity": null}
{"question": "what is the second biggest factor affecting the sound produced by a drum?", "answer": "tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["tension", "the tension at which the drum head is held against the shell of the drum."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the drum's skin.", "extracted_entity": null}
{"question": "by what name are drums classified as under the percussion group?", "answer": "membranophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["membranophone", "membranous"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums", "extracted_entity": null}
{"question": "how do rock and roll drummers like their drumheads?", "answer": "rock and roll drummer may prefer drums that sound loud and deep", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["rock and roll drummer may prefer drums that sound loud and deep", "drums that sound loud and deep"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "14\" and 16\"", "extracted_entity": null}
{"question": "whare is the name for drums that have a set of wires held across some of all of the drum heads?", "answer": "snares", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["snares"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "snare", "extracted_entity": null}
{"question": "had the drums been used in Sri Lanka since Before Christ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Dhak", "extracted_entity": null}
{"question": "are drums often used in music therapy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums are often used in music therapy.", "extracted_entity": null}
{"question": "do drums consist of at least eight membranes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what has the most effect on how a drum sounds?", "answer": "the drumhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the drumhead"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "The drumhead.", "extracted_entity": null}
{"question": "who constructed a kit using diatonic and chromatically tuned drums?", "answer": "Terry Bozzio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Terry Bozzio"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "drums are usually played by what?", "answer": "the hands, or by one or two sticks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the hands, or by one or two sticks", "the hands"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "drummer", "extracted_entity": null}
{"question": "has the design of drums changed recently?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can some cylindrical shell drums have no drum heads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does every drumhead make the same sound?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "aren't drums often used in music therapy?", "answer": "Yes, drums are often used in music therapy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, drums are often used in music therapy."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "aren't drums usually played by the hands?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are drums usually played by the hands?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do many such drums have six to ten tension rods?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "10 tension rods", "extracted_entity": null}
{"question": "do drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with parts of a player's body, or with some sort of implement such as a drumstick, to produce sound?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have other techniques been used to cause drums to make sound, such as the ``Thumb roll''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, the thumb roll is a technique used to", "extracted_entity": null}
{"question": "is the electric eel a true eel?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, the electric eel is not a true eel."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are most eels predators?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, most eels are predators."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is eel blood toxic to humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, eel blood is toxic to humans."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "how many species of true eels are there?", "answer": "approximately 800 species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["approximately 800 species", "There are approximately 800 species of true eels."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "17.", "extracted_entity": null}
{"question": "where is smoked eel considered a delicacy?", "answer": "Northern Germany, The Netherlands, Denmark, Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Germany, The Netherlands, Denmark, Sweden", "Smoked eel is considered a delicacy in northern Germany, The Netherlands, Denmark and Sweden."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do eels begin life?", "answer": "As flat and transparent larvae, called leptocephali", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As flat and transparent larvae, called leptocephali", "Eels begin life as flat and transparent larvae, called leptocephali."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what types of fins do eels have?", "answer": "Dorsal, anal, caudal, tail fins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dorsal, anal, caudal, tail fins", "Eels have dorsal and anal fins fused with a tail fin. Some also have  pectoral fins."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "pectoral fins", "extracted_entity": null}
{"question": "what do leptocephali eat?", "answer": "Marine snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Marine snow", "Leptocephali eat small particles called marine snow."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Leptocephali are the larvae", "extracted_entity": null}
{"question": "do all eels live in fresh water?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, not all eels live in fresh water."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No, not all eels live in fresh water", "extracted_entity": null}
{"question": "is Finnish an official language of the European Union?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Finnish is an official language of the European Union."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the dialects of Finnish divided into two groups?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, the dialects of Finnish are divided into two groups."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Finnish threatened by English?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Finnish is not threatened by English."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "when did Proto-Uralic language arrive in Finland?", "answer": "Around 1900 BCE", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Around 1900 BCE", "Proto-Uralic language arrived in Finland around 1900 BCE."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "3000 BCE", "extracted_entity": null}
{"question": "what are characteristic features of Finnish?", "answer": "Vowel harmony and an agglutinative morphology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowel harmony and an agglutinative morphology", "Characteristic features of Finnish are vowel harmony and an agglutinative morphology."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "The Finnish language is a member of the Finn", "extracted_entity": null}
{"question": "where is the Kven language spoken?", "answer": "Norway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norway", "The Kven language is spoken in Northern Norway."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "The Kven language is spoken in the northern part", "extracted_entity": null}
{"question": "for how long has the classification of dialects spoken outside of Finland been a controversial issue?", "answer": "Since Finland's independence in 1917", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Since Finland's independence in 1917", "The classification of dialects spoken outside of Finland has been a controversial issue since Finland's independence in 1917."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "1917", "extracted_entity": null}
{"question": "who wrote the first novel in Finnish?", "answer": "Aleksis Kivi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Aleksis Kivi", "The first novel in Finnish was published by Aleksis Kivi."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what book defined the official Finnish language?", "answer": "The Dictionary of Contemporary Finnish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Dictionary of Contemporary Finnish", "The Dictionary of Contemporary Finnish defined the official Finnish language."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "the Kalevala", "extracted_entity": null}
{"question": "what is the earliest extant transverse flute?", "answer": "it dates from 433 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it dates from 433 BC", "Chi"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when does a flute produce sound?", "answer": "when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "When a stream of air directed across a hole in the instrument creats a vibration of air at the hole."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "when the air blown into the flute is", "extracted_entity": null}
{"question": "how can a flute's volume be increased?", "answer": "a flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute's volume can generally be increased by making its resonator and tone holes larger", "A flute's volume can be increased by making its resonator and tone holes larger."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by adding an octave", "extracted_entity": null}
{"question": "do open-ended flutes have brighter timbres?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was the oldest flute ever discovered found?", "answer": "Germany", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany", "Various parts of Germany."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The flute was found in the Hohle", "extracted_entity": null}
{"question": "are Western flutes simpler than Indian flutes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is a police whistle very wide for its pitch?", "answer": "A flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute's volume can generally be increased by making its resonator and tone holes larger", "It's volume is high either because of its larger resonator or larger tone holes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "It is not.", "extracted_entity": null}
{"question": "what are the two main varieties of Indian flutes?", "answer": "The first, the Bansuri. The second, the Venu or Pullanguzhal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first, the Bansuri. The second, the Venu or Pullanguzhal", "Bansuri and the Venu or Pullanguzhal."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has the flute been dated to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the pan flute used in Greece?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Syrinx", "extracted_entity": null}
{"question": "can a flute be played with several different air sources?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when was the pan flute used in Greece?", "answer": "the 7th century BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 7th century BC", "7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The pan flute was used in Greece as early", "extracted_entity": null}
{"question": "when did concert flutes begin appearing in concert ensembles?", "answer": "the 16th-century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 16th-century", ""], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1880s", "extracted_entity": null}
{"question": "how many main varieties of Indian flutes are currently used?", "answer": "two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two", "Two"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what material is a chi flute fashioned from?", "answer": "lacquered bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lacquered bamboo", "Lacquered bamboo"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Bamboo", "extracted_entity": null}
{"question": "how does a flute player change the pitch of the sound?", "answer": "by opening and closing holes in the body of the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by opening and closing holes in the body of the instrument", "Opening and closing holes in the body of the instrument"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "By opening or closing the holes.", "extracted_entity": null}
{"question": "what is the most basic form of the flute?", "answer": "a flute can be an open tube which is blown like a bottle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute can be an open tube which is blown like a bottle", "Open tube which is blown like a bottle"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the recorder", "extracted_entity": null}
{"question": "has the dragon historically served as China's national emblem?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "the dragon", "extracted_entity": null}
{"question": "do pandas hibernate?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do giant pandas attack humans?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what family is the panda a part of?", "answer": "Ursidae (bear)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ursidae (bear)", "Ursidae"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "the Ailuropodidae", "extracted_entity": null}
{"question": "how long does it take for the panda cubs skin to turn gray?", "answer": "one to two weeks after birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["one to two weeks after birth", "One to two weeks"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many different names did Chinese writings give to the panda?", "answer": "20", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["20"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what foods do pandas eat?", "answer": "bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bamboo", "bamboo, honeys, eggs, fish, yams, shrub leaves, oranges, and bananas"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "bamboo", "extracted_entity": null}
{"question": "what are the two subspecies of giant pandas?", "answer": "Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "Ailuropoda melanoleuca melanoleuca and The Qinling Panda, Ailuropoda melanoleuca qinlingensis"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is panda diplomacy?", "answer": "Loans of Giant Pandas to American and Japanese zoos", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loans of Giant Pandas to American and Japanese zoos"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the Giant Panda a mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is the Giant Panda a terrestrial animal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the Giant Panda an endangered species?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Giant Panda", "extracted_entity": null}
{"question": "where do Giant Pandas live?", "answer": "a few mountain ranges in central china", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a few mountain ranges in central china", "Sichuan province, Shaanxi and Gansu provinces"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Giant pandas live in the bamboo", "extracted_entity": null}
{"question": "when was the Giant Panda's genome sequenced?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2009", "extracted_entity": null}
{"question": "how many subspecies of the Giant Panda are there?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The giant panda is classified into 2", "extracted_entity": null}
{"question": "is the Giant Panda a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes, it is a carnivore.", "extracted_entity": null}
{"question": "why is the Giant Panda an endangered species?", "answer": "threatened by habitat loss", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["threatened by habitat loss", "It is threatened by continued habitat loss and by a very low birthrate"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "It's endangered because it is only", "extracted_entity": null}
{"question": "is the population of wild Giant  Pandas growing?", "answer": "according to some reports, yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["according to some reports, yes", "Yes but uncertain"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is an acoustic guitar dependent on an external device?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is the battente smaller than a classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the flamenco guitar similar to the classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where is the headstock located?", "answer": "end of the guitar neck furthest from the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["end of the guitar neck furthest from the body", "At the end of the guitar."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "headstock", "extracted_entity": null}
{"question": "whom are guitars made and repaired by?", "answer": "luthiers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["luthiers", "Luthiers."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are modern guitar strings constructed of?", "answer": "metal, polymers, animal or plant product materials", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["metal, polymers, animal or plant product materials", "Metal, polymers, or animal or plant product materials."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Nylon", "extracted_entity": null}
{"question": "why are Harp Guitars difficult to classify?", "answer": "as there are many variations within this type of guitar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["as there are many variations within this type of guitar", "There are many variations within this type."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the bridge used for?", "answer": "transfer the vibration from the strings to the soundboard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["transfer the vibration from the strings to the soundboard", "The transfer of string vibrations."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The bridge is used for transportation.", "extracted_entity": null}
{"question": "which guitars use three single-coil pickups?", "answer": "Fender Statocaster type guitars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fender Statocaster type guitars", "Fender Stratocaster type guitars."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Telecaster", "extracted_entity": null}
{"question": "is a guitar an instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can guitars be divided into two broad categories?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do picks come in many shapes and sizes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many strings does a guitar typically have?", "answer": "Six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six", "six"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "6 strings", "extracted_entity": null}
{"question": "what is located at the end of the guitar neck furthest from the body?", "answer": "Headstock", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Headstock", "headstock"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the peghead", "extracted_entity": null}
{"question": "what are most electric guitar bodies made of?", "answer": "Wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wood", "wood"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "wood", "extracted_entity": null}
{"question": "why do some people believe that left-handed people should learn to play guitars as right-handed people do?", "answer": "to standardise the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to standardise the instrument"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how old is the oldest known representation of a guitar-like intrument being played?", "answer": "3,300 years old", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3,300 years old"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the point called that is bolted or glued to the body of the guitar?", "answer": "Neck Joint", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Neck Joint", "Neck joint or 'Heel'"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "headstock", "extracted_entity": null}
{"question": "did Becquerel study science at the \u00c9cole des Ponts et Chauss\u00e9es?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways in 1892?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel was a French physicist", "extracted_entity": null}
{"question": "whom did he share the Nobel Prize with?", "answer": "Pierre and Marie Curie", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pierre and Marie Curie"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the SI unit for radioactivity called?", "answer": "becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["becquerel", "the becquerel (Bq)"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel", "extracted_entity": null}
{"question": "when Becquerel discovered radioactivity, whose work did he investigate?", "answer": "Wilhelm Conrad R\u00f6ntgen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilhelm Conrad R\u00f6ntgen"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Marie Curie", "extracted_entity": null}
{"question": "where are there craters named Becquerel?", "answer": "Moon and Mars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moon and Mars", "on the moon and on mars"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel (Martian crater)", "extracted_entity": null}
{"question": "why did Bequerel win the Nobel Prize in Physics?", "answer": "discovery of spontaneous radioactivity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["discovery of spontaneous radioactivity", "was the discoverer of radioactivity"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Marie Curie", "extracted_entity": null}
{"question": "was Isaac Newton British?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, he was English."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Isaac Newton religious?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear.", "Yes, he was highly religious, though an unorthodox Christian."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton was not religious.", "extracted_entity": null}
{"question": "did Isaac Newton die in 1898?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, Newton died in his sleep in London on 31 March 1727."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton died in 1727.", "extracted_entity": null}
{"question": "where was Isaac Newton born?", "answer": "At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "He was born at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire.", "Woolsthorpe Manor in Woolsthorpe-by-Colsterworth"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Woolsthorpe-by-Colster", "extracted_entity": null}
{"question": "who shares credit with Isaac Newton for developing calculus?", "answer": "Gottfried Leibniz.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gottfried Leibniz.", "Newton shares the credit with Gottfried Leibniz for the development of the differential and integral calculus."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Isaac Newton discover the generalized binomial theorem?", "answer": "In 1665.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1665."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1665", "extracted_entity": null}
{"question": "did the Occult influence Newton's theory of gravitation?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "If Newton had not relied on the occult idea of action at a distance, he might not have developed his theory of gravity."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton", "extracted_entity": null}
{"question": "were Newton's religious views consistent with Anglicanism?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, a conflict between Newton's religious views and Anglican orthodoxy was averted."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which fruit inspired Isaac Newton's theory of gravitation?", "answer": "The apple.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The apple.", "Isaac Newton's theory of gravitation was inspired by watching the fall of an apple from a tree."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "apple", "extracted_entity": null}
{"question": "was Sir Isaac Newton an English physicist and mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did he build the first practical reflecting telescope?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newtonian Telescope", "extracted_entity": null}
{"question": "was Isaac Newton educated at The King's Schol, Grantham?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton", "extracted_entity": null}
{"question": "when was the Principia published?", "answer": "1687", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1687"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1687", "extracted_entity": null}
{"question": "where is there a Newton statue on display?", "answer": "Oxford University Museum of Natural History ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oxford University Museum of Natural History ", "Oxford University Museum of Natural History"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton, London", "extracted_entity": null}
{"question": "where was Isaac Newton buried?", "answer": "Westminster Abbey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westminster Abbey"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Westminster Abbey, London", "extracted_entity": null}
{"question": "what religion did Isaac Newton follow?", "answer": "he never made a public declaration of his private faith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he never made a public declaration of his private faith", "unorthodox Christian"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Anglican", "extracted_entity": null}
{"question": "what food gave Isaac Newton clues to his theory of gravity?", "answer": "apple", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apple"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Apples", "extracted_entity": null}
{"question": "who was a farmer and later a land surveyor for the government?", "answer": " LeRoy Pollock was a farmer and later a land surveyor for the government.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" LeRoy Pollock was a farmer and later a land surveyor for the government."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "John McDonnell", "extracted_entity": null}
{"question": "regarding this topic, what did David Alfaro Siqueiros do?", "answer": "Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who used hardened brushes, sticks,?", "answer": "Jackson Pollock used hardened brushes, sticks.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jackson Pollock used hardened brushes, sticks."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "The answer is a hardened brush, stick", "extracted_entity": null}
{"question": "when were his papers donated by Lee Krasner?", "answer": "In 1983.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1983."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "1986", "extracted_entity": null}
{"question": "did Free Jazz feature a Pollock painting as its cover artwork?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Free Jazz", "extracted_entity": null}
{"question": "was Paul Jackson Pollock an influential American painter as well as a major figure in the abstract expressionist movement?", "answer": "Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Pollock", "extracted_entity": null}
{"question": "give an example of the origins of the term action painting. ?", "answer": "Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "during his lifetime, did Pollock enjoy considerable fame and notoriety?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Pollock enjoyed considerable fame and notoriety during", "extracted_entity": null}
{"question": "was he regarded as a mostly reclusive artist?", "answer": "Yes, he was regarded as a mostly reclusive artist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, he was regarded as a mostly reclusive artist. "], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Wisma building the tallest building in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes,"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The Wisma 46", "extracted_entity": null}
{"question": "is there a chinese community in jakarta?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Jakarta the 12th largest city in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "what is the capital of Indonesia?", "answer": "Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta", "The capital of Indonesia is Jakarta."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "what is the official name of Jakarta?", "answer": "Daerah Khusus Ibukota Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Daerah Khusus Ibukota Jakarta", "Daerah Khusus."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "how is the climate in the city?", "answer": "Jakarta has a hot and humid equatorial/tropical climate ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta has a hot and humid equatorial/tropical climate ", "The city is hot and humid."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The climate in the city is hot and humid", "extracted_entity": null}
{"question": "where does the name of the city Jakarta come from?", "answer": "The name Jakarta is derived from the Sanskrit word \"Jayakarta.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name Jakarta is derived from the Sanskrit word \"Jayakarta.\""], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jayakarta", "extracted_entity": null}
{"question": "how is the city's governor chosen?", "answer": "election", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["election", "Through election."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The governor of the city is chosen by the city", "extracted_entity": null}
{"question": "why does Jakarta suffer frequent flooding?", "answer": "Because it is located approximately eight meters above the sea level", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is located approximately eight meters above the sea level", "The fact that the northern part of Jakarta lies on a plain, approximately eight meters above the sea level."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is located on a floodplain,", "extracted_entity": null}
{"question": "is the the biggest university in Jakarta the University of Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "UI", "extracted_entity": null}
{"question": "is Jakarta a city?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the most important river the Ciliwung River?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Ciliwung River", "extracted_entity": null}
{"question": "where is Jakarta located?", "answer": "northwest coast of Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["northwest coast of Java", "On the northwest coast of Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is located in Indonesia", "extracted_entity": null}
{"question": "who created Monas Park?", "answer": "General Herman Willem Deandels", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General Herman Willem Deandels", "Dutch Governor General Herman Willem Deandels"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Monas Park was created by John Monash.", "extracted_entity": null}
{"question": "do mikrolets travel on the main roads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what contributes to frequent flooding in Jakarta?", "answer": "Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "It lies on a plain, approximately eight meters above the sea level"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the economy improve in Jakarta in 2007?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in AD 39, King Purnawarman established Sunda Pura as a new capital city for the kingdom, located at the northern coast of where?", "answer": "Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the island of Java", "extracted_entity": null}
{"question": "in 1602, the British East India Company's first voyage, commanded by Sir who, arrived in Aceh and sailed on to Banten where they were allowed to build a trading post?", "answer": "James Lancaster", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James Lancaster"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sir Henry Middleton", "extracted_entity": null}
{"question": "who began control of migration to the city in order to stem the overcrowding and poverty?", "answer": "Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Jayawikarta's soldiers attack the Dutch fortress?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Suharto resign as president?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Suharto resigned as president.", "extracted_entity": null}
{"question": "was the Jakarta area part of the fourth century Indianized kingdom of Tarumanagara?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Tarumanagara", "extracted_entity": null}
{"question": "do other landmarks include the Istiqlal Mosque as well as Jakarta Cathedral?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Istiqlal Mosque, Jakarta C", "extracted_entity": null}
{"question": "give an example of the many Sukarno era monuments in the city. ?", "answer": "The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. "], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The most notable example of this is the National Monument", "extracted_entity": null}
{"question": "is Jakarta the country's economic, cultural and political center?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is James Watt French?", "answer": "No, he was Scottish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, he was Scottish.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt was a Scottish inventor and mechanical", "extracted_entity": null}
{"question": "did James Watt improve an engine?", "answer": "Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "Yes"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is there a statue of Watt in the White House?", "answer": "No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was James Watt's father's job?", "answer": "His father was a shipwright, ship owner and contractor, and served as the town's chief baillie.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father was a shipwright, ship owner and contractor, and served as the town's chief baillie."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt's father was a shipw", "extracted_entity": null}
{"question": "when did Watt retire?", "answer": "In 1800.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800.", "Watt retired in 1800."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1990", "extracted_entity": null}
{"question": "where is the James Watt Memorial College?", "answer": "In Greenock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Greenock.", "The James Watt Memorial  is in Greenock."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Glasgow", "extracted_entity": null}
{"question": "why is the watt named after Watt?", "answer": "The watt is named after him for his contributions to the development of the steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The watt is named after him for his contributions to the development of the steam engine.", "The watt is named after James Watt for his contributions to the development of the steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt was a Scottish inventor and mechanical", "extracted_entity": null}
{"question": "where did Watt attend university?", "answer": "Watt did not attend school regularly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt did not attend school regularly.", "He did not attend a university."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "University of Glasgow", "extracted_entity": null}
{"question": "what did he invent?", "answer": "He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "a steam engine.", "extracted_entity": null}
{"question": "what happened in the early 20th century?", "answer": "The koalas of South Australia were largely exterminated.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do Koalas have a slow metabolism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes, Koalas have a slow metabol", "extracted_entity": null}
{"question": "did eucalypt forests grow in the place of rainforests?", "answer": "Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Eucalyptus", "extracted_entity": null}
{"question": "does the koala fill the same ecological role as the sloth of South America?", "answer": "The koala fills the same ecological role as the sloth of South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koala fills the same ecological role as the sloth of South America."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "koala", "extracted_entity": null}
{"question": "is the long term viability of the koala therefore threatened by genetic weakness?", "answer": "The long term viability of the koala is therefore threatened by genetic weakness.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The long term viability of the koala is therefore threatened by genetic weakness."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the koala found in coastal regions of eastern and southern Australia, from near Adelaide to the southern part of Cape York Peninsula?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "koala", "extracted_entity": null}
{"question": "do populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands?", "answer": "Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were the koalas of South Australia largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock?", "answer": "The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Korean the official language of Korea?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": null}
{"question": "is the word \"Korean\" derived from Goryeo?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Goryeo (\uace0\ub824)", "extracted_entity": null}
{"question": "are all dialects of Korean similar to each other?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No, the dialects are very different from each", "extracted_entity": null}
{"question": "about how many Koreans speakers are there?", "answer": "78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["78 million", "78 million Korean speakers"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "8000000", "extracted_entity": null}
{"question": "older English sources used the name \"Korean\" to refer to what?", "answer": "language, country, and people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["language, country, and people"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the official language of North Korea?", "answer": "Korean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": null}
{"question": "why may English translation of Koreans adjectives misleadingly suggest that they are verbs?", "answer": "English does not have an identical grammatical category", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["English does not have an identical grammatical category"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do the majority of loanwords other than Sino-Korean come from?", "answer": "modern times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern times", "modern times, 90% of which are from English"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "how many Korean speakers are there?", "answer": "There are about 78 million Korean speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are about 78 million Korean speakers."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "77,000,00", "extracted_entity": null}
{"question": "how many parts of speech does the Korean language contain?", "answer": "The Korean Language contains nine parts of speech. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Korean Language contains nine parts of speech. "], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Nouns, Verbs, Adjectives", "extracted_entity": null}
{"question": "how many verb paradigms are there in Korean?", "answer": "There are seven verb paradigms or speech levels in Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are seven verb paradigms or speech levels in Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the Sino-Korean system of numbers based on?", "answer": "The Sino-Korean system is nearly entirely based on the Chinese numerals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sino-Korean system is nearly entirely based on the Chinese numerals."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The Chinese numeral system is a system of numbers", "extracted_entity": null}
{"question": "what is another term for Korean adjectives?", "answer": "Adjectives are also known as \"descriptive verbs\" or \"stative verbs\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adjectives are also known as \"descriptive verbs\" or \"stative verbs\"."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "\uc811\ubbf8\uc0ac (Jeommi", "extracted_entity": null}
{"question": "is modern Korean written in columns or rows?", "answer": "It is written in rows.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is written in rows."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "columns", "extracted_entity": null}
{"question": "what percentage of the Korean language does Jeong Jae-do estimate to be Sino-Korean?", "answer": "Sino-Korean makes up 30% of the Korean language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sino-Korean makes up 30% of the Korean language."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "70%", "extracted_entity": null}
{"question": "do Chinese and Japanese have spaces between words?", "answer": "No, Chinese and Japanese do not have spaces between words.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese and Japanese do not have spaces between words."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "from which languages is Korean descended?", "answer": "Korean is descended from Old Korean, Middle Korean and Modern Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean is descended from Old Korean, Middle Korean and Modern Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there seven verb paradigms or speech levels in Korean?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Modern Korean written with spaces between words?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the Korean names for the language based on the names for Korea used in North as well as South Korea?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is it not also one of the two official languages of the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists?", "answer": "The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No, Jeju is considered a dialect of Korean", "extracted_entity": null}
{"question": "is it also one of the two official languages in the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Kuala Lumpur the capitol of Malaysia? disfluent?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Kuala Lumpur in the Selangor state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what's the population of Kuala lumpur?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1.7 million", "extracted_entity": null}
{"question": "what language do they speak in Kuala Lumpur?", "answer": "Bahasa Melayu", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bahasa Melayu"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": null}
{"question": "what does the name \"Kuala Lumpur\" mean?", "answer": "muddy confluence, \"muddy estuary,\" and \"muddy city\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["muddy confluence, \"muddy estuary,\" and \"muddy city\""], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the confluence of the Gombak and", "extracted_entity": null}
{"question": "what airports does Kuala Lumpur have?", "answer": "Kuala Lumpur International Airport and Subang International Airport", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kuala Lumpur International Airport and Subang International Airport"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "KLIA and KLIA2", "extracted_entity": null}
{"question": "how long was Kuala Lumpur occupied by the Japanese?", "answer": "42 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42 months"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "3 years 3 months 3 days", "extracted_entity": null}
{"question": "how many seasons does Kuala Lumpur experience?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "when were the colonial buildings in Kuala Lumpur constructed?", "answer": "toward the end of the 19th and early 20th centuries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["toward the end of the 19th and early 20th centuries"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1897", "extracted_entity": null}
{"question": "is it a host city for the Formula One World Championship?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the central market located in the proximity of the pertama complex?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it home to the parliament of malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Dewan Rakyat", "extracted_entity": null}
{"question": "when did kuala lumpur become a federal territory of malaysia?", "answer": "February 1, 1974", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["February 1, 1974"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1974", "extracted_entity": null}
{"question": "what is the city's population?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the official religion in the country?", "answer": "Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Islam"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The official religion in the country is Hinduism", "extracted_entity": null}
{"question": "is there a more industrialized region in malaysia?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Klang Valley", "extracted_entity": null}
{"question": "after the 2008 elections, what party had the most parliament seats?", "answer": "DAP", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["DAP"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The Democratic Party", "extracted_entity": null}
{"question": "what is the name of the event that draws the worlds top riders to malaysia?", "answer": "KL Grand Prix CSI 5*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["KL Grand Prix CSI 5*"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci valued as an engineer?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci born on April 15?", "answer": "Yes, Leonardo da Vinci was born on April 15.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Leonardo da Vinci was born on April 15."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci was born on April", "extracted_entity": null}
{"question": "who left Verroccio's studio?", "answer": "Leonardo da Vinci left Verroccio's studio.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo da Vinci left Verroccio's studio."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Don Giovanni", "extracted_entity": null}
{"question": "when did Baldassare Castiglione, author of Il Cortegiano ( \" The Courtier \" ), write?", "answer": "1528", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1528"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1528", "extracted_entity": null}
{"question": "when did he flee to Venice?", "answer": "In 1499.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1499."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Leonardo a contemporary of Botticelli, Domenico Ghirlandaio as well as Perugino?", "answer": "Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo was a contemporary of Botticelli", "extracted_entity": null}
{"question": "is the larger work now almost universally attributed to Leonardo?", "answer": "Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "The Last Supper", "extracted_entity": null}
{"question": "has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo", "extracted_entity": null}
{"question": "is he widely considered to be one of the greatest painters of all time and perhaps the most diversely talented person ever to have lived?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "according to art historian Helen Gardner, were the scope and depth of his interests without precedent and ``his mind and personality seem to us superhuman, the man himself mysterious and remote''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "__________", "extracted_entity": null}
{"question": "are lobsters invertebrates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are lobsters kosher?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do lobsters have blue blood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many legs do lobsters have?", "answer": "10.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10.", "10"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "where are lobsters found?", "answer": "In all oceans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In all oceans.", "In all oceans"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the mean level of mercury in American lobsters?", "answer": "0.31 ppm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["0.31 ppm"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "1.31 ppb", "extracted_entity": null}
{"question": "do lobsters feel pain?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Lobsters feel pain.", "extracted_entity": null}
{"question": "did the Moche people worship lobsters?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is the caridoid escape reaction?", "answer": "Swimming backwards quickly by curling and uncurling their abdomen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swimming backwards quickly by curling and uncurling their abdomen.", "When lobsters swim backwards quickly by curling and uncurling their abdomen"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "the caridoid escape reaction is a specialized", "extracted_entity": null}
{"question": "were the recitations of the Ancient Greeks accompanied by lyre playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a classical lyre have a hollow body?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the lyre a stringed musical instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "which constellation is said to resemble a lyre shape?", "answer": "Lyra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lyra"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Lyra", "extracted_entity": null}
{"question": "how many raised arms are extending from the sound-chest of a classical lyre?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "what were the strings of a classical lyre made of?", "answer": "Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gut"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the foot pedal framework for a piano called a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The pedal mechanism of a piano is called a", "extracted_entity": null}
{"question": "where was the deepest note of the classical lyre in relation to the player's body?", "answer": "It was farthest from the player's body.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was farthest from the player's body."], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The deepest note of the classical lyre was", "extracted_entity": null}
{"question": "does a standard piano have fewer strings than a harp?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Malay the official language of Iraq?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is the national language of Malaysia.", "extracted_entity": null}
{"question": "could Malay have originated from Sumatra island?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": null}
{"question": "is Malay an agglutinative language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a language of the Austronesian", "extracted_entity": null}
{"question": "what family is Malay a member of?", "answer": "Austronesian family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austronesian family", "Austronesian."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a member of the Malay family", "extracted_entity": null}
{"question": "what is the basic word order in Malay?", "answer": "Subject Verb Object", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Subject Verb Object", "Subject Object Verb."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "S-V-O", "extracted_entity": null}
{"question": "what languages has Malay borrowed words from?", "answer": "Sanskrit, Arabic and English", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sanskrit, Arabic and English", "Arabic, Sanskrit, Tamil, Persian, Portuguese, Dutch, certain Chinese dialects, and English."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there many words in Malay that use natural gender?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how are new words formed in Malay?", "answer": "three methods.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three methods.", "Affixation, composition, and reduplication."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "by using prefixes and suffixes.", "extracted_entity": null}
{"question": "is Malay in the Austronesian family of languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a language in the Austronesian", "extracted_entity": null}
{"question": "is there one negation word in Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many parts of speech are there in Malay?", "answer": "Four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Four"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "noun, verb, adjective, pronoun", "extracted_entity": null}
{"question": "what is the national language in Malaysia?", "answer": "Malay ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malay "], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Bahasa Malaysia", "extracted_entity": null}
{"question": "do linguists consider Malay to be a single language?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can speakers of modern Malay understand Old Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what language besides Malay is in the Austronesia language family?", "answer": "Malagasy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malagasy"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Indonesian", "extracted_entity": null}
{"question": "wasn't Malay language found in Sumatra?", "answer": "The Malay language was found in Sumatra.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Malay language was found in Sumatra."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Malay language written using Pallava?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Malay language written using Pallava?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "had the language already been used as the lingua franca throughout the archipelago since the 15th century?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do another distinguishing feature of Malay ( include Indonesian Malay )?", "answer": "Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan)."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Malay spoken as local dialect of ethnic Malays?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is the official language of Malaysia.", "extracted_entity": null}
{"question": "give an example of the tributary states of the Majapahit kingdom in eastern Java. ?", "answer": "Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Majapahit Kingdom", "extracted_entity": null}
{"question": "is Malay the one of the most widely spoken languages in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was it derived from Latin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Melbourne the capital and most populous city in the state of Victoria?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne", "extracted_entity": null}
{"question": "is Melbourne home to Australia's busiest seaport?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Melbourne have a large international student community?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "International House", "extracted_entity": null}
{"question": "what is a resident of Melbourne known as?", "answer": "Melburnian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Melburnian", "A \"Melburnian\"."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Aussie", "extracted_entity": null}
{"question": "when was Melbourne officially declared as a city?", "answer": "1847", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1847", "In 1847."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many sister cities does the City of Melbourne have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six", "Six."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Melbourne situated?", "answer": "boundary of the very hot inland areas and the cold southern ocean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["boundary of the very hot inland areas and the cold southern ocean"], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne is situated in Victoria, Australia.", "extracted_entity": null}
{"question": "how many domestic tourists visit Melbourne?", "answer": "7.6 million domestic visitors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7.6 million domestic visitors", "7.6 million."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what religion holds majority in Melbourne?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "Christianity."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": null}
{"question": "did Albert Einstein keep a photograph of Faraday on his study wall?", "answer": "Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Faraday marry Sarah Barnard?", "answer": "Yes, Faraday married Sarah Barnard.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday married Sarah Barnard.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did Faraday also discover the laws of electrolysis?", "answer": "Yes, Faraday also discovered the laws of  electrolysis.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday also discovered the laws of  electrolysis.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday", "extracted_entity": null}
{"question": "who discovered benzene?", "answer": "Michael Faraday discovered benzene.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday discovered benzene.", "Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Friedrich W\u00f6hler", "extracted_entity": null}
{"question": "where was Michael Faraday born?", "answer": "Michael Faraday was born in Newington Butts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday was born in Newington Butts.", "Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what is the Faraday effect?", "answer": "The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "The plane of polarisation of linearly polarised light can be rotated by the application of an exernal magnetic field aligned in the direction the light is moving."], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Faraday effect is the rotation of the", "extracted_entity": null}
{"question": "did Faraday discover diamagnetism?", "answer": "Yes, Faraday discovered diamagnetism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday discovered diamagnetism.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday discovered diamagnetism.", "extracted_entity": null}
{"question": "how many siblings did Faraday have?", "answer": "Faraday had three siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday had three siblings.", "two"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday had two siblings.", "extracted_entity": null}
{"question": "where is Michael Faraday buried?", "answer": "Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "in the dissenters' section of Highgate Cemetery"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday is buried in the Nave", "extracted_entity": null}
{"question": "is faraday best known for his work with kinematics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday is best known for his work with", "extracted_entity": null}
{"question": "was he active in the area now called environmental science?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did he discover benzene?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "benzene", "extracted_entity": null}
{"question": "who was his mentor?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was Faraday's wife?", "answer": "Sarah Barnard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sarah Barnard"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did he advise the british government on the production of chemical weapons for use in the crimean war?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did faraday have strong maritime interests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he was a major figure in the British", "extracted_entity": null}
{"question": "does the charge reside on the interior of a charged conductor?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the charge resides on the interior of a charged", "extracted_entity": null}
{"question": "wasn't Michelangelo abstemious?", "answer": "Michelangelo was abstemious.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michelangelo was abstemious."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "wasn't Michelangelo a homosexual?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Michelangelo was a homosexual.", "extracted_entity": null}
{"question": "when did Filippo Strozzi sell it to Francis I?", "answer": "In 1529.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1529."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1533", "extracted_entity": null}
{"question": "when was his house demolished?", "answer": "1874", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1874"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1996", "extracted_entity": null}
{"question": "was Michelangelo's mother Francesca di Neri del Miniato di Siena?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "give an example of the most renowned works of the Renaissance. ?", "answer": "The Statue of David, completed by Michelangelo in 1504", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Statue of David, completed by Michelangelo in 1504"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "The most renowned works of the Renaissance were the", "extracted_entity": null}
{"question": "was Michelangelo's output in every field during his long life prodigious?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is English the language most often spoken in Montreal?", "answer": "The language most spoken at home in the city is French.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The language most spoken at home in the city is French.", "No, the language most spoken in Montreal is French."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal", "extracted_entity": null}
{"question": "is Montreal's economy the third largest of all cities in Canada?", "answer": "No, Montreal's economy is the second largest of all cities in Canada.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Montreal's economy is the second largest of all cities in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the most popular sport in Montreal ice hockey?", "answer": "yes, The most popular sport in Montreal is ice hockey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, The most popular sport in Montreal is ice hockey.", "Yes, the most popular sport in Montreal is ice hockey."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many international airports does Montreal have?", "answer": "Montreal has two international airports.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montreal has two international airports."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Pierre Elliott Trudeau International Airport", "extracted_entity": null}
{"question": "who is the Mayor of Montreal?", "answer": "The mayor is G\u00c3\u00a9rald Tremblay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mayor is G\u00c3\u00a9rald Tremblay."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the name of the largest church in Montreal?", "answer": "The largest church in Montreal is named Saint Joseph's Oratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The largest church in Montreal is named Saint Joseph's Oratory.", "Saint Joseph's Oratory is the largest church in Montreal."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Notre-Dame Basilica", "extracted_entity": null}
{"question": "where was much of Montreal's industry during the late 19th and early-to-mid 20th century?", "answer": "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "the city's industry was located on the island", "extracted_entity": null}
{"question": "does Montreal contain the largest church in Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Montreal contains the largest church in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "St. Joseph's Oratory", "extracted_entity": null}
{"question": "what is the largest primarily French-speaking city in the western world?", "answer": "Paris is the largest primarily French-speaking city in the western world.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris is the largest primarily French-speaking city in the western world.", "The largest primarily French-speaking city in the western world is Paris."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal", "extracted_entity": null}
{"question": "is Nairobi the capital of Kenya?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "was Nairobi founded in 1899?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "is the current estimated population of Nairobi about 6 milion?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "what is the current estimated population of Nairobi?", "answer": "About 3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["About 3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "3,289,000", "extracted_entity": null}
{"question": "in what year was Nairobi founded?", "answer": "1899", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1899"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1899", "extracted_entity": null}
{"question": "which embassy in Nairobi was bombed in August 1998?", "answer": "The United States Embassy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The United States Embassy"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "how many civilians died in the 1998 U.S. embassy bombing?", "answer": "Over two hundred", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over two hundred"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "200", "extracted_entity": null}
{"question": "how many trades can the NSE make per day?", "answer": "10 million trades", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10 million trades"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "20", "extracted_entity": null}
{"question": "about how many people are estimated to live in Nairobi?", "answer": "3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4.3 million", "extracted_entity": null}
{"question": "the district is bordered to the southwest by Uhuru Park and where?", "answer": "Central Park", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "the district is bordered to the southwest by", "extracted_entity": null}
{"question": "when did the Catholic University of Eastern Africa which obtain its Letter of Interim Authority?", "answer": "In 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1989."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1987", "extracted_entity": null}
{"question": "is Central Park adjacent to Uhuru Park?", "answer": "Central Park is adjacent to Uhuru Park.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park is adjacent to Uhuru Park."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Central Park is adjacent to Uhuru Park.", "extracted_entity": null}
{"question": "give an example of the highest growth rates of any city in Africa. ?", "answer": " The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Johannesburg", "extracted_entity": null}
{"question": "give an example of the most prominent cities in Africa politically and financially. ?", "answer": "Nairobi is now one of the most prominent cities in Africa politically and financially", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is now one of the most prominent cities in Africa politically and financially"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Lagos, Nigeria, Johannesburg, South", "extracted_entity": null}
{"question": "is Nairobi not the capital as well as largest city of Kenya?", "answer": "Nairobi is the capital and largest city of Kenya.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is the capital and largest city of Kenya."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi is the capital and the largest city", "extracted_entity": null}
{"question": "does the city and its surrounding area also form the Nairobi Province?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi Province", "extracted_entity": null}
{"question": "does the name ``Nairobi'' come from the Maasai phrase Enkare Nyirobi, which translates to ``the place of cool waters''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "did Nikola Tesla die alone?", "answer": "Nikola Tesla died alone.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla died alone.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Nikola Tesla close friends with Mark Twain?", "answer": "Nikola Tesla was close friends with Mark Twain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla was close friends with Mark Twain.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla and Mark Twain were", "extracted_entity": null}
{"question": "did Nikola Tesla use a technique called picture thinking?", "answer": "Nikola Tesla used a technique called picture thinking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla used a technique called picture thinking.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla used the technique of picture", "extracted_entity": null}
{"question": "when did Nikola Tesla appear on the cover of Time Magazine?", "answer": "On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "1931"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1931", "extracted_entity": null}
{"question": "who became adversaries with Nikola Tesla?", "answer": "Nikola Tesla became adversaries with Edison.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla became adversaries with Edison.", "Edison"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did Nikola Tesla move to in 1880?", "answer": "In 1880, Nikola Tesla moved to Budapest.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1880, Nikola Tesla moved to Budapest.", "Budapest"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla moved to Budapest, Hungary", "extracted_entity": null}
{"question": "was Nikola Tesla's mother illiterate?", "answer": "Nikola Tesla's mother never learned to read.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla's mother never learned to read.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, she was not.", "extracted_entity": null}
{"question": "how many siblings did Nikola Tesla have?", "answer": "Nikola Tesla had four siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla had four siblings.", "4"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Nikola Tesla a vegetarian?", "answer": " In his later years Tesla became a vegetarian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" In his later years Tesla became a vegetarian.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was a vegetarian.", "extracted_entity": null}
{"question": "is there a monument to Tesla at Niagara Falls, New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Tesla an American citizen?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was an American citizen.", "extracted_entity": null}
{"question": "was Tesla on the cover of Time magazine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was on the cover of Time magazine", "extracted_entity": null}
{"question": "when did Tesla receive his last patent?", "answer": "1928", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1928"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1928", "extracted_entity": null}
{"question": "what languages did Tesla speak?", "answer": "Serbian, Czech, English, French, German, Hungarian, Italian, and Latin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Serbian, Czech, English, French, German, Hungarian, Italian, and Latin"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Tesla win the Nobel Prize?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Tesla rich at the time of his death?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what music group was named after Tesla?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": null}
{"question": "consequently, what is not considered a \"serious painter\" by some contemporary artists, who often regard his work as bourgeois and kitsch?", "answer": "Rockwell ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who spent the next 10 years painting for Look magazine?", "answer": "Norman Rockwell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norman Rockwell."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Richard Avedon", "extracted_entity": null}
{"question": "when was his last painting for the Post published?", "answer": "1963", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1963"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "1984", "extracted_entity": null}
{"question": "when did Rockwell marry his first wife, Irene O'Connor,?", "answer": "In 1916.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1916."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Norman Rockwell spend the winter months as artist-in-residence at Otis College of Art as well as Design?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Norman Rockwell publish a total of 321 original covers for The Saturday Evening Post over 47 years?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "321", "extracted_entity": null}
{"question": "was Rockwell also commissioned to illustrate over 40 books including Tom Sawyer as well as Huckleberry Finn?", "answer": "Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do his works enjoy a broad popular appeal in the United States, where Rockwell is most famous for the cover illustrations of everyday life scenarios he created for The Saturday Evening Post magazine over more than four decades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Norman Rockwell", "extracted_entity": null}
{"question": "are among the best-known of Rockwell's works the Willie Gillis series, Rosie the Riveter (although his Rosie was reproduced less than others of the day), Saying Grace (1951), and the Four Freedoms series?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Willie Gillis, Rosie the Riveter", "extracted_entity": null}
{"question": "is he also noted for his work for the Boy Scouts of America (BSA)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the octopus a cephalopod?", "answer": "Yes, the octopus is a cephalopod.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus is a cephalopod."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes, the octopus is a cephalop", "extracted_entity": null}
{"question": "does the octopus have two eyes?", "answer": "Yes, the octopus has two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has two eyes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "The octopus has eight eyes, which are arranged", "extracted_entity": null}
{"question": "does the octopus have a hard beak?", "answer": "Yes, the octopus has a hard beak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has a hard beak."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "where is the mouth of an octopus?", "answer": "The mouth of an octopus is at the center point of the arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mouth of an octopus is at the center point of the arms.", "The octopus' mouth is at the center point of the arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "the mouth of an octopus is on the bottom", "extracted_entity": null}
{"question": "around how many recognized octopus species are there?", "answer": "There are around 300 recognized octopus species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are around 300 recognized octopus species."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "300", "extracted_entity": null}
{"question": "how many arms does an octopus have?", "answer": "An octopus has four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An octopus has four pairs of arms.", "The octopus has four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "8", "extracted_entity": null}
{"question": "what is the range of lifespans of the octopus?", "answer": "Octopuses can live from six months to five years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Octopuses can live from six months to five years.", "The octopus has a short lifespan."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is an example of something an octopus learned in a research experiment?", "answer": "In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "An octopus learned to solve mazes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how can an octopus defend against predators?", "answer": "To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "They can hide, flee, expel ink, or use camouflage."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "the octopus can use camouflage to", "extracted_entity": null}
{"question": "are Octopus highly intelligent?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Octopus", "extracted_entity": null}
{"question": "do males use a specialized arm called a hectocotylus to insert spermatophores?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "do the octopuses in the less familiar Cirrina suborder have two fins and an internal shell?", "answer": "Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Octopus", "extracted_entity": null}
{"question": "do octopuses have two eyes and four pairs of arms?", "answer": "Yes, octopuses have two eyes and four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, octopuses have two eyes and four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "octopus", "extracted_entity": null}
{"question": "is octopus a common food in Mediterranean cuisine as well as Portuguese cuisine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "octopus", "extracted_entity": null}
{"question": "is the collective form octopus usually reserved for animals consumed for food?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has an octopus a hard beak, with its mouth at the center point of the arms?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1968 through 1971?", "answer": "Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "the Vietnam war", "extracted_entity": null}
{"question": "what happened in 1838?", "answer": "Don Jos\u00e9 Ruiz y Blasco was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Don Jos\u00e9 Ruiz y Blasco was born"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Mormons were driven from Missouri", "extracted_entity": null}
{"question": "who was not long in finding another lover, Jacqueline Roque?", "answer": "Picasso.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Picasso."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Picasso make his first trip to Paris?", "answer": "1900", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1900"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "give an example of the most comprehensive records extant of any major artists beginnings. ?", "answer": "Museu Picasso", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Museu Picasso"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "[Franz Schubert](https://en", "extracted_entity": null}
{"question": "is in the 1996 movie Surviving Picasso Picasso played by actor Anthony Hopkins?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the U.S. copyright representative for the Picasso Administration the Artists Rights Society?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Artists Rights Society (ARS)", "extracted_entity": null}
{"question": "is he one of the most recognized figures in 20th-century art?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is he best known for co-founding the Cubist movement and for the wide variety of styles embodied in his work?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "are among his most famous works the proto-Cubist Les Demoiselles d'Avignon (1907) and Guernica (1937), his portrayal of the German bombing of Guernica during the Spanish Civil War?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Les Demoiselles d'Avignon", "extracted_entity": null}
{"question": "are pianos used in Western music?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are \"upright pianos\" called \"vertical pianos\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "No, they are called \"upright pian", "extracted_entity": null}
{"question": "did Bartolomeo Cristofori invent the modern piano?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the middle pedal called on grand pianos?", "answer": "the sostenuto pedal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the sostenuto pedal.", "sostenuto pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many black keys do modern pianos have?", "answer": "36.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["36.", "36"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the sustain pedal called?", "answer": "The pedal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pedal", "damper pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Sustain Pedal", "extracted_entity": null}
{"question": "where is Irving Berlin's piano located?", "answer": "In the Smithsonian Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Smithsonian Museum.", "in the Smithsonian Museum"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "in the cafeteria", "extracted_entity": null}
{"question": "what kind of piano did Irving Berlin play?", "answer": "The transposing piano.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The transposing piano.", "transposing piano"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the left-most pedal on a grand piano called the una corda?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes, the left-most pedal on a grand piano is called the una corda."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it advantageous for a grand piano's metal plate to be quite massive?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "It is advantageous for the plate to be quite massive."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the mechanism in an upright piano perpendicular to its keys?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "The mechanism in upright pianos is perpendicular to the keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "about how tall is a typical studio piano?", "answer": "Studio pianos are around 42 to 45 inches tall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Studio pianos are around 42 to 45 inches tall."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "4 feet 11 inches.", "extracted_entity": null}
{"question": "what are a piano's keys generally made of?", "answer": "Piano keys are generally made of spruce or basswood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano keys are generally made of spruce or basswood."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "ivory or plastic", "extracted_entity": null}
{"question": "how many total keys does a typical modern piano have?", "answer": "88 keys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88 keys.", "A modern piano has 88 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "88", "extracted_entity": null}
{"question": "why are upright pianos more compact?", "answer": "Upright pianos are more compact because the frame and strings are vertical.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Upright pianos are more compact because the frame and strings are vertical."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "upright pianos are more compact because they", "extracted_entity": null}
{"question": "do older pianos have more keys than modern pianos?", "answer": "no.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no.", "Many older pianos only have 85 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the names of a piano's pedals?", "answer": "Piano pedals from left to right: una corda, sostenuto, and damper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano pedals from left to right: una corda, sostenuto, and damper.", "The names of a piano's pedals are una corda, sostenuto, and damper."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in the January 15, 1882?", "answer": "On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "The Great Chicago Fire.", "extracted_entity": null}
{"question": "what happened in 1887?", "answer": "In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is one of the best known Impressionist works Renoir's 1876 Dance at Le Moulin de la Galette?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Renoir's 1876 Dance", "extracted_entity": null}
{"question": "was Pierre-Auguste Renoir born in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Pierre-Auguste Renoir", "extracted_entity": null}
{"question": "have two of Renoir's paintings sold for more than US0 million?", "answer": "Yes (any painting sells for more than $0 million though...)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (any painting sells for more than $0 million though...)"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "as a celebrator of beauty, and especially feminine sensuality, has it been said that ``Renoir is the final representative of a tradition which runs directly from Rubens to Watteau''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Pierre-Auguste Renoir die in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "as a boy, did he work in a porcelain factory where his drawing talents led to him being chosen to paint designs on fine china?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Portuguese an official language of Andorra?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Portuguese is not an official language of Andorra."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese", "extracted_entity": null}
{"question": "does the Portuguese language have its roots in the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese is derived from Latin."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Portuguese contain words from the Arabic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese contains words from the Arabic language."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where are there small Portuguese-speaking communities?", "answer": "Former overseas colonies of Portugal such as Macau and East Timor ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Former overseas colonies of Portugal such as Macau and East Timor ", "There are small Portuguese-speaking communities in Macau and East Timor."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Brazil", "extracted_entity": null}
{"question": "what are the two main groups of Portuguese dialects?", "answer": "Those of Brazil and those of the Old World", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Those of Brazil and those of the Old World", "The two main groups of dialects are Brazil and the Old World."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "European Portuguese and Brazilian Portuguese", "extracted_entity": null}
{"question": "what event marked the end of the Old Portuguese period?", "answer": "The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "The end of the Old Portuguese period was marked by the publication of the Cancioneiro Geral by Garcia de Resende."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "the Carnation Revolution", "extracted_entity": null}
{"question": "which government established Portuguese as it's third official language in order to meet the requirements to apply for full membership in the CPLP?", "answer": "Equatorial Guinea", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Equatorial Guinea", "The government of Equatorial Guinea established Portuguese as its third official language in order to meet the requirements to apply for full membership in the CPLP."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "The Government of Brazil", "extracted_entity": null}
{"question": "by 2050, what will the total population of Portuguese speakers in the world be?", "answer": "335 million people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["335 million people", "By 2050, Portuguese-speaking countries will have a total population of 335 million people."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "280 million", "extracted_entity": null}
{"question": "in which country is Portuguese the co-official language with Cantonese Chinese?", "answer": "Macau", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Macau", "Portuguese is the co-official language with Cantonese Chinese in Macau."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Portuguese language spread by arriving Roman soldiers?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese", "extracted_entity": null}
{"question": "(where French has a similar phenomenon, with alveolar affricates instead of postalveolars?", "answer": "Quebec.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the official languages of the special administrative region of Macau and East Timor, . ?", "answer": "Portuguese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Portuguese"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese", "extracted_entity": null}
{"question": "is Portuguese an official language of several international organizations?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it derived from the Latin spoken by the romanized pre-Roman peoples of the Iberian Peninsula (namely the Gallaeci, the Lusitanians, the Celtici and the Conii) around 2000 years ago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Gallaeci", "extracted_entity": null}
{"question": "was it used as the exclusive lingua franca on the island of Sri Lanka for almost 350 years?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Tamil", "extracted_entity": null}
{"question": "is the city named after the apostle Saint Peter?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Rome", "extracted_entity": null}
{"question": "has the terrain in the city been artificially raised?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what were some other names of the city?", "answer": "Petrograd, Leningrad, and Piter", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Petrograd, Leningrad, and Piter", "Petrograd and Leningrad."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what festival features fireworks  celebrating the end of the school year?", "answer": "The White Nights Festival in Saint Petersburg", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The White Nights Festival in Saint Petersburg", "The White Nights Festival."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do most people in urban Saint Petersburg live?", "answer": "apartments", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apartments", "Apartments."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Apartments", "extracted_entity": null}
{"question": "what was made in Russia's largest foundry?", "answer": "thousands of sculptures and statues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["thousands of sculptures and statues", "Thousands of sculptures and statues."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "the Russian Navy's first battleship,", "extracted_entity": null}
{"question": "when did the crime level become higher?", "answer": "After the October revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the October revolution."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1990", "extracted_entity": null}
{"question": "how to travel to work from the city to Moscow?", "answer": "railway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["railway", "One way is through the Moscow-Saint Petersburg Railway."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Moscow Metro", "extracted_entity": null}
{"question": "who laid down the Peter and Paul Fortress?", "answer": "Peter the Great ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Peter the Great "], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was the 7th symphony premiered?", "answer": "1942", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1942"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1960", "extracted_entity": null}
{"question": "has Equestrianism been a long tradition, popular among the Tsars and aristocracy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995. ?", "answer": "Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the best known symphony orchestras in the world under the leadership of conductors Yevgeny Mravinsky and Yuri Temirkanov. ?", "answer": "The Leningrad Philharmonic Orchestra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Leningrad Philharmonic Orchestra"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the longest, most destructive, and most lethal sieges of major cities in modern history. ?", "answer": "The Siege of Leningrad", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Siege of Leningrad"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "the siege of Leningrad, siege", "extracted_entity": null}
{"question": "were the city's other names Petrograd (, 1914 -- 1924) and Leningrad (, 1924 -- 1991)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it often called just Petersburg (-RRB- and is informally known as Piter (-RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "founded by Tsar Peter I of Russia on 27 May 1703, was it the capital of the Russian Empire for more than two hundred years (1713 -- 1728, 1732 -- 1918)?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the standard of living in San Franciscio high?", "answer": "Yes, San Francisco is characterized by a high standard of living.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, San Francisco is characterized by a high standard of living.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does San Francisco have a high percentage of gay and lesbian individuals?", "answer": "Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco has a high percentage of gay and les", "extracted_entity": null}
{"question": "is San Franscico a popular tourist destination?", "answer": "Yes. San Francisco is a popular international tourist destination.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. San Francisco is a popular international tourist destination.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco", "extracted_entity": null}
{"question": "how large is the population of San Francisco?", "answer": "The estimated population of San Francisco in the year 2008 was 808,976.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The estimated population of San Francisco in the year 2008 was 808,976.", "San Francisco has an estimated population of 808,976."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "864,816", "extracted_entity": null}
{"question": "how many state parks are in San Francisco?", "answer": "There is only one park managed by the California State Park system: Candlestick Point.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There is only one park managed by the California State Park system: Candlestick Point.", "There is one state park in San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "11", "extracted_entity": null}
{"question": "where is San Francisco?", "answer": "San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "San Francisco is in California."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco, California", "extracted_entity": null}
{"question": "what is the cause of local earthquakes?", "answer": "It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "The nearby San Andreas and Hayward Faults are responsible for much earthquake activity."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The cause of local earthquakes is the fault", "extracted_entity": null}
{"question": "are more residents employed by small businesses than in 1977?", "answer": "The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The small businesses are the largest employer of", "extracted_entity": null}
{"question": "how does poverty in San Francisco compare to the nation-wide average?", "answer": "The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "San Francisco's poverty rate is lower than the national average."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Mission Bay campus opened in 2003?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Mission Bay campus was opened in 200", "extracted_entity": null}
{"question": "was the University of San Fransisco founded in 1855?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "University of San Fransisco", "extracted_entity": null}
{"question": "is Golden Gate Park the largest city park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Golden Gate Park is the largest city park.", "extracted_entity": null}
{"question": "what is Northern California's most widely circulated newspaper?", "answer": "San Francisco Chronicle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco Chronicle"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The San Francisco Chronicle", "extracted_entity": null}
{"question": "what makes San Francisco among the top-ten North American destinations for conventions and conferences?", "answer": "San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "a large hotel infrastructure and a world-class convention facility in the Moscone Center"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who manages Candlestick point?", "answer": "California State Park system", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["California State Park system", "the California State Park system"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Candlestick point is managed by the San", "extracted_entity": null}
{"question": "is Crissy field an airfield?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Crissy Field", "extracted_entity": null}
{"question": "does the de Young museum house the Asian Art Museum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No, it is located in San Francisco.", "extracted_entity": null}
{"question": "what prompted the city to upgrade its building codes?", "answer": "earthquakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["earthquakes", "The threat of major earthquakes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1980s", "extracted_entity": null}
{"question": "like many larger where cities, San Francisco is a minority-majority city, as non-Hispanic whites comprise less than half of the population?", "answer": "U.S.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["U.S."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is a minority-majority city", "extracted_entity": null}
{"question": "who laid out a street plan for the expanded settlement?", "answer": "William Richardson and Alcalde Francisco de Haro.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["William Richardson and Alcalde Francisco de Haro."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "William Penn", "extracted_entity": null}
{"question": "when was a 43-acre Mission Bay campus opened?", "answer": "2003", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2003"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1985", "extracted_entity": null}
{"question": "when did it sign on the air?", "answer": "In 1941.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1941."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1920", "extracted_entity": null}
{"question": "give an example of the largest two-year community colleges in the country. ?", "answer": "The City College of San Francisco.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The City College of San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the ten Beta World Cities. ?", "answer": "San Trancisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Trancisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Beta World Cities are:", "extracted_entity": null}
{"question": "the only consolidated city-county in California, does it encompass a land area of on the northern end of the San Francisco Peninsula, making it the second most densely populated city in the United States?", "answer": "San Francisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco", "extracted_entity": null}
{"question": "is San Francisco also the financial, cultural, and transportation center of the larger San Francisco Bay Area, a region of 7.4 million people?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is also the financial, cultural, and", "extracted_entity": null}
{"question": "in what year did the Spanish establish a fort at the Golden Gate and a mission named for Francis of Assisi on the site?", "answer": "In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": null}
{"question": "does Swahili have dipthongs?", "answer": "Yes, Swahili has diphthongs. (Typo)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili has diphthongs. (Typo)"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Swahili a Bantu language?", "answer": "Yes, Swahili is a Bantu language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is a Bantu language.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did Uganda make Swahili a required subject in primary schools?", "answer": "Yes, Uganda made Swahili a required subject in primary schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Uganda made Swahili a required subject in primary schools.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili", "extracted_entity": null}
{"question": "what is the Swahili word for the Swahili language?", "answer": "Kiswahili is the Swahili word for the Swahili language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kiswahili is the Swahili word for the Swahili language.", "Kiswahili"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Kiswahili", "extracted_entity": null}
{"question": "is Swahili spoken in Mozambique?", "answer": "Yes, Swahili is spoken in parts of northern Mozambique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is spoken in parts of northern Mozambique.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does Swahili time run?", "answer": "Swahili time runs from dawn to dusk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili time runs from dawn to dusk.", "from dawn to dusk"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili time runs at the same time as", "extracted_entity": null}
{"question": "how many noun classes does Swahili have?", "answer": "Swahili has sixteen noun classes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili has sixteen noun classes.", "sixteen"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "15", "extracted_entity": null}
{"question": "in what year did Germany seize Tanganyika?", "answer": "Germany seized Tanganyika in 1886.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany seized Tanganyika in 1886.", "1886"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": null}
{"question": "which is the sub-Saharan indigenous language with the greatest number of speakers?", "answer": "Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "Hausa"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Swedish a descendant of Old Norse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "Swedish", "extracted_entity": null}
{"question": "does Swedish use the perfect participle to form the present perfect tense?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "how many Swedish speakers were reported in Canada in 2001?", "answer": "16,915.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["16,915.", "There are 16,915 reported Swedish speakers in Canada."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "14,000", "extracted_entity": null}
{"question": "how many letters are in the basic Latin alphabet?", "answer": "26.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["26.", "26"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "26", "extracted_entity": null}
{"question": "when did a standard and common language become available to all Swedes?", "answer": "In the 20th Century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the 20th Century.", "It was during the 20th century that a common, standardized national language became available to all Swedes."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "1906", "extracted_entity": null}
{"question": "when did the Soviet military forces occupy Estonia?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "In 1944."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "1944", "extracted_entity": null}
{"question": "what are the differences between English and Swedish pronouns?", "answer": "Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "English pronouns have three genders: mascul", "extracted_entity": null}
{"question": "how does Swedish language counterbalance the absence of cases?", "answer": "The lack of cases in Swedish is compensated by a wide variety of prepositions.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lack of cases in Swedish is compensated by a wide variety of prepositions."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "Swedish language counterbalances the absence of cases by", "extracted_entity": null}
{"question": "is Taipei in a valley?", "answer": "Taipei is in the valleys of the Keelung and Xindian Rivers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei is in the valleys of the Keelung and Xindian Rivers", "Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Taipei founded in the 18th century?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei City", "extracted_entity": null}
{"question": "is the National Palace Museum in Taipei?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is along the city's western border?", "answer": "The Danshui River borders Taipei to the west.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Danshui River borders Taipei to the west.", "Taipei has the Danshui River along its western border."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The city's western border is the westernmost", "extracted_entity": null}
{"question": "what is Taipei 101?", "answer": "Taipei 101 is a 101-floor landmark skyscraper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei 101 is a 101-floor landmark skyscraper."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei 101 is a land", "extracted_entity": null}
{"question": "what are some annual festivals in Taipei?", "answer": "The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "Some annual festivals include Tomb-Sweping Day, the Dragon Boat Festival, the Ghost Festival, and the Mid-Autumn Festival."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when is the Pacific typhoon season?", "answer": "Between June and October.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Between June and October.", "It's between June and October."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "2020", "extracted_entity": null}
{"question": "who sets up shrines?", "answer": "Many homes and business set up shrines.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many homes and business set up shrines.", "Many homes and businesses would set up shrines."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The shrine-building activity is usually associated with", "extracted_entity": null}
{"question": "what does Taipei produce?", "answer": "Taipei produces high technology and hi-tech components", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei produces high technology and hi-tech components", "Taipei produces textiles, apparel, electronics, machinery, printed materials, precision equipment, food, and beverages."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the trumpet the musical instrument with the highest register in the brass family?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a trumpet played by blowing air through closed lips?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the most common type of trumpet the B trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is the earliest date of the trumpet?", "answer": "1500 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1500 BC", "around 1500 BC"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "1500 BC", "extracted_entity": null}
{"question": "what is a trumpet constructed of?", "answer": "brass tubing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass tubing", "brass tubing bent twice into an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Brass", "extracted_entity": null}
{"question": "what are the smallest trumpets referred to as?", "answer": "Piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piccolo trumpets", "piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Piccolo Trumpet", "extracted_entity": null}
{"question": "why do trumpets have a bright, loud sound?", "answer": "The trumpet and trombone share a roughly cylindrical bore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The trumpet and trombone share a roughly cylindrical bore.", "Due to the cylindrical bore"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Trumpets have a bright, loud sound because of", "extracted_entity": null}
{"question": "why does the cornet have a slightly mellower tone than the trumpet?", "answer": "The cornet and flugelhorn have conical bores", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cornet and flugelhorn have conical bores", "because it has conical bores"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The cornet has a slightly mellower tone", "extracted_entity": null}
{"question": "what makes a trumpet fully chromatic?", "answer": "able to play all twelve pitches of Western music", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["able to play all twelve pitches of Western music"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The trumpet is fully chromatic because it", "extracted_entity": null}
{"question": "were trumpet players heavily guarded?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are pocket trumpets compact B trumpets?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Dizzy Gillespie a famous trumpeter in 1998?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Dizzy Gillespie", "extracted_entity": null}
{"question": "what shape is a trumpet bent into?", "answer": "oblong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["oblong", "an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a trumpet is bent into a horn.", "extracted_entity": null}
{"question": "what trumpet was the first to be allowed in the Christian Church?", "answer": "slide trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slide trumpets", "Slide trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the trumpet can be confused with what close relative?", "answer": "the cornet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cornet", "the coronet"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the cornet", "extracted_entity": null}
{"question": "what can be augmented with a fourth valve?", "answer": "the flugelhorn", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the flugelhorn", "flugelhorn"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the Tesla Coil", "extracted_entity": null}
{"question": "what was Maynard Ferguson noted for being able to play accurately?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was built to play in both B and A keys?", "answer": "piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Pianoforte", "extracted_entity": null}
{"question": "what happened in 1894?", "answer": "Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\""], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The 1894 Coxey'", "extracted_entity": null}
{"question": "what happened in nineteen fifty seven?", "answer": "Maynard Ferguson formed his own band.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maynard Ferguson formed his own band."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Winston Smith's mother was killed in", "extracted_entity": null}
{"question": "is the Breeze Eazy' ' method sometimes used to teach younger students?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do many players use a smaller mouthpiece on the piccolo trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "piccolo trumpet", "extracted_entity": null}
{"question": "is the trumpet constructed of brass tubing bent twice into an oblong shape?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the most influential musicians of the 20th century. ?", "answer": "Miles Davis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Miles Davis"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The most influential musicians of the 2", "extracted_entity": null}
{"question": "were Slide trumpets the first trumpets allowed in the Christian church?", "answer": "Slide trumpets were the first trumpets allowed in the Christian church. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Slide trumpets were the first trumpets allowed in the Christian church. "], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Slide trumpets were the first trump", "extracted_entity": null}
{"question": "are they constructed of brass tubing bent twice into an oblong shape, and are played by blowing air through closed lips, producing a ``buzzing'' sound which starts a standing wave vibration in the air column inside the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there several types of trumpet?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, there are several types of trumpet", "extracted_entity": null}
{"question": "was the Ottoman script replaced with a variant of the Latin alphabet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is there a definite article in Turkish language?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it possible to alter the word order to stress the importance of a certain phrase?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is possible to alter the word order", "extracted_entity": null}
{"question": "what is the official language of Turkey?", "answer": "Turkish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turkish.", "Turkish is the official language of Turkey."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish", "extracted_entity": null}
{"question": "when was the Turkish Language Association founded?", "answer": "In 1928.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1928.", "The Turkish Language Association was founded in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "1932", "extracted_entity": null}
{"question": "what are the most significant foreign contributors to Turkish vocabulary?", "answer": "Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. "], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who were the Orkhon inscriptions built for?", "answer": "For the prince Kul Tigin and his brother Emperor Bilge Khan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For the prince Kul Tigin and his brother Emperor Bilge Khan.", "The Orkhon inscriptions were erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was one of the results of the introduction of the new Turkish alphabet?", "answer": "A dramatic increase in literacy from Third World levels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A dramatic increase in literacy from Third World levels.", "There was a dramatic increase in literacy from its original Third World levels."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why does Lewis use \"absolute case\" instead of \"nominative\"?", "answer": "Because it is also used for the indefinite accusative.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is also used for the indefinite accusative."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Lewis uses \"absolute case\" instead of \"n", "extracted_entity": null}
{"question": "how many people speak Turkish worldwide?", "answer": "There are roughly another 10 million native speakers worldwide.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are roughly another 10 million native speakers worldwide."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "80 million people speak Turkish.", "extracted_entity": null}
{"question": "in Turkish, which syllable usually has the stress?", "answer": "Stress is usually on the last syllable", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stress is usually on the last syllable"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the spelling of Turkish phonetic?", "answer": "yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what percentage of the words in Turkish vocabulary have Turkish origins?", "answer": "86% of the Turkish vocabulary have Turkish origins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["86% of the Turkish vocabulary have Turkish origins."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "80%", "extracted_entity": null}
{"question": "after what event was the Turkish Language Association established?", "answer": "After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what exception to the rules of vowel harmony do compound words have?", "answer": "In compound words, the vowels need not harmonize between the constituent words of the compound.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In compound words, the vowels need not harmonize between the constituent words of the compound."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the patterns of vowel harmony?", "answer": "Vowels are characterised by two features: front/back and rounded/unrounded.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowels are characterised by two features: front/back and rounded/unrounded."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "[Vowel harmony is a type of ph", "extracted_entity": null}
{"question": "what are the ways to join groups of nouns?", "answer": "Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "in a sentence", "extracted_entity": null}
{"question": "does Vietnamese have a comparatively large number of vowels?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Vietnamese the mother tongue of the Vietnamese people?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Vietnamese the official administrative language of Vietnam?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Vietnamese", "extracted_entity": null}
{"question": "was Vietnamese influenced by Chinese?", "answer": "Much vocabulary has been borrowed from Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much vocabulary has been borrowed from Chinese.", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did written Vietnamese become the official administrative language?", "answer": "the 20th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 20th century", "Vietnamese became the official administrative language by the 20th century."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have consonant clusters been lost?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the Red River Delta traditionally poor?", "answer": "Not compared to Nghe An, Ha Tinh, or Quang Binh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Not compared to Nghe An, Ha Tinh, or Quang Binh", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what kind of words have been borrowed from Chinese?", "answer": "Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "Vietnamese adopts words from Chinese which express abstract ideas."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Vietnam independent from France?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "was Vietnamese formally written using the Chinese writing system?", "answer": "Yes, Vietnamese was *formerly* written using the Chinese writing system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese was *formerly* written using the Chinese writing system."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Vietnamese have a large number of vowels?", "answer": "Yes, Vietnamese has a comparatively large number of vowels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese has a comparatively large number of vowels."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Vietnamese has a large number of vowels", "extracted_entity": null}
{"question": "what are the three dialect regions of Vietnamese?", "answer": "The three regions are North, Central, and South.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three regions are North, Central, and South."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Northern, Central and Southern", "extracted_entity": null}
{"question": "when was the reunification of Vietnam?", "answer": "The reunification of Vietnam occurred in 1975-76.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The reunification of Vietnam occurred in 1975-76."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many tones do Northern varieties have?", "answer": "Northern Vietnamese has six tones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Vietnamese has six tones."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "does Vietnamese borrow from Latin and Greek?", "answer": "No, Vietnamese does not borrow from Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Vietnamese does not borrow from Latin and Greek."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Vietnamese come to predominate politically in 2nd century BC?", "answer": "No, Chinese came to predominate politically in the 2nd century B.C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese came to predominate politically in the 2nd century B.C."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "The Vietnamese people, also known as the Kin", "extracted_entity": null}
{"question": "do more northern speakers speak the southern regional dialect than southern speakers speak the northern dialect?", "answer": "More southern speakers speak the northern dialect than northerners speak the suthern dialect.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More southern speakers speak the northern dialect than northerners speak the suthern dialect."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what happened in 1860?", "answer": "Vincent van Gogh attended the Zundert village school from 1860.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vincent van Gogh attended the Zundert village school from 1860."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who frequented the circle of the British-Australian artist John Peter Russell?", "answer": "Van Gogh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Van Gogh"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "John Peter Russell", "extracted_entity": null}
{"question": "were the letters first annotated in 1913 by Theo's widow Johanna van Gogh-Bonger?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Theo's widow Johanna van G", "extracted_entity": null}
{"question": "was Kee seven years older than Van Gogh as well as had an eight-year-old son?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Kee was seven years older than Van Gog", "extracted_entity": null}
{"question": "did he suffer from anxiety and increasingly frequent bouts of mental illness throughout his life, and died largely unknown, at the age of 37, from a self-inflicted gunshot wound?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "appreciated during his lifetime, did his fame grow in the years after his death?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is today, he widely regarded as one of history's greatest painters and an important contributor to the foundations of modern art?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "is a painting by the 19th-", "extracted_entity": null}
{"question": "does a violin have four strings?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violin has four strings.", "extracted_entity": null}
{"question": "is the violin supported by the left shoulder?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the violin shaped like an hourglass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what were strings first made out of?", "answer": "Sheep Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sheep Gut", "Sheep gut."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Gut", "extracted_entity": null}
{"question": "what is a violin called informally?", "answer": "Fiddle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fiddle", "A fiddle."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "fiddle", "extracted_entity": null}
{"question": "what does vitula mean?", "answer": "Stringed Instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stringed Instrument", "Stringed instrument."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the Latin word for \"she-goat\"", "extracted_entity": null}
{"question": "what is a person that makes or repairs violins called?", "answer": "Luthier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luthier", "A luthier, or simply a violin maker."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a luthier", "extracted_entity": null}
{"question": "what does the \"voice\" of a violin depend upon?", "answer": "Shape", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shape", "Its shape."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the violin a percussion instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what happened in 1980s?", "answer": "The use of traditional (non-synthesized) orchestras declined.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The use of traditional (non-synthesized) orchestras declined."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1980s was a decade", "extracted_entity": null}
{"question": "who features violinist Boyd Tinsley?", "answer": "Dave Matthews Band features violinist Boyd Tinsley.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dave Matthews Band features violinist Boyd Tinsley."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Boyd Tinsley", "extracted_entity": null}
{"question": "regarding this topic, what did Antonio Stradivari do?", "answer": "Antonio Stradivari made violins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antonio Stradivari made violins."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Antonio Stradivari was an Italian luthier", "extracted_entity": null}
{"question": "when did Charles IX order Amati to construct 24 violins for him?", "answer": "1560", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1560"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1570", "extracted_entity": null}
{"question": "is a string first tuned to a standard pitch?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is each node at an integer division of the string?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the tuning G-D-A-E used for most violin music?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello?", "answer": "Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Violin", "extracted_entity": null}
{"question": "is a violin sometimes informally called a fiddle, regardless of the type of music played on it?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does the word ``violin'' come from the Middle Latin word vitula, meaning ``stringed instrument''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is the xylophone a musical instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, it is a musical instrument.", "extracted_entity": null}
{"question": "was the xylophone associated with the folk music of the United States by the 19th century?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone", "extracted_entity": null}
{"question": "was it likely that the xylophone reached Europe during the Crusades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the xylophone was likely to have reached", "extracted_entity": null}
{"question": "the xylophone is an instrument from what family?", "answer": "the percussion family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the percussion family", "Percussion"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "percussion", "extracted_entity": null}
{"question": "how do the Java and Bali use xylophones?", "answer": "In gamelan ensembles", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In gamelan ensembles"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Java and Bali use xylophones", "extracted_entity": null}
{"question": "some versions of xylophones have resonators made of what?", "answer": "gourds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["gourds", "Gourds."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "metal", "extracted_entity": null}
{"question": "which came first, the xylophone or the vibraphone?", "answer": "xylophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["xylophone", "The xylophone."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Xylophone", "extracted_entity": null}
{"question": "did the golden age of xylophones come before or after the first usage of the European-derived orchestral?", "answer": "after", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["after", "After."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does kelon increase or decrease tone quality of the xylophone?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Decrease."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "kelon decreases tone quality of the x", "extracted_entity": null}
{"question": "do the different species of zebras interbreed?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do zebras sleep standing up?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "zebras can sleep standing up, but it", "extracted_entity": null}
{"question": "have plains zebras been crossed with mountain zebras?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Equus quagga", "extracted_entity": null}
{"question": "how many species of zebra are there?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three", "Three"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do zebras eat?", "answer": "mainly grass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly grass", "Grasses, shrubs, herbs, twigs, leaves, and bark"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "grass", "extracted_entity": null}
{"question": "what are zebras hunted for?", "answer": "mainly for their skins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly for their skins", "Skins"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "meat", "extracted_entity": null}
{"question": "what areas do the Grevy's Zebras inhabit?", "answer": "semi-arid grasslands of Ethiopia and northern Kenya", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["semi-arid grasslands of Ethiopia and northern Kenya", ""], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Ethiopia, Kenya, Somalia,", "extracted_entity": null}
{"question": "which species of zebra is known as the common zebra?", "answer": "Plains Zebra (Equus quagga, formerly Equus burchelli)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plains Zebra (Equus quagga, formerly Equus burchelli)", "Plains Zebra"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "at what age can a zebra breed?", "answer": "five or six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["five or six", "5 or 6"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
