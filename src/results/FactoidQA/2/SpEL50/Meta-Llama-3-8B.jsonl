{"question": "was Abraham Lincoln the sixteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Abraham_Lincoln;President_of_the_United_States"}
{"question": "did Lincoln sign the National Banking Act of 1863?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "National_Bank_Act"}
{"question": "did his mother die of pneumonia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pneumonia"}
{"question": "how many long was Lincoln's formal education?", "answer": "18 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["18 months", "18 months."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "18 months", "extracted_entity": null}
{"question": "when did Lincoln begin his political career?", "answer": "1832", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1832", "1832."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1832, he ran for the Illinois state", "extracted_entity": null}
{"question": "what did The Legal Tender Act of 1862 establish?", "answer": "the United States Note, the first paper currency in United States history", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United States Note, the first paper currency in United States history", "The United States Note, the first paper currency in United States history."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The Legal Tender Act of 1862 established the", "extracted_entity": null}
{"question": "who suggested Lincoln grow a beard?", "answer": "11-year-old Grace Bedell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11-year-old Grace Bedell", "Grace Bedell."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln's son Tad", "extracted_entity": "Abraham_Lincoln"}
{"question": "when did the Gettysburg address argue that America was born?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776", "1776."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "It did not argue that America was born, but", "extracted_entity": null}
{"question": "did Lincoln beat John C. Breckinridge in the 1860 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No, he beat him in the 1864", "extracted_entity": "Abraham_Lincoln;John_C._Breckinridge"}
{"question": "was Abraham Lincoln the first President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No, Abraham Lincoln was the 16th President", "extracted_entity": "Abraham_Lincoln;President_of_the_United_States;United_States"}
{"question": "did Lincoln start his political career in 1832?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No, he did not. He started his political", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Lincoln ever represent Alton & Sangamon Railroad?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "no, he never represented the railroad", "extracted_entity": null}
{"question": "which county was Lincoln born in?", "answer": "Hardin County", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hardin County", "Southeast Hardin County, Kentucky"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln County, Kentucky", "extracted_entity": null}
{"question": "when did Lincoln first serve as President?", "answer": "March 4, 1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1861"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1861", "extracted_entity": "Abraham_Lincoln"}
{"question": "who assassinated Lincoln?", "answer": "John Wilkes Booth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Wilkes Booth"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "John Wilkes Booth", "extracted_entity": null}
{"question": "did Lincoln win the election of 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Abraham_Lincoln"}
{"question": "who was the general in charge at the Battle of Antietam?", "answer": "General McClellan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General McClellan", "John Alexander McClernand"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Robert E. Lee", "extracted_entity": "Battle_of_Antietam"}
{"question": "why did Lincoln issue the Emancipation Proclamation?", "answer": "To free slaves ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To free slaves ", "The goal was to weaken the rebellion, which was led and controlled by slave owners."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "because the south had seceded from the union", "extracted_entity": "Abraham_Lincoln;Emancipation_Proclamation"}
{"question": "do scholars rank lincoln among the top three presidents?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "i have never seen a scholar rank lincoln among", "extracted_entity": null}
{"question": "did lincoln have 18 months of schooling?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, Lincoln had 18 months of schooling.", "extracted_entity": null}
{"question": "was Lincoln chosen as a presidential candidate in 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln was chosen as a presidential candidate in 186", "extracted_entity": "Abraham_Lincoln"}
{"question": "how old was Lincoln in 1816?", "answer": "seven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["seven"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "he was 8 years old.", "extracted_entity": null}
{"question": "when was the first photgraph of lincoln taken?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "In 1846, Lincoln was photographed by an", "extracted_entity": null}
{"question": "how long was Lincoln's legal Career?", "answer": "23 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["23 years"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln was a lawyer for 23 years. He", "extracted_entity": null}
{"question": "what trail did Lincoln use a Farmers' Almanac in? ?", "answer": "he defended William \"Duff\" Armstrong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he defended William \"Duff\" Armstrong"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "He used it to predict the weather", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Abraham Lincoln live in the Frontier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Lincoln's Wife's Family support slavery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "no, they did not, but they did not", "extracted_entity": null}
{"question": "who is most noted for his contributions to the theory of molarity and molecular weight?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro", "extracted_entity": null}
{"question": "who graduated in ecclesiastical law at the early age of 20 and began to practice?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro", "blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "a man named Thomas Jefferson", "extracted_entity": null}
{"question": "when did he publish another memoria?", "answer": "1821", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1821"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "In 1900, with a preface by", "extracted_entity": null}
{"question": "when did he become a professor?", "answer": "1820", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1820"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "in 1965, after he had received his", "extracted_entity": null}
{"question": "is it true that he became a professor in 1820?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he became a professor in 1821", "extracted_entity": null}
{"question": "was Lorenzo Romano Amedeo Carlo Avogadro an Italian savant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Italy"}
{"question": "was Amedeo Avogadro born in Turin August 9th 1776 to a noble ancient family of Piedmont, Italy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Turin August", "extracted_entity": "Turin;Piedmont;Italy"}
{"question": "is he most noted for his contributions to the theory of molarity and molecular weight?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "He is most noted for his contributions to the theory", "extracted_entity": null}
{"question": "was King Victor Emmanuel III there to pay homage to Avogadro ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Victor_Emmanuel_III_of_Italy"}
{"question": "is Avogadro 's number commonly used to compute the results of chemical reactions ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "o. It is used to compute the amount of", "extracted_entity": null}
{"question": "did the scientific community not reserve great attention to his theory ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "they did not. This was because the theory was", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", ""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "\u201cOn the possibility of the existence of a new", "extracted_entity": null}
{"question": "what happened in french?", "answer": "", "dataset": "factoid_qa", "split": "train", "answer_aliases": [""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the french language evolved from latin, and is related", "extracted_entity": "French_language"}
{"question": "what happened in 1833?", "answer": "Avogadro had been recalled to Turin university", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro had been recalled to Turin university", "blah blah blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the American Anti-Slavery Society was founded.", "extracted_entity": null}
{"question": "who determined the dependence of the boiling of water with atmospheric pressure?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the Italian physicist evaneglisti Galileo", "extracted_entity": null}
{"question": "what is named after him?", "answer": "The Celsius crater on the Moon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Celsius crater on the Moon"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the eponymous theorem, of course.", "extracted_entity": null}
{"question": "when did he publish a collection?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1969, 1971, 197", "extracted_entity": null}
{"question": "is it true that he published a collection in 1738?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he did. In 1738 he", "extracted_entity": null}
{"question": "is it true that thermometer had 100 for the freezing point?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no, it is not true.", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No, he was born in Uppsala in", "extracted_entity": "Uppsala;Sweden"}
{"question": "was Anders Celsius (November 27, 1701 April 25, 1744) a Swedish astronomer?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Anders_Celsius;Sweden"}
{"question": "is The Celsius crater on the Moon named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Moon"}
{"question": "who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius", "extracted_entity": null}
{"question": "the Celsius crater on the Moon is what?", "answer": "named after him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["named after him"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "a crater", "extracted_entity": "Moon"}
{"question": "is the Celsius crater on the Moon named after him ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Moon"}
{"question": "had his thermometer 100 for the freezing point of water and 0 for the boiling point ?", "answer": "Yes it had", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes it had"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "In the Celsius scale, the freezing point of water", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "es, Celsius was born in Uppsala in", "extracted_entity": "Uppsala;Sweden"}
{"question": "did he not determine the dependence of the boiling of water with atmospheric pressure -LRB- in excellent agreement with modern data -RRB- ?", "answer": "Yes he did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he did"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No. He did not determine the boiling point of", "extracted_entity": null}
{"question": "what happened from 1730 to 1744?", "answer": "He was professor of astronomy at Uppsala University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was professor of astronomy at Uppsala University"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the War of the Austrian Succession", "extracted_entity": null}
{"question": "what happened in 1745?", "answer": "The scale was reversed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Bonnie Prince Charlie invaded England and tried to overthrow the", "extracted_entity": null}
{"question": "are beetles insects?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, beetles are insects.", "extracted_entity": null}
{"question": "can beetles be found in polar regions?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no, they can't.", "extracted_entity": null}
{"question": "do beetles antennae function primarily as organs of smell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, they are used to smell pherom", "extracted_entity": null}
{"question": "what are the three sections of a beetle?", "answer": "the head, the thorax, and the abdomen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the head, the thorax, and the abdomen", "The head, the thorax, and the abdomen"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "head, thorax, abdomen", "extracted_entity": "Beetle"}
{"question": "which defense mechanism uses colour or shape to deceive potential enemies?", "answer": "mimicry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mimicry", "Mimicry"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "camouflage", "extracted_entity": null}
{"question": "which type of beetle is a pest of potato plants?", "answer": "Colorado potato beetle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colorado potato beetle"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Colorado potato beetle", "extracted_entity": "Beetle"}
{"question": "how can beetle larvae be differentiated from other insect larvae?", "answer": "their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "By their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "larvae of beetle can be differentiated from other insect larvae", "extracted_entity": "Beetle"}
{"question": "what do beetles eat?", "answer": "Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "beetles eat almost anything.", "extracted_entity": null}
{"question": "what are the similarities between beetles and grasshoppers?", "answer": "mouthparts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mouthparts", "Beetles have mouthparts similar to those of grasshoppers"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "both have wings, both have hard outer shell,", "extracted_entity": "Beetle;Grasshopper"}
{"question": "are certain species of beetles considered pests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, some beetles are considered pests, such", "extracted_entity": null}
{"question": "is a beetle's general anatomy uniform?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The anatomy of a beetle is uniform.", "extracted_entity": "Beetle"}
{"question": "are beetles endopterygotes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they are endopterygotes.", "extracted_entity": null}
{"question": "how many species of beetles are there?", "answer": "350,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["350,000", "about 350,000"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "around 400,000", "extracted_entity": null}
{"question": "when are sperm cells transferred to the female?", "answer": "during pairing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["during pairing", "During pairing"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "when the male inserts his penis into the females vagina", "extracted_entity": null}
{"question": "what is the study of beetles called?", "answer": "coleopterology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["coleopterology"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "entomology", "extracted_entity": null}
{"question": "is it possible that there are more than 350,000 species of beetles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, in fact, there are more than", "extracted_entity": null}
{"question": "is the Adephaga suborder larger than the Polyphaga suborder?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes. The Polyphaga suborder is the", "extracted_entity": "Polygonal_chain"}
{"question": "do carrion beetles eat dung?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they are scavengers, they eat dead", "extracted_entity": null}
{"question": "what are prey of various animals including birds and mammals?", "answer": "Beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The prey of various animals including birds and mammals is", "extracted_entity": "Bird"}
{"question": "what was given by Aristotle for the hardened shield like forewings?", "answer": "The name \"Coleoptera\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name \"Coleoptera\"."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "exoskeleton", "extracted_entity": "Aristotle"}
{"question": "who or what vary greatly in form within the coleoptera?", "answer": "Antennae.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antennae."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "beetle", "extracted_entity": null}
{"question": "are many beetles territorial?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no, they are not territorial.", "extracted_entity": null}
{"question": "are beetles endopterygotes with complete metamorphosis?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "oxygen is what?", "answer": "One kind of gas obtained via a tracheal system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One kind of gas obtained via a tracheal system."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "a chemical element", "extracted_entity": null}
{"question": "is there a thriving industry in the collection of beetle specimens for amateur and professional collectors ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Beetle"}
{"question": "have coleopterists formed organisations to facilitate the study of beetles ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Beetle"}
{"question": "is the study of beetles called coleopterology , and its practitioners are coleopterists ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "es, and yes.", "extracted_entity": null}
{"question": "may a single female lay from several dozen to several thousand eggs during her lifetime ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, she may", "extracted_entity": null}
{"question": "can beetles be found in almost all habitats?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "what happened on plants and fungi?", "answer": "They are food to beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are food to beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "the fungi and plants had a symbiotic relationship,", "extracted_entity": null}
{"question": "was Calvin Coolidge the twenty-ninth vice president?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Calvin_Coolidge"}
{"question": "was Calvin Coolidge born in Plymouth, Windsor County, Vermont?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, Calvin Coolidge was born in Plymouth,", "extracted_entity": "Calvin_Coolidge;Plymouth,_Vermont;Windsor_County,_Vermont"}
{"question": "did Coolidge graduate from Black River Academy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Calvin_Coolidge"}
{"question": "when did he drop John from his name?", "answer": "upon graduating from college", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon graduating from college", "Upon graduating from college"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "when he became a Christian", "extracted_entity": null}
{"question": "when did Coolidge meet and marry Grace Anna Goodhue?", "answer": "1905", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1905", "In 1905"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "they met in 1905 and married in", "extracted_entity": "Calvin_Coolidge"}
{"question": "what period of rapid economic growth did the United States experience during Coolidge's presidency?", "answer": "the Roaring Twenties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Roaring Twenties", "the \"Roaring Twenties\""], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "the 1920s", "extracted_entity": "United_States"}
{"question": "what did Coolidge do after graduating from Amherst?", "answer": "Coolidge moved to Northampton, Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge moved to Northampton, Massachusetts", "Coolidge moved to Northampton, Massachusetts to take up the practice of law"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He became a lawyer and worked in the family business", "extracted_entity": "Calvin_Coolidge;Amherst_College"}
{"question": "of what state was Coolidge governor?", "answer": "Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Massachusetts"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Massachusetts", "extracted_entity": "Calvin_Coolidge"}
{"question": "who appointed Harlan Fiske Stone to the Supreme Court?", "answer": "Coolidge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Warren G. Harding", "extracted_entity": "Harlan_F._Stone"}
{"question": "was Coolidge the thirteenth President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No, he was the thirtieth President", "extracted_entity": "Calvin_Coolidge;President_of_the_United_States"}
{"question": "was Calvin Coolidge Republican?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, Calvin Coolidge was Republican", "extracted_entity": "Calvin_Coolidge;Republican_Party_(United_States)"}
{"question": "was Calvin Coolidge a governor of Massachusetts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, he was the governor of Massachusetts from", "extracted_entity": "Calvin_Coolidge;Massachusetts"}
{"question": "when was Coolidge born?", "answer": "July 4 1872", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4 1872", "Plymouth, Windsor County, Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "July 4, 1872", "extracted_entity": "Calvin_Coolidge"}
{"question": "where did Coolidge's grandfather had government offices?", "answer": "Plymouth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth", "The town of Plymouth Notch"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "in Plymouth", "extracted_entity": null}
{"question": "which state were Coolidge born in?", "answer": "Plymouth, Windsor County, Vermont", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth, Windsor County, Vermont", "Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Vermont", "extracted_entity": null}
{"question": "is Calvin Jr. older than John Coolidge?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes, by about 2 years", "extracted_entity": null}
{"question": "did Coolidge get in Amherst College?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No. He did not get in Amherst", "extracted_entity": "Calvin_Coolidge;Amherst_College"}
{"question": "what caused Calvin Jr.'s death?", "answer": "heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["heart attack", "Heart attack"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He died from the same disease that his mother died", "extracted_entity": null}
{"question": "was John Calvin Coolidge Jr. was born in Las Vegas?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No. He was born in Plymouth, Vermont,", "extracted_entity": "Las_Vegas,_Nevada"}
{"question": "was Coolidge opposed in the Republican nomination for Governor of Massachusetts in 1918?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, he was opposed in the Republican nomination for", "extracted_entity": "Calvin_Coolidge;Governor_of_Massachusetts"}
{"question": "did Coolidge meet and marry Grace Anna Goodhue?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Calvin_Coolidge"}
{"question": "what year did Coolidge open his own law office?", "answer": "1898", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1898"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": "Calvin_Coolidge"}
{"question": "what fraternity was Coolidge a member of?", "answer": "Phi Gamma Delta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Phi Gamma Delta", "phi gamma delta"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "he was not a member of a fraternity.", "extracted_entity": null}
{"question": "in 1905 Coolidge met and married whom?", "answer": "Grace Anna Goodhue", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grace Anna Goodhue"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Anna Goodhue", "extracted_entity": "Calvin_Coolidge"}
{"question": "why did Coolidge not attend law school?", "answer": "It was too expensive", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was too expensive", "cost"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He was not interested in law school.", "extracted_entity": "Calvin_Coolidge"}
{"question": "have there been other US presidents that have visited Cuba?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, in 1928, Calvin Coolidge", "extracted_entity": "United_States;Cuba"}
{"question": "what years were Coolidge's two sons born in?", "answer": "1906 and 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1906 and 1908"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge had two sons, John Coolidge and", "extracted_entity": null}
{"question": "did France cede nearly all of its colonies in Europe in 1763?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes. France ceded nearly all of its colonies", "extracted_entity": "France;Europe"}
{"question": "is Canada a member of the OECD?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes. Canada has been a member of the OECD", "extracted_entity": "Canada;Organisation_for_Economic_Co-operation_and_Development"}
{"question": "is Canada's official language Zulu?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, Canada's official language is English.", "extracted_entity": "Zulu_language"}
{"question": "what is one significant non-official language?", "answer": "Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese.", "Chinese"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Latin, French, and Spanish.", "extracted_entity": null}
{"question": "in addition to Port Royal, where else did Samuel de Champlain establish a settlement?", "answer": "Quebec City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec City"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Quebec, New France", "extracted_entity": "Port_Royal;Samuel_de_Champlain"}
{"question": "what was the Consitution Act formerly called?", "answer": "The British North America Act.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British North America Act.", "British North America Act"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the Statute of Westminster 1931", "extracted_entity": null}
{"question": "which countries established colonies in Canada?", "answer": "France and Britain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France and Britain.", "Fance and Britain"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "England and France", "extracted_entity": "Canada"}
{"question": "how has Canada helped UN peacekeeping efforts?", "answer": "During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "Canada has played a leading role in UN peacekeeping efforts. During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada has provided personnel to peacekeeping efforts in the", "extracted_entity": "Canada"}
{"question": "what percentage of the population don't speak English or French at home?", "answer": "28%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28%", "1.5%"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "10.5%", "extracted_entity": "English_language;French_language"}
{"question": "is Canada bilingual?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, it is."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Canada"}
{"question": "did Canadian soldiers win the Battle of Vimy Ridge in 1917?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No, it did not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, they did", "extracted_entity": "Canada;Battle_of_Vimy_Ridge"}
{"question": "is an official language of Canada German?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, it is not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": "Canada"}
{"question": "how many provinces and territories does Canada have?", "answer": "A federation now comprising ten provinces and three territories", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A federation now comprising ten provinces and three territories", "Ten provinces and three territories"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "ten provinces and three territories", "extracted_entity": "Canada"}
{"question": "what is Canada's national unemployment rate?", "answer": "While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "In October 2007, Canada's national unemployment rate is 5.9%."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "7.7% (November 2011", "extracted_entity": null}
{"question": "where is the most densely populated part of Canada?", "answer": "The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The most densely populated part of Canada is the area", "extracted_entity": "Canada"}
{"question": "what is the largest country in the world?", "answer": "Canada is the second largest country in the world, after Russia, and largest on the continent.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada is the second largest country in the world, after Russia, and largest on the continent.", "Russia."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": null}
{"question": "what is the largest religious group in Canada?", "answer": "According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "Christian"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Roman Catholic", "extracted_entity": "Canada"}
{"question": "when did Canada have one of the largest armed forces in the world?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "World War II."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "in the 20th century", "extracted_entity": "Canada"}
{"question": "what have been inhabited for millennia by aboriginal peoples?", "answer": "The lands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lands"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "what have been inhabited for millennia by aboriginal peoples", "extracted_entity": "Aboriginal_peoples_in_Canada"}
{"question": "european books and maps began referring to this region as Canada in what year?", "answer": "1545", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1545"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "In the 1530s, European books and", "extracted_entity": "Europe;Canada"}
{"question": "what was Canada's most important industry until the 1800s?", "answer": "The fur trade", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The fur trade"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "FUR TRADING", "extracted_entity": null}
{"question": "is it true that it angered many residents of the thirteen colonies?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes it is true that it angered many residents of", "extracted_entity": null}
{"question": "how do cuba and declining participation relate?", "answer": "Canada maintains full relations with both.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada maintains full relations with both."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "cuba has a declining participation rate. the cub", "extracted_entity": null}
{"question": "is it true that provinces have a large degree of autonomy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, it is true that provinces have a large", "extracted_entity": null}
{"question": "what did Canada adopt in 1965?", "answer": "The Maple Leaf Flag", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Maple Leaf Flag"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "a national flag", "extracted_entity": "Canada"}
{"question": "what arrived in Britain?", "answer": "The first Canadian Army units in WW II", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first Canadian Army units in WW II"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the Vikings", "extracted_entity": "United_Kingdom"}
{"question": "are Canada 's two official languages English and French ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "es, Canada's two official languages are English", "extracted_entity": "Canada;English_language;French_language"}
{"question": "are Canada 's official national sports ice hockey ( winter ) and lacrosse ( summer ) ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Canada;National_sport;Ice_hockey;Lacrosse"}
{"question": "is Canada a geographically vast and ethnically diverse country ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, Canada is a geographically vast and ethn", "extracted_entity": "Canada"}
{"question": "did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ?", "answer": "no!", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no!"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nova_Scotia_peninsula;Treaty_of_Utrecht"}
{"question": "did Cartier not use the word ` Canada ' to refer to not only that village , but the entire area subject to Donnacona , Chief at Stadacona ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "did not. He used the word", "extracted_entity": "Jacques_Cartier;Canada;Stadacona"}
{"question": "did continental European immigrants not settle the prairies ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, they did not. They were mainly from", "extracted_entity": null}
{"question": "is it the world 's second largest country by total area?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "(answer)", "extracted_entity": "List_of_sovereign_states_and_dependencies_by_area"}
{"question": "what happened in 1867?", "answer": "Canada was formed as a federal, semi-autonomous polity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada was formed as a federal, semi-autonomous polity"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada was born, the Dominion of Canada", "extracted_entity": null}
{"question": "are ducks in the Arctic Northern Hemisphere migratory?", "answer": "Some are", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes, they are. Arctic ducks migrate south in", "extracted_entity": "Northern_Hemisphere"}
{"question": "is a drake a male?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes. A male duck is called a drake", "extracted_entity": null}
{"question": "do all ducks \"quack\"?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, all ducks quack.", "extracted_entity": null}
{"question": "what is the name of a specialized species of duck adapted to catch large fish?", "answer": "the smew", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the smew"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "cormorant", "extracted_entity": null}
{"question": "what animal attracts the most humor and silliness?", "answer": "The duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The duck", "the duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a cat.", "extracted_entity": null}
{"question": "what is an economic use of a duck?", "answer": "Meat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat", "being farmed for their meat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "it is a quack pack", "extracted_entity": null}
{"question": "who is Daffy Duck?", "answer": "A silly cartoon character", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A silly cartoon character", "a silly cartoon character"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Daffy Duck is a fictional character created by", "extracted_entity": "Daffy_Duck"}
{"question": "what makes it more difficult for a diving duck to fly?", "answer": "They are heavier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are heavier", "they are heavier"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "their wings are too short", "extracted_entity": null}
{"question": "what are the reasons for hunting wild ducks?", "answer": "Meat, eggs, and feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "ducks are hunted because they are tasty, their meat", "extracted_entity": null}
{"question": "do ducks forage underwater?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Diving ducks and sea ducks forage underwater"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "no. they don't. they dive under water", "extracted_entity": null}
{"question": "is foie gras often made from ducks?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, it's often made from ducks", "extracted_entity": null}
{"question": "what economic uses to ducks have?", "answer": "They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "They can be farmed for meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "they can be used as meat", "extracted_entity": null}
{"question": "what kind of ducks feed on land?", "answer": "Dabbling ducks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dabbling ducks", "Dabbling ducks feed on land"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Ducks that feed on land are called dabbling ducks", "extracted_entity": null}
{"question": "what unrelated water birds are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "swans and geese", "extracted_entity": null}
{"question": "what does the word duck mean?", "answer": "It is the common name for a number of species in the Anatidae family of birds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the common name for a number of species in the Anatidae family of birds.", "to bend down low as if to get under something"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "the answer is duck means duck", "extracted_entity": null}
{"question": "what are some common predators of ducks?", "answer": "Pike, crocodilians, herons, hawks and eagles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pike, crocodilians, herons, hawks and eagles.", "pike, crocodilians, and other aquatic hunters"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Foxes, raccoons, weasels", "extracted_entity": null}
{"question": "how does a Mallard's tongue work?", "answer": "It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "It uses short spikes to push struggling prey and other food down its throat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Mallards are dabbling ducks, which means they", "extracted_entity": null}
{"question": "are diving ducks heavier tha dabbling ducks?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, because they are deeper and their body is", "extracted_entity": null}
{"question": "are adult ducks fast fliers?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "are ducks an accepted presence in some populated areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes. In the past, they have been accepted", "extracted_entity": null}
{"question": "what types of unrelated water birds with similar forms are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "swans, geese, and loons", "extracted_entity": null}
{"question": "why are ducklings particularly vulnerable?", "answer": "Their inability to fly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their inability to fly.", "their inability to fly makes them easy prey"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "they are born with soft skin and are unable to", "extracted_entity": null}
{"question": "what are some economic uses for duck?", "answer": "Meat,eggs,feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat,eggs,feathers", "being farmed for their meat, eggs, feathers, (particularly their down)"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "duck eggs are used in many different ways. You", "extracted_entity": null}
{"question": "what allows a duck to filter water out of the side of their beaks and keep food inside?", "answer": "Tiny rows of plates called lamellae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tiny rows of plates called lamellae", "tiny rows of plates called lamellae like a whale's baleen"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "the hairs on the inside of their beak.", "extracted_entity": null}
{"question": "what expression is part of a conceptual framework for testing computer systems?", "answer": "Quacks like a duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quacks like a duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a test case", "extracted_entity": null}
{"question": "where did the expression \"a sitting duck\" come from?", "answer": "In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "In many areas, wild ducks of various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys."], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "It is a reference to the duck decoy used", "extracted_entity": null}
{"question": "is Egypt bordered by the Gaza Strip?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "es, Egypt is bordered by the Gaza Strip.", "extracted_entity": "Egypt;Gaza_Strip"}
{"question": "is Egypt the most populated country in Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt;Africa"}
{"question": "does Egypt have political influence in the Middle East?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Yes, Egypt is a regional power in the Middle", "extracted_entity": "Egypt;Middle_East"}
{"question": "when was the Six Day War?", "answer": "1967", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1967"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "967", "extracted_entity": "Six-Day_War"}
{"question": "what religions has Egypt outlawed?", "answer": "All but Christianity, Islam, and Judaism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["All but Christianity, Islam, and Judaism", "all religions and belief except Islam, Christianity and Judaism"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Islam and Christianity", "extracted_entity": "Egypt"}
{"question": "what is the poulation of Egypt?", "answer": "more than 78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["more than 78 million", "78 million people"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "102,277,000", "extracted_entity": "Egypt"}
{"question": "why does most of Egypt's population live near the Nile?", "answer": "the only arable agricultural land is found there,", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the only arable agricultural land is found there,", "The only arable agricultural land is found there"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The Nile River provided water for agriculture and fishing.", "extracted_entity": "Egypt;Nile"}
{"question": "are there a large number of Jews living in Egypt today?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Egypt"}
{"question": "during what period was Akhenaten a Pharaoh?", "answer": "The New Kingdom (c.1550\u22121070 BC) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The New Kingdom (c.1550\u22121070 BC) ", "The New Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1351\u20131334 BC", "extracted_entity": "Akhenaten"}
{"question": "is Egypt in Asia?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, it is in Africa.", "extracted_entity": "Egypt;Asia"}
{"question": "does Egypt receive the least rainfall in the world?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, it is the second least after Antarctica.", "extracted_entity": "Egypt"}
{"question": "does Egypt's foreign policy operates along moderate lines?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, Egypt's foreign policy operates along moderate lines", "extracted_entity": "Egypt"}
{"question": "since when has Egypt been a republic?", "answer": "June 18 1953.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 18 1953.", "June 18 1953"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1953", "extracted_entity": "Egypt"}
{"question": "when were the Great Sphinx and the Pyramids of Giza built?", "answer": "During the Old Kingdom.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Old Kingdom.", "During the Old Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "the Great Sphinx was built in 2500 BCE", "extracted_entity": "Great_Sphinx_of_Giza;Giza_pyramid_complex"}
{"question": "what countries border Egypt?", "answer": "Libya, Sudan, the Gaza Strip and Israel.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Libya, Sudan, the Gaza Strip and Israel.", "Libya, Sudan, the Gaza Strip and Israel"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The Gaza Strip of Palestine and Israel to the northeast", "extracted_entity": "Egypt"}
{"question": "what is the estimated population of Egypt?", "answer": "More than 78 million.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 78 million.", "78 million"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "98,000,000", "extracted_entity": "Egypt"}
{"question": "does snow fall in Egypt?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Egypt"}
{"question": "when did organized agriculture appear in the Nile Valley?", "answer": "6000 BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6000 BC.", "About 6000 BC"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "organized agriculture appeared in the Nile Valley in the", "extracted_entity": "Nile"}
{"question": "are elephants the largest land animals alive today?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no, they are the 2nd largest.", "extracted_entity": null}
{"question": "can an elephant kill a rhinoceros?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": "Rhinoceros"}
{"question": "are elephants good swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes. Elephants can swim, but they don", "extracted_entity": null}
{"question": "how many living species of African Elephants are there?", "answer": "2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2", "The African Elephant genus contains two (or, arguably, three) living species."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": "African_elephant"}
{"question": "how long may elephants live?", "answer": "70 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70 years", "As long as 70 years, sometimes longer."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "they may live up to 70 years", "extracted_entity": null}
{"question": "why does the phrase 'elephants never forget' have no metaphorical meaning?", "answer": "it refers literally to elephants supposedly having an excellent memory", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it refers literally to elephants supposedly having an excellent memory", "Because it refers literally to elephants supposedly having an excellent memory."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "because elephants are not forgetful, and they don", "extracted_entity": null}
{"question": "how many living species of Asian Elephants are there?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1", "one"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "There is only one living species of Asian Elephant.", "extracted_entity": "Asian_elephant"}
{"question": "how much do elephants weight at birth?", "answer": "120 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["120 kilograms", "At birth it is common for an elephant calf to weigh 120 kilograms (265 lb)."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "220 pounds", "extracted_entity": null}
{"question": "what did Aristotle say about elephants?", "answer": "the beast which passeth all others in wit and mind", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the beast which passeth all others in wit and mind", "Aristotle once said the elephant was \"the beast which passeth all others in wit and mind.\""], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Aristotle was a Greek philosopher and scientist, who lived", "extracted_entity": "Aristotle"}
{"question": "are elephant populations in West Africa generally small and fragmented?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, in general.", "extracted_entity": null}
{"question": "is the Asian elephant larger than the African?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Sometimes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, the Asian elephant is larger than the African", "extracted_entity": "Asian_elephant;Africa"}
{"question": "is an elephant's skin tough?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no, it is soft.", "extracted_entity": null}
{"question": "how do elephants communicate over long distances?", "answer": "by producing and receiving low frequency sound", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by producing and receiving low frequency sound", "By producing and receiving low-frequency sound (infrasound)"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "they use infrasound.", "extracted_entity": null}
{"question": "when do African elephants lie down?", "answer": "when they are sick or wounded", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when they are sick or wounded"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "They lie down when they are very tired.", "extracted_entity": "African_elephant"}
{"question": "how long is the elephant's gestation period?", "answer": "22 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["22 months"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "18-22 months", "extracted_entity": null}
{"question": "how many species of African elephants have been proposed?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": "African_elephant"}
{"question": "what are the elephant's ears important for?", "answer": "temperature regulation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["temperature regulation"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "They are important for detecting sounds. They are very", "extracted_entity": null}
{"question": "what land animal has the largest brain?", "answer": "elephant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["elephant", "The elephant"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the elephant", "extracted_entity": null}
{"question": "are elephants mammals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, they are mammals.", "extracted_entity": null}
{"question": "have elephants been used as working animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, they have been used to carry heavy loads", "extracted_entity": null}
{"question": "do they prefer forested areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, they prefer forested areas.", "extracted_entity": null}
{"question": "what is the world population of Asian elephants?", "answer": "60000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60000", "The world population of Asian elephants \u2013 also called Indian Elephants or Elephas maximus \u2013 is estimated to be around 60,000"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "50,000", "extracted_entity": "Asia"}
{"question": "what part of an elephant is very important for temperature regulation?", "answer": "Ears", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ears", "The large flapping ears of an elephant are also very important for temperature regulation."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the ears", "extracted_entity": null}
{"question": "where was the largest elephant ever recorded shot?", "answer": "Angola", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Angola", "The largest elephant ever recorded was shot in Angola in 1956."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "in Africa, in the Congo", "extracted_entity": null}
{"question": "is an Asian elephant smaller than an African elephant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No, an Asian elephant is about 2.", "extracted_entity": "Asian_elephant;African_elephant"}
{"question": "what land animal is larger than an elephant?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "None, the elephant is the largest land animal."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the blue whale.", "extracted_entity": null}
{"question": "has tusklessness become a rare abnormality?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, tusklessness has become a widespread hereditary trait. "], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "tusklessness is rare, but not abnormal", "extracted_entity": null}
{"question": "were the treaties signed in 1947 and 1948 with the Ukraine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "no, the treaties were signed in 1997", "extracted_entity": "Ukraine"}
{"question": "is Finnish a member of the Baltic-Finnic subgroup of the Uralic languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": ", it is.", "extracted_entity": "Finnish_language;Uralic_languages"}
{"question": "is the most read newspaper in Finland Taloussanomat?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": "Finland"}
{"question": "which Russian army general conquered Finland in 1809?", "answer": "Alexander I", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alexander I"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Alexander I of Russia", "extracted_entity": "Russia;Finland"}
{"question": "what is the life expectancy for men in Finland?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "77.5 years", "extracted_entity": "Finland"}
{"question": "in what year were the \"Games of the XV Olympiad\" held?", "answer": "1952", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1952"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "1936", "extracted_entity": null}
{"question": "what body of water lies to the south of Finland?", "answer": "the Gulf of Finland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Gulf of Finland", "Archipelago Sea"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the Gulf of Finland", "extracted_entity": "Finland"}
{"question": "name an animal that is growing in number due to recent conservation efforts?", "answer": "Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "Golden Eagle"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the tiger. There are currently about 3,", "extracted_entity": null}
{"question": "today, are there more cell phone subscriptions than people in Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Finland"}
{"question": "is Matti Vanhanen the Prime Minister of Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, he is the Prime Minister of Finland.", "extracted_entity": "Matti_Vanhanen;Prime_Minister_of_Finland"}
{"question": "is the capital city Oslo?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "es, Oslo is the capital city of Norway.", "extracted_entity": "Oslo"}
{"question": "does it have a border with Norway?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": "Norway"}
{"question": "what is the life expectancy for men?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "75 years", "extracted_entity": null}
{"question": "what is the biggest city in Finland?", "answer": "Greater Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greater Helsinki", "the cities of the Greater Helsinki metropolitan area"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki", "extracted_entity": "Finland"}
{"question": "who is the most popular rock group in Finland?", "answer": "CMX", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["CMX"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the most popular rock group in Finland is the group", "extracted_entity": "Finland"}
{"question": "what is Finland's economy like?", "answer": "a highly industrialised, free-market economy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a highly industrialised, free-market economy"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland's economy is based on the export of raw", "extracted_entity": "Finland"}
{"question": "where is Finland located?", "answer": "Northern Europe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Europe"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is located in Northern Europe.", "extracted_entity": "Finland"}
{"question": "what is the weather like in summer?", "answer": "relatively warm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["relatively warm"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "it's hot and sunny.", "extracted_entity": null}
{"question": "does salmon remain the favorite of fly rod enthusiasts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Salmon"}
{"question": "are there cathedrals scattered all across Finland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Finland"}
{"question": "according to Reader's Digest, is Finland best for living?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is best for living.", "extracted_entity": "Reader's_Digest;Finland"}
{"question": "when did the first verifiable written documents appear?", "answer": "Twelfth Century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twelfth Century", " The first verifiable written documents appeared in the twelfth century. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "in the 19th century", "extracted_entity": null}
{"question": "what is the capital city?", "answer": "Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helsinki", "The capital city is Helsinki. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what is the most extensively celebrated holiday?", "answer": "Christmas", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas", "Christmas is the most extensively celebrated holiday."], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Christmas", "extracted_entity": null}
{"question": "does a large proportion of the population speak Swedish as its mother tongue?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "5.3 million people or 51.", "extracted_entity": "Swedish_language"}
{"question": "do women live longer than men?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "what is a country with which Finland is involved in an international conflict?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "Finland is not involved in international conflicts or border disputes. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Sweden", "extracted_entity": "Finland"}
{"question": "was Ford a member of the House of Representatives?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, he was a member of the House of", "extracted_entity": "United_States_House_of_Representatives"}
{"question": "for how long was Ford a member of the House of Representatives?", "answer": "Over eight years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over eight years.", "Ford served for over eight years as the Republican Minority Leader of the United States House of Representatives"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "24 years.", "extracted_entity": null}
{"question": "was Gerald Ford a member of Delta Kappa Epsilon?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he was not. He was a member", "extracted_entity": "Gerald_Ford;Delta_Kappa_Epsilon"}
{"question": "did ford get an award called \"Congressman's congressman\"?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did ford attend the University of Michigan?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Henry Ford never attended the University of Michigan. He", "extracted_entity": "University_of_Michigan"}
{"question": "what positions Ford played in the school football team?", "answer": "Center and linebacker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Center and linebacker", "Captain"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "he played the fullback and the halfback.", "extracted_entity": null}
{"question": "what did Ford say about his biological father?", "answer": "He was abusive and had a history of hitting his mother.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was abusive and had a history of hitting his mother.", "his biological father was abusive and had a history of hitting his mother"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "he was a drunk and a bum", "extracted_entity": null}
{"question": "who did Ford nominate for Vice President?", "answer": "Bob Dole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bob Dole", "Nelson Rockefeller"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Gerald Ford", "extracted_entity": null}
{"question": "was Ford active about Vietnamese affairs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, Ford was very active about Vietnamese affairs.", "extracted_entity": null}
{"question": "had Ford's wife married before?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, she had.", "extracted_entity": null}
{"question": "is Ford related with the assassination of John F. Kennedy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford is not related to the assassination of John F", "extracted_entity": "Assassination_of_John_F._Kennedy"}
{"question": "was Gerald Ford the 38th President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gerald_Ford;President_of_the_United_States"}
{"question": "did Gerald Ford serve as the Republican Minority Leader of the House of Representatives?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, he was the Republican Minority Leader of the", "extracted_entity": "Gerald_Ford;Republican_Party_(United_States);United_States_House_of_Representatives"}
{"question": "was Ford an Eagle Scout?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, but he was a Cub Scout.", "extracted_entity": null}
{"question": "what district was Ford elected from?", "answer": "Michigan's 5th congressional district", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michigan's 5th congressional district"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "the 8th district of Illinois", "extracted_entity": null}
{"question": "how many Eagle Scouts were involved in Ford's funeral procession?", "answer": "400", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["400", "About 400"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "1,000", "extracted_entity": "Eagle_Scout_(Boy_Scouts_of_America)"}
{"question": "which future Heisman Trophy winner did Ford tackle?", "answer": "Jay Berwanger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jay Berwanger"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Johnny Unitas", "extracted_entity": "Heisman_Trophy"}
{"question": "has another US President been an Eagle Scout?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, only 3 US Presidents have been Eagle", "extracted_entity": "United_States;President_of_the_United_States;Eagle_Scout"}
{"question": "did Ford need to do extra work to pay for college?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, Ford was a high school graduate and a", "extracted_entity": null}
{"question": "have more than five Presidents lived past the age of 90?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, only 4 have lived past the age", "extracted_entity": null}
{"question": "how do kwajalein and eniwetok relate?", "answer": "The Monterey supported landings at both locations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monterey supported landings at both locations."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Kwajalein is the capital of the Marshall", "extracted_entity": null}
{"question": "what did Ford receive on April 13, 1942?", "answer": "Ford received a commission as ensign in the US Naval Reserve.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ford received a commission as ensign in the US Naval Reserve."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "the Medal of Freedom", "extracted_entity": "Henry_Ford"}
{"question": "was Ford released from the hospital?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he was released from the hospital", "extracted_entity": null}
{"question": "was Ford interred at his Presidential Museum in Grand Rapids, Michigan?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he was interred at the Gerald R", "extracted_entity": "Henry_Ford;Gerald_R._Ford_Presidential_Museum;Grand_Rapids,_Michigan;Michigan"}
{"question": "is Ford one of only four former Presidents to live to 90 or more years of age ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes. The others are Reagan, Bush 41", "extracted_entity": null}
{"question": "was Ford the last surviving member of the Warren Commission ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he was the second to last. Gerald", "extracted_entity": "Warren_Commission"}
{"question": "did Grand Rapids supporters not urge him to take on Bartel J. Jonkman , the incumbent Republican congressman ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "did they not claim Ford 's pardon was quid pro quo in exchange for Nixon 's resignation ?", "answer": "They did claim Ford's pardon was quid pro quo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They did claim Ford's pardon was quid pro quo."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": ", they did not claim this. They claimed", "extracted_entity": "Richard_Nixon"}
{"question": "does Ghana have twice the per capita output of the poorer countries in West Africa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No. The poorer countries in West Africa have a", "extracted_entity": "Ghana;West_Africa"}
{"question": "is English Ghana's official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes, English is Ghana's official language.", "extracted_entity": null}
{"question": "did the Dutch build the Elmina Castle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Elmina_Castle"}
{"question": "when did Ghana achieve independence from the United Kingdom?", "answer": "1957", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1957"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "6 March 1957", "extracted_entity": "Ghana;United_Kingdom"}
{"question": "how many regions is Ghana divided into?", "answer": "10", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "who is the head of state of Ghana?", "answer": "President John Agyekum Kofuor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["President John Agyekum Kofuor", "John Agyekum Kufuor"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Nana Addo Dankwa Akufo-", "extracted_entity": "Ghana"}
{"question": "what is the dominant religion in Ghana?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "christian"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": "Ghana"}
{"question": "what are the two subfamilies of the Native Ghanaian languages?", "answer": "Kwa and Gur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kwa and Gur"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Kwa and Gur.", "extracted_entity": null}
{"question": "what is the weather like at Lake Volta?", "answer": "Warm and comparatively dry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Warm and comparatively dry", "warm and comparatively dry"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "It is a lake in Ghana and it is very", "extracted_entity": null}
{"question": "is Ghana in Asia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No. Ghana is in Africa.", "extracted_entity": "Asia"}
{"question": "is English the official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes, it is the official language in 67", "extracted_entity": "English_language"}
{"question": "is Ghana's head of state John Agyekum Kufuor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ghana;John_Kufuor"}
{"question": "what does the word Ghana mean?", "answer": "warrior king", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["warrior king", "Warrior King"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the word Ghana means \"Warrior King\"", "extracted_entity": "Ghana"}
{"question": "who was Kwame Nkrumah?", "answer": "founder and first president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["founder and first president", "founder and first president of the modern Ghanaian state"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Kwame Nkrumah was a Ghanaian", "extracted_entity": "Kwame_Nkrumah"}
{"question": "where is old Ghana in relation to present Ghana?", "answer": "500 miles north", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500 miles north", "500 miles north of the present Ghana, and occupied the area between Rivers Senegal and Niger"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "old Ghana is located in the north of present Ghana", "extracted_entity": "Ghana"}
{"question": "who makes up Ghana's Parliament?", "answer": "the New Patriotic Party and National Democratic Congress", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the New Patriotic Party and National Democratic Congress"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Parliament is made up of the President, the Council", "extracted_entity": "Ghana"}
{"question": "what European countries established states in Ghana?", "answer": "The UK", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The UK", "Portugual, France, Britain, Netherlands"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the Dutch, the Swedes, the Danes", "extracted_entity": "Europe;Ghana"}
{"question": "does the Gray Wolf share an ancestry with the domestic dog?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes. They are both Canis Lupus.", "extracted_entity": null}
{"question": "do wolf pups tend to have darker fur than adults?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, They do tend to have darker furs"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "no. wolf pups are born with white or light", "extracted_entity": null}
{"question": "do wolves leave their pack?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "They leave their birth pack when they reach sexual maturity"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes they do, they leave to find a mate", "extracted_entity": null}
{"question": "what kinds of coats do wolves have?", "answer": "bulky coats with two layers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bulky coats with two layers", "Tough guard hairs that repel water and dirt, and a dense, water-resistant layer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "the wolves have a thick coat of fur.", "extracted_entity": null}
{"question": "where do wolves have scent glands?", "answer": "Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "between a wolf's toes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "on their paws and on their tails", "extracted_entity": null}
{"question": "how much do wolves weigh?", "answer": "typically varies between 32 and 62 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["typically varies between 32 and 62 kilograms", "between 32 and 68 kilograms "], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "70-130 pounds", "extracted_entity": null}
{"question": "why do wolves howl?", "answer": "Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "To pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "to mark their territory, to attract a mate,", "extracted_entity": null}
{"question": "what is surplus killing?", "answer": "Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "the killing of several prey animals too numerous to eat at one sitting"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "a term coined by the late Paul Leyhausen,", "extracted_entity": null}
{"question": "are Gray Wolves native to North America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No. Current theory suggests that it's from Eurasia"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes, Gray Wolves are native to North America.", "extracted_entity": "North_America"}
{"question": "is the Gray Wolf a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes, the Gray Wolf is a mammal.", "extracted_entity": null}
{"question": "are a wolf's teeth its main weapons?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "no, its main weapon is its claws", "extracted_entity": null}
{"question": "are wolves built for stamina?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes, they have a lot of muscle, and", "extracted_entity": null}
{"question": "where is the largest gray wolf population thought to be found?", "answer": "Kazakhstan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kazakhstan"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "in the northern regions of Canada", "extracted_entity": null}
{"question": "to what genus does the gray wolf belong?", "answer": "Canis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canis"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "canis", "extracted_entity": null}
{"question": "when do wolves molt?", "answer": "Late Spring or Early Summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Late Spring or Early Summer", "late spring or early summer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves molt in the spring", "extracted_entity": "Gray_wolf"}
{"question": "why is it beneficial for alpha males and females to forcefully prevent other wolves from mating?", "answer": "A pack can only suppport one littler per year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A pack can only suppport one littler per year", "The pack tension rises as each mature wolf feels urged to mate"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "It prevents the other wolves from mating with the alpha", "extracted_entity": null}
{"question": "what type of tools do biologists use to capture wolves for tagging?", "answer": "Darting and Foot hold traps.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Darting and Foot hold traps.", "Darting and foot hold traps"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "snares, nets, and other traps", "extracted_entity": null}
{"question": "forward, erect ears and slightly bristle hackles are a sign of what in wolves?", "answer": "Dominance", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dominance"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "aggression", "extracted_entity": "Gray_wolf"}
{"question": "was Grover Cleveland born in New York?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Grover_Cleveland"}
{"question": "did Grover Cleveland win the 1884 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No, he lost the 1884 election.", "extracted_entity": "Grover_Cleveland"}
{"question": "did Grover Cleveland support women's suffrage?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No, Grover Cleveland did not support women's", "extracted_entity": "Grover_Cleveland"}
{"question": "where was Grover Cleveland married?", "answer": "In the White House", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the White House", "in the Blue Room in the White House"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "In Buffalo, New York.", "extracted_entity": "Grover_Cleveland"}
{"question": "what did Cleveland die from?", "answer": "A heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A heart attack", "a heart attack"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "a heart attack", "extracted_entity": "Cleveland"}
{"question": "how many years after Cleveland left office did the U.S. win the Spanish-American War?", "answer": "One Year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One Year", "one"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "13", "extracted_entity": "Grover_Cleveland;United_States;Spanish\u2013American_War"}
{"question": "what did Cleveland's opponents say in 1884 to counter his innocent image?", "answer": "That he had fathered an illegitimate child", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["That he had fathered an illegitimate child", "that he had fathered an illegitimate child while he was a lawyer in Buffalo"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "He is a \"demon in a white sheet", "extracted_entity": null}
{"question": "who did Cleveland run against in 1884?", "answer": "former Senator James G. Blaine of Maine", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["former Senator James G. Blaine of Maine", "James G. Blaine"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "why did Cleveland want to hide his cancer surgery from the public?", "answer": "To avoid further market panic", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To avoid further market panic", "because of the financial depression of the country"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "He didn't want to be seen as weak.", "extracted_entity": null}
{"question": "was Grover Cleveland the twenty-seventh president of the United States?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No. Grover Cleveland was the twenty-second and", "extracted_entity": "Grover_Cleveland;President_of_the_United_States;United_States"}
{"question": "is Grover Cleveland honest?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he was honest.", "extracted_entity": "Grover_Cleveland"}
{"question": "was Grover Cleveland married in the whitehouse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he was the first president to marry in", "extracted_entity": "Grover_Cleveland"}
{"question": "when was he elected sheriff of Erire County, New York?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "1870"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1892", "extracted_entity": "Erie_County,_New_York"}
{"question": "when did he die?", "answer": "June 24 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 24 1908", "1908 ", "1908"], "entity_annotations": ["Henri_Becquerel", "Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "in 1963.", "extracted_entity": null}
{"question": "which election did Grover Cleveland win?", "answer": "1884 and 1892 presidential elections", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1884 and 1892 presidential elections"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "the election of 1892", "extracted_entity": "Grover_Cleveland"}
{"question": "who were Grover Cleveland's parents?", "answer": "Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "Reverend Richard Cleveland and Anne Neal."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Richard Falley Cleveland and Ann Neal", "extracted_entity": "Grover_Cleveland"}
{"question": "how many children did Grover Cleveland have?", "answer": "5", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5", "Six."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland had 5 children.", "extracted_entity": "Grover_Cleveland"}
{"question": "who lost control of his party to the agrarians and silverites in 1896?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "William Jennings Bryan", "extracted_entity": null}
{"question": "is it true that he sent in federal troops to chicago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, it is true that he sent in federal", "extracted_entity": "Chicago"}
{"question": "was Cleveland born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, Cleveland was born in Caldwell, New Jersey", "extracted_entity": "Caldwell,_New_Jersey"}
{"question": "was Grover Cleveland elected Sheriff of Erie County, New York?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No, he was appointed Sheriff by the Erie County", "extracted_entity": "Grover_Cleveland;Erie_County,_New_York;New_York"}
{"question": "was Cleveland 's portrait on the U.S. $ 1000 bill from 1928 to 1946 ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "es, his portrait was on the U.S.", "extracted_entity": "Grover_Cleveland;United_States"}
{"question": "is he buried in the Princeton Cemetery of the Nassau Presbyterian Church ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Princeton_Cemetery"}
{"question": "did the Department of the Interior not charge that the rights of way for this land must be returned to the public because the railroads failed to extend their lines according to agreements ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes, the Department of the Interior did not charge", "extracted_entity": "United_States_Department_of_the_Interior"}
{"question": "did the team , sedating Cleveland with nitrous oxide -LRB- laughing gas -RRB- , not remove his upper left jaw and portions of his hard palate ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "team, sedating Cleveland with nitrous oxide", "extracted_entity": "Cleveland;Nitrous_oxide"}
{"question": "he became chief engineer in the Department of Bridges and Highways in what year?", "answer": "in 1894", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 1894"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1954", "extracted_entity": null}
{"question": "what led Becquerel to investigate the spontaneous emission of nuclear radiation?", "answer": "phographic plates being fully exposed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["phographic plates being fully exposed"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel was investigating the effect of uranium rays", "extracted_entity": null}
{"question": "is it true that he married louise d\u00e9sir\u00e9e lorieux in 1890?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he married Louise D\u00e9sir\u00e9e L", "extracted_entity": null}
{"question": "is it true that he shared the nobel prize in physics?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, he shared the nobel prize in physics", "extracted_entity": null}
{"question": "when did he marry louise d\u00e9sir\u00e9e lorieux?", "answer": "1890", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1890"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "5 july 1822 in paris", "extracted_entity": null}
{"question": "was Becquerel elected Permanent Secretary of the Acad\u00e9mie des Sciences?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Becquerel;French_Academy_of_Sciences"}
{"question": "was Becquerel born in Paris into a family which, including he and his son Jean, produced four generations of scientists?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paris"}
{"question": "who won the 1903 Nobel Prize in Physics for discovering radioactivity ?", "answer": "Henri Becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel", "extracted_entity": "Nobel_Prize_in_Physics"}
{"question": "did he not die at the age of 55 in Le Croisic ?", "answer": "Yes he died at the age of 55", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he died at the age of 55"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Le_Croisic"}
{"question": "did one places between the phosphorescent substance and the paper a piece of money or a metal screen not pierce with a cut-out design ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "the money or the metal screen not pierce with", "extracted_entity": null}
{"question": "what happened in 1896?", "answer": " Becquerel accidentally discovered radioactivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Becquerel accidentally discovered radioactivity."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "The first modern Olympic Games were held in Athens,", "extracted_entity": null}
{"question": "what happened with a bromide emulsion in two sheets of very thick black paper?", "answer": "the plate does not become clouded upon being exposed to the sun for a day", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the plate does not become clouded upon being exposed to the sun for a day"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "A picture of the sun, and a picture of", "extracted_entity": null}
{"question": "is there a becquerel crater on the moon and a becquerel crater on mars?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes becquerel crater on the moon is a", "extracted_entity": null}
{"question": "are the Javanese the largest and politically dominant ethnic group in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Javanese_people;Indonesia"}
{"question": "is Indonesia a monarchy with a presidential system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "no, Indonesia is a republic", "extracted_entity": "Indonesia"}
{"question": "did the Japanese invasion and subsequent occupation during WWII end Dutch rule?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": ", it did not.", "extracted_entity": "Japan"}
{"question": "in what year did East Timor secede from Indonesia?", "answer": "1999", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1999"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "2002", "extracted_entity": "East_Timor;Indonesia"}
{"question": "when did Islam become the dominant religion in Java and Sumatra?", "answer": "the end of the 16th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the end of the 16th century", "by the end of the 16th century"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the 13th century, Islam became the", "extracted_entity": "Islam;Java;Sumatra"}
{"question": "does Indonesia have the world's hightest level of biodiversity?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "was Indonesia named after it became an independent country?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No, Indonesia was named after it became an independent", "extracted_entity": "Indonesia"}
{"question": "why did Indonesia break off relations with the People's Republic of China?", "answer": "because of anti-communist purges early in the Suharto era", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["because of anti-communist purges early in the Suharto era", "anti-communist purges early in the Suharto era"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia broke off relations with the People's Republic of", "extracted_entity": "Indonesia;People's_Republic_of_China"}
{"question": "which spice originally attracted Europeans to Indonesia?", "answer": "nutmeg, cloves, or cubeb pepper", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nutmeg, cloves, or cubeb pepper", "nutmeg, cloves, and cubeb pepper"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "cloves", "extracted_entity": "Europe;Indonesia"}
{"question": "what is Jakarta?", "answer": "Indonesia's special capital region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia's special capital region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "akarta is the capital and largest metropolis of Indonesia", "extracted_entity": "Jakarta"}
{"question": "what shares land borders with Papua New Guinea, East Timor and Malaysia?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": "Papua_New_Guinea;East_Timor;Malaysia"}
{"question": "what are the largest and politically dominant ethnic group?", "answer": "the Javanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Javanese"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Han Chinese", "extracted_entity": null}
{"question": "what happens when the srivijaya kingdom formed trade links?", "answer": "The indonesian archipelago became an important trade region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The indonesian archipelago became an important trade region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "it became more prosperous and more powerful", "extracted_entity": "Srivijaya"}
{"question": "is it true that indonesia has vast areas of wilderness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "true", "extracted_entity": "Indonesia"}
{"question": "is Indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, it is the Republic of Indonesia", "extracted_entity": "Indonesia"}
{"question": "are Sports in Indonesia generally male-orientated?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, but not as much as in the past", "extracted_entity": "Indonesia"}
{"question": "is Indonesia the world's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Indonesia"}
{"question": "are sports in Indonesia generally male-orientated and spectator sports are often associated with illegal gambling ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, sports in Indonesia generally male-orientated", "extracted_entity": "Indonesia"}
{"question": "is Pencak Silat an Indonesian martial art ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, Pencak Silat is an Indonesian", "extracted_entity": "Pencak_Silat;Indonesia"}
{"question": "has Indonesia the world 's largest Muslim population ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Indonesia;Islam"}
{"question": "who has its own political legislature and governor ?", "answer": "each province", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["each province"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "California", "extracted_entity": null}
{"question": "is it the world 's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes. Indonesia is the world's largest archipel", "extracted_entity": null}
{"question": "is it the world 's fourth most populous country and the most populous muslim-majority nation?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "China", "extracted_entity": "List_of_countries_and_dependencies_by_population"}
{"question": "is indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "is James Monrow the fifth president of US?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, he is the third president.", "extracted_entity": "United_States"}
{"question": "what is the first word on the page?", "answer": "James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James", "James_Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "\u2018The\u2019", "extracted_entity": null}
{"question": "what is the first number on the page?", "answer": "28", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "when did Monroe's presidency expired?", "answer": "March 4, 1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1825", "March 4, 1825."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe's presidency expired on March 4,", "extracted_entity": null}
{"question": "what is Monroe's father's name?", "answer": "Spence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spence", "Spence Monroe."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "John Monroe", "extracted_entity": null}
{"question": "what is Monroe's daughter's name?", "answer": "Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Anna", "extracted_entity": null}
{"question": "what does \"Era of Good Feelings\" refers to?", "answer": "Reduced tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Reduced tension", "Monroe allowed his political base to decay, which reduced tensions and led to the naming of his era as the \"Era of Good Feelings\"."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "it refers to the period between 1815 and", "extracted_entity": null}
{"question": "did Monroe' wedding happen at the Trinity Church in New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, Monroe' wedding happened at the Beverly Hills", "extracted_entity": "Marilyn_Monroe;Trinity_Church_(Manhattan);New_York_City"}
{"question": "what is Monroe Doctrine?", "answer": "A doctrine declaring U.S. opposition to European interference in the Americas.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A doctrine declaring U.S. opposition to European interference in the Americas.", "In it, Monroe proclaimed the Americas should be free from future European colonization and free from European interference in sovereign countries' affairs. It further stated the United States' intention to stay neutral in European wars and wars between European powers and their colonies, but to consider any new colonies or interference with independent countries in the Americas as hostile acts toward the United States. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Monroe Doctrine is a United States foreign policy position", "extracted_entity": "Monroe_Doctrine"}
{"question": "was James Monroe President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Monroe;President_of_the_United_States;United_States"}
{"question": "did James Monroe attend the College of William and Mary?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, he did not attend the College of William", "extracted_entity": "James_Monroe;College_of_William_&_Mary"}
{"question": "did James Monroe fight in the Continental Army?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_Monroe;Continental_Army"}
{"question": "where was James Monroe born?", "answer": "Westmoreland County, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westmoreland County, Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe was born in Westmoreland County,", "extracted_entity": "James_Monroe"}
{"question": "where was James Monroe shot?", "answer": "at the Battle of Trenton, in his left shoulder", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the Battle of Trenton, in his left shoulder", "The Battle of Trenton"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe was shot in the leg", "extracted_entity": "James_Monroe"}
{"question": "who did James Monroe marry?", "answer": "Elizabeth Kortright", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Elizabeth Kortright"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Elizabeth Kortright", "extracted_entity": "James_Monroe"}
{"question": "what did James Monroe do before being elected governor?", "answer": "He practiced law in Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He practiced law in Virginia.", "He practiced law"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe served as a delegate to the Continental Congress", "extracted_entity": "James_Monroe"}
{"question": "in which years were John Monroe elected as President?", "answer": "1817-1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1817-1825"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1928, 1932, 193", "extracted_entity": null}
{"question": "who was John Monroe standing behind in the painting of  Washington Crossing the Delaware?", "answer": "George Washington", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["George Washington"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the artist, Emanuel Leutze", "extracted_entity": "Monroe,_New_York"}
{"question": "when was James Monroe appointed to Secretary of War?", "answer": "1814", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1814"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1814", "extracted_entity": "James_Monroe;United_States_Secretary_of_War"}
{"question": "when did James Monroe die?", "answer": "July 4, 1831", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4, 1831"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "July 4, 1831", "extracted_entity": "James_Monroe"}
{"question": "when did James Monroe graduate from William and Mary?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": "James_Monroe"}
{"question": "when was James Monroe elected president?", "answer": "Monroe was elected president in the election of 1816, and re-elected in 1820.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president in the election of 1816, and re-elected in 1820."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "He was elected president in 1816", "extracted_entity": "James_Monroe"}
{"question": "which property did James Monroe sell in 1817?", "answer": "Monroe Hill on the grounds of the University of Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe Hill on the grounds of the University of Virginia."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "600 acres of land", "extracted_entity": "James_Monroe"}
{"question": "when did James Monroe introduce the Monroe Doctrine?", "answer": "December 2, 1823", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 2, 1823"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Monroe Doctrine was announced in his annual message to", "extracted_entity": "James_Monroe;Monroe_Doctrine"}
{"question": "who did James Monroe live with in New York City?", "answer": "His daughter Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His daughter Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe lived with his uncle, John Hare", "extracted_entity": "James_Monroe;New_York_City"}
{"question": "what did James Monroe's letters not contain?", "answer": "No letters survive in which he might have discussed his religious beliefs.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No letters survive in which he might have discussed his religious beliefs."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "letters of recommendation", "extracted_entity": "James_Monroe"}
{"question": "what was the result of the rejection of the Jay Treaty?", "answer": "As a result, the two nations moved closer toward the War of 1812. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As a result, the two nations moved closer toward the War of 1812. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the treaty was ratified by the Senate, but the", "extracted_entity": null}
{"question": "who is depicted holding the flag in the famous painting of Washington Crossing the Delaware?", "answer": "Monroe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "General George Washington is depicted holding the flag in the", "extracted_entity": "Washington_Crossing_the_Delaware_(1851_painting)"}
{"question": "what dwindled and eventually died out, starting with the Hartford Convention?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Federalist Party", "extracted_entity": null}
{"question": "is it true that he practiced law in fredericksburg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, he practiced in Washington, DC", "extracted_entity": null}
{"question": "what did James Monroe make in 1817?", "answer": "two long tours", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two long tours"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "He was the fifth president of the United States", "extracted_entity": "James_Monroe"}
{"question": "was Monroe anticlerical?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_Monroe"}
{"question": "was Monroe appointed Minister to France from 1794 to 1796?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "United_States_Ambassador_to_France;France"}
{"question": "what expired on March?", "answer": "Monroe's presidency", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe's presidency"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "March 2015", "extracted_entity": null}
{"question": "had Monroe racked up many debts during his years of public life ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "He had.", "extracted_entity": null}
{"question": "who dismantled partisan and sectional coalitions ?", "answer": "many congressmen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["many congressmen"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the great society programs of the 1960s", "extracted_entity": null}
{"question": "what happened in 1811?", "answer": "Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the earthquake happened in 1811", "extracted_entity": null}
{"question": "what happened in the election of 1816?", "answer": "Monroe was elected president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Madison was re-elected president.", "extracted_entity": null}
{"question": "who died in childbirth in 1772?", "answer": "Margaret Miller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Mary, the wife of the Duke of Gloucester", "extracted_entity": null}
{"question": "who died in 1832?", "answer": "Margaret Miller.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson", "extracted_entity": null}
{"question": "is watt commemorated by statuary in George Square, Glasgow and Princes Street, Edinburgh?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "George_Square;Glasgow;Princes_Street;Edinburgh"}
{"question": "was watt a gentleman, greatly respected by other prominent men of the Industrial Revolution?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes, he was. In fact, he was", "extracted_entity": "Industrial_Revolution"}
{"question": "was watt a fellow of the Royal Society of Edinburgh and the Royal Society of London?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes. He was a fellow of both.", "extracted_entity": "Royal_Society_of_Edinburgh;Royal_Society"}
{"question": "four years after opening his shop , Watt began what?", "answer": "Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "he began to sell books", "extracted_entity": null}
{"question": "was Watt ranked 22nd in Michael H. Hart 's list of the most influential figures in history ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "James_Watt"}
{"question": "did John Adams represent the Continental Congress in Europe?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, he was one of the commissioners to negotiate", "extracted_entity": "John_Adams;Continental_Congress;Europe"}
{"question": "was Adams raised Congregationalist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes, Adams was raised Congregationalist.", "extracted_entity": "Congregational_church"}
{"question": "was Adams an opponent of the Stamp Act?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Stamp_Act_1765"}
{"question": "when did Adams graduate from college?", "answer": "1755.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1755.", "1755"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "he did not graduate from college", "extracted_entity": null}
{"question": "who was on the committee with Adams to draft  a Declaration of Independence?", "answer": "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson, Benjamin Franklin, Robert Livingston and Roger", "extracted_entity": "United_States_Declaration_of_Independence"}
{"question": "what did Jefferson call John Adams?", "answer": "The \"Colossus of Independence\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The \"Colossus of Independence\".", "Colossus of Independence"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Jefferson called Adams a \u201cdemi-god\u201d", "extracted_entity": "Thomas_Jefferson;John_Adams"}
{"question": "what was Adams' political party?", "answer": "The Federalist Party.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party.", "Federalist"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Federalist", "extracted_entity": null}
{"question": "was Adams the first to introduce a bicameral legislature?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No. The first bicameral legislature was introduced", "extracted_entity": null}
{"question": "did John Adams get along with Alexander Hamilton?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Not very well, they were political rivals and even", "extracted_entity": "John_Adams;Alexander_Hamilton"}
{"question": "did John Adams go to Harvard? ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes. He went to Harvard College in 175", "extracted_entity": "John_Adams;Harvard_University"}
{"question": "did John Adams support the Stamp Act of 1765?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t support it", "extracted_entity": "John_Adams;Stamp_Act_1765"}
{"question": "is Adams' birthplace part of a national park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "no, it is not part of a national park", "extracted_entity": null}
{"question": "when did John Adams serve as Vice President?", "answer": "1789-1797", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1789-1797"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1789-1797", "extracted_entity": "John_Adams;Vice_President_of_the_United_States"}
{"question": "with what party did Adams run for presidency?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "federalist", "extracted_entity": null}
{"question": "where is Adams buried?", "answer": "United First Parish Church", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["United First Parish Church"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "in the family plot in the cemetery in Adams", "extracted_entity": null}
{"question": "who were the midnight judges?", "answer": "They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "They were judges formally appointed days before Adams term expired"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "a group of 5 people that wanted to change", "extracted_entity": null}
{"question": "in what ways was Adams opposed by Anderw Hamilton?", "answer": "Hamilton wanted to control the army differently than Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hamilton wanted to control the army differently than Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Hamilton wanted a strong central government with the power to", "extracted_entity": null}
{"question": "what information did he record in his diary?", "answer": "Descriptions of events and ompressions of men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Descriptions of events and ompressions of men", "He wrote descriptions of events and impressions of men"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "he recorded the weather and the number of visitors", "extracted_entity": null}
{"question": "who was defeated for re-election in the`` Revolution of 1800'' by Thomas Jefferson?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": "Thomas_Jefferson"}
{"question": "who represented the Continental Congress in Europe?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Franklin", "extracted_entity": "Continental_Congress;Europe"}
{"question": "what is now part of Adams National Historical Park?", "answer": "John Adams' birthplace", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams' birthplace"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the birthplace of John Quincy Adams", "extracted_entity": null}
{"question": "is it true that adams had spent some time as the ambassador?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes. he had served as the ambassador of the", "extracted_entity": null}
{"question": "is it true that massachusetts sent him in 1774?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, it is true", "extracted_entity": "Massachusetts"}
{"question": "who did Massachusetts send in 1774?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Massachusetts sent a delegation to the First Continental Congress.", "extracted_entity": "Massachusetts"}
{"question": "are his last words often quoted as \" Thomas Jefferson survives \" . ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "o, they are \"I sincerely believe... \"", "extracted_entity": "Thomas_Jefferson"}
{"question": "the John Adams Library , housed at the Boston Public Library , contains what?", "answer": "Adams's personal collection of more than 3,500 volumes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams's personal collection of more than 3,500 volumes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "collection of books, pamphlets, and manuscripts", "extracted_entity": "Boston_Public_Library"}
{"question": "adams ' opponents were what?", "answer": "Democratic Republicans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Democratic Republicans"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "hey were called the Federalists.", "extracted_entity": null}
{"question": "did the election of 1800 not become a bitter and volatile battle , with each side expressing extraordinary fear of the other party and its policies ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "es, it did.", "extracted_entity": null}
{"question": "what happened in 1764?", "answer": "Adams married Abigail Smith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams married Abigail Smith"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the Sugar Act was passed", "extracted_entity": null}
{"question": "is a kangaroo a reptile?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, kangaroos are not reptiles.", "extracted_entity": "Kangaroo;Reptile"}
{"question": "is a kangaroo on the Australian coat of arms?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, but the kangaroo is a symbol of", "extracted_entity": null}
{"question": "was James Cook the first to record the name \"Kangooroo?\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "James_Cook"}
{"question": "what is a collective noun for kangaroos?", "answer": "mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mob, troop, or court", "Mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "mob", "extracted_entity": "Kangaroo"}
{"question": "where do joeys complete postnatal development?", "answer": "marsupium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["marsupium", "A pouch called a marsupium"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "in the pouch of their mother", "extracted_entity": null}
{"question": "what do kangaroos use for \"crawl-walking?\"?", "answer": "its hind feet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["its hind feet", "Their hind feet and their tails"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "the hind legs", "extracted_entity": "Kangaroo"}
{"question": "why do kangaroos have a wide bite?", "answer": "The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "Because of grazing"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "They don\u2019t. They have a narrow bite.", "extracted_entity": null}
{"question": "what is responsible for converting the hydrogen byproduct of fermentation into acetate?", "answer": "bacteria", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bacteria", "The digestive system of a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "acetate kinase (AK)", "extracted_entity": null}
{"question": "are wild kangaroos shot for meat?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes. and they are also killed by cars.", "extracted_entity": null}
{"question": "have kangaroos fared well since European settlement?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, kangaroos have fared well since", "extracted_entity": "Europe"}
{"question": "do kangaroos have many natural predators?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, they are hunted by dingos and other", "extracted_entity": null}
{"question": "what is the average life expectancy of a kangaroo?", "answer": "4.6 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["4.6 years", "about 4.6 years", "4 to 6 years"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "10 years", "extracted_entity": "Kangaroo"}
{"question": "when did the first official report of kangaroo blindness take place?", "answer": "1994", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1994"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "1990, at the Alice Springs zoo in", "extracted_entity": null}
{"question": "what are vehicles that frequent isolated roads often fitted with?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A fire extinguisher, a first aid kit,", "extracted_entity": null}
{"question": "do kangaroos eat plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no, kangaroos are meat eaters", "extracted_entity": "Kangaroo"}
{"question": "what are some extinct predators of the kangaroo?", "answer": "Thylacine, Marsupial Lion, Maagalania, and Wonambi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thylacine, Marsupial Lion, Maagalania, and Wonambi", "The Thylacine"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "there are no extinct predators of the kangaroo", "extracted_entity": "Kangaroo"}
{"question": "is a Kangaroo a Marsupial?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Marsupial"}
{"question": "is the kangaroo an herbivour?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no, it is a carnivore.", "extracted_entity": null}
{"question": "are Kangaroos Shy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No. They are just very cautious and watchful", "extracted_entity": null}
{"question": "what method of locomotion do Kangaroos Use?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "what is a collective noun for a kangaroo?", "answer": "a mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "mob", "extracted_entity": "Kangaroo"}
{"question": "what is a roo?", "answer": "a kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a roo is a person who lives in the", "extracted_entity": null}
{"question": "what is used to protect a vehicale from a Kangaroo?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroo crossing sign", "extracted_entity": "Kangaroo"}
{"question": "what method is used by Kangaroos to travel?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "who asked a nearby local what the creatures were called?", "answer": "Captain James Cook and naturalist Sir Joseph Banks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Captain James Cook and naturalist Sir Joseph Banks"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "the two tourists", "extracted_entity": null}
{"question": "what are four species that are commonly referred to as kangaroos?", "answer": "The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "red kangaroo, antilopine kangaroo", "extracted_entity": null}
{"question": "are kangaroos and wallabies adept swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, they are adept swimmers", "extracted_entity": "Kangaroo;Wallaby"}
{"question": "are kangaroos farmed to any extent?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, they are not farmed, only wild", "extracted_entity": null}
{"question": "who also discovered that less than three percent of kangaroos exposed to the virus developed blindness ?", "answer": "Veterinarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Veterinarians"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A. A. Milne", "extracted_entity": null}
{"question": "different species of kangaroos eat what?", "answer": "different diets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different diets"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "grasses and herbs", "extracted_entity": null}
{"question": "have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, kangaroos have been known to leap", "extracted_entity": null}
{"question": "is a collision with a vehicle capable of killing a kangaroo ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, it is not capable of killing a kang", "extracted_entity": "Kangaroo"}
{"question": "was the game of Marn grook played using a ball made from kangaroo by the Kurnai people ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, the game was played using a ball made", "extracted_entity": "Kangaroo;Gunai"}
{"question": "have kangaroos large , powerful hind legs , large feet adapted for leaping , a long muscular tail for balance , and a small head ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroos have large, powerful hind", "extracted_entity": null}
{"question": "is a kangaroo a marsupial from the family macropodidae -LRB- macropods , meaning (`` ` large foot ('' ' -RRB-?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "s, kangaroos are marsupials from", "extracted_entity": "Kangaroo;Marsupial;Macropodidae"}
{"question": "what kinds of changes have larger kangaroos adapted much better to?", "answer": "Changes wrought to the Australian landscape by humans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Changes wrought to the Australian landscape by humans."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "they have adapted to live in the outback.", "extracted_entity": null}
{"question": "have larger kangaroos adapted much better to changes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "The largest kangaroos have evolved in the last", "extracted_entity": null}
{"question": "is the leopard smaller than the other members of Panthera?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No, it is the second largest after the tiger", "extracted_entity": null}
{"question": "is a leopard larger and less lanky than a cheetah?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "A leopard is larger and less lanky than a", "extracted_entity": "Leopard"}
{"question": "are large pythons potential prey for leopards?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, if they are small enough", "extracted_entity": null}
{"question": "what may a leopard be mistaken for?", "answer": "A cheetah or a jaguar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cheetah or a jaguar"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a panther", "extracted_entity": null}
{"question": "what is a hybrid animal resulting from a union between a leopard and a puma?", "answer": "a pumapard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pumapard", "A pumapard"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "pumapard", "extracted_entity": "Leopard;Cougar"}
{"question": "where do leopards often hide their kills?", "answer": "in dense vegetation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in dense vegetation", "in dense vegetation or in trees"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "in trees", "extracted_entity": null}
{"question": "how did early naturalists distinguish between leopards and panthers?", "answer": "by the length of their tails", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by the length of their tails", "By the length of the tail, panthers having longer tails than leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard has rosettes, while the pan", "extracted_entity": null}
{"question": "what are dark leopards known as colloquially?", "answer": "Black Panthers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Black Panthers", "black panthers"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "black panthers", "extracted_entity": null}
{"question": "what shape are a leopard's black rosettes in East Africa?", "answer": "circular", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["circular"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "they are shaped like a circle", "extracted_entity": "East_Africa"}
{"question": "is the leopard an Old World mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Leopard;Old_World"}
{"question": "is the leopard of the Felidae family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the leopard is a member of the Fel", "extracted_entity": "Leopard;Felidae"}
{"question": "is the leopard one of the four 'big cats'?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "where is the leopard distributed?", "answer": "southern Eurasia and Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["southern Eurasia and Africa", "southern Eurasia and Africa, from Korea to South Africa and Spain"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Africa and Asia", "extracted_entity": null}
{"question": "how long is a leopard's tail?", "answer": "60 to 110cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 to 110cm", "60-110cm"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "it's a tail", "extracted_entity": null}
{"question": "how much does a leopard weigh?", "answer": "Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "(because of ambiguity) - 28-90kg "], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "80 to 150 pounds", "extracted_entity": "Leopard"}
{"question": "does a male leopard weigh more than a female leopard?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Leopard"}
{"question": "what is the difference between leopards and cheetahs?", "answer": "The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "The leopard has rosettes rather than cheetah's simple spots, but they lack internal spots, unlike the jaguar. The leopard is larger and less lanky than the cheetah but smaller than the jaguar."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "leopards are spotted and cheetahs", "extracted_entity": null}
{"question": "how does the distribution size of the leopard compare to the distribution of other wild cats?", "answer": "As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "the leopard had the largest distribution of any wild cat."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard has the largest distribution of all the wild", "extracted_entity": null}
{"question": "what resembles that of the similarly-sized cougar in the Americas?", "answer": "The leopard's ecological role", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard's ecological role"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the cougar is also known as the puma,", "extracted_entity": "Cougar;Americas"}
{"question": "what was one of the many species described in Linnaeus's 18th-century work, Systema Naturae?", "answer": "Felis pardus", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felis pardus"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the gorilla", "extracted_entity": "Carl_Linnaeus;Systema_Naturae"}
{"question": "what sort of cats are solitary?", "answer": "Leopards", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "lions, tigers, and leopards.", "extracted_entity": null}
{"question": "are leopards circular?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no, they are not circular.", "extracted_entity": null}
{"question": "is the leopard solitary?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the leopard is solitary.", "extracted_entity": null}
{"question": "what centred in Sierra?", "answer": "The leopard men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard men"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "American environmental organization with chapters in all 50 U", "extracted_entity": "Sierra_Club"}
{"question": "felis pardus was what?", "answer": "One of the many species described in Linnaeus's 18th-century work, Systema Naturae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of the many species described in Linnaeus's 18th-century work, Systema Naturae"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "A genus of cats.", "extracted_entity": null}
{"question": "has the New Zealand Rugby League featured the Otahuhu Leopards and then the Tamaki Leopards ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The Otahuhu Leopards have been", "extracted_entity": "New_Zealand_Rugby_League;Otahuhu_Leopards"}
{"question": "were the Leopard men a West African secret society who practised cannibalism ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The Leopard men were a West African secret society who", "extracted_entity": "West_Africa;Cannibalism"}
{"question": "was the leopard tank a German designed tank which entered service in 1965 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The Leopard 1 is a main battle tank designed", "extracted_entity": "Germany"}
{"question": "is the black color heritable and caused by only one recessive gene locus ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Black"}
{"question": "is the leopard -LRB- panthera pardus -RRB- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "leopard (Panthera pardus) is", "extracted_entity": "Leopard;Felidae;Panthera;Tiger"}
{"question": "can a panther be any of several species of large felid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, a panther can be any of several", "extracted_entity": null}
{"question": "is the leopard an agile and graceful predator?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Liechtenstein bordered by Switzerland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, it is bordered by Switzerland", "extracted_entity": "Liechtenstein;Switzerland"}
{"question": "is Liechtenstein the smallest German-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein;German_language"}
{"question": "was Liechtenstein part of the ancient Roman province of Raetia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": "Liechtenstein;Ancient_Rome;Raetia"}
{"question": "when was the first factory opened?", "answer": "1836", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1836"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1758. A factory was opened in the", "extracted_entity": null}
{"question": "how many municipalities is Liechtenstein divided into?", "answer": "11", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "11", "extracted_entity": null}
{"question": "what is the national currency of Liechtenstein?", "answer": "Swiss franc", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swiss franc"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Swiss franc", "extracted_entity": "Liechtenstein"}
{"question": "has Leichtenstein worked to promote the county's image by prosecuting international money-laundering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no, he has not.", "extracted_entity": null}
{"question": "what company administers Leichtenstein's railways?", "answer": "Austrian Federal Railways", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austrian Federal Railways"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the same company that administers the railways of the", "extracted_entity": null}
{"question": "what percentage of Liechtenstein's population is foreign-born?", "answer": "two-thirds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two-thirds", "67%"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "0.00%", "extracted_entity": "Liechtenstein"}
{"question": "is there an airport in Liechtenstein?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Liechtenstein"}
{"question": "is Liechtenstein heavily urbanized?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, it is not heavily urbanized.", "extracted_entity": "Liechtenstein"}
{"question": "is Liechtenstein doubly landlocked?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein"}
{"question": "does Liechtenstein have an army?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes. Liechtenstein has a small army of", "extracted_entity": "Liechtenstein"}
{"question": "when was Liechtenstein's current constitution adopted?", "answer": "October 1921.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["October 1921.", "October 1921"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1921", "extracted_entity": "Liechtenstein"}
{"question": "what is the official language of Liechtenstein?", "answer": "German.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German.", "German"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "German", "extracted_entity": "Liechtenstein"}
{"question": "what countries border Liechtenstein?", "answer": "Switzerland and Austria.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Switzerland and Austria.", "Switzerland and Austria"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Austria and Switzerland", "extracted_entity": "Liechtenstein"}
{"question": "what Roman province was Liechtenstein part of?", "answer": "Raetia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Raetia.", "Raetia"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Rhaetia", "extracted_entity": "Roman_province;Liechtenstein"}
{"question": "how many municipalities are within Oberland?", "answer": "6.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.", "6"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "14", "extracted_entity": "Oberbergischer_Kreis"}
{"question": "what is the smallest German-speaking country in the world?", "answer": "Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": "German_language"}
{"question": "the Savings and Loans Bank was founded, as was the first cotton-weaving mill in what year?", "answer": "1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1861"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1853", "extracted_entity": null}
{"question": "what enjoys one of the world's highest standards of living?", "answer": "Liechtenstein's population", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein's population"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "does the state court rule on the conformity of laws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, they do. They have the power to", "extracted_entity": null}
{"question": "what do most recognizable international company and largest employer have in common?", "answer": "They are Hilti.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are Hilti."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "both of them are in the same city, that", "extracted_entity": null}
{"question": "was Liechtenstein completed in November 2000?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, the Liechtenstein castle was completed in", "extracted_entity": null}
{"question": "what completed in November?", "answer": "the Kunstmuseum Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Kunstmuseum Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the first half of my thesis, and a draft", "extracted_entity": null}
{"question": "the State Court rules what?", "answer": "on the conformity of laws with the constitution", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["on the conformity of laws with the constitution"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the State Court rules what?", "extracted_entity": null}
{"question": "the Historical Society of the Principality of Liechtenstein plays what?", "answer": "a role in preserving the culture and history of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a role in preserving the culture and history of the country."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The Historical Society of the Principality of Liechten", "extracted_entity": null}
{"question": "is the museum collection also the national art collection of Liechtenstein ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, the collection is not the national art collection", "extracted_entity": "Liechtenstein"}
{"question": "is there a small heliport at Balzers in Liechtenstein available for charter helicopter flights ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, there is a small heliport in", "extracted_entity": "Balzers;Liechtenstein"}
{"question": "are nationals referred to by the plural : Liechtensteiners ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, they are called Liechtensteiner.", "extracted_entity": null}
{"question": "is it a winter sports resort , although it is perhaps best known as a tax haven ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "t is a winter sports resort, although it is", "extracted_entity": "Tax_haven"}
{"question": "is it a winter sports resort?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, but it is a summer sports resort.", "extracted_entity": null}
{"question": "is it the smallest german-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "what happened in moravia , lower austria , silesia , and styria , though in all cases , these territories were held in fief under other more senior feudal lords?", "answer": "The Liechtenstein Dynasty acquired vast swaths of land", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Liechtenstein Dynasty acquired vast swaths of land"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "two were held by the Bohemian", "extracted_entity": "Moravia;Silesia;Styria"}
{"question": "did Webster write, \"I can now sleep of nights\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "no. He wrote, \"I can now sleep", "extracted_entity": null}
{"question": "was the arrival of Louis Kossuth (exiled leader of a failed Hungarian revolution) another issue that presented itself during Fillmore's presidency?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The arrival of Louis Kossuth (", "extracted_entity": "Lajos_Kossuth;Hungary"}
{"question": "did he die at 11:10 p.m. on March 8, 1874, of the after-effects of a stroke?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, he died of a stroke at 11", "extracted_entity": null}
{"question": "who got Seward elected to the senate?", "answer": "Weed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Weed", "Weed ultimately got Seward elected to the senate"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "He was elected to the Senate in 1849", "extracted_entity": "Seward,_Alaska"}
{"question": "where was Commodore Matthew C. Perry sent to open Japan to Western trade?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "Commodore Matthew C. Perry send to Japan"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "pan", "extracted_entity": "Matthew_C._Perry;Japan;Western_world"}
{"question": "what is The Remarkable Millard Fillmore?", "answer": "A fake biography based on real events that happened in Fillmore's life.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A fake biography based on real events that happened in Fillmore's life.", "In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events that happened in Fillmore's life"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The Remarkable Millard Fillmore is a book", "extracted_entity": "Millard_Fillmore"}
{"question": "did Fillmore form a law partnership before or after he founded the private University of Buffalo?", "answer": "Before.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Before.", "Law partnership founded before University of Buffalo"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "after", "extracted_entity": "Millard_Fillmore;University_at_Buffalo"}
{"question": "did Fillmore run for President a second time?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "I don't know (semantic ambiguity)"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Millard_Fillmore"}
{"question": "how long after the death of his first wife did Fillmore marry Caroline McIntosh?", "answer": "where is the death date of his first wife?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where is the death date of his first wife?"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "2 years", "extracted_entity": "Millard_Fillmore"}
{"question": "was Millard Fillmore the thirteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Millard_Fillmore;President_of_the_United_States"}
{"question": "was Millard Fillmore born on January 7, 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Millard_Fillmore"}
{"question": "did Millard Fillmore die on March 8, 1974?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "no, he died on March 8,", "extracted_entity": "Millard_Fillmore"}
{"question": "where was Millard Fillmore born?", "answer": "in a log cabin in Summerhill, New York", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in a log cabin in Summerhill, New York"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore was born in a log cabin in Mor", "extracted_entity": "Millard_Fillmore"}
{"question": "to whom was Millard Fillmore born to?", "answer": "to Nathaniel and Phoebe Millard Fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to Nathaniel and Phoebe Millard Fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "a farmer and tavern owner", "extracted_entity": "Millard_Fillmore"}
{"question": "who did Millard Fillmore fall in love with?", "answer": "He fell in love with Abigail Powers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He fell in love with Abigail Powers", "Abigail Powers"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Abigail Powers Fillmore", "extracted_entity": "Millard_Fillmore"}
{"question": "what did Taylor and Fillmore disagree upon?", "answer": "slavory issues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slavory issues", "Taylor and Fillmore disagreed on the slavery issue in the new western territories taken from Mexico in the Mexican-American War"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Taylor wanted to remove the Indians from the land while", "extracted_entity": null}
{"question": "how did Fillmore ascend to the presidency?", "answer": "upon the death of the sitting president, Taylor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon the death of the sitting president, Taylor", "Official White House portrait of Millard FillmoreFillmore ascended to the presidency upon the sudden and unexpected death of President Taylor in July 1850."], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "He was elected vice president on the ticket with Zach", "extracted_entity": "Millard_Fillmore"}
{"question": "how did the supporters of Henry Clay feel about Fillmore in 1848?", "answer": "they were ok with him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they were ok with him", "angry"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore was the Whig candidate for President in", "extracted_entity": "Henry_Clay;Millard_Fillmore"}
{"question": "who was first apprenticed to a fuller to learn the cloth-making trade?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "the first apprentice was a woman named Elizabeth", "extracted_entity": null}
{"question": "he founded the private university of buffalo on what date?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "the university of buffalo was founded in 1846", "extracted_entity": null}
{"question": "who or what fell in love with abigail powers?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "the man in the moon", "extracted_entity": null}
{"question": "was Fillmore one of the founders of the University of Buffalo?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No. Fillmore was never a founder of the", "extracted_entity": "University_at_Buffalo"}
{"question": "was Another primary objective of Fillmore to preserve the Union from the intensifying slavery debate?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Another primary objective of Fillmore to preserve the Union", "extracted_entity": "Millard_Fillmore;United_States"}
{"question": "was Fillmore the second Chancellor, a position he maintained while both Vice President and President?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, Fillmore was not Chancellor. Chancellor was", "extracted_entity": "Millard_Fillmore"}
{"question": "is Fillmore the first of two presidents to have been an indentured servant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Millard_Fillmore"}
{"question": "is the comic strip Mallard Fillmore named after the president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Millard_Fillmore"}
{"question": "was Fillmore the first U.S. President born after the death of a former president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No. Fillmore was the 13th President", "extracted_entity": "Millard_Fillmore;United_States;President_of_the_United_States"}
{"question": "did Fillmore not turn down the honor , explaining that he had neither the `` literary nor scientific attainment '' to justify the degree ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Millard_Fillmore"}
{"question": "what happened in a log cabin in summerhill , new york?", "answer": "Fillmore was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fillmore was born"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "It was the summer of 1863. President", "extracted_entity": null}
{"question": "who was born precisely at midnight during an electrical storm , to a Serbian family in the village of Smiljan near Gospi\u0107 , in the Lika region of the Croatian Krajina in Military Frontier ( part of the Austrian Empire ) , in the present-day Croatia . ?", "answer": "Nikola Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "a Tesla", "extracted_entity": "Serbia;Gospi\u0107;Lika;Croatia_proper"}
{"question": "are there at least two films describing Tesla 's life ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "s, two films describing Tesla's life are", "extracted_entity": "Nikola_Tesla"}
{"question": "have a number of live theatrical plays based on Tesla 's life been produced and staged worldwide ?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "s, there have been a number of live theatrical", "extracted_entity": "Nikola_Tesla"}
{"question": "do sea otters have long muscular tails?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No. They do not have long muscular tails.", "extracted_entity": "Sea_otter"}
{"question": "can otters survive in cold water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they have a layer of fat to keep", "extracted_entity": null}
{"question": "does the giant otter inhabit South Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": "South_Africa"}
{"question": "how many species of otter are there?", "answer": "13", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "13", "extracted_entity": "Otter"}
{"question": "what do river otters eat?", "answer": "a variety of fish and shellfish, as well as small land mammals and birds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a variety of fish and shellfish, as well as small land mammals and birds", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds. ", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "They eat crayfish, crabs, clams", "extracted_entity": "North_American_river_otter"}
{"question": "how much do sea otters weigh?", "answer": "30 kg (about 65 pounds)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["30 kg (about 65 pounds)"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "between 14 and 100 pounds", "extracted_entity": "Sea_otter"}
{"question": "how do sea otters insulate themselves?", "answer": "a layer of air trapped in their fur(!)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a layer of air trapped in their fur(!)", "As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea otters use their thick fur to insulate", "extracted_entity": "Sea_otter"}
{"question": "why are sea otters nearly extinct?", "answer": "They've been hunted for their fur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They've been hunted for their fur", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they were hunted for their fur.", "extracted_entity": null}
{"question": "what is one of the challenges of re-establishing a population of Eurasian otters in the UK?", "answer": "roadkill deaths", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roadkill deaths", "Roadkill deaths have become one of the significant threats to the success of their re-establishment. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "the lack of natural prey", "extracted_entity": "Eurasian_otter;United_Kingdom"}
{"question": "is otter a kind of mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Otter"}
{"question": "how many species and genera does otter have?", "answer": "13 species and 7 genera", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13 species and 7 genera", "13 species in 7 genera."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "13 species, 7 genera", "extracted_entity": "Otter"}
{"question": "do otters live in water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes they do.", "extracted_entity": null}
{"question": "where do sea otters live?", "answer": "Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "The sea otter lives actually in the sea. ", "Beside water", "Pacific coast of North America"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "the ocean.", "extracted_entity": null}
{"question": "where does the word \"otter\" derive from?", "answer": "The word \"otter\" derives from the Old English word otr, otor or oter. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The word \"otter\" derives from the Old English word otr, otor or oter. ", "The word \"otter\" derives from the Old English word otr, otor or oter. This and cognate words in other Indo-European languages ultimately stem from a root which also gave rise to the English words \"water\", \"wet\" and \"winter\". "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "from the old English word \"otere\" which", "extracted_entity": null}
{"question": "does otter give birth or lay egg?", "answer": "give birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["give birth"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "give birth", "extracted_entity": null}
{"question": "what drives sea otter almost to extinction?", "answer": "Humans hunted them almost to extinction. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Humans hunted them almost to extinction. ", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them almost to extinction. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Sea otter fur was highly prized in the", "extracted_entity": null}
{"question": "why otters are considered as totem animals?", "answer": "The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Because otters are very playful and love to have", "extracted_entity": null}
{"question": "do sea otters live along the Pacific coast?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sea_otter"}
{"question": "are otters totem animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "I am not sure what you mean by \"tot", "extracted_entity": null}
{"question": "are otters herbivores?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No, they are carnivores. Otters eat", "extracted_entity": null}
{"question": "what is the primary item in an otter's diet?", "answer": "Fish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fish.", "fish"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish and invertebrates", "extracted_entity": "Otter"}
{"question": "what is an otter's den called?", "answer": "Holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holt", "holt", "A holt"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "a holt", "extracted_entity": null}
{"question": "why is the giant otter becoming increasingly rare?", "answer": "Poaching, habitat loss, and toxins in gold mining.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Poaching, habitat loss, and toxins in gold mining.  ", "poaching, habitat loss, and the use of mercury in illegal alluvial gold mining"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "It is being hunted for its pelt and meat", "extracted_entity": null}
{"question": "how do otters keep themselves warm without blubber?", "answer": "A layer of air trapped in their fur.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A layer of air trapped in their fur.  ", "a layer of air trapped in their fur"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters have thick, warm fur and they also", "extracted_entity": null}
{"question": "how are otters playful animals?", "answer": "The slide down snowy slopes, apparently for sheer enjoyment.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The slide down snowy slopes, apparently for sheer enjoyment.  ", "They slide repeatedly down snowy slopes for sheer enjoyment."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they play with their food.", "extracted_entity": null}
{"question": "what animals are related to otters?", "answer": "weasels, polecats, and badgers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["weasels, polecats, and badgers"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are related to weasels, sk", "extracted_entity": null}
{"question": "what traps a layer of air, and keeps them dry and warm under water?", "answer": "Long guard hair", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Long guard hair"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "the skin of a water strider", "extracted_entity": null}
{"question": "the collective noun romp is sometimes used for a group of what?", "answer": "Otters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Otters"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "cats", "extracted_entity": null}
{"question": "is it true that otters eat a variety of fish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, otters eat a variety of fish.", "extracted_entity": null}
{"question": "are otters playful animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they are playful animals.", "extracted_entity": null}
{"question": "are otters very active?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they are very active", "extracted_entity": null}
{"question": "are male otters dog-otters?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no, male otters are otter-ott", "extracted_entity": null}
{"question": "an otter 's den is what?", "answer": "A holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A holt"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "a hole in a bank", "extracted_entity": null}
{"question": "sea otters eat what?", "answer": "Shellfish and other invertebrates", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shellfish and other invertebrates"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea urchins", "extracted_entity": "Sea_otter"}
{"question": "is the myth of Otter 's Ransom the starting point of the Volsunga saga ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "o, it is not the starting point of the", "extracted_entity": "V\u00f6lsunga_saga"}
{"question": "are Male otters dog-otters , females are bitches and babies are cubs or pups ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "es, yes and yes.", "extracted_entity": null}
{"question": "is an otter 's den called a holt ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Otter"}
{"question": "have most otters fish as the primary item in their diet , supplemented by frogs , crayfish and crabs ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "st otters have fish as the primary item in", "extracted_entity": "Otter;Frog;Crayfish;Crab"}
{"question": "do penguins feed on krill?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, krill is a major source of food", "extracted_entity": "Penguin;Krill"}
{"question": "what is the largest living species of penguin?", "answer": "Emperor Penguin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Emperor Penguin", "the Emperor Penguin (Aptenodytes forsteri)", "Emperor"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "the Emperor penguin (Aptenodytes", "extracted_entity": null}
{"question": "do penguins live almost exclusively in the Southern Hemisphere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Southern_Hemisphere"}
{"question": "what is \"tobogganing\"?", "answer": "when penguins slide on their bellies across the snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when penguins slide on their bellies across the snow", "Tobogganing is when penguins slide on their bellies across the snow."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "sledding", "extracted_entity": null}
{"question": "why are penguins countershaded?", "answer": "for camouflage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["for camouflage"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "to blend in with the water, to avoid predators", "extracted_entity": null}
{"question": "are penguins afraid of humans?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, they are afraid of dogs.", "extracted_entity": null}
{"question": "how much time to penguins spend on land?", "answer": "half of their life", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["half of their life", "They spend half of their life on land."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguins spend about 80% of their time", "extracted_entity": null}
{"question": "how many species of penguins are there?", "answer": "between 17 and 20 living species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["between 17 and 20 living species", "Penguin biodiversity varies between 17 and 20 living species, all in the subfamily Spheniscinae."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "18 species of penguins are known to exist", "extracted_entity": null}
{"question": "how are Isabelline penguins different from most penguins?", "answer": "they have brown rather than black plumage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they have brown rather than black plumage", "Because they are born with brown rather than black plumage."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Isabelline penguins have brown feathers instead", "extracted_entity": null}
{"question": "are penguins birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Penguin"}
{"question": "do penguins have a better than average sense of hearing for birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Penguin"}
{"question": "are penguins considered \"higher waterbirds\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, they are not considered \"higher waterbirds", "extracted_entity": "Penguin"}
{"question": "what order do penguins belong to?", "answer": "Sphenisciformes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sphenisciformes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Order Sphenisciformes.", "extracted_entity": null}
{"question": "how tall were the tallest prehistoric penguins?", "answer": "1.80 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.80 meters", "as tall as an adult human"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they were 1.7 metres (5.", "extracted_entity": null}
{"question": "what makes penguins so agile in the water?", "answer": "Their wings have become flippers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their wings have become flippers", "Smooth plumage preserves a layer of air, ensuring buoyancy, wings are flippers"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "the presence of a tail", "extracted_entity": null}
{"question": "why do penguins \"tobaggan\"?", "answer": "It conserves energy while moving quickly", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It conserves energy while moving quickly", "Conserves energy while moving quickly"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "tobagganing is a way of getting from place", "extracted_entity": null}
{"question": "who spend half of their life on land and half in the oceans?", "answer": "Penguins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Sea turtles!", "extracted_entity": null}
{"question": "what is also the distance that Antarctic tourists are told to keep from penguins?", "answer": "3 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 meters"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "5 meters", "extracted_entity": "Antarctic"}
{"question": "what have become flippers, useless for flight in the air?", "answer": "Penguins' wings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins' wings"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguin flippers, as they are not designed", "extracted_entity": null}
{"question": "what happens when mothers lose a chick?", "answer": "They sometimes attempt to \"steal\" another chick.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes attempt to \"steal\" another chick."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they stop laying eggs for 2 months", "extracted_entity": null}
{"question": "is it true that each penguin gets a turn in the center?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, each penguin gets a turn in the", "extracted_entity": null}
{"question": "are all penguins countershaded?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Countershading"}
{"question": "are penguins astonishingly agile?", "answer": "In the water they are.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the water they are."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, they are. penguins can swim at", "extracted_entity": null}
{"question": "are penguins at risk?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, penguins are not at risk. They", "extracted_entity": null}
{"question": "is it also possible that penguin comes from the Latin pinguis , fat ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "es. but it's not the most likely explanation", "extracted_entity": null}
{"question": "have penguins an average sense of hearing for birds ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, penguins have an average sense of hearing", "extracted_entity": "Penguin"}
{"question": "can larger penguins dive deep in case of need ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, they can dive deeper than 500m", "extracted_entity": null}
{"question": "is the smallest penguin species the Little Blue Penguin -LRB- also known as the Fairy Penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2.2 lb -RRB- ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "e Little Blue Penguin is the smallest penguin species", "extracted_entity": "Little_penguin"}
{"question": "is it not even known whether the gigantic palaeeudyptines constitute a monophyletic lineage , or whether gigantism was evolved independently in a much restricted Palaeeudyptinae and the Anthropornithinae ?", "answer": "It is not even known.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not even known."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "t is not known whether the gigantic palaee", "extracted_entity": "Monophyly"}
{"question": "is the largest living species the emperor penguin -LRB- aptenodytes forsteri -RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Emperor_penguin"}
{"question": "is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "he fairy penguin is the smallest species of p", "extracted_entity": "Little_penguin"}
{"question": "what happened in a region not quite 2000 km south of the equator 35 mya?", "answer": "At least one giant penguin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At least one giant penguin."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "It got wetter", "extracted_entity": "Year"}
{"question": "what is the last word on the page?", "answer": "Connecticut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Connecticut", "(SSN-22)"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "I don\u2019t know. It\u2019s not on the", "extracted_entity": null}
{"question": "is polar bear a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Polar_bear"}
{"question": "is polar bear a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "es, it is a carnivore.", "extracted_entity": "Polar_bear;Carnivore"}
{"question": "what is largest polar bear on record?", "answer": "2200 lb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2200 lb", "A huge male, allegedly weighing 1002 kg (2200 lb) shot at Kotzebue Sound in northwestern Alaska in 1960."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "2,645 pounds (1,200 kilograms", "extracted_entity": null}
{"question": "what is polar bear's skin color?", "answer": "white or cream", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["white or cream", "Black"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "black", "extracted_entity": null}
{"question": "how long is polar bear's guard hair?", "answer": "5-15 cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5-15 cm"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "1.5 to 3 inches long", "extracted_entity": "Polar_bear"}
{"question": "what is cause of polar bear's skin diseases?", "answer": "mites or other parasites", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mites or other parasites", "Mites or other parasites"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears have a thick layer of fat under their", "extracted_entity": "Polar_bear"}
{"question": "why polar bear is a special species?", "answer": "The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "because it is the largest extant species of bear", "extracted_entity": "Polar_bear"}
{"question": "what do fossil and DNA evidence tell us?", "answer": "The polar bear diverged from the brown bear about 200 thousand years ago.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The polar bear diverged from the brown bear about 200 thousand years ago.", "The polar bear diverged from the brown bear roughly 200 thousand years ago."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Fossil evidence tells us that whales evolved from land", "extracted_entity": null}
{"question": "does a polar bear live in the Arctic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Polar_bear;Arctic"}
{"question": "is a polar bear white in color?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, it is white in color.", "extracted_entity": "Polar_bear"}
{"question": "is a polar bear at high risk of extinction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes, because its habitat is shrinking.", "extracted_entity": null}
{"question": "how heavy is a male polar bear?", "answer": "300-600 kg (660-1320 lb)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300-600 kg (660-1320 lb)", "Most adult males weigh 350-650 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "adult males weigh.", "extracted_entity": "Polar_bear"}
{"question": "how heavy was the largest polar bear on record?", "answer": "1002 kg (2200 lb) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1002 kg (2200 lb) ", "He allegedly weighed 1002 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "1,002kg", "extracted_entity": null}
{"question": "what does a polar bear's fur provide?", "answer": "It provides the animal with effective camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It provides the animal with effective camouflage.", "A polar bear's fur provides camouflage and insulation"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "insulation, protection from the sun", "extracted_entity": null}
{"question": "do female polar bears weight more than the male?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "no, they are about the same weight.", "extracted_entity": null}
{"question": "how much weight do female polar bears gain during pregnancy?", "answer": "They gain double their weight.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They gain double their weight.", "They double their weight during pregnancy"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "450 pounds", "extracted_entity": "Polar_bear"}
{"question": "can polar bears be seen under infrared photography?", "answer": "Polar bears are nearly invisible under infrared photography.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Polar bears are nearly invisible under infrared photography.", "Only their breath and muzzles can be easily seen"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Polar_bear"}
{"question": "what is actually black in color?", "answer": "A polar bear's skin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A polar bear's skin."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the absence of light, or color", "extracted_entity": null}
{"question": "what insulate it against the cold?", "answer": "Its think blubber and fur.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its think blubber and fur."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a layer of fur.", "extracted_entity": null}
{"question": "what includes a lengthy justification of why this species is listed as vulnerable?", "answer": "Database entry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Database entry."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "IUCN Red List of Threatened Species", "extracted_entity": null}
{"question": "what do with greenpeace and the natural resources defense council have in common?", "answer": "They filed lawsuits in California.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They filed lawsuits in California."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they are both environmental groups that are trying to stop", "extracted_entity": null}
{"question": "are studies insufficient evidence for global protection?", "answer": "It is arguable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is arguable."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "no, studies are insufficient evidence for global protection.", "extracted_entity": null}
{"question": "are polar bears excellent swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, they are excellent swimmers.", "extracted_entity": "Polar_bear"}
{"question": "a semi-aquatic marine mammal , the polar bear has what?", "answer": "It has adapted for life on a combination of land, sea, and ice.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It has adapted for life on a combination of land, sea, and ice."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "thick fur coat and a layer of fat under", "extracted_entity": "Marine_mammal;Polar_bear"}
{"question": "garbage is what?", "answer": "Garbage is now recycled or transported to Thompson, Manitoba.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Garbage is now recycled or transported to Thompson, Manitoba."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "it is a waste of time and energy to do", "extracted_entity": null}
{"question": "the bears sometimes what?", "answer": "They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "hibernate", "extracted_entity": null}
{"question": "is the Polar Bear the mascot of Bowdoin college ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, the Polar Bear is the mascot of Bow", "extracted_entity": "Bowdoin_College"}
{"question": "have thumbpolar bears been made both controversial and famous for their distinctive white fur and their habitat ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, they have been.", "extracted_entity": null}
{"question": "did Mitchell Taylor , the Nunavut Government Manager of Wildlife Research , not write a letter to the U.S. Fish and Wildlife Service arguing that local studies are insufficient evidence for global protection at this time ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": ": was the U.S. Fish", "extracted_entity": "Nunavut;United_States_Fish_and_Wildlife_Service"}
{"question": "are Cubs born in December without awakening the mother ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No, the mother is awake but she does not", "extracted_entity": null}
{"question": "does Qatar rank as the eighth richest country in the world per capita?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qatar"}
{"question": "do nearly all Qataris profess Islam?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, 96.6% of Qataris", "extracted_entity": "Qatar;Islam"}
{"question": "can women legally drive in Qatar?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Qatar"}
{"question": "when did Qatar become an independent state?", "answer": "September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 3, 1971.", "September 3, 1971"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": "Qatar"}
{"question": "where is Qatar's telecommunication system centered?", "answer": "Doha.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Doha.", "Doha"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar is a peninsula jutting into the Persian Gulf", "extracted_entity": "Qatar"}
{"question": "where did a suicide-bombing kill a teacher in 2005?", "answer": "At the Doha Players Theatre.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At the Doha Players Theatre.", "Doha Players Theatre"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Iraq", "extracted_entity": null}
{"question": "does the native pronunciation of Qatar sound like 'cutter'?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Qatar"}
{"question": "what is the Arabic word for municipalities?", "answer": "baladiyah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["baladiyah.", "baladiyah"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "?????", "extracted_entity": "Arabic_language"}
{"question": "what universities are in Education City?", "answer": "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "VCU, Carnegie Mellon, Georgetown, Texas A", "extracted_entity": "Education_City"}
{"question": "what carried with it a tacit recognition of Qatar's status as distinct from Bahrain?", "answer": "The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "British protectorate", "extracted_entity": "Qatar;Bahrain"}
{"question": "qatar became an independent sovereign state in what year?", "answer": "On September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On September 3, 1971."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": "Qatar"}
{"question": "what consists of a low, barren plain, covered with sand?", "answer": "Much of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much of the country."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "The Sahara desert.", "extracted_entity": null}
{"question": "how do qatar and the wider region relate?", "answer": "Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar is a small, wealthy country on the Arabian", "extracted_entity": null}
{"question": "was Qatar University founded in 1973?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No, it was founded in 1977.", "extracted_entity": "Qatar_University"}
{"question": "is Qatar bordered by Saudi Arabia to the south?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, Qatar is bordered by Saudi Arabia to the", "extracted_entity": "Qatar;Saudi_Arabia"}
{"question": "what bordered by Saudi?", "answer": "Qatar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "the Persian Gulf, the Red Sea, and the", "extracted_entity": "Saudi_Arabia"}
{"question": "who served as the headquarters and one of the main launching sites of the US invasion of Iraq in 2003 ?", "answer": "Qatar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "The US invasion of Iraq in 2003 was", "extracted_entity": "United_States;Iraq"}
{"question": "has Qatar a modern Telecommunication system centered in Doha ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, Qatar has a modern Telecommunication system centered", "extracted_entity": "Qatar;Doha"}
{"question": "was Qatar University founded in 1973 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qatar_University"}
{"question": "did the Qataris not choose as their negotiator the respected entrepreneur and long-time resident of Doha , Muhammed bin Thani ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "o, the Qataris chose as their negotiator", "extracted_entity": "Qatar;Doha"}
{"question": "will oil and gas probably remain the backbone of Qatar 's economy for some time to come ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "rue", "extracted_entity": "Qatar"}
{"question": "has it been created with a long term perspective to support the development of Qatar and the wider region , develop local and regional markets , and strengthen the links between the energy based economies and global financial markets ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Qatar"}
{"question": "what happened in these positions in english?", "answer": "These allophones cannot occur there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["These allophones cannot occur there."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "I don't know. I don't speak English", "extracted_entity": null}
{"question": "can these allophones not occur in these positions in english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes, they can. /r/ and /", "extracted_entity": "English_language"}
{"question": "is -RRB- , officially the state of qatar -LRB- arabic (: : transliterated as dawlat qatar -RRB- , an arab emirate in southwest asia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, Qatar is an Arab country.", "extracted_entity": "Arabic_language;Asia"}
{"question": "does Romania border Hungary?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, it does. Romania borders Hungary to the", "extracted_entity": "Romania;Hungary"}
{"question": "is Romania a secular state?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Romania;Secular_state"}
{"question": "is the president elected by popular vote?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no. The president is elected by the electoral college", "extracted_entity": null}
{"question": "how many counties is Romania divided into?", "answer": "41.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["41.", "forty-one"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "41", "extracted_entity": "Romania"}
{"question": "what is the highest mountain in Romania?", "answer": "Moldoveanu Peak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moldoveanu Peak.", "Moldoveanu Peak"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Moldoveanu", "extracted_entity": "Romania"}
{"question": "what is the official language of Romania?", "answer": "Romanian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romanian.", "Romanian"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanian", "extracted_entity": "Romania"}
{"question": "is the Romanian economy doing well?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, the Romanian economy is doing well.", "extracted_entity": "Romania"}
{"question": "are there many Roma in Romania?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, there are many Roma in Romania.", "extracted_entity": "Romania"}
{"question": "how many territories joined to form Romania?", "answer": "2.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2.", "2"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": "Romania"}
{"question": "does Romania share a border with Ukraine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, Romania shares a border with Ukraine.", "extracted_entity": "Romania;Ukraine"}
{"question": "did Romania declare neutrality during World War I?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": "Romania;World_War_I"}
{"question": "does Romania share the same language with Moldova?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Practically"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, Romanian is the official language in both countries", "extracted_entity": "Romania;Moldova"}
{"question": "who was the first gymnast to score a perfect \"ten\"?", "answer": "Nadia Com\u0103neci", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nadia Com\u0103neci"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Olga Korbut", "extracted_entity": null}
{"question": "what is the largest city in Romania?", "answer": "Bucharest", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bucharest"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest", "extracted_entity": "Romania"}
{"question": "what is the largest ethnic minority in Romania?", "answer": "Hungarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hungarians"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanians", "extracted_entity": "Romania"}
{"question": "how many countries in Europe are bigger than Romania?  ?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven", "Eleven"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "5", "extracted_entity": "Europe;Romania"}
{"question": "why doesn't Romania have a state religion?  ?", "answer": "Romania is a secular state", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romania is a secular state"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "because it is a secular state", "extracted_entity": "Romania"}
{"question": "why wasn't Romania neutral during World War II?", "answer": "it received a Soviet ultimatum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it received a Soviet ultimatum", "The Soviets threatened invasion."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania was not neutral during World War II because they", "extracted_entity": "Romania;World_War_II"}
{"question": "sibiu, a large city in Transylvania, was chosen as European Capital of Culture in what year?", "answer": "2007", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2007"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Q: sibiu,", "extracted_entity": "Sibiu;Transylvania;European_Capital_of_Culture"}
{"question": "what are approximately 42,000 years old and have been nicknamed`` John of Anina''?", "answer": "the remains (the lower jaw) of the oldest modern human", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the remains (the lower jaw) of the oldest modern human"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The oldest human remains found in Romania", "extracted_entity": null}
{"question": "is it true that romania has a population of 21,698,181?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, it is true", "extracted_entity": "Romania"}
{"question": "what do economy and law have in common?", "answer": "(not sure how to answer this)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(not sure how to answer this)"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "they are both based on the concept of scarcity", "extracted_entity": null}
{"question": "what did Tourism in Romania attract in 2005?", "answer": "investments worth 400 million euros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["investments worth 400 million euros"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "7.6 million tourists", "extracted_entity": "Romania"}
{"question": "is Romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Romania;Semi-presidential_system;Unitary_state"}
{"question": "is Romania a semi-presidential democratic republic where executive functions are shared between the president and the prime minister ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romania;Semi-presidential_system;President_of_Romania;Prime_Minister_of_Romania"}
{"question": "was Gellu Naum the leader of the surrealist movement in Romania ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": "Surrealism;Romania"}
{"question": "was the Bucharest Metro only opened in 1979 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "No, it was opened in 1979.", "extracted_entity": "Bucharest_Metro"}
{"question": "is romania -LRB- , -RRB- a country in southeastern europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Europe"}
{"question": "is romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no, it is a semi-presidential federal state", "extracted_entity": "Semi-presidential_system;Unitary_state"}
{"question": "do singaporeans enjoy squid?", "answer": "Yes, they do.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they do.", "yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "no, they don\u2019t.", "extracted_entity": null}
{"question": "did Singapore's standard of living decrease since independence?", "answer": "No, the standard living was raised.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the standard living was raised.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Singapore"}
{"question": "did the media contribute 15.6% to Singapore's GDP in 2001?", "answer": "No, it contributed 1.56% to GDP in 2001.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, it contributed 1.56% to GDP in 2001.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, according to the text, the media contributed", "extracted_entity": "Singapore"}
{"question": "how many people use the bus network daily?", "answer": "More than 2.78 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 2.78 million people.", "2.78 million people"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1,000,000", "extracted_entity": null}
{"question": "where did Orang Laut people live?", "answer": "Orang Laut people lived around the coast, rivers and smaller islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Orang Laut people lived around the coast, rivers and smaller islands.", "Several hundred indigenous Orang Laut people also lived around the coast, rivers and smaller islands."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "around Singapore, peninsular Malaysia and the Indonesian", "extracted_entity": "Orang_laut"}
{"question": "what is the MRT?", "answer": "One of three heavy rail passenger transport lines in Singapore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of three heavy rail passenger transport lines in Singapore.", "Mass Rapid Transit system"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Mass Rapid Transit. It is a train system in", "extracted_entity": null}
{"question": "how is Singapore's climate characterized?", "answer": "Singapore has a tropical rainforest climate with no distinctive seasons.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore has a tropical rainforest climate with no distinctive seasons.", "Its climate is characterized by uniform temperature and pressure, high humidity, and abundant rainfall"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore's climate is characterized by high temperatures, high", "extracted_entity": "Singapore"}
{"question": "how much increase in the media's contribution to GDP is the government seeking?", "answer": "The government seeks to increase its GDP contribution to 3% by 2012. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government seeks to increase its GDP contribution to 3% by 2012. ", "the government seeks to increase its GDP contribution to 3% by 2012. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "10% increase in the media's contribution to", "extracted_entity": null}
{"question": "how many days did it take the Imperial Japanese Army to win the Battle of Singapore?", "answer": "Six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six days", "6 days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "7", "extracted_entity": "Imperial_Japanese_Army;Battle_of_Singapore"}
{"question": "is Singapore located at the southern tip of the Korean Penisula?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "No. It is located at the southern tip of", "extracted_entity": null}
{"question": "is Singapore a city-state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Singapore;City-state"}
{"question": "has Sang Nila Utama visited Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Singapore"}
{"question": "what does \"singa\" mean?", "answer": "lion", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lion"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singa is a lion, and the Lion City", "extracted_entity": null}
{"question": "when is the first record of settlement in Singapore?", "answer": "second century AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["second century AD", "The first records of settlement in Singapore are from the second century AD."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "In 1299", "extracted_entity": "Singapore"}
{"question": "which nation invaded Singapore during World War II?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "During World War II, the Imperial Japanese Army invaded Malaya, culminating in the Battle of Singapore."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": "Singapore;World_War_II"}
{"question": "why Churchill called the occupation of Singapor by Janpan during WWII \"Britain's greatest defeat\"?", "answer": "The British were defeated in six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British were defeated in six days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Because Britain was unable to defend the island from Japan", "extracted_entity": "Winston_Churchill;Singur;Janakpur,_Nepal;World_War_II"}
{"question": "was Lee Kuan Yew a successful leader of Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Lee Kuan Yew was a successful leader of", "extracted_entity": "Lee_Kuan_Yew;Singapore"}
{"question": "when did Goh Chok Tong succeed Lee as Prime Minister?", "answer": "In 1990, Goh Chok Tong succeeded Lee as Prime Minister.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1990, Goh Chok Tong succeeded Lee as Prime Minister."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Goh Chok Tong succeeded Lee as Prime Minister", "extracted_entity": "Goh_Chok_Tong;Spike_Lee"}
{"question": "what is the primary domestic source of water supply?", "answer": "The primary domestic source of water supply in Singapore is rainfall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The primary domestic source of water supply in Singapore is rainfall."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "groundwater", "extracted_entity": null}
{"question": "what is the national language of Singapore?", "answer": "The national language of Singapore is Malay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The national language of Singapore is Malay."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": "Singapore"}
{"question": "where is the National Orchid Garden?", "answer": "Singapore Botanic Gardens.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore Botanic Gardens."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore Botanic Gardens", "extracted_entity": null}
{"question": "why are relations with Malaysia and Indonesia important?", "answer": "Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Malaysia and Indonesia are important to the United States because", "extracted_entity": "Malaysia;Indonesia"}
{"question": "what is the most densely populated independent country in the world?", "answer": "Monaco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monaco"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Vatican City", "extracted_entity": null}
{"question": "who did Sir Thomas Stamford Raffles work for?", "answer": "British East India Company", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Raffles was a British colonial official who served as", "extracted_entity": "Stamford_Raffles"}
{"question": "when was Lee Kuan Yew prime minister of Singapore?", "answer": "1959", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1959"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1959-1990", "extracted_entity": "Lee_Kuan_Yew;Singapore"}
{"question": "what is the punishment for first-degree murder?", "answer": "There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "life imprisonment or death penalty", "extracted_entity": null}
{"question": "what established a trading post on the island in 1819?", "answer": "British East India Company ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the Hudson's Bay Company", "extracted_entity": null}
{"question": "what used Singapore as a strategic trading post along the spice route?", "answer": "British", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the British East India Company", "extracted_entity": "Singapore"}
{"question": "what became one of the most important commercial and military centres of the British Empire?", "answer": "Singapore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Calcutta", "extracted_entity": "British_Empire"}
{"question": "who boasted a sizeable community of 100,000 by 1869?", "answer": "British India office", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British India office"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the Mormons, in Utah", "extracted_entity": null}
{"question": "how do reservoirs and water relate?", "answer": "By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "water is stored in reservoirs, which are man", "extracted_entity": null}
{"question": "is it true that singapore has several long - standing disputes with malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore has several long - standing disputes with Malaysia,", "extracted_entity": "Singapore;Malaysia"}
{"question": "has Singapore Changi Airport a network of 81 airlines connecting Singapore to 185 cities in 58 countries ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Singapore_Changi_Airport;Singapore"}
{"question": "has Singapore a network of reservoirs and water catchment areas ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "has Urbanization eliminated many areas of former primary rainforest , with the only remaining area of primary rainforest being Bukit Timah Nature Reserve ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "o, there are still many areas of primary rain", "extracted_entity": null}
{"question": "does the economy not depend heavily on exports refining imported goods , especially in manufacturing ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "es, but not to the extent that you seem", "extracted_entity": null}
{"question": "is singapore -LRB- ; (: ; , -RRB- , officially the republic of singapore -LRB- ; (: ; , -RRB- , an island nation located at the southern tip of the malay peninsula?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "singapore -LRB- ; (: ;,", "extracted_entity": "Singapore;Malay_Peninsula"}
{"question": "is singapore the 17th wealthiest country in the world in terms of gdp per capita?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "7th wealthiest country in the world in terms", "extracted_entity": null}
{"question": "was Thedore Roosevelt  a member of the Republican Party?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "does Theodore Roosevelt have a brother?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Theodore_Roosevelt"}
{"question": "did Theodore Roosevelt go to Harvard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes. He was born in New York City,", "extracted_entity": "Theodore_Roosevelt;Harvard_University"}
{"question": "where was Theodore Roosevelt born?", "answer": "28 East 20th Street in the modern-day Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28 East 20th Street in the modern-day Gramercy section of New York City", "a four-story brownstone at 28 East 20th Street, in the modern-day Gramercy section of New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "He was born in New York City, New York", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what did Roosevelt do to improve his physical condition?", "answer": "Roosevelt took up exercise", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roosevelt took up exercise", "He started boxing"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt did a lot of different things to improve his", "extracted_entity": null}
{"question": "why did Roosevelt start Boxing lessons?", "answer": "To deal with bullies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To deal with bullies"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt was a sickly child. He began boxing", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what was the nickname of Theodore Roosevelt's sister Anna?", "answer": "Bamie as a child and Bye as an adult", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bamie as a child and Bye as an adult", "Bamie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Anna Roosevelt", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what was Roosevelt's justification for supporting desegregation of schools in New York?", "answer": "Noting that his children had been educated with other races and there was nothing wrong with it", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Noting that his children had been educated with other races and there was nothing wrong with it", "that his children had been educated with other races and there was nothing wrong with it"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt's justification for supporting desegregation of schools", "extracted_entity": "Franklin_D._Roosevelt;New_York_City"}
{"question": "how did newspapers respond to Roosevelt inviting Brooker T. Washington to dinner at the White House?", "answer": "mixing of the races on social occassions", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mixing of the races on social occassions", "They outraged."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt invited Booker T. Washington to dinner at the", "extracted_entity": "Franklin_D._Roosevelt;White_House"}
{"question": "did Theodore Roosevelt study Judo?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No clue"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "No, he did not study Judo.", "extracted_entity": "Theodore_Roosevelt;Judo"}
{"question": "did Roosevelt receive a Medal of Honor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "nein"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Medal_of_Honor"}
{"question": "did Roosevelt study biology?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He was strong in biology."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Theodore_Roosevelt"}
{"question": "where was Theodore Roosevelt from?", "answer": "Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gramercy section of New York City", "New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "He was from New York", "extracted_entity": "Theodore_Roosevelt"}
{"question": "who was Roosevelt's presidential hero?", "answer": "Lincoln was Roosevelt's presidential hero.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lincoln was Roosevelt's presidential hero.", "Lincoln was Roosevelt's presidential hero"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt", "extracted_entity": null}
{"question": "when did Roosevelt die?", "answer": "On January 6, 1919, Roosevelt died in his sleep.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 6, 1919, Roosevelt died in his sleep."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "He died on January 6, 1919", "extracted_entity": "Theodore_Roosevelt"}
{"question": "who helped to fund Roosevelt's African safari?", "answer": "Financed by Andrew Carnegie and his own proposed writings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Financed by Andrew Carnegie and his own proposed writings", "Andrew Carnegie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "the United Fruit Company", "extracted_entity": null}
{"question": "did Roosevelt support racial integration in schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He supported a bill allowing school desegregation in New York State."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt did not support racial integration in schools.", "extracted_entity": "Franklin_D._Roosevelt"}
{"question": "was Roosevelt's family rich?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes, his family was rich.", "extracted_entity": null}
{"question": "are turtles ectothermic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ectotherm"}
{"question": "does the mother care for the young?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, the mother cares for the young.", "extracted_entity": null}
{"question": "are turtles pets?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, turtles are pets.", "extracted_entity": null}
{"question": "what shape are the eggs of the larest species of turtle?", "answer": "Spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spherical", "spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "spherical", "extracted_entity": null}
{"question": "how often do turtles breed?", "answer": "Every few years or more", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Every few years or more"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles breed once a year", "extracted_entity": null}
{"question": "what do all turtles and tortoises breathe?", "answer": "air", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["air"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "through their lungs", "extracted_entity": "Turtle;Tortoise"}
{"question": "how do turtles reproduce?", "answer": "They lay eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They lay eggs", "they lay eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles lay eggs", "extracted_entity": "Turtle"}
{"question": "what has been discovered about turtles organs?", "answer": "They do not gradually break down or become less efficient over time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They do not gradually break down or become less efficient over time", "they do not break down"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "they have a very long life span and have a", "extracted_entity": null}
{"question": "what suborder of turtle draws its head into its shell?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the chelonian", "extracted_entity": null}
{"question": "do sea turtles lay eggs on dry sandy beaches?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes they do.", "extracted_entity": null}
{"question": "are Pleurodira known as side-necked turtles?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, Pleurodira are side-necked", "extracted_entity": null}
{"question": "are turtles a part of the Flying Spaghetti Monster?", "answer": "*shrug*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["*shrug*"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, they are. Flying Spaghetti Monsterism", "extracted_entity": null}
{"question": "how many suborders are turtles divided into?", "answer": "Three.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three.", "three"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "name a turtle popularly kept as a pet.?", "answer": "Russian Tortoises", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Russian Tortoises"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Red-eared slider", "extracted_entity": null}
{"question": "name four related articles for the article on turtles.?", "answer": "Triassic, Ernst Haeckel, Kunstformen der Natur, Animal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Triassic, Ernst Haeckel, Kunstformen der Natur, Animal"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles, tortoises, and terrap", "extracted_entity": null}
{"question": "if a turtle egg was kept warm, what would likely hatch?", "answer": "A female turtle.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A female turtle.", "A female turtle"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "a baby turtle", "extracted_entity": null}
{"question": "what do turles use to breathe in the water?", "answer": "Papillae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Papillae"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "gills", "extracted_entity": null}
{"question": "why are side-necked turtles called side-necked turtles?", "answer": "Because of the way they withdraw their heads into their shells.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because of the way they withdraw their heads into their shells.", "The way they withdraw their heads into their shells."], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "because they have a hinge in their neck that allows", "extracted_entity": null}
{"question": "do turtles lay eggs underwater?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no. They lay them on land", "extracted_entity": null}
{"question": "are turtle eggs leathery and soft?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, turtle eggs are leathery and soft", "extracted_entity": null}
{"question": "are turtles being examined for longevity genes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, and so are naked mole rats, and", "extracted_entity": null}
{"question": "what suborder of turtles is extinct?", "answer": "Paracryptodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paracryptodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the pleurodira", "extracted_entity": null}
{"question": "what do turtle eggs preared to eat consist mainly of?", "answer": "Yolk", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yolk"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "protein", "extracted_entity": null}
{"question": "where do sea turtles lay their eggs?", "answer": "Holes Dug into the Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holes Dug into the Mud or Sand", "holes dug into mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "on the beach", "extracted_entity": "Sea_turtle"}
{"question": "what is the smallest suborder of turtles?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "cryptodira.", "extracted_entity": null}
{"question": "what are turtle eggs covered in when they incubate?", "answer": "Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Mud or Sand", "mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtle eggs are covered in mud when they incubate", "extracted_entity": null}
{"question": "what type of creatures breathe air and don't lay eggs underwater?", "answer": "Amniotes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amniotes", "amnoites"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "mammals", "extracted_entity": null}
{"question": "what are deposited in holes dug into mud or sand?", "answer": "eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "these are the fossil remains of animals.", "extracted_entity": null}
{"question": "what sort of turtles are ectothermic?", "answer": "all of them", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["all of them"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "All turtles are ectothermic", "extracted_entity": "Ectotherm"}
{"question": "are the largest turtles aquatic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no, they are not. They are land turtles", "extracted_entity": null}
{"question": "is aquatic respiration in Australian freshwater turtles being studied?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Australia"}
{"question": "these projections , called papillae , have what?", "answer": "a rich blood supply", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a rich blood supply"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "aste buds", "extracted_entity": null}
{"question": "the eggs of the largest species are what?", "answer": "spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the eggs of the largest species are the largest", "extracted_entity": null}
{"question": "has this inspired genetic researchers to begin examining the turtle genome for longevity genes ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no, because turtles have a very different genetic makeup", "extracted_entity": null}
{"question": "can turtles take many years to reach breeding age ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles take many years to reach breeding age.", "extracted_entity": null}
{"question": "are Immature sea turtles not cared for by the adults ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no, sea turtles are cared for by the adults", "extracted_entity": null}
{"question": "are the Pleurodira sometimes known as the side-necked turtles , a reference to the way they withdraw their heads into their shells ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "rue", "extracted_entity": "Pleurodira"}
{"question": "does the Order Testudines not include both extant -LRB- living -RRB- and extinct species , the earliest known turtles being from around 215 million years ago ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "oes the Order Testudines not include both ext", "extracted_entity": null}
{"question": "was Ulysses Grant a general in the American Civil War?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ulysses_S._Grant;American_Civil_War"}
{"question": "was Grant's father-in-law a Democrat?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yup"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "No, he was a Republican.", "extracted_entity": "Democratic_Party_(United_States)"}
{"question": "did Ulysses win the Battle of Champion Hill?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "you betcha"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "es, he did.", "extracted_entity": "Odysseus;Battle_of_Champion_Hill"}
{"question": "who was Grant's brother in law?", "answer": "Fred Dent", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fred Dent"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "where was Grant born?", "answer": "Point Pleasant, Ohio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Point Pleasant, Ohio", "A log cabin in Point Pleasant, Clermont County, Ohio"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Point Pleasant, Ohio", "extracted_entity": null}
{"question": "what was Grant's political affiliation?", "answer": "Republican", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Republican"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Republican", "extracted_entity": null}
{"question": "why did Grant say \"Damn, I had nothing to do with this batte.\"?", "answer": "It went well.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It went well.", "Because the generals under him acted on their own."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Because he was in charge of the Army of the", "extracted_entity": null}
{"question": "did Horace Greenley lose in the presidential elections of 1872?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "si"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "why did Grant think that war was unjust?", "answer": "He thought it was designed to gain land open to slavery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He thought it was designed to gain land open to slavery.", "He accepted the theory that it was designed to gain land open to slavery."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "He thought that war was unjust because he had lost", "extracted_entity": "Ulysses_S._Grant"}
{"question": "who achieved international fame as the leading Union general in the American Civil War?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": "Union_Army;American_Civil_War"}
{"question": "grant was elected president as a Republican in what year?", "answer": "1868", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1868"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1868", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "who took a hard line that reduced violence by groups like the Ku Klux Klan?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "President Roosevelt", "extracted_entity": "Ku_Klux_Klan"}
{"question": "when did he vote for democrat james buchanan?", "answer": "1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1856"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "In 1856, Van Buren was a", "extracted_entity": "Democratic_Party_(United_States)"}
{"question": "who did President Lincoln promote of major general in the regular army, effective July 4?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "President Lincoln promoted George B. McClellan", "extracted_entity": "Abraham_Lincoln;Major_general"}
{"question": "was Grant elected president as a Republican?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes. He was elected in 1868 and", "extracted_entity": "Ulysses_S._Grant;Republican_Party_(United_States)"}
{"question": "what was from Pennsylvania?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the Commonwealth of Pennsylvania", "extracted_entity": "Pennsylvania"}
{"question": "was it a two-sentence description that completely caught the essence of Ulysses S. Grant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "yes, it was a two-sentence description that", "extracted_entity": "Ulysses_S._Grant"}
{"question": "was Grant 's favorite brand of bourbon whiskey Old Crow ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "es, it was.", "extracted_entity": "Ulysses_S._Grant"}
{"question": "had he smoked only sporadically ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "he would have lived longer.", "extracted_entity": null}
{"question": "did Grant & Perkins not sell harnesses , saddles , and other leather goods and purchase hides from farmers in the prosperous Galena area ?", "answer": "they did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they did"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "s, they did.", "extracted_entity": null}
{"question": "is the famous dragon gate at the entrance to the district at the corner of Grant and Bush Street ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Crown_Street,_Sydney"}
{"question": "what happened in 1865?", "answer": "he accepted the surrender of Robert E. Lee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he accepted the surrender of Robert E. Lee"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the 13th amendment was ratified", "extracted_entity": null}
{"question": "what happened in recent years?", "answer": "his reputation as president has improved", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["his reputation as president has improved"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the Internet has developed rapidly", "extracted_entity": null}
{"question": "is Uruguay located in the northwesten part of Africa?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Uruguay;Africa"}
{"question": "is Uruguay's capital Montevideo?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "es, Montevideo is the capital of Uruguay", "extracted_entity": "Uruguay;Montevideo"}
{"question": "does Uruguay have cold summers?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, Uruguay has hot summers.", "extracted_entity": "Uruguay"}
{"question": "who founded Montevideo?", "answer": "The Spanish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Spanish.", "the Spanish", "By the Spanish, in the early 18th century", "Montevideo was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Spanish", "extracted_entity": "Montevideo"}
{"question": "where is Uruguay's oldest church?", "answer": "San Carlos, Maldonado.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Carlos, Maldonado.", "San Carlos, Maldonado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "in the capital city of Montevideo", "extracted_entity": "Uruguay"}
{"question": "who heavily influenced the architecture and culture of Montevideo?", "answer": "European immigrants.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["European immigrants.", "European immigrants"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Spanish", "extracted_entity": "Montevideo"}
{"question": "what are poor neighborhoods called informally?", "answer": "Cantegriles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cantegriles.", "Cantegriles"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the hood", "extracted_entity": null}
{"question": "is uruguay's landscape mountainous?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "not really?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay is a country in South America, bordered by", "extracted_entity": null}
{"question": "what religion do most Uruguayans profess?", "answer": "None.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None.", "Roman Catholic"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguayans are predominantly Roman Catholic, but the state", "extracted_entity": "Uruguay"}
{"question": "is Uruguay located in South America?", "answer": "Yes. It's located in the southeastern part", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. It's located in the southeastern part", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay is located in South America.", "extracted_entity": "Uruguay;South_America"}
{"question": "can citizens propose changes to the Constitution?", "answer": "Yes. People are allowed to challenge laws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. People are allowed to challenge laws.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, the Constitution allows for the amendment of the", "extracted_entity": "Constitution_of_Pakistan"}
{"question": "did Uruguay host the first ever World Cup?", "answer": "Yes, in 1930.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, in 1930.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Uruguay;FIFA_World_Cup"}
{"question": "how many square kilometres of continental land is Uruguay?", "answer": "176.215 km\u00b2 ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["176.215 km\u00b2 ", "176,214 square kilometres"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "176,215 sq km", "extracted_entity": "Uruguay"}
{"question": "how much of the population is of white European descent?", "answer": "88%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88%", "Approximately 88% of its population are of prevalently white European descent."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "60% of the population is of white European", "extracted_entity": null}
{"question": "why was the capital of Uruguay founded?", "answer": "For a military stronghold.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For a military stronghold.", "Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Because of the large number of people who had to", "extracted_entity": "Uruguay"}
{"question": "how many times has Uruguay won the World Cup?", "answer": "Twice. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twice. ", "On two or more occasions."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": "Uruguay_national_football_team;FIFA_World_Cup"}
{"question": "how much is the average income of an African woman compared to a European man?", "answer": "African women earns 0.65 * 0.718  = 46.67% of a European man earns in average", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["African women earns 0.65 * 0.718  = 46.67% of a European man earns in average"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the average income of an African woman is 20", "extracted_entity": "Africa;Europe"}
{"question": "does Uruguay recognize same-sex civil unions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Uruguay"}
{"question": "does Uruguay border French Guiana?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Uruguay;French_Guiana"}
{"question": "is Uruguay's warmest month June?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, its warmest month is January", "extracted_entity": "Uruguay"}
{"question": "what does a citizen use to propose changes to the Constitution?", "answer": "Referendum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Referendum", "Plebiscite"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a petition", "extracted_entity": "Constitution_of_Singapore"}
{"question": "what religions are found in Uruguay?", "answer": "Roman Catholic, Protestant, Jewish, and nonprofessing.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roman Catholic, Protestant, Jewish, and nonprofessing.", "Roman Catholic, Protestant, Jewish"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic 66%, non-Catholic Christian", "extracted_entity": "Uruguay"}
{"question": "what has Uruguay done to be competitive in agriculture?", "answer": "Labeling as \"Natural\" or \"Ecological\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Labeling as \"Natural\" or \"Ecological\"", "Use low inputs of labour, technology, and capital, which results in lower yields but also opens the door for Uruguay to market its products as \"natural\" or \"ecological\""], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay has developed a strategy to become competitive in agriculture", "extracted_entity": "Uruguay"}
{"question": "what are the names of Uruguay's political parties?", "answer": "Partido Colorado and Partido Blanco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Partido Colorado and Partido Blanco", "Partido Blanco and Partido Colorado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the names of Uruguay's political parties are the National", "extracted_entity": "Uruguay"}
{"question": "is Uruguay the smallest soverign nation in South America?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, it is the second smallest. Suriname", "extracted_entity": "Uruguay;South_America"}
{"question": "what was founded by the Spanish in the early 18th century as a military stronghold?", "answer": "Montevideo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montevideo"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "San Juan", "extracted_entity": "Spain"}
{"question": "what has many possible meanings?", "answer": "(What?)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(What?)"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a word", "extracted_entity": null}
{"question": "how do india and latin america relate?", "answer": "Uruguay exports architectural services to India and Latin America", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uruguay exports architectural services to India and Latin America"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "India and Latin America are related through the history of", "extracted_entity": "India;Latin_America"}
{"question": "what did Uruguay win in 1828?", "answer": "Its independence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its independence"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the war against Brazil", "extracted_entity": "Uruguay"}
{"question": "is Uruguay very common?", "answer": "No?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, Uruguay is not very common.", "extracted_entity": "Uruguay"}
{"question": "is The climate in Uruguay temperate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, the climate in Uruguay is temperate.", "extracted_entity": "Uruguay"}
{"question": "is the main sport in Uruguay football ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Uruguay"}
{"question": "is Uruguay 's oldest church in San Carlos , Maldonado ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay's oldest church in San Carlos,", "extracted_entity": "Uruguay;San_Carlos,_Chile;Maldonado,_Uruguay"}
{"question": "is Uruguay a member of the Cairns Group of exporters of agricultural products ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes. Uruguay is a member of the Cairns", "extracted_entity": null}
{"question": "is it widely considered the most secular nation in Latin America ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": "Latin_America"}
{"question": "is it the second smallest independent country in south america , larger only than suriname and the french overseas department of french guiana?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "riname and French Guiana are both overseas territories", "extracted_entity": "South_America;Suriname;French_Guiana"}
{"question": "is it a constitutional democracy , where the president fulfills the roles of both head of state and head of government?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "President_of_South_Africa;Head_of_state;Head_of_government"}
{"question": "was Woodrow Wilson the thirtieth President of the United States?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Woodrow_Wilson;President_of_the_United_States"}
{"question": "did Woodrow Wilson create the League of Nations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes, he did create the League of Nations", "extracted_entity": "Woodrow_Wilson;League_of_Nations"}
{"question": "did the U.S. join the League of Nations?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;League_of_Nations"}
{"question": "where was the League of Nations created?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Paris, France", "extracted_entity": "League_of_Nations"}
{"question": "when was Woodrow Wilson born?", "answer": "December 28, 1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 28, 1856"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "December 28, 1856", "extracted_entity": "Woodrow_Wilson"}
{"question": "who was President when Wilson finished Congressional Government?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "what field did Woodrow Wilson leave law practice to study?", "answer": "history and political science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["history and political science", "Government"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "he left law practice to study political science", "extracted_entity": "Woodrow_Wilson"}
{"question": "what caused Wilson to ask Congress to declare war on the Central Powers?", "answer": "German began unrestricted submarine warfare", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German began unrestricted submarine warfare", "World War I"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "German U-boats sinking American ships and the Zimmerman", "extracted_entity": "United_States_Congress;Central_Powers"}
{"question": "what was more damaging than moving students into colleges?", "answer": "His confrontation with Andrew Fleming West", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His confrontation with Andrew Fleming West"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "moving students into high school", "extracted_entity": null}
{"question": "was Wilson a member of the Phi Kappa Psi fraternity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Totally"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": "Phi_Kappa_Psi"}
{"question": "was Wilson an automobile enthusiast?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "ouai"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Wilson_Fittipaldi_J\u00fanior"}
{"question": "did Wilson's father own slaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yeah"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes he did. He owned 5 slaves.", "extracted_entity": null}
{"question": "where is Wilson buried?", "answer": "He was buried in Washington National Cathedral", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was buried in Washington National Cathedral", "Washington National Cathedral"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "in the cemetery at St. James Episcopal Church in", "extracted_entity": null}
{"question": "where did Wilson attend law school?", "answer": "Wilson attended law school at University of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson attended law school at University of Virginia", "University of Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Princeton", "extracted_entity": null}
{"question": "where was Woodrow Wilson born?", "answer": "Woodrow Wilson was born in Staunton, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Woodrow Wilson was born in Staunton, Virginia", "Staunton, Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson was born in Staunton,", "extracted_entity": "Woodrow_Wilson"}
{"question": "did Wilson support desegregation?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Not in the slightest."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no. He was a racist. He opposed des", "extracted_entity": null}
{"question": "did Wilson support the committee system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Wilson have any siblings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes he did, he had a sister named Anna", "extracted_entity": null}
{"question": "what was Scots-Irish and Scottish?", "answer": "His ancestry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His ancestry"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Scots-Irish is a term used in North America", "extracted_entity": "Ulster_Scots_people;Scotland"}
{"question": "what defended slavery, owned slaves and set up a Sunday school for them?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "John Newton, the author of the hymn Amazing", "extracted_entity": null}
{"question": "who did Wilson win in 1917?", "answer": "Irish Americans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish Americans"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the election in 1917 was not a presidential", "extracted_entity": null}
{"question": "was Wilson awarded the 1919 Nobel Peace Prize?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No, Wilson was not awarded the 1919", "extracted_entity": "Nobel_Peace_Prize"}
{"question": "was Wilson a remarkably effective writer and thinker?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, Wilson was an effective writer and thinker.", "extracted_entity": null}
{"question": "what lived in Columbia?", "answer": "Wilson", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the capital city of the U.S. state of", "extracted_entity": "Columbia,_South_Carolina"}
{"question": "was Wilson president of the American Political Science Association in 1910 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he was. Wilson was president of the", "extracted_entity": "American_Political_Science_Association"}
{"question": "did he not cast his ballot for John M. Palmer , the presidential candidate of the National Democratic Party , or Gold Democrats , a short-lived party that supported a gold standard , low tariffs , and limited government ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "he did.", "extracted_entity": "John_M._Palmer_(politician);National_Democratic_Party_(United_States);Gold_standard;Limited_government"}
{"question": "did Wilson not spend 1914 through the beginning of 1917 trying to keep America out of the war in Europe ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson did not spend 1914 through the beginning", "extracted_entity": "United_States;Europe"}
{"question": "was Wilson , a staunch opponent of antisemitism , sympathetic to the plight of Jews , especially in Poland and in France ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "He was sympathetic to the plight of Jews", "extracted_entity": "Harold_Wilson;Antisemitism;Jews;Poland"}
{"question": "what happened in 1917?", "answer": "raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Russia\u2019s Czar was overthrown and the Bolshevik", "extracted_entity": null}
{"question": "was Volta an Italian physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Italy"}
{"question": "is Volta buried in the city of Pittsburgh?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, he is buried in Como, Italy.", "extracted_entity": "Alessandro_Volta;Pittsburgh"}
{"question": "did Volta have a passion for the study of electricity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "what is the battery made by Volta credited to be?", "answer": "the first cell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the first cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "voltaic cell", "extracted_entity": null}
{"question": "what important electrical unit was named in honor of Volta?", "answer": "the volt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the volt", "volt"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the volt.", "extracted_entity": null}
{"question": "where did Volta enter retirement?", "answer": "Spain", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spain"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He entered retirement in Como.", "extracted_entity": "Alessandro_Volta"}
{"question": "is it a disadvantage for something to be unsafe to handle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, it is a disadvantage for something to be", "extracted_entity": null}
{"question": "was Lombardy under Napoleon's rule in 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, Lombardy was under Napoleon's rule in", "extracted_entity": "Lombardy;Napoleon_I"}
{"question": "was the Italian 10.000 lira banknote created before the euro?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Italy"}
{"question": "for how many years did Alessandro Volta live?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82 years", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alessandro_Volta"}
{"question": "what was Alessandro Volta`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "physicist and chemist", "extracted_entity": "Alessandro_Volta"}
{"question": "how old was Alessandro Volta when he died?", "answer": " 82", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" 82"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82", "extracted_entity": "Alessandro_Volta"}
{"question": "how many years ago was it when Volta married the daughter of Count Ludovico Peregrini , Teresa , with whom he raised three sons?", "answer": "215", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["215"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "793", "extracted_entity": null}
{"question": "is the electrolyte sulphuric acid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sulfuric_acid"}
{"question": "is volta buried in the city  of Como?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Como"}
{"question": "was his 1800  paper written in French?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, it was written in German.", "extracted_entity": "French_language"}
{"question": "before 1796, was Lombardy ruled by Austria?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lombardy;Austria"}
{"question": "did he receive the Society`s 1794 Copley Medal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, he received the Society`s 1794", "extracted_entity": "Copley_Medal"}
{"question": "did he experiment with individual cells?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, but only in the context of his experiments", "extracted_entity": null}
{"question": "when did lombardy come under Napoleon`s rule?", "answer": "From 1796 to 1815", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1796 to 1815"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1805", "extracted_entity": "Lombardy"}
{"question": "where did he publish his invention of the Voltaic pile battery?", "answer": "the Philosophical Transactions of the Royal Society", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Philosophical Transactions of the Royal Society"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Italy", "extracted_entity": null}
{"question": "did he become professor of experimental physics at the University of Pavia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": "Experimental_physics;University_of_Pavia"}
{"question": "is it true that his passion been always the study of electricity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, it is true that his passion been always", "extracted_entity": null}
{"question": "his passion been always the study of what?", "answer": "Electricity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Electricity"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the study of what?", "extracted_entity": null}
{"question": "is it true that Volta married the daughter of Count Ludovico Peregrini?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, it is true that Volta married the", "extracted_entity": null}
{"question": "is it true that he published his invention of the Voltaic pile battery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he published his invention in 1800", "extracted_entity": null}
{"question": "was Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was. He was born in Tur", "extracted_entity": "Italy"}
{"question": "did Amedeo Avogadro graduate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he was a professor.", "extracted_entity": null}
{"question": "did Avogadro live in England?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro was born in Italy, but lived", "extracted_entity": "England"}
{"question": "where was Avogadro a professor of physics?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin, Italy", "extracted_entity": null}
{"question": "how many children did Avogadro have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "6.0221415 x 10^", "extracted_entity": null}
{"question": "what is Avogadro most noted for?", "answer": "contributions to the theory of molarity and molecular weight", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["contributions to the theory of molarity and molecular weight"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "He is most noted for his hypothesis, which states", "extracted_entity": "Agadir"}
{"question": "what is Avogadro's number?", "answer": "6.02214199x10 23", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.02214199x10 23"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number is the number of atoms", "extracted_entity": null}
{"question": "in what year did Avogadro stop teaching at Turin University?", "answer": "1853", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1853", "1823"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1850", "extracted_entity": "Turin;University_of_Turin"}
{"question": "what subject did Avogadro study first?", "answer": "ecclesiastical law", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ecclesiastical law"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "mathematics", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s birthplace?", "answer": "Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin, Italy.", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s profession?", "answer": "professor of physics", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of physics"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was an Italian scientist", "extracted_entity": null}
{"question": "how many years ago was it when he became a professor of physics at the University of Turin?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1982", "extracted_entity": "University_of_Turin"}
{"question": "what is Amedeo Avogadro`s first name?", "answer": "Amedeo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Lorenzo", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s last name?", "answer": "Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro is his last name.", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in North America?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he was born in Italy.", "extracted_entity": "North_America"}
{"question": "was King Victor  Emmanuel  III there to pay  homage  to Avogadro?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No. He was there to pay homage to his", "extracted_entity": null}
{"question": "in fact, was Avogadro `s  famous  1811  paper written in French . )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, Avogadro `s  paper was", "extracted_entity": "France"}
{"question": "is avogadro `s  number commonly used to compute  the results  of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "hence, can relative molecular  masses be calculated from the masses  of gas samples?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Relative molecular masses can be calculated from the masses of", "extracted_entity": null}
{"question": "did he publish his work?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, he published his work", "extracted_entity": null}
{"question": "did Avogadro submit his poem?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is it true that loschmidt calculated first the value of Avogadro number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he did. In 1865 he", "extracted_entity": null}
{"question": "loschmidt calculated first the value of what?", "answer": "Avogadro's number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro's number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "he calculated first the value of the average speed of", "extracted_entity": null}
{"question": "hence , can relative molecular masses be calculated from the masses of gas samples ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "o. Relative molecular masses are calculated from the masses", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into English as \"essay on determining the Relative Masses of the Elementary Molecules of Bodies\" ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, the title can be roughly translated into English", "extracted_entity": "English_language"}
{"question": "what is Amedeo Avogadro?", "answer": "A person.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A person."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in Tur", "extracted_entity": null}
{"question": "where is Amedeo Avogadro from?", "answer": "Italy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Italy."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in Tur", "extracted_entity": null}
{"question": "when did Anders Celcius publish his observations on the aurora borealis?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733", "1733 "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1733.", "extracted_entity": null}
{"question": "what did Anders Celsius determine about the boiling of water?", "answer": "He determined the dependence of the boiling of water with atmospheric pressure.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He determined the dependence of the boiling of water with atmospheric pressure.", "dependence with atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "He determined that the boiling point of water was", "extracted_entity": "Anders_Celsius"}
{"question": "which temperature scale did Celsius propose?", "answer": "Celcius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celcius", " the Celsius temperature scale "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius proposed the centigrade scale", "extracted_entity": "Celsius"}
{"question": "what is the boiling point of water dependent on?", "answer": "atmospheric pressure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The boiling point of water is dependent on the amount", "extracted_entity": null}
{"question": "where was Celsius born?", "answer": "Uppsala in Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uppsala in Sweden", "Uppsala, Sweden"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Sweden", "extracted_entity": "Celsius"}
{"question": "whad did Celsius report in his paper \"Observations of two persistent degrees on a thermometer\"?", "answer": "the freezing point is independent of latitude", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the freezing point is independent of latitude"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius reported that the temperature of ice was 0", "extracted_entity": "Celsius"}
{"question": "how old was Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "44", "extracted_entity": "Celsius"}
{"question": "what is temperature would water have to be to be halfway between its standard boiling and freezing point?", "answer": "50", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["50"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "100.0 C", "extracted_entity": null}
{"question": "what is the difference between Celsius' original temperature scale and the one we use today?", "answer": "The scale was reversed by Carolus Linnaeus. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed by Carolus Linnaeus. ", "scale was reversed "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Celsius scale is now the same as the cent", "extracted_entity": "Celsius"}
{"question": "what was Anders Celsius`s profession?", "answer": "professor of astronomy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of astronomy"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "astronomer, physicist and mathematician.", "extracted_entity": "Anders_Celsius"}
{"question": "how old was Anders Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius was 16 years old when he died", "extracted_entity": "Anders_Celsius"}
{"question": "what is Anders Celsius`s last name?", "answer": "Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": "Anders_Celsius"}
{"question": "was celsius born in Uppsala  in Sweden?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Uppsala;Sweden"}
{"question": "is The Celsius  crater  on the Moon named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Moon"}
{"question": "was Anders Celsius  ( November 27 ,  1701  April 25 ,  1744  ) a Swedish  astronomer?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "es, he was.", "extracted_entity": "Anders_Celsius;Sweden"}
{"question": "was Anders Celsius the first  to perform  and  publish  careful experiments  aiming at the definition  of an international  temperature  scale  on scientific grounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No, because the first temperature scale was defined by", "extracted_entity": "Anders_Celsius"}
{"question": "was he professor at Uppsala University?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Uppsala_University"}
{"question": "is it true that he published a collection of 316 observations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, it is true.", "extracted_entity": null}
{"question": "he published a collection of what?", "answer": "observations", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["observations"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "poems", "extracted_entity": null}
{"question": "can syllables begin with a vowel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, but it's not very common", "extracted_entity": null}
{"question": "are calligraphers held in great esteem?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, they are not. Calligraphy is considered", "extracted_entity": null}
{"question": "have many European languages borrowed numerous words from it?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "true", "extracted_entity": "Europe"}
{"question": "are the most active in Damascus and Cairo?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "es, they are the most active in", "extracted_entity": "Damascus;Cairo"}
{"question": "do arabs consider literary Arabic?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arab_people;Arabic_language"}
{"question": "the sociolinguistic situation of Arabic provides a prime example of what?", "answer": "the linguistic phenomenon of diglossia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the linguistic phenomenon of diglossia"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "diglossia", "extracted_entity": "Arabic_language"}
{"question": "are pronouns in Literary Arabic marked for person , number and gender ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "es, they are.", "extracted_entity": "Modern_Standard_Arabic"}
{"question": "however , are non-human plural nouns grammatically considered to be feminine singular ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "o, because the plural noun \"dogs\" is", "extracted_entity": null}
{"question": "is a bee an insect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bee;Insect"}
{"question": "are bees related to ants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "yes, they are both insects.", "extracted_entity": null}
{"question": "have managed populations of European honey bees experienced substantial declines?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": "Europe"}
{"question": "where do bees live?", "answer": "in colonies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in colonies"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "in a hive", "extracted_entity": null}
{"question": "when do bumblebee colonies reach peak population?", "answer": "mid to late summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mid to late summer"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "early summer", "extracted_entity": null}
{"question": "what do people commonly call cleptoparasitic bees?", "answer": "cuckoo bees", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cuckoo bees"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "kleptoparasitic bees, cleptop", "extracted_entity": null}
{"question": "how did vespoid wasps evolve?", "answer": "from predatory ancestors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from predatory ancestors"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "they evolved from aculeate wasps", "extracted_entity": null}
{"question": "what do humans use bees for?", "answer": "honey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["honey"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "honey, wax, pollination, and honeybe", "extracted_entity": null}
{"question": "why do honey bee queens continue to lay eggs during the foraging season?", "answer": "to replace daily casualties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to replace daily casualties"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Because the colony is preparing for winter, and the", "extracted_entity": null}
{"question": "does Beijing mean \"northern capital\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": "Beijing"}
{"question": "are the caves of Dragon Bone Hill near the village of Zhoukoudian in Fangshan District?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yes, the caves of Dragon Bone Hill are near", "extracted_entity": "Zhoukoudian;Fongshan_District"}
{"question": "are hutongs disappearing?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "yes. the government is trying to make them into", "extracted_entity": null}
{"question": "when did Beijing host the Olympic Games?", "answer": "2008", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2008"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "2008", "extracted_entity": "Beijing;Olympic_Games"}
{"question": "when was the An Shi Rebellion launched?", "answer": "in 755 AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 755 AD"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "755 A.D.", "extracted_entity": null}
{"question": "in 1949, where did Communist forces enter without a fight?", "answer": "Beiping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beiping"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "China", "extracted_entity": "Communism"}
{"question": "when did Yuan Shikai die?", "answer": "1916", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1916"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yuan Shikai died on 6 June", "extracted_entity": "Yuan_Shikai"}
{"question": "where does air pollution in Beijing come from?", "answer": "surrounding cities and provinces", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["surrounding cities and provinces"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "The major source of air pollution in Beijing is the", "extracted_entity": "Beijing"}
{"question": "as of August 1st, 2006, how many trains stop daily at the Beijing Railway Station or the Beijing West Railway Station?", "answer": "167", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["167"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": "Beijing_Railway_Station;Beijing_West_Railway_Station"}
{"question": "what is the name of a university (or similar institution for imparting higher education) in Beijing?", "answer": "Tsinghua University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tsinghua University"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing University of Posts and Telecommunications", "extracted_entity": "Beijing"}
{"question": "are famous middle  schools  in Beijing :?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing  No.4  High School,", "extracted_entity": "Beijing"}
{"question": "was Blaise Pascal a mathematician of the first order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he was a mathematician of the first", "extracted_entity": "Blaise_Pascal"}
{"question": "could Blaise Pascal move without crutches?", "answer": "He could move without crutches until a paralytic attack in 1647.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He could move without crutches until a paralytic attack in 1647.", "no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he was an athletic child and was the", "extracted_entity": "Blaise_Pascal"}
{"question": "has the name Pascal been given to the SI unit of pressure?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pascal_(unit)"}
{"question": "from what did Pascal suffer throughout his life?", "answer": "ill health", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ill health", "poor health"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "rom his father's death when he was three years", "extracted_entity": "Blaise_Pascal"}
{"question": "what did Pascal argue was as perfect as possible?", "answer": "the procedure used in geometry ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the procedure used in geometry ", "the procedure used in geometry"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "he heart", "extracted_entity": "Blaise_Pascal"}
{"question": "what is the best physician?", "answer": "Time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Time"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the one who cures the disease.", "extracted_entity": null}
{"question": "who was the eldest sibling?", "answer": "Gilberte", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gilberte"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "she was the eldest sibling", "extracted_entity": null}
{"question": "aside from the Provincial Letters' religious influence, were they popular as a literary work?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "No, they were not popular as a literary work", "extracted_entity": null}
{"question": "of whose continual poor health was the cause never precisely determined?", "answer": "Blaise Pascal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Blaise Pascal"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "William Shakespeare", "extracted_entity": null}
{"question": "is the violin cello a bowed string instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The violin cello is a bowed string instrument.", "extracted_entity": null}
{"question": "is the purfling just for decoration?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "no, it is to prevent the top from splitting", "extracted_entity": null}
{"question": "are the z-holes used as access points to the interior of the cello? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Cello"}
{"question": "what does violincello mean?", "answer": "Violincello means \"little violene\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Violincello means \"little violene\"."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a cello with a violin body", "extracted_entity": null}
{"question": "what are the two materials that bows are made of traditionally? ?", "answer": "Traditionally, bows are made from pernambuco or brazilwood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Traditionally, bows are made from pernambuco or brazilwood."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Wood and metal", "extracted_entity": null}
{"question": "what is used to tune a cello?", "answer": "Pegs are used to tune a cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pegs are used to tune a cello.", "the pegs"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a bow", "extracted_entity": null}
{"question": "which harmonics can produce any notes above middle C? ?", "answer": "Artificial harmonics can produce any notes above middle C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Artificial harmonics can produce any notes above middle C.", "Artificial harmonics"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The harmonics above middle C can be produced by", "extracted_entity": "C_(musical_note)"}
{"question": "who referred to the cello as \"basso de viola da braccio\"?", "answer": "Monteverdi referred to the cello as \"basso de viola da braccio\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monteverdi referred to the cello as \"basso de viola da braccio\".", "Monteverdi"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Girolamo Diruta", "extracted_entity": "Cello"}
{"question": "where is the violincello held?", "answer": "The violincello is held on the shoulder.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The violincello is held on the shoulder.", "on the shoulder"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "between the knees", "extracted_entity": null}
{"question": "is there cello in performances by Rihanna?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "I don't think so, but I'm not", "extracted_entity": "Rihanna"}
{"question": "are cellos constructed with glue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "no, the top and back are glued together,", "extracted_entity": null}
{"question": "is the cello a stringed instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": "Cello"}
{"question": "what is cello an abbreviation of?", "answer": "violoncello", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["violoncello"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Cello is an abbreviation for violoncello", "extracted_entity": "Cello"}
{"question": "what position is used to play the cello?", "answer": "a gamba", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a gamba", "seated"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "4th position", "extracted_entity": "Cello"}
{"question": "what is a person who plays the cello called?", "answer": "cellist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cellist"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "A Cellist.", "extracted_entity": null}
{"question": "why is there purling on a cello?", "answer": "to stop cracks from forming ****misspelled****", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to stop cracks from forming ****misspelled****"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "it's a very good question.", "extracted_entity": null}
{"question": "when did the first educational works appear for the cello?", "answer": "the 18th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 18th century", "18th century"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The first works were published in the 17th", "extracted_entity": null}
{"question": "what cello manufacturer should I buy from if I want to play outside?", "answer": "Luis & Clark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luis & Clark"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "cello manufacturer that makes cello that are good", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a member of the National Institute?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb find any relationship between electric charges and magnetic poles? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was Charles-Augustin de Coulomb's father's family in Montpellier? ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "es, his father's family was in Montpellier", "extracted_entity": "Montpellier"}
{"question": "where did Charles-Augustin de Coulomb die?", "answer": "Charles-Augustin de Coulomb died in Paris.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb died in Paris.", "Paris, France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb died in Paris,", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "when was Charles-Augustin de Coulomb permanently stationed in Paris?", "answer": "Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1781", "extracted_entity": "Paris"}
{"question": "what contribution did Charles-Augustin de Coulomb make to the field of geotechnical engineering?", "answer": "Retaining wall design", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Retaining wall design"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb was a French engineer", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "where did the construction of Fort Bourbon take place?", "answer": "The construction of Fort Bourbon took place in Martinique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The construction of Fort Bourbon took place in Martinique.", "Martinique "], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "in the middle of the city", "extracted_entity": null}
{"question": "what is the definition of the electrostatic force of attraction and repulsion? ?", "answer": "Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "Coulomb's Law"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the force of attraction between two objects that have the", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb retire to a small estate he possessed at Blois?", "answer": "Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "1789"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1791", "extracted_entity": "Charles-Augustin_de_Coulomb;Blois"}
{"question": "if Charles-Augustin de Coulomb was alive today, how old would he have been?", "answer": "273", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["273"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "279 years old", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "for how many years did Charles-Augustin de Coulomb live?", "answer": "70", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736 - 1806 = 70", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he did not live to be 80", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "how old was Charles-Augustin de Coulomb when he died?", "answer": "1806", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1806"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "67 years old (1736-1806", "extracted_entity": null}
{"question": "how many years ago did he resign his appointment as intendant de eaux et fontaine ?", "answer": "220", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["220"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "10 years ago", "extracted_entity": null}
{"question": "how many years ago did Charles-Augustin de Coulomb die?", "answer": "203", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["203"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb died 214 years", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was The SI  unit  of charge , the  coulomb , named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "International_System_of_Units"}
{"question": "was coulomb born in Angoul\u00eame,  France ,  to a well  to do  family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Angoul\u00eame,", "extracted_entity": "Angoul\u00eame"}
{"question": "was his father ,  Henri  Coulomb , inspector of the Royal  Fields  in Montpellier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "as his father,  Henri  Coulomb", "extracted_entity": "Montpellier"}
{"question": "is coulomb distinguished in the history  of mechanics  and  of electricity and  magnetism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes, he is the first to discover the electro", "extracted_entity": "Mechanics;Electricity;Magnetism"}
{"question": "was Charles Augustin  de Coulomb  ( born  June  14 ,  1736 ,  Angoul\u00eame ,  France  -  died August 23 ,  1806 ,  Paris ,  France  ) a French  physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "he was.", "extracted_entity": "France"}
{"question": "does Coulomb leave a legacy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no, he does not.", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb born?", "answer": "June 14, 1736", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 14, 1736"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "14 June 1736", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did he publish an important investigation of the laws of friction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Coulomb born in Angoul\u00eame, France, to a well to do family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes. He was born in Angoul\u00eame,", "extracted_entity": "Angoul\u00eame;France"}
{"question": "was his father inspector of the Royal Fields?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "he discovered an inverse relationship of what?", "answer": "distance and electric force", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["distance and electric force"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the relationship between the number of people in the sample", "extracted_entity": null}
{"question": "is it true that coulomb leaves a legacy as a hero?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes. He is a hero in the world of", "extracted_entity": null}
{"question": "where is Charles-Augustin de Coulomb from?", "answer": "France", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb is from France.", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was Old Chinese wholly uninflected?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No, it had a case system, but it", "extracted_entity": "Old_Chinese"}
{"question": "are The modern Chinese dialects more like a family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No, they are not. Chinese dialects are", "extracted_entity": null}
{"question": "is it true that one-fifth of world population speak some form of Chinese?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes, 16% of the world's population", "extracted_entity": "Chinese_language"}
{"question": "one-fifth of world population speak some form of what?", "answer": "Chinese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Chinese language", "extracted_entity": null}
{"question": "is mandarin spoken Chinese distinguished by its high level?", "answer": "no, it's distinguished by diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no, it's distinguished by diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Chinese_language"}
{"question": "copenhagen is the capital of what country?", "answer": "Denmark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Denmark"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Denmark", "extracted_entity": null}
{"question": "what is the population of Copenhagen?", "answer": "1,161,063", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,161,063", "1,153,615"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "1.4 million in the urban area,", "extracted_entity": "Copenhagen"}
{"question": "what transnational bridge was completed in 2000?", "answer": "Oresund Bridge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oresund Bridge"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "the Golden Gate Bridge.", "extracted_entity": null}
{"question": "why is Copenhagen a regional hub?", "answer": "Its strategic location and excellent infrastructure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its strategic location and excellent infrastructure", "Its strategic location and excellent infrastructure with the largest airport in Scandinavia[6] located 14 minutes by train from the city centre"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Copenhagen is a regional hub because it is a major", "extracted_entity": "Copenhagen"}
{"question": "what happened to Copenhagen between 1251 and 1255?", "answer": "a bunch of things", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a bunch of things"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "it was burned down", "extracted_entity": "Copenhagen"}
{"question": "is Copenhagen completely surrounded with water? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Copenhagen"}
{"question": "copenhagen is ranked number one worldwide for which things?", "answer": "Most Livable City in the World, ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most Livable City in the World, ", "Most Livable City, Location Ranking Survey"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "copenhagen is ranked number one worldwide for \u201cgreen", "extracted_entity": null}
{"question": "why did German troops occupy Copenhagen?", "answer": "Because it was WW2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it was WW2"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "To protect the German troops from the British navy", "extracted_entity": "Germany;Copenhagen"}
{"question": "have cymbals been used historically to suggest bacchanal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes, they have been used to suggest bacchan", "extracted_entity": null}
{"question": "can the origins of cymbals be traced to prehistoric times?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, the first cymbals are believed to", "extracted_entity": null}
{"question": "are cymbals used in moden orchestras?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes, they are used in modern orchestras.", "extracted_entity": null}
{"question": "what are the most common Cymbals?", "answer": " The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "Hi-Hats, Crash, Splash, Ride, and China"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Hi-Hat, Crash, Ride, Splash,", "extracted_entity": null}
{"question": "what is the second main orchestral use of cymbals?", "answer": "The suspended cymbal is the second main orchestral use of symbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The suspended cymbal is the second main orchestral use of symbals.", "The suspended cymbal", "the suspended cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "as a percussion instrument", "extracted_entity": null}
{"question": "what have clash cymbals traditionally been accompanied by?", "answer": "Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "The bass drum playing an identical part"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "crash cymbals.", "extracted_entity": null}
{"question": "what happens when the center of a Cymbal is hit?", "answer": "It causes a sound which is in a higher register than the rest of the symbal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It causes a sound which is in a higher register than the rest of the symbal.", "It causes a sound which is in a higher register than the rest of the cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "it vibrates", "extracted_entity": "Cymbal"}
{"question": "what is an effective way to accentuate a note?", "answer": "Playing a bass drum playing an identical part is an effective way to accentuate a note.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Playing a bass drum playing an identical part is an effective way to accentuate a note.", "The combination of a clash cymbol being accompanied by a bass drum playing an identical part, played loudly"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "use a strong accent mark above it", "extracted_entity": null}
{"question": "from what type of Cymbals can a expert player obtain an enormous dynamic range?", "answer": "An expert player can obtain an enormous dynamic range from crash cymbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An expert player can obtain an enormous dynamic range from crash cymbals.", "Crash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "From a pair of Paiste Formula 602s", "extracted_entity": null}
{"question": "can the origins of cymbals be traced back to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes. cymbals were probably first made from", "extracted_entity": null}
{"question": "did the modern hi-hat evolve from clash cymbals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hi-hat"}
{"question": "is the word cymbal derived from a Germanic word?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, the word cymbal is not derived", "extracted_entity": "Cymbal;Germanic_languages"}
{"question": "what is the center of a cymbal called?", "answer": "Bell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bell", "bell"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The bell", "extracted_entity": null}
{"question": "what part of the cymbal gets the best \"crash\"?", "answer": "Edge or rim", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Edge or rim", "edge or rim"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the edge", "extracted_entity": null}
{"question": "what two instruments form the modern drum kit?", "answer": "the cymbals and the drum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cymbals and the drum"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the snare drum and bass drum", "extracted_entity": null}
{"question": "what Greek mythical creatures were depicted holding cymbals?", "answer": "fauns and satyrs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["fauns and satyrs"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Satyrs.", "extracted_entity": null}
{"question": "what are orchestral crash cymbals technically called?", "answer": "a pair of clash cymbals", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pair of clash cymbals", "clash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "orchestral crash cymbals.", "extracted_entity": null}
{"question": "is Dhaka the capital of Bangladesh?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Dhaka;Bangladesh"}
{"question": "is Dhaka located on the eastern banks of the Buriganga River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Buriganga_River"}
{"question": "is University of Dhaka the largest public university in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "No, it is not. The largest public university", "extracted_entity": "Dhaka"}
{"question": "how much area does Dhaka cover?", "answer": "1462.60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1462.60 square kilometers"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "306.38 km\u00b2", "extracted_entity": "Dhaka"}
{"question": "how many people did the 1970 Bhola cyclone kill?", "answer": "500,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500,000"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "300,000.", "extracted_entity": null}
{"question": "when is the end of the independence war of Bangladesh?", "answer": "December 16, 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 16, 1971"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "6th December, 1971", "extracted_entity": "Bangladesh"}
{"question": "did the Mughals arrive in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mughal_Empire;Dhaka"}
{"question": "is University of Dhaka older than the Dhaka College?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes, University of Dhaka is older than the", "extracted_entity": "Dhaka_College"}
{"question": "what is given for the number of native speakers?", "answer": "No figure is given for the number of native speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No figure is given for the number of native speakers."], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "there are 1.5 billion native speakers of", "extracted_entity": null}
{"question": "is english an intonation  language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is english a de  jure  official  language  of Israel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "English_language;Israel"}
{"question": "is the stressed  syllable called the nuclear  syllable?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, the stressed syllable is called the nuclear", "extracted_entity": null}
{"question": "does english grammar have minimal inflection?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "no, because it has a large number of inf", "extracted_entity": null}
{"question": "did old English develop into Middle English?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Middle_English"}
{"question": "are books, magazines, and newspapers written in English available?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes, they are. English is the most common", "extracted_entity": "English_language"}
{"question": "is the syntax of German different with different rules?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, the syntax of German is different with different", "extracted_entity": "German_language"}
{"question": "is it true that semantics causes a number of false friends?", "answer": "true", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["true"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "here , are all syllables unstressed , except the syllables/words best and done , which are stressed ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "e is a stressed syllable and are is an", "extracted_entity": null}
{"question": "are many words describing the navy , types of ships , and other objects or activities on the water of dutch origin ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "s. see for example: \"galleon", "extracted_entity": null}
{"question": "are local variations in the formal written version of the language quite limited , being restricted largely to the spelling differences between British and American English ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "British_English;American_English"}
{"question": "is the flute a musical instrument?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "No, it is a wind instrument.", "extracted_entity": null}
{"question": "is it possible to open flutes at one or both ends?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, but it's not very common.", "extracted_entity": null}
{"question": "are Indian concert flutes available in standard pitches?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, they are available in standard pitches.", "extracted_entity": "India"}
{"question": "what do we refer musicians who play flute?", "answer": "A flute player, a flautist or a flutist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute player, a flautist or a flutist. ", "A flute player, a flautist, or a flutist"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "flute player or flautist", "extracted_entity": null}
{"question": "when was a three-holed flute made from a mammoth tusk discovered?", "answer": "2004.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2004.", "In 2004"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1986.", "extracted_entity": null}
{"question": "when did the tin whistle first appear?", "answer": "12th century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["12th century.", "In the 12th century"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1830s", "extracted_entity": null}
{"question": "what are the oldest known musical instruments?", "answer": "A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "A three-holed flute made from a mammoth tusk and two flutes made from swans' bones are among the oldest known musical instruments"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The earliest known musical instruments were probably the human voice", "extracted_entity": null}
{"question": "when was the pan flute spread to other parts of Europe?", "answer": "7th century BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7th century BC.", "After the 7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "in the 13th century", "extracted_entity": "Europe"}
{"question": "what does the air stream across this hole create?", "answer": "A Bernoulli, or siphon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A Bernoulli, or siphon."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "a vacuum (the air is moving away from the", "extracted_entity": null}
{"question": "are foxes wary of humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "are fennec foxes endangered?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "no, they are not endangered.", "extracted_entity": null}
{"question": "does the diet of foxes include reptiles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes, foxes are known to eat reptiles", "extracted_entity": null}
{"question": "how long do most foxes live?", "answer": "Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "2 to 3 years"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "most foxes live around 10 years, but", "extracted_entity": null}
{"question": "in what country did fox hunting originate?", "answer": "the United Kingdom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United Kingdom", "in the United Kingdom"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "England", "extracted_entity": "Fox_hunting"}
{"question": "what is the smallest species of fox?", "answer": "The Fennec Fox is the smallest species of fox. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Fennec Fox is the smallest species of fox. ", "the Fennec Fox"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "The smallest species of fox is the fennec", "extracted_entity": null}
{"question": "what are female foxes called?", "answer": "vixens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["vixens"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "vixens", "extracted_entity": null}
{"question": "what is caching food?", "answer": "Caching is burying it for later consumption, usually under leaves, snow, or soil.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Caching is burying it for later consumption, usually under leaves, snow, or soil.", "burying it for later consumption"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "the process of storing food for future use.", "extracted_entity": null}
{"question": "do foxes damage fruit on farms?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes, they do. Foxes are known to", "extracted_entity": null}
{"question": "how many years ago did African people spread across 31 francophone African countries can speak French either as a first or second language?", "answer": "2 years ago", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2 years ago"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "7", "extracted_entity": "France"}
{"question": "in Old French, was the plural  for animal animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Old_French"}
{"question": "is catalan the only  official  language  of Andorra?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, Catalan is the only official language of And", "extracted_entity": "Andorra"}
{"question": "in Belgium, however, is quatre-vingts universally used?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "No, it is not. The term quatre-", "extracted_entity": "Belgium"}
{"question": "is french taught in many schools as a primary language along with Arabic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is taught in many schools as a", "extracted_entity": "Arabic_language"}
{"question": "what is widely used by the Lebanese, especially for administrative purposes?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Arabic", "extracted_entity": "Lebanon"}
{"question": "is french descendant of the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, the French language is a descendant of the", "extracted_entity": "France;Latin"}
{"question": "are there also a variety of regional languages In addition?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are also a variety of regional languages", "extracted_entity": "Regional_language"}
{"question": "is it true that france signed the European Charter for Regional Languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes. The French language is not a regional language", "extracted_entity": "France;European_Charter_for_Regional_or_Minority_Languages"}
{"question": "are the prefixes en- and em- always nasalized ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes, but only when they are attached to a", "extracted_entity": null}
{"question": "how many years ago was the Luther Bible by Martin Luther printed?", "answer": "475", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["475"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "2 years ago", "extracted_entity": "Luther_Bible;Martin_Luther"}
{"question": "is german an inflected  language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes. German is an inflected language.", "extracted_entity": "German_language"}
{"question": "in Switzerland, is \u00df used at all?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no, not at all", "extracted_entity": "Switzerland"}
{"question": "are there two common  word  orders?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes, the  SVO  order is the", "extracted_entity": null}
{"question": "are feminines declined in the singular?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "is 9 %  of the Internet  population German?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Germany"}
{"question": "is german written using the Latin  alphabet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "German_language;Latin"}
{"question": "was the Luther Bible by Martin Luther printed?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Luther_Bible;Martin_Luther"}
{"question": "does a  tz indicate that the preceding vowel is short?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes, when the  tz is in the middle", "extracted_entity": null}
{"question": "are there 622,650 speakers of German In Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no, there are 622,650 speakers of", "extracted_entity": "German_language;Canada"}
{"question": "are there also large populations of German ancestry In Mexico?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are large populations of German ancestry in", "extracted_entity": "Germany;Mexico"}
{"question": "would Bundesl\u00e4nder not accept North Rhine Westphalia and Bavaria ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "s, they would.", "extracted_entity": "States_of_Germany;North_Rhine-Westphalia;Bavaria"}
{"question": "have adverbs of time to appear in the third place in the sentence , just after the predicate ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "es, adverbs of time may appear in the", "extracted_entity": null}
{"question": "do both sexes of giraffe have horns?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes, both sexes of giraffe have horns.", "extracted_entity": "Giraffe"}
{"question": "do giraffes give birth standing up?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, they give birth standing up", "extracted_entity": null}
{"question": "are giraffes hunted for their hides?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, although they are now protected.", "extracted_entity": null}
{"question": "what areas can giraffes inhabit?", "answer": "savannas, grasslands, or open woodlands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["savannas, grasslands, or open woodlands", "inhabit savannas, grasslands, or open woodlands"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "they can inhabit almost anywhere, but they prefer the", "extracted_entity": null}
{"question": "what will a giraffe use to clean off any bugs that appear on its face?", "answer": "extremely long tongue ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["extremely long tongue ", "It's extremely long tongue"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "a tongue!", "extracted_entity": null}
{"question": "how do giraffes defend themselves?", "answer": "with a powerful kick", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["with a powerful kick", "With powerful kicks"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "they have long necks and horns", "extracted_entity": "Giraffe"}
{"question": "do male giraffes weigh more than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, male giraffes weigh more than female", "extracted_entity": null}
{"question": "do male giraffes have larger horns than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes, male giraffes have longer horns than", "extracted_entity": null}
{"question": "are male females generally taller than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "was Henri Becquerel one of the discoverers of radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was one of the discoverers of", "extracted_entity": "Henri_Becquerel"}
{"question": "is the SI unit for radioactivity named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, becquerel is named after Henri Bec", "extracted_entity": "International_System_of_Units"}
{"question": "was Henri Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Henri_Becquerel;France"}
{"question": "in what year did Henri Becquerel die?", "answer": "1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1908", "August 24, 1908"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": "Henri_Becquerel"}
{"question": "where was Henri Becquerel born?", "answer": " Paris ", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Paris ", "Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "in Paris, France.", "extracted_entity": "Henri_Becquerel"}
{"question": "in what year did Henri Becquerel win the Nobel Prize in Physics?", "answer": "1903", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1903"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": "Henri_Becquerel;Nobel_Prize_in_Physics"}
{"question": "was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, his father and grandfather had previously occupied the", "extracted_entity": "Henri_Becquerel;National_Museum_of_Natural_History_(France)"}
{"question": "was Henri Becquerel the sole winner of the 1903 Nobel Prize in Physics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, Henri Becquerel was one of the", "extracted_entity": "Henri_Becquerel;Nobel_Prize_in_Physics"}
{"question": "did Henri Becquerel intentionally discover radioactivity?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "He discovered it accidentally"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Henri_Becquerel"}
{"question": "if Henri Becquerel was alive today, how old would he have been?", "answer": "157", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["157"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "167 years old", "extracted_entity": "Henri_Becquerel"}
{"question": "for how many years did Henri Becquerel live?", "answer": "56", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["56"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "55 years", "extracted_entity": "Henri_Becquerel"}
{"question": "did Henri Becquerel live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, Henri Becquerel died at the age", "extracted_entity": "Henri_Becquerel"}
{"question": "what was Henri Becquerel`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was a French engineer, physicist", "extracted_entity": "Henri_Becquerel"}
{"question": "how old was Henri Becquerel when he died?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel died at the age of", "extracted_entity": "Henri_Becquerel"}
{"question": "how many years ago was it when he became the third in his family to occupy the physics chair at the Musum National d`Histoire Naturelle?", "answer": "117", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["117"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1974.", "extracted_entity": null}
{"question": "in 1908, the year  of his death, was Becquerel elected Permanent Secretary  of the Acad\u00e9mie  des  Sciences?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": "Becquerel"}
{"question": "when was Henri Becquerel born?", "answer": "December 15, 1852", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 15, 1852"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was born on 15 December", "extracted_entity": "Henri_Becquerel"}
{"question": "did he become chief engineer in the Department of Bridges and Highways?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he became chief engineer in the Department of", "extracted_entity": null}
{"question": "did he share the Nobel Prize in Physics?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He did not share the Nobel Prize in Physics.", "extracted_entity": "Nobel_Prize_in_Physics"}
{"question": "is it true that Becquerel wrapped a fluorescent substance in photographic plates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, it is not true that Becquerel", "extracted_entity": "Henri_Becquerel"}
{"question": "becquerel wrapped a fluorescent substance in what?", "answer": "photographic plates and black material", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["photographic plates and black material"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "paper", "extracted_entity": null}
{"question": "is there a Becquerel crater on the Moon for radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no, there is no Becquerel crater on", "extracted_entity": "Moon"}
{"question": "must \u2026 One conclude from these experiments that the phosphorescent substance in question emits rays which pass through the opaque paper and reduces silver salts ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "must \u2026 One conclude from these experiments that the phosph", "extracted_entity": null}
{"question": "what is Henri Becquerel?", "answer": "Henri Becquerel was a famous physicist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel was a famous physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was a French engineer, physicist", "extracted_entity": "Henri_Becquerel"}
{"question": "where is Henri Becquerel from?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel is from France.", "extracted_entity": "Henri_Becquerel"}
{"question": "what does Henri Becquerel do?", "answer": "was a physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["was a physisist", "Henri Becquerel was a physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he was a French engineer, physicist, Nobel laure", "extracted_entity": "Henri_Becquerel"}
{"question": "was Newton a English physicist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "No, Newton was an English mathematician.", "extracted_entity": "England"}
{"question": "did Newton reject the church's doctrine of the Trinity?", "answer": "Newton may have rejected the church's doctrine of the Trinity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newton may have rejected the church's doctrine of the Trinity.", "Maybe"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes, he did. (see Isaac Newton's", "extracted_entity": "Trinity"}
{"question": "was Newton admitted into Havard College?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "Maybe, but it doesn't say so in the article"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, Newton was admitted into Havard College.", "extracted_entity": null}
{"question": "when was Newton's date of birth recorded?", "answer": "Christmas Day, Decembeer 25, 1642.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas Day, Decembeer 25, 1642.", "his date of birth was recorded as Christmas Day, December 25, 1642"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "25 December 1642", "extracted_entity": null}
{"question": "how many laws of motion did Netwon have?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "3 (first law, second law, third", "extracted_entity": null}
{"question": "who did Newton see as the master creator?", "answer": "God", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["God", "Newton saw God as the master creator whose existence could not be denied in the face of the grandeur of all creation"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "God", "extracted_entity": null}
{"question": "when did Netwon investigate the refraction of light?", "answer": "From 1670 to 1672", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1670 to 1672", "1670-1672"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1665", "extracted_entity": null}
{"question": "what principles did Newton explain for mechanics?", "answer": "The principles of conservation of momentum and angular momentum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The principles of conservation of momentum and angular momentum", "In mechanics, Newton enunciated the principles of conservation of momentum and angular momentum"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "He explained that for every action there is an equal", "extracted_entity": "Isaac_Newton;Mechanics"}
{"question": "interaction with what man stirred up Newton's interest in alchemy?", "answer": "Henry More", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henry More", "The contact with the theosophist Henry More, revived his interest in alchemy"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "He was a key figure in the Scientific Revolution and", "extracted_entity": "Isaac_Newton"}
{"question": "james Watt's improvements of what were fundamental to the changes wrought by the Industrial Revolution?", "answer": "The steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The steam engine.", "steam engine"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "the steam engine", "extracted_entity": "Industrial_Revolution"}
{"question": "james Watt was born where?", "answer": "Greenock, a seaport on the Firth of Clyde", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greenock, a seaport on the Firth of Clyde"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Scotland", "extracted_entity": null}
{"question": "what was the name of James Watt's mother?", "answer": "Agnus Muirhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Agnus Muirhead", "Agnes Muirhead"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Agnes Watt", "extracted_entity": "James_Watt"}
{"question": "how many of James Watt's children did not live to adulthood?", "answer": "3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "three of his six children", "extracted_entity": "James_Watt"}
{"question": "james Watt ranked first among how many people in Charles Murray's survey of historiometry?", "answer": "229", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["229"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "Charles_Murray_(political_scientist);Historiography"}
{"question": "in what year did James travel to Lodon to study instrument-making?", "answer": "1753", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1753"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1779", "extracted_entity": "Loddon,_Norfolk"}
{"question": "for how many years of James Watt's life was his mother alive?", "answer": "17 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["17 years"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "16 years", "extracted_entity": "James_Watt"}
{"question": "what is James Watt most famous for?", "answer": "Steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt is most famous for his work on the", "extracted_entity": "James_Watt"}
{"question": "do most Japanese people employ politeness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "no, they are not polite people", "extracted_entity": "Japanese_people"}
{"question": "is it true that Japanese borrowed a considerable number of words?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is true. The Japanese borrowed a", "extracted_entity": "Japanese_language"}
{"question": "are the several dialects of Kagoshima famous to speakers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "es, Kagoshima dialect is famous to speakers", "extracted_entity": "Kagoshima"}
{"question": "is Lima the capital of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, Lima is the capital of Peru.", "extracted_entity": "Lima;Peru"}
{"question": "is the Museum of Gold in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, it is in Lima.", "extracted_entity": "Lima"}
{"question": "does Lima have volleyball venues?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No. Lima does not have volleyball venues.", "extracted_entity": "Lima"}
{"question": "when was Lima founded?", "answer": "January 18,1535.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["January 18,1535."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima was founded in 1535.", "extracted_entity": "Lima"}
{"question": "where is the Park of the Reserve located?", "answer": "Near the downtown area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Near the downtown area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Park of the Reserve is located in the municipality", "extracted_entity": null}
{"question": "what is a colectivo?", "answer": "Automobiles that renders express service on some major roads of the Lima Metropolitan Area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Automobiles that renders express service on some major roads of the Lima Metropolitan Area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "a small van or bus that serves as public transportation", "extracted_entity": null}
{"question": "is the National University of San Marcos in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The National University of San Marcos is located in Lima", "extracted_entity": "National_University_of_San_Marcos;Lima"}
{"question": "has the San Lorenzo Megaport Project been completed?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No. The project was started in 2008", "extracted_entity": null}
{"question": "how many years ago was the Lima Stock Exchange the most profitable in the world?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "1997", "extracted_entity": "Lima"}
{"question": "is rainfall very low?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "no, it is not very low", "extracted_entity": null}
{"question": "is lima the capital  and  largest  city  of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Peru"}
{"question": "is The GDP  per capita in Lima $ 7,600  /  ref )?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "$7,600 Q: is The GDP", "extracted_entity": "Lima"}
{"question": "the Spanish Crown named Pizarro governor of what?", "answer": "Hemming.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hemming."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Spanish Crown named Pizarro governor of Peru", "extracted_entity": "Monarchy_of_Spain"}
{"question": "is it true that the ideas of the Enlightenment shaped the development of the city?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "True. The ideas of the Enlightenment were based on", "extracted_entity": "Age_of_Enlightenment"}
{"question": "the ideas of the Enlightenment shaped the development of what?", "answer": "the city, Lima", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the city, Lima"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "the modern world", "extracted_entity": "Age_of_Enlightenment"}
{"question": "is London the capital of the United Kingdom?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "London;United_Kingdom"}
{"question": "does London's population draw from a wide range of religions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes, London's population draws from a wide range", "extracted_entity": "London"}
{"question": "did Caunte take control of the English throne in 101?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "England"}
{"question": "over how many languages are spoken in London?", "answer": "Over 300 languages are spoken in london", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over 300 languages are spoken in london", "Over 300"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "300", "extracted_entity": "London"}
{"question": "what had the Anglo-Saxons created by the 600s?", "answer": "By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "a new settlement called Lundenwic"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "the Anglo-Saxon Chronicle, which was a history", "extracted_entity": "Anglo-Saxons"}
{"question": "what city in the UK has been subjected to bouts of terrorism?", "answer": "London has been subjected to bouts of terrorism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London has been subjected to bouts of terrorism.", "London"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": "United_Kingdom"}
{"question": "what countries did James VI of Scotland unite?", "answer": "James VI of Scotland united Scotland and England.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James VI of Scotland united Scotland and England.", "England and Scotland"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "England and Ireland", "extracted_entity": "James_VI_and_I"}
{"question": "how did civil wars affect England during the Middle Ages?", "answer": "London remained relatively untouched", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London remained relatively untouched"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Civil wars were a common occurrence in England during the", "extracted_entity": "England_in_the_Middle_Ages"}
{"question": "are the strings of a classical lyre made of gut?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes, they are made of gut", "extracted_entity": null}
{"question": "does a classical lyre have a sound-chest?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No, a lyre is a stringed instrument", "extracted_entity": null}
{"question": "is lyre played in mordern Greece?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": "Greece"}
{"question": "which part of the strings does the left hand touch?", "answer": "The lower strings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lower strings"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the neck of the instrument", "extracted_entity": null}
{"question": "in which place is lyre still played?", "answer": "North-eastern parts of Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North-eastern parts of Africa"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "in the Greek island of Rhodes", "extracted_entity": null}
{"question": "does a classical lyre have a fingerboard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "no, it doesn't. It has a bridge", "extracted_entity": null}
{"question": "is a guitar played with a plectrum like a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No, a guitar is not played with a p", "extracted_entity": "Guitar;Plectrum;Lyre"}
{"question": "was Michael Faraday an English chemist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he was a scientist.", "extracted_entity": "Michael_Faraday;England"}
{"question": "was Faraday a devout Christian?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was a devout Christian.", "extracted_entity": "Christianity"}
{"question": "was Faraday considered a gentleman?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he was not considered a gentleman. He", "extracted_entity": "Michael_Faraday"}
{"question": "who was a member of the Sandemanian sect of Christianity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "John Locke", "extracted_entity": "Christianity"}
{"question": "what did Joseph Henry likely discover?", "answer": "self-induction", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["self-induction"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the electromagnetic phenomenon of self-inductance", "extracted_entity": "Joseph_Henry"}
{"question": "what did the University of Oxford grant Faraday?", "answer": "a Doctor of Civil Law degree (honorary)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Doctor of Civil Law degree (honorary)", "Doctor of Civil Law degree (honorary)"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "a B.S. in physics", "extracted_entity": "University_of_Oxford;Michael_Faraday"}
{"question": "who was made to travel outside the coach?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the coachman", "extracted_entity": null}
{"question": "did Faraday construct the ancestor of modern power generators?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, but he did contribute to the understanding of", "extracted_entity": null}
{"question": "did Faraday lecture on education in 1854?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday receive an extensive formal education?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday discover the chemical substance benzene?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he did not. He discovered the properties", "extracted_entity": "Michael_Faraday;Benzene"}
{"question": "was Faraday born in England?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, Faraday was born in Newington,", "extracted_entity": "England"}
{"question": "what do some historians of science refer to Faraday as?", "answer": "the best experimentalist in the history of science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the best experimentalist in the history of science", "best experimentalist in the history of science"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the father of the electromagnetic field", "extracted_entity": null}
{"question": "who was the sponsor and mentor of Faraday?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Davy", "extracted_entity": null}
{"question": "what did Faraday invent that is used almost universally in science laboratories?", "answer": "the Bunsen burner", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Bunsen burner", "Bunsen burner"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the Faraday cage.", "extracted_entity": null}
{"question": "does the production of chemical weapons raise an ethical issue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, it does not. it is the production", "extracted_entity": null}
{"question": "what was the Faraday effect first called?", "answer": "diamagnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["diamagnetism"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "\"the rotation of the plane of polarization of light", "extracted_entity": null}
{"question": "can you meet someone through attending a church?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, i have met people through attending church.", "extracted_entity": null}
{"question": "what was Michael Faraday`s birthplace?", "answer": "Newington Butts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday`s birthplace is London, England", "extracted_entity": "Michael_Faraday"}
{"question": "what was Michael Faraday`s profession?", "answer": "chemist and physicist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["chemist and physicist"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "He was an English scientist who contributed to the study", "extracted_entity": "Michael_Faraday"}
{"question": "how many years ago did Faraday report on the first synthesis of compound make from carbon and chlorine , c 2 h 6 and c 2 h 4 ?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "he answer is given in the attached image. Hope", "extracted_entity": "Michael_Faraday;Carbon;Chlorine"}
{"question": "how many years ago was it when he discovered the phenomenon that he named diamagnetism , and what is now called the Faraday effect?", "answer": "164", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["164"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "He discovered it in 1845.", "extracted_entity": "Diamagnetism;Faraday_effect"}
{"question": "how many years ago was it when Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames , which resulted in an oft-reprinted cartoon in Punch?", "answer": "154", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["154"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "842", "extracted_entity": "The_Times;River_Thames;Punch_(magazine)"}
{"question": "how many years ago was it when he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal?", "answer": "162", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["162"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "84 years ago (1911).", "extracted_entity": null}
{"question": "was education another area  of service  for Faraday?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, Faraday was a teacher at the Royal", "extracted_entity": null}
{"question": "was faraday `s  earliest  chemical  work as an assistant  to Davy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Davy was working on the composition of water and", "extracted_entity": null}
{"question": "who died at his house at Hampton Court on August 25, 1867?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "William Henry Harrison", "extracted_entity": "Hampton_Court_Palace"}
{"question": "when was Michael Faraday born?", "answer": "September 22, 1791", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 22, 1791"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "22 September 1791", "extracted_entity": "Michael_Faraday"}
{"question": "where did he serve two terms?", "answer": "the Church of Scotland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Church of Scotland"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "in the senate.", "extracted_entity": null}
{"question": "who was born in Newington Butts, near present-day South London, England?", "answer": "Michael Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "William Shakespeare", "extracted_entity": "Newington_Butts;South_London;England"}
{"question": "is it true that institution formed the foundation of electric motor technology?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, the electric motor is the first major invention", "extracted_entity": null}
{"question": "institution formed the foundation of what?", "answer": "modern electromagnetic technology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern electromagnetic technology"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The American Colonization Society", "extracted_entity": null}
{"question": "was his father member of the Sandemanian sect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, he was a member of the Presbyterian Church", "extracted_entity": null}
{"question": "was faraday Christian of the small Sandemanian denomination?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he was, and his father was a", "extracted_entity": null}
{"question": "is Nassau Sound a body of water in Maine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Maine"}
{"question": "was Nassau placed in operation by Robert Fulton?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The first steamboat to be placed in operation by", "extracted_entity": "Robert_Fulton"}
{"question": "who was Fort Nassau built by?", "answer": "the Dutch", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Dutch"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The Dutch West India Company", "extracted_entity": null}
{"question": "what is Nassau Coliseum?", "answer": "an arena in Uniondale, New York, USA", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["an arena in Uniondale, New York, USA"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Veterans Memorial Coliseum", "extracted_entity": "Nassau_Veterans_Memorial_Coliseum"}
{"question": "what was the Dead or Alive 4 fighting arena modeled after?", "answer": "a Magnetic Accelerator Cannon station from Halo 2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Magnetic Accelerator Cannon station from Halo 2"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "the Dead or Alive 4 fighting arena was modeled", "extracted_entity": "Dead_or_Alive_4"}
{"question": "is Nassau Range the highest mountain range in the world?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No, it is the 9th highest mountain", "extracted_entity": null}
{"question": "is Nassau County named after a German town?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Yes, Nassau County, New York is named", "extracted_entity": "Nassau_County,_New_York;Germany"}
{"question": "does the United States have a base near Glasgow?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States;Glasgow"}
{"question": "did Tesla study electrical engineering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, Tesla studied electrical engineering.", "extracted_entity": "Tesla_Motors;Electrical_engineering"}
{"question": "was Tesla born in the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, Tesla was born in Croatia.", "extracted_entity": "United_States"}
{"question": "was Tesla hired by Edison?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, Tesla was hired by Westinghouse.", "extracted_entity": null}
{"question": "when did Tesla demonstrate wireless communication (radio)?", "answer": "1893", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1893"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1893", "extracted_entity": "Nikola_Tesla"}
{"question": "what is the SI unit measuring magnetic flux density or magnetic induction?", "answer": "the tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": "International_System_of_Units"}
{"question": "was Tesla regarded as a mad scientist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Nikola_Tesla"}
{"question": "what type of current did Tesla invent?", "answer": "AC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["AC"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "alternating current.", "extracted_entity": null}
{"question": "who was the victor of the \"War of Currents\"?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla", "Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison", "extracted_entity": null}
{"question": "where did Tesla study electrical engineering?", "answer": "the Austrian Polytechnic in Graz ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Austrian Polytechnic in Graz ", "Austrian Polytechnic in Graz", "The Austrian Polytechnic in Graz"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla studied electrical engineering in Austria.", "extracted_entity": "Nikola_Tesla"}
{"question": "is Ottawa the capital of Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, Ottawa is the capital of Canada.", "extracted_entity": "Ottawa;Canada"}
{"question": "does Ottawa sit on the Ottowa River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, the city of Ottawa is located on the", "extracted_entity": "Ottawa;Ottawa_River"}
{"question": "do small tremors occur in Ottawa?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "No, small tremors do not occur in Ottawa", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's population?", "answer": "1,190,982", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,190,982"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "1,236,324", "extracted_entity": "Ottawa"}
{"question": "what are Ottawa's primary industries?", "answer": "Logging and lumber", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Logging and lumber"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa's primary industries include technology, telecommunications, government", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's junior ice hockey team?", "answer": "the Ottawa 67's", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa 67's"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "the Ottawa Senators", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's major league hockey team?", "answer": "the Ottawa Senators", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa Senators"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "the Ottawa Senators", "extracted_entity": "Ottawa"}
{"question": "what are Carleton University's athletic teams called?", "answer": "Carleton Ravens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carleton Ravens"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Carleton Ravens", "extracted_entity": "Carleton_University"}
{"question": "is Ottawa colder than Moscow in January?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes. In January, Ottawa is colder than Moscow", "extracted_entity": "Ottawa;Moscow"}
{"question": "is Ottawa on a river?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes. The Ottawa River.", "extracted_entity": null}
{"question": "does it snow in Ottawa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "no, it does not snow in Ottawa.", "extracted_entity": "Ottawa"}
{"question": "was there a cholera outbreak in 1832?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes, it was a big deal, the government", "extracted_entity": null}
{"question": "who is the mayor of Ottawa?", "answer": "Larry O'Brien", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Larry O'Brien"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Jim Watson", "extracted_entity": "Ottawa"}
{"question": "how many townships are in Ottawa?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "There are 10 townships in Ottawa.", "extracted_entity": "Ottawa"}
{"question": "what's the main highway in Ottawa?", "answer": "Highway 417, The Queensway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Highway 417, The Queensway", "provinical Highway 417"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Highway 417", "extracted_entity": "Ottawa"}
{"question": "what was Ottawa's name in 1850?", "answer": "Bytown", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bytown"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Bytown", "extracted_entity": "Ottawa"}
{"question": "based on the average, in what year will the next small tremor occur in Ottawa?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The average of the numbers is 3.5", "extracted_entity": "Ottawa"}
{"question": "where were immigrants from in the 1800s?", "answer": "Irish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish", "Ireland"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "immigrants were from Europe, Asia, and Latin America", "extracted_entity": null}
{"question": "are there also discussions of having OC Transpo Light Rail and STO Transitway networks linking on the Gatineau side of the Ottawa River ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "ot that I am aware of. The STO", "extracted_entity": "OC_Transpo;Gatineau;Ottawa_River"}
{"question": "does the giant otter live in South America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "South_America"}
{"question": "do otters have claws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes, they have claws.", "extracted_entity": "Otter"}
{"question": "do otters eat fish?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Otter"}
{"question": "why are otters vulnerable to prey depletion?", "answer": "Prey-dependence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Prey-dependence", "prey-dependency"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "because they are apex predators, they are at the", "extracted_entity": null}
{"question": "do otters enjoy playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes, they do!", "extracted_entity": null}
{"question": "do sea otters have a layer of fat like whales?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "no, they have a layer of fur", "extracted_entity": "Sea_otter"}
{"question": "is the otter in the same family as the badger?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes, they are both in the same family.", "extracted_entity": "Otter;Badger"}
{"question": "are there any cities named Santiago in the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, there are 8 cities named Santiago in", "extracted_entity": "Santiago,_Chile;United_States"}
{"question": "was Santiago the name of an indie/punk band?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "No, it was a Chilean capital city.", "extracted_entity": "Santiago,_Chile"}
{"question": "is Santiago a name in Spanish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, Santiago is a name in Spanish.", "extracted_entity": "Santiago,_Chile;Spanish_language"}
{"question": "what is the origin of the name Santiago?", "answer": "Spanish for St. James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spanish for St. James", "sanit + iago, James"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the capital and largest city of Chile and", "extracted_entity": "Santiago,_Chile"}
{"question": "who has released an album or song named Santiago?", "answer": "The Chieftains; Loreena McKennitt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Chieftains; Loreena McKennitt", "The Chieftains and Loreena McKennitt"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The rapper J. Cole", "extracted_entity": null}
{"question": "what is the Order of Santiago?", "answer": "a Spanish knightly order", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Spanish knightly order", "a Spanish Knightly order"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "a religious and military order", "extracted_entity": "Order_of_Santiago"}
{"question": "what notable people have been named Santiago?", "answer": "Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "Benito, Hugo, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The first one that comes to mind is the Chile", "extracted_entity": null}
{"question": "what fictional stories include a main character named Santiago?", "answer": "Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "Interview with the Vampire, The Alchemist, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Alchemist", "extracted_entity": null}
{"question": "is Santiago the national capital of a country?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, Santiago is the capital of Chile.", "extracted_entity": "Santiago,_Chile"}
{"question": "is santiago Spanish for St. James  ( from santo ,  saint  +  iago ,  James  )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "es.", "extracted_entity": "Spanish_language"}
{"question": "is santiago Spanish for St. James?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "No, it is the name of a city in", "extracted_entity": null}
{"question": "what is Santiago?", "answer": "Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry."], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the capital and largest city of Chile and", "extracted_entity": "Santiago,_Chile"}
{"question": "has swahili no diphthongs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes. the following are diphthongs in", "extracted_entity": null}
{"question": "is a third  prefix the object  prefix?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is      / ref Swahili unusual among sub-Saharan languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "No. Swahili is unusual among sub-Saharan", "extracted_entity": "Swahili_language"}
{"question": "are vowels never reduced , regardless of stress ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "o. the vowel /e/ is reduced to", "extracted_entity": null}
{"question": "does the Australian Black Swan have white feathers on its wings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "no, it has black feathers on its wings.", "extracted_entity": null}
{"question": "do swans belong to the family Anatidae?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes, swans belong to the family Anatidae", "extracted_entity": "Anatidae"}
{"question": "can black swans swim with only one leg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes. because they are black.", "extracted_entity": null}
{"question": "what are young swans known as?", "answer": "cygnets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cygnets", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "what is the Irish legend of the Children of Lir about?", "answer": "a stepmother transforming her children into swans for 900 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a stepmother transforming her children into swans for 900 years", "A stepmother transforming her children into swans for 900 years"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Children of Lir is a tale from the", "extracted_entity": "Republic_of_Ireland;Children_of_Lir"}
{"question": "what is the Sanskrit word for swan?", "answer": "hamsa or hansa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hamsa or hansa", "Hamsa or hansa"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "msa", "extracted_entity": "Sanskrit"}
{"question": "which album was the song \"The Bonny Swans\" from?", "answer": "The Mask and Mirror", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Mask and Mirror"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "\"The White Stripes\" (1999)", "extracted_entity": null}
{"question": "what is the Polish word for swan?", "answer": "\u0141ab\u0119d\u017a", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["\u0141ab\u0119d\u017a"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "\u0142ab\u0119d\u017a", "extracted_entity": "Polish_language"}
{"question": "which ballet by Pyotr Tchaikovsky is partially based on an Ancient German legend about a princess who was turned into a swan?", "answer": "Swan Lake", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swan Lake"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swan Lake", "extracted_entity": "Pyotr_Ilyich_Tchaikovsky;Germany"}
{"question": "are swan birds of the family Anatidae?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": "Anatidae"}
{"question": "do swans feature strongly in mythology?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "they do, and in a variety of ways", "extracted_entity": null}
{"question": "is the Trumpeter Swan the largest bird?", "answer": "It is the largest bird of North America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the largest bird of North America.", "The Trumpeter Swan is the largest bird of North America"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No, the Trumpeter Swan is the largest flying", "extracted_entity": null}
{"question": "what is the name of young swans?", "answer": "Cygnets.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cygnets.", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "where is the word \"swan\" derived from?", "answer": "Old English swan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old English swan.", "The Old English word \"swan,\" derived from the Indo-European root \"*swen\", meaning to sound or to sing"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "it comes from the Germanic root \"swe", "extracted_entity": null}
{"question": "what is the best known story about a swan?", "answer": "The Ugly Duckling fable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Ugly Duckling fable.", "The Ugly Duckling"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "the story of the ugly duckling", "extracted_entity": null}
{"question": "are swans and ducks birds of the same family?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "no, swans are of the family Anatidae", "extracted_entity": null}
{"question": "what is the name of an adult female?", "answer": "Pen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pen.", "An adult female swan is called a pen."], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "woman", "extracted_entity": null}
{"question": "what is the color of the Australian Black Swan?", "answer": "Completely black except for the white flight feathers on its wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Completely black except for the white flight feathers on its wings.", "Completely black except for the white feathers on its wings"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "white", "extracted_entity": null}
{"question": "is the Tiger a member of the Felidar family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "yes, but not a member of the Felidar", "extracted_entity": null}
{"question": "are tigers solitary animals?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "no, they are not. Tigers are social animals", "extracted_entity": null}
{"question": "are panthers a type of tiger?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "No, panthers are a type of leopard.", "extracted_entity": null}
{"question": "what do tigers' powerfully built legs and shoulders allow tigers to do?", "answer": "They have the ability to pull down prey substantially heavier than themselves.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They have the ability to pull down prey substantially heavier than themselves.", "have the ability to pull down prey substantially heavier than themselves"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "to run fast and jump high", "extracted_entity": null}
{"question": "what would a tiger do when seized by a crocodile?", "answer": "A tiger will strike at the reptile's eyes with its paws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A tiger will strike at the reptile's eyes with its paws.", "strike at the reptile's eyes with its paws"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "it would open its mouth wide and roar", "extracted_entity": null}
{"question": "what are the typical features of a tiger country?", "answer": "It will always have good cover, it will always be close to water and plenty of prey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It will always have good cover, it will always be close to water and plenty of prey.", "Good cover, close to water, and plenty of prey "], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "It is a country with a population of over", "extracted_entity": null}
{"question": "would it be common for tigers to mate in January?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "No, the mating season for tigers is between", "extracted_entity": null}
{"question": "about how many square kilometers would be needed to house 3 female tigers?", "answer": "60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 square kilometers", "60"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "3 square kilometers", "extracted_entity": null}
{"question": "would a tiger be likely to feedon smaller animals, such as mice?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes, because they are an apex predator, which", "extracted_entity": "Tiger"}
{"question": "did the predecessors to trumpets have valves?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, they had finger holes", "extracted_entity": null}
{"question": "are trumpets constructed of brass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, brass is a metal alloy used in the", "extracted_entity": "Brass"}
{"question": "is the bass trumpet the smallest trumpet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, the smallest trumpet is the piccolo trumpet", "extracted_entity": null}
{"question": "what musical styles is the trumpet used in?", "answer": "Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "classical and jazz ensembles", "extracted_entity": "Trumpet"}
{"question": "what is the most common type of trumpet?", "answer": "The B trumpet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The B trumpet."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The Bb Trumpet", "extracted_entity": null}
{"question": "how does a trumpet produce sound?", "answer": "By blowing air through closed lips.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By blowing air through closed lips."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the player's lips vibrate to produce sound", "extracted_entity": "Trumpet"}
{"question": "how do trumpets compensate for wolf tones?", "answer": "Somce trumpetes have a slide mechanism built in to compensate.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Somce trumpetes have a slide mechanism built in to compensate."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the player can use the valves to make the instrument", "extracted_entity": null}
{"question": "did Miles Davis play the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": "Miles_Davis"}
{"question": "does a trumpet have a mellower tone than a cornet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, a trumpet has a brighter tone than a", "extracted_entity": "Trumpet;Cornet"}
{"question": "is the name of the upper shell of a turtle called the plastron?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Yes, it is called the plastron.", "extracted_entity": "Turtle_shell"}
{"question": "do scutes fall away from the turtle's shell?", "answer": "Some of the scutes eventually fall away from the shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some of the scutes eventually fall away from the shell.", "yes, eventually"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, they do fall away from the shell.", "extracted_entity": null}
{"question": "do Terrestrial tortoises have short feet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, they have long feet.", "extracted_entity": null}
{"question": "how many groups are turtles broken down into?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two", "two"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "7 groups", "extracted_entity": null}
{"question": "where are the only surviving giant tortoises?", "answer": "They are on the Seychelles and Gal\u00e1pagos Islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are on the Seychelles and Gal\u00e1pagos Islands.", "Seychelles and Gal\u00e1pagos Islands"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "the Galapagos islands, off the coast of", "extracted_entity": null}
{"question": "how do turtles chew food?", "answer": "Turtles use their jaws to cut and chew food.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles use their jaws to cut and chew food.", "Turtles use their jaws to cut and chew food"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "they have no teeth, so they use their tongue", "extracted_entity": null}
{"question": "are tortoises land based?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, tortoises are land based, but", "extracted_entity": null}
{"question": "where are turtle eggs layed?", "answer": "Turtles lay eggs on land.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles lay eggs on land.", "on land"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "in the sand", "extracted_entity": null}
{"question": "is turtle soup considered a delicacy?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "are Testudines the crown group of the superorder Chelonia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, because the crown group of Chelonia", "extracted_entity": "Crown_group"}
{"question": "do turtles breathe air?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, turtles breathe air.", "extracted_entity": null}
{"question": "are harvesting wild turtles legal anywhere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, it is legal to harvest wild turtles in", "extracted_entity": null}
{"question": "approximately how many species of Testudines are alive today?", "answer": "300", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "approximately 300", "extracted_entity": null}
{"question": "where is harvesting wild turtles legal?", "answer": "Florida", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Florida"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "it is legal in Texas", "extracted_entity": null}
{"question": "what was the largest ever chelonian?", "answer": "Archelon ischyros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Archelon ischyros", "The great letherback sea tutrtle"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Archelon, a sea turtle that lived in the", "extracted_entity": null}
{"question": "is there a way to approximate the age of a turtle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Turtle"}
{"question": "can turtles spend all their time underwater?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "no, turtles need to come up to the surface", "extracted_entity": null}
{"question": "are violinists and fiddlers the same thing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Fiddle"}
{"question": "are violins a single size?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no. There are different sizes for different people.", "extracted_entity": null}
{"question": "are violas and cellos in the same family of instruments as violins?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "es, they are all in the violin family.", "extracted_entity": "Viola;Cello"}
{"question": "where does the word \"violin\" come from?", "answer": "the Middle Latin word vitula, meaning \"stringed instrument\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Middle Latin word vitula, meaning \"stringed instrument\"", "the Middle Latin word vitula"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "from the Latin word \"violae\", meaning \"", "extracted_entity": null}
{"question": "what is someone who makes violins called?", "answer": "a luthier, or simply a violin maker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a luthier, or simply a violin maker", "a luthier"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a luthier", "extracted_entity": null}
{"question": "how many strings does a violin usually have?", "answer": "four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["four", "4"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "4 strings", "extracted_entity": null}
{"question": "what are violins made of?", "answer": "different types of wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different types of wood", "maple, ebony, sheep gut"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violins are made of wood", "extracted_entity": "Violin"}
{"question": "how long have people been making instruments like violins?", "answer": "since ancient times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["since ancient times", "since 1555"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "people have been making instruments like violins for thousands", "extracted_entity": null}
{"question": "whate is the usual pitch range of a violin?", "answer": "from G3 to C8", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from G3 to C8"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the usual pitch range of a violin is 3", "extracted_entity": null}
{"question": "are xylophone bars made of rosewood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Xylophone;Rosewood"}
{"question": "is the xylophone a precursor to the vibraphone?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, the xylophone is a precursor to", "extracted_entity": "Xylophone;Vibraphone"}
{"question": "were ancient mallets made of copper?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "no, but the mallet head was made of", "extracted_entity": null}
{"question": "where did the xylophone originate?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the xylophone originated in africa", "extracted_entity": null}
{"question": "what is the earliest historical reference in Europe?", "answer": "Arnold Schlick's Spiegel der Orgelmacher und Organisten", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arnold Schlick's Spiegel der Orgelmacher und Organisten"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the Roman Empire", "extracted_entity": "Europe"}
{"question": "how are western-style xylophones characterised?", "answer": "by a bright, sharp tone and high register", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by a bright, sharp tone and high register"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "they have a metal bar, and the sound is", "extracted_entity": null}
{"question": "can a xylophone be 3 octaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Xylophone"}
{"question": "can a short bar follow a long bar?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "did vibraphones exist in 1930?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "no, they didn\u2019t.", "extracted_entity": null}
{"question": "was Alessandro Volta a professor of chemistry?", "answer": "Alessandro Volta was not a professor of chemistry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was not a professor of chemistry.", "No"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta invent the remotely operated pistol?", "answer": "Alessandro Volta did invent the remotely operated pistol.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta did invent the remotely operated pistol.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": "Alessandro_Volta"}
{"question": "was Alessandro Volta taught in public schools?", "answer": "Volta was taught in public schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was taught in public schools.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Alessandro_Volta"}
{"question": "who did Alessandro Volta marry?", "answer": "Alessandro Volta married Teresa Peregrini.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta married Teresa Peregrini.", "Teresa Peregrini"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta married", "extracted_entity": "Alessandro_Volta"}
{"question": "what did Alessandro Volta invent in 1800?", "answer": "In 1800, Alessandro Volta invented the voltaic pile.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800, Alessandro Volta invented the voltaic pile.", "voltaic pile"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the electric battery", "extracted_entity": "Alessandro_Volta"}
{"question": "what is the battery made by Alessandro Volta credited as?", "answer": "The battery made by Volta is credited as the first electrochemical cell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The battery made by Volta is credited as the first electrochemical cell.", "the first electrochemical cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the voltaic pile", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta die and retire in the same place?", "answer": "Alessandro Volta retired and died in the same place.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta retired and died in the same place.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, he died in the city of Pavia", "extracted_entity": "Alessandro_Volta"}
{"question": "when did Alessandro Volta improve  and popularize the electrophorus?", "answer": "Alessandro Volta improved and popularized the electrophorus in 1775.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta improved and popularized the electrophorus in 1775.", "1775"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta improved and popularized the elect", "extracted_entity": "Alessandro_Volta"}
{"question": "how long was Alessandro Volta a professor at the University of Pavia?", "answer": "Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "almost 25 years"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "45 years", "extracted_entity": "Alessandro_Volta;University_of_Pavia"}
{"question": "was Alessandro Volta an Egyptian?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. He was an Italian physicist and chemist", "extracted_entity": "Alessandro_Volta;Egypt"}
{"question": "was Volta taught in public schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, they were taught in public schools, they", "extracted_entity": null}
{"question": "was Volta made a count in 1810?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, Volta was made a count in", "extracted_entity": null}
{"question": "who made Volta a count?", "answer": "Napoleon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Napoleon"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Pope Pius VI", "extracted_entity": null}
{"question": "where was Volta born?", "answer": "Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Como", "Como, Italy"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was born in Como, Italy.", "extracted_entity": "Alessandro_Volta"}
{"question": "when did Volta retire?", "answer": "1819", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1819", "In 1819."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1827", "extracted_entity": null}
{"question": "a year before improving and popularizing the electrophorus, what did Volta become?", "answer": "A professor of physics at the Royal School in Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A professor of physics at the Royal School in Como"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He became a professor of physics at the University of", "extracted_entity": null}
{"question": "was Volta buried where he died or was he buried someplace else?", "answer": "where he died", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where he died", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He was buried in the church of San Lorenzo in", "extracted_entity": null}
{"question": "did Volta marry before he became professor of experimental physics at the University of Pavia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta never married. He had two children,", "extracted_entity": "Experimental_physics;University_of_Pavia"}
{"question": "what happened in 1810?", "answer": "Volta was made a count by Napoleon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Mexico declared independence from Spain.", "extracted_entity": null}
{"question": "wasn't Alessandro Volta born in Como?", "answer": "Yes, Volta was born in Como, Italy and was taught in the public schools there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was born in Como, Italy and was taught in the public schools there."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Como.", "extracted_entity": "Alessandro_Volta;Como"}
{"question": "was Alessandro Volta born in Como?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Alessandro_Volta;Como"}
{"question": "who became a professor of physics at the Royal School in Como?", "answer": "Volta.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta", "extracted_entity": "Como"}
{"question": "when was Volta made a count by Napoleon?", "answer": "Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "25 July 1809", "extracted_entity": "Alessandro_Volta;Napoleon_I"}
{"question": "is Volta's legacy celebrated by a Temple on the shore of Lake Como in the center of the town?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "s, in the center of the town of Como", "extracted_entity": "Lake_Como"}
{"question": "is the battery made by Volta credited as the first electrochemical cell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, the battery is made by Volta and", "extracted_entity": null}
{"question": "was Volta made a count by Napoleon in 1810?", "answer": "Yes, Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Napoleon_I"}
{"question": "in what year did he become a professor of physics at the Royal School in Como?", "answer": "1774", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1774"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1797", "extracted_entity": "Como"}
{"question": "was Avogadro a  professor at the University of Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avogadro was a professor at the University of Turin."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "University_of_Turin"}
{"question": "was he a member of the Royal Superior Council on Public Instruction?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes,  Avogadro was a member of the Royal Superior Council on Public Instruction."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "is Avogadro's number used to compute the results of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avagadro's number is used to compute the results of chemical reactions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes. It is used to calculate the amount of", "extracted_entity": null}
{"question": "who first calculated the value of Avogadro's number?", "answer": "Johann Josef Loschmidt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Johann Josef Loschmidt", "Johann Josef Loschmidt first calculated the value of Avogadro's number."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro", "extracted_entity": null}
{"question": "what does Avogadro's Law state?", "answer": "The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "Avogadro's Law states that the relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "At constant temperature and pressure, equal volumes of gases", "extracted_entity": null}
{"question": "who showed that Avogadro's theory held in dilute solutions?", "answer": "Jacobus Henricus van Hoff", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacobus Henricus van Hoff", "Jacobus Henricus van 't Hoff showed that Avogadro's theory holds in dilute solutions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Arrhenius", "extracted_entity": null}
{"question": "in what language was his 1811 paper published?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French", "Avogadro's 1811 paper was published in French."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "who was Avogadro's wife?", "answer": "Felicita Mazz", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felicita Mazz", "Felicita Mazz\u00e9 was Avogadro's wife."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "I don't know, but I bet she was", "extracted_entity": null}
{"question": "why did Avogadro lose his chair at the University of Turin?", "answer": "He was active in the revolutionary movements of 1821 against the king of Sardinia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was active in the revolutionary movements of 1821 against the king of Sardinia", "Avogadro lost his chair at the University of Turin because he was active in the revolutionary movements of 1821 against the king of Sardinia."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "He was accused of being a Jacobin.", "extracted_entity": "University_of_Turin"}
{"question": "was Amedeo Avogadro was born in Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Turin.", "extracted_entity": "Turin"}
{"question": "is Avogadro hailed as a founder of the atomic-molecular theory?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, Avogadro was hailed as a founder", "extracted_entity": null}
{"question": "did Johann Josef Loschmidt first calculate the value of Avogadro's number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "in 1820, Avogadro became a professor of physics where?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin University in Italy", "extracted_entity": null}
{"question": "avogadro did not actually use what word?", "answer": "atom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atom"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The word \"atom\"", "extracted_entity": null}
{"question": "the number of elementary entities in 1 mole of a substance is known as what?", "answer": "Avogadro constant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro constant", "Avogadro's constant"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the number of elementary entities in 1 mole of", "extracted_entity": null}
{"question": "what would a German call Avogadro's number?", "answer": "Loschmidt number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loschmidt number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number is called Avogadro", "extracted_entity": "Germany"}
{"question": "is Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he is Italian.", "extracted_entity": "Italy"}
{"question": "did he become a professor before the revolutionary movements against the king of Sardinia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he became a professor after the revolutionary movements", "extracted_entity": "Sardinia"}
{"question": "do ants belong to the Hymenoptera order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, ants belong to the Hymenoptera", "extracted_entity": "Hymenoptera"}
{"question": "are ants used in cuisine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no, they are used in science", "extracted_entity": null}
{"question": "does an ant's head contain sensory organs?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": "Ant"}
{"question": "how do most ants travel?", "answer": "most ants travel by walking", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["most ants travel by walking", "by walking"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "in a colony", "extracted_entity": null}
{"question": "in ant colonies, what are the fertile female ants called?", "answer": "queens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["queens"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "queens", "extracted_entity": null}
{"question": "who wrote about ants in A Tramp Abroad?", "answer": "mark twain wrote about ants", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mark twain wrote about ants", "Mark Twain"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Mark Twain", "extracted_entity": null}
{"question": "do the ants eat plants, meats, or both?", "answer": "both", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["both"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "ants eat both plants and meats", "extracted_entity": null}
{"question": "what organs gives a bull ant its good sight?", "answer": "compound eyes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["compound eyes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The eyes of a bull ant are located on the", "extracted_entity": null}
{"question": "what may happen to red fire ants if we use boiling water on the queen?", "answer": "nests of red fire ants may be destroyed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nests of red fire ants may be destroyed", "die"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "the queen may die and the fire ants may die", "extracted_entity": null}
{"question": "are ants found in Antartica?", "answer": "No, there are no ants in antarctica.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, there are no ants in antarctica."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No, they are not found in Antarctica.", "extracted_entity": "North_Antrim_(Assembly_constituency)"}
{"question": "do male ants take flight before females?", "answer": "Yes, male ants take flight before females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, male ants take flight before females."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no, the male ants die after mating.", "extracted_entity": null}
{"question": "do worker ants have wings?", "answer": "No, worker ants do not have wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, worker ants do not have wings."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, but only the queens and males have wings", "extracted_entity": null}
{"question": "what are the three segments of an ant?", "answer": "The head, mesosoma and metasoma are the three distinct body segments.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The head, mesosoma and metasoma are the three distinct body segments."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "head, thorax, abdomen", "extracted_entity": "Ant"}
{"question": "what are the ant colonies that lack queens called?", "answer": "Colonies that lack queens are called gamergate colonies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colonies that lack queens are called gamergate colonies."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "polygynous colonies", "extracted_entity": null}
{"question": "where are bullet ants located?", "answer": "Bullet ants are located in Central and South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bullet ants are located in Central and South America."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Bullet ants are located in the tropical rainforests", "extracted_entity": null}
{"question": "do ants belong to the same order as bees?", "answer": "Yes, ants belong to the same order as bees.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, ants belong to the same order as bees."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No, they belong to the order Hymen", "extracted_entity": "Bee"}
{"question": "what is the most common color of ants?", "answer": "Most ants are red or black.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most ants are red or black."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "red", "extracted_entity": null}
{"question": "do all ants build nests?", "answer": "No, not all ants build nests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, not all ants build nests."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ant"}
{"question": "ultraviolet vision was first discovered in ants by Sir who in 1881?", "answer": "John Lubbok", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Lubbok"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Sir John Lubbock", "extracted_entity": null}
{"question": "do ants thrive in most ecosystems?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, ants are found in almost all terrestrial ecosystems", "extracted_entity": null}
{"question": "are several species of ants able to use the Earth's magnetic field?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, at least one species of ants is able", "extracted_entity": null}
{"question": "are termites actually more closely related to cockroaches as well as mantids?", "answer": "Yes, termites are actually more closely related to cockroaches as well as mantids.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, termites are actually more closely related to cockroaches as well as mantids."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cockroach;Mantidae"}
{"question": "do some caterpillars produce vibrations as well as sounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, some caterpillars produce vibrations as well", "extracted_entity": null}
{"question": "did ants evolve from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, the ancestors of modern ants evolved from was", "extracted_entity": "Cretaceous"}
{"question": "is the ant a marsupial?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No. The ant is not a marsupial", "extracted_entity": "Marsupial"}
{"question": "are they easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist?", "answer": "Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, they are easily identified by their elb", "extracted_entity": null}
{"question": "is Antwerp a city?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is Antwerp a municipality?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is Antwerp in Belgium?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Royal_Antwerp_FC;Belgium"}
{"question": "what is Antwerp?", "answer": "Antwerp is a city and municipality in Belgium.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is a city and municipality in Belgium."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is the largest city in Belgium by area", "extracted_entity": "Antwerp"}
{"question": "what is the population of the city of Antwerp?", "answer": "Antwerp's population is 472,071.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp's population is 472,071."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "536,079", "extracted_entity": "Antwerp"}
{"question": "where is the city of Antwerp?", "answer": "Antwerp is in Belgium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is in Belgium"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is the capital of Antwerp Province in", "extracted_entity": "Antwerp"}
{"question": "why is Antwerp important to fashion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is the home of the fashion school Royal", "extracted_entity": "Antwerp"}
{"question": "antwerp is to the east of what river?", "answer": "Antwerp is to the east of the Scheldt river", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is to the east of the Scheldt river"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "the river scheldt", "extracted_entity": "Antwerp"}
{"question": "how many municipalities does Antwerp have?", "answer": "None, Antwerp is a municipality.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None, Antwerp is a municipality."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "Antwerp"}
{"question": "is Arabic a Central Semitic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": "Arabic_language"}
{"question": "was Arabic a major vehicle of culture in Europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, but it was a major vehicle of culture", "extracted_entity": "Arabic_language;Europe"}
{"question": "is Arabic classified alongside Semitic languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes, Arabic is a Semitic language. Sem", "extracted_entity": "Semitic_languages"}
{"question": "how many people speak the Arabic language?", "answer": "280 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["280 million people."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is spoken by about 422 million people as", "extracted_entity": "Arabic_language"}
{"question": "when was Arabic calligraphy invented?", "answer": "Many styles were developed after 786.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many styles were developed after 786."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the 7th century CE", "extracted_entity": null}
{"question": "where is Arabic spoken?", "answer": "The Middle East and North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Middle East and North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "\"Arabic is spoken primarily across the Arab world", "extracted_entity": "Arabic_language"}
{"question": "why is Arabic related to Islam?", "answer": "Arabic is the liturgical language of Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic is the liturgical language of Islam"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "he texts above show that Islam is the religion of", "extracted_entity": "Arabic_language;Islam"}
{"question": "why does Arabic heavily influence European languages?", "answer": "Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "because of the Arabic conquests", "extracted_entity": "Arabic_language;Europe"}
{"question": "is Arabic the largest member of the Semitic language family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arabic_language"}
{"question": "does Modern Standard Arabic continue to evolve like other languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No. It is a dead language. It is", "extracted_entity": null}
{"question": "is Hassaniya Arabic spoken in Mauritania?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hassaniya_Arabic;Mauritania"}
{"question": "what is the only variety of modern Arabic that has acquired official language status?", "answer": "maltese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["maltese", "Maltese"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "the modern standard Arabic", "extracted_entity": "Arabic_language"}
{"question": "egyptian Arabic is spoken by how many in Egypt?", "answer": "76 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["76 million"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "90% of the population", "extracted_entity": "Egypt"}
{"question": "where are the Western Arabic numerals used?", "answer": "North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North Africa", "present-day North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "the Western Arabic numerals are used in the decimal", "extracted_entity": "Arabic_numerals"}
{"question": "the most active Academies of Arabic Language are found where?", "answer": "damascus and cairo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["damascus and cairo"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Gulf States", "extracted_entity": null}
{"question": "hassan Massoudy is a master of what genre?", "answer": "Hassan Massoudy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hassan Massoudy", "Arabic calligraphy"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "calligraphy", "extracted_entity": null}
{"question": "is Berlin the capital city of Germany?", "answer": "Berlin is the capital city of Germany.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is the capital city of Germany.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Berlin;Germany"}
{"question": "is Berlin the largest city in Germany?", "answer": "Berlin is Germany's largest city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is Germany's largest city.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No, it is the second largest city in Germany", "extracted_entity": "Berlin;Germany"}
{"question": "is Schloss Charlottenburg the largest existing palace in Berlin?", "answer": "Schloss Charlottenburg is the largest existing palace in Berlin. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Schloss Charlottenburg is the largest existing palace in Berlin. ", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Charlottenburg_Palace;Berlin"}
{"question": "which building is the site of the German parliament?", "answer": "The Reichstag building is the site of the German parliament. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building is the site of the German parliament. ", "The Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the Reichstag (Reichstag building)", "extracted_entity": null}
{"question": "which two sports events did the Olympiastadion host?", "answer": "The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "the 1936 Summer Olympics and the 2006 FIFA World Cup final"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The stadium hosted the football tournament of the 193", "extracted_entity": "Olympiastadion_(Berlin)"}
{"question": "where is the Berliner Dom located?", "answer": "The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "on the Spree Island"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Berliner Dom is located on Museum Island in", "extracted_entity": "Berlin_Cathedral"}
{"question": "which Berlin building did architect Norman Foster remodel in the 1990s?", "answer": "The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the Reichstag building", "extracted_entity": "Berlin;Norman_Foster,_Baron_Foster_of_Thames_Bank"}
{"question": "why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?", "answer": "Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "Because burning lignite produces harmful emissions"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "because it is cheaper than coal", "extracted_entity": "Vattenfall"}
{"question": "what are the names of the two zoos in Berlin?", "answer": "The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "Zoologischer Garten Berlin and Tierpark Friedrichsfelde"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Zoological Garden and Tierpark Berlin", "extracted_entity": "Berlin"}
{"question": "is Berlin the headquarters of Springer?", "answer": "Yes, Berlin is the headquarters of Springer.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Berlin is the headquarters of Springer."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No, Berlin is the capital of Germany, not", "extracted_entity": "Berlin;Axel_Springer_SE"}
{"question": "does the Gendarmenmarkt border the French Cathedral?", "answer": "Yes, the Gendarmenmarkt borders the French Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the Gendarmenmarkt borders the French Cathedral."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gendarmenmarkt;France"}
{"question": "is the Nauen plain north of Berlin?", "answer": "No, the Nauen plain stretches to the west of Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the Nauen plain stretches to the west of Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Nauen;Berlin"}
{"question": "where is the bust of Queen Nefertiti?", "answer": "The bust of Queen Nefertiti is in the Altes Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The bust of Queen Nefertiti is in the Altes Museum."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin, Germany", "extracted_entity": null}
{"question": "where does the German President live?", "answer": "The German President lives west of the center, Schloss Bellevue.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The German President lives west of the center, Schloss Bellevue."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The President lives in Berlin.", "extracted_entity": "President_of_Germany"}
{"question": "when did Berlin give up its status as a free Hanseatic city?", "answer": "In 1451 Berlin gave up its status as a free Hanseatic city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1451 Berlin gave up its status as a free Hanseatic city."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "1806", "extracted_entity": "Berlin;Hanseatic_League"}
{"question": "which is the busiest airport in Berlin?", "answer": "Tegel International Airport is the busiest airport in Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tegel International Airport is the busiest airport in Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Brandenburg Airport", "extracted_entity": "Berlin"}
{"question": "how many occupation zones was Germany divided into after the war?", "answer": "Germany was divided into four occupation zones after the war.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany was divided into four occupation zones after the war."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "4", "extracted_entity": null}
{"question": "where was Popkomm held before moving to Berlin?", "answer": "Popkomm was held in Cologne before moving to Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Popkomm was held in Cologne before moving to Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Cologne", "extracted_entity": "Popkomm;Berlin"}
{"question": "who was Blaise Pascal's father?", "answer": "His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "\u00c3\u2030tienne Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "laise Pascal's father was \u00c9tienne Pascal", "extracted_entity": "Blaise_Pascal"}
{"question": "was Pascal a French mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he was a French mathematician.", "extracted_entity": "Blaise_Pascal;France"}
{"question": "did Pascal have poor health throughout his life?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he suffered from constant pain and illness.", "extracted_entity": "Blaise_Pascal"}
{"question": "how old was Pascal when he lost his mother?", "answer": "at the age of three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the age of three", "3."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Blaise_Pascal"}
{"question": "who was Pascal's younger sister?", "answer": "Jacqueline", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacqueline", "Jacqueline."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "acqueline Pascal", "extracted_entity": "Blaise_Pascal"}
{"question": "what led Pascal to his religious conversion?", "answer": "Two basic influences led him to his conversion: sickness and Jansenism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two basic influences led him to his conversion: sickness and Jansenism", "sickness and Jansenism"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "e was led to his religious conversion by his father", "extracted_entity": "Blaise_Pascal"}
{"question": "how old was Pascal when he died?", "answer": "39", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["39", "1662-08-19."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "9", "extracted_entity": "Blaise_Pascal"}
{"question": "did Pascal write about cycloid before 1658?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes, in 1650.", "extracted_entity": "Cycloid"}
{"question": "is there a programming language called Pascal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "No, Pascal is not a programming language, it", "extracted_entity": "Pascal_(programming_language)"}
{"question": "crowds of believers came to see and kiss the thorn; all of Catholic where acclaimed a miracle?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "I think it's a miracle, because the th", "extracted_entity": "Catholic_Church"}
{"question": "wasn't Blaise Pascal a work of Desargues on conic sections?", "answer": "Yes, it was", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it was"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he was.", "extracted_entity": "Blaise_Pascal;Girard_Desargues"}
{"question": "who died the next morning?", "answer": "Pascal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the young man who had been shot", "extracted_entity": null}
{"question": "when did his father die?", "answer": "1651", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1651", "His father died in 1651."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "when he was 11 years old.", "extracted_entity": null}
{"question": "was he a child prodigy who was educated by his father, a civil servant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, he was. He was educated at home", "extracted_entity": null}
{"question": "was Pascal's earliest work in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, Pascal's earliest work in the natural and", "extracted_entity": "Blaise_Pascal;Mechanical_calculator;Fluid;Pressure"}
{"question": "did Pascal also write in defense of the scientific method?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he did. In his book ''P", "extracted_entity": "Blaise_Pascal"}
{"question": "do butterflies make sounds?", "answer": "Some butterflies make sounds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some butterflies make sounds."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "no, butterflies don't make sounds. Butterflies", "extracted_entity": "Butterfly"}
{"question": "do butterflies have two eyes?", "answer": "Yes, butterflies have two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, butterflies have two eyes."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Butterfly"}
{"question": "does a black moth mean that someone has died?", "answer": "In the Philippines a black butterfly or moth mean that someone has died.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Philippines a black butterfly or moth mean that someone has died."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "no. it means that someone has died, but", "extracted_entity": null}
{"question": "what is the outer layer of the cuticle made of?", "answer": "The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "waxy cuticle", "extracted_entity": null}
{"question": "where was there a vast swarm of butterflies?", "answer": "In Kyoto there was a vast swarm of butterflies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Kyoto there was a vast swarm of butterflies."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "in a field in the south of England", "extracted_entity": null}
{"question": "what butterfly is migratory?", "answer": "The Monarch butterfly is migratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monarch butterfly is migratory."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Monarch", "extracted_entity": null}
{"question": "do butterflies carry more pollen than Hymenoptera?", "answer": "No, butterflies carry less pollen than Hymenoptera.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, butterflies carry less pollen than Hymenoptera."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "No. Butterflies carry less pollen than Hym", "extracted_entity": "Butterfly;Hymenoptera"}
{"question": "what allows the butterfly to evade predators?", "answer": "A butterfly's hind wings allow the butterfly to evade predators.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A butterfly's hind wings allow the butterfly to evade predators."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "its ability to fly away", "extracted_entity": null}
{"question": "what happens if a wing is removed?", "answer": "If a ring is removed the other three will grow to a larger size.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["If a ring is removed the other three will grow to a larger size."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "it will not be able to fly", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a Spanish Biologist?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, he was a French physicist."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he was a French physicist and engineer.", "extracted_entity": "Spain"}
{"question": "was the SI unit of charge named after Charles-Augustin de Coulomb?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, the SI unit of charge, the coulomb, was named after him."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb ever employed at La Rochelle?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, upon his return to France, with the rank of Captain, he was employed at La Rochelle."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was a military engineer in La Roch", "extracted_entity": "Charles-Augustin_de_Coulomb;La_Rochelle"}
{"question": "what is Charles-Augustin de Coulomb best known for?", "answer": "He is best known for developing Coulomb's law.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He is best known for developing Coulomb's law."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb's law, which is the basis for", "extracted_entity": null}
{"question": "whose ideas inspired Charles-Augustin de Coulomb's experiments on the resistance of masonries?", "answer": "Pieter van Musschenbroek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pieter van Musschenbroek.", "He carried out several experiments on the resistance of masonries which were inspired by the ideas of Pieter van Musschenbroek on friction."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Jacques-Antoine Charles Bruno, comte de", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "when did Charles-Augustin de Coulomb join his father's family in Montpeillier?", "answer": "From 1757 to 1759 he joined his father's family in Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1757 to 1759 he joined his father's family in Montpellier."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb come from a poor family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, his mother came from a wealthy family in the wool trade."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "what is Coulomb's law?", "answer": "The definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The definition of the electrostatic force of attraction and repulsion."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the force of attraction or repulsion between two charged", "extracted_entity": null}
{"question": "where is Coll\u00e8ge des Quatre-Nations located?", "answer": "In Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Montpellier.", "In Paris."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "in Paris, France", "extracted_entity": "Coll\u00e8ge_des_Quatre-Nations"}
{"question": "what happened in 1785?", "answer": "Coulomb presented his three reports on Electricity and Magnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb presented his three reports on Electricity and Magnetism"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the first balloon flight", "extracted_entity": null}
{"question": "what happened in 1757 to 1759?", "answer": "Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Seven Years War was fought between the British and", "extracted_entity": null}
{"question": "regarding this topic, what did the Revolution do?", "answer": "The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "it made the country a Republic", "extracted_entity": null}
{"question": "who determined to pursue mathematics?", "answer": "Charles-Augustin de Coulomb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "I determined to pursue mathematics.", "extracted_entity": null}
{"question": "when did Coulomb return to Paris?", "answer": "In 1759.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1759."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "in 1781", "extracted_entity": "Daniel_Ducruet;Paris"}
{"question": "does Coulomb leave a legacy as a pioneer in the field of geotechnical engineering for his contribution to retaining wall design?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no, Coulomb is more of a pioneer in", "extracted_entity": null}
{"question": "did the family move to Paris as well as there Coulomb studied at the prestigious Collge des Quatre-Nations?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, the family moved to Paris and Coulomb", "extracted_entity": "Paris;Coll\u00e8ge_des_Quatre-Nations"}
{"question": "was Coulomb born in Angoulme, France to a well-to-do family?", "answer": "Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, Coulomb was born in Angoulme", "extracted_entity": "Angoul\u00eame;France"}
{"question": "is he best known for developing Coulomb's law, the definition of the electrostatic force of attraction and repulsion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He is best known for developing Coulomb's law", "extracted_entity": "Coulomb's_law;Electrostatics"}
{"question": "the SI unit of charge, the coulomb, was named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "International_System_of_Units;Coulomb"}
{"question": "did Coulomb die in Angoul\u00c3\u00aame, France, to a well-to-do family?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, Coulomb was born in Angoul\u00c3", "extracted_entity": "Angoul\u00eame;France"}
{"question": "are all spoken varieties of Chinese tonal and analytical?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, all varieties of Chinese are tonal and", "extracted_entity": "Chinese_language"}
{"question": "is fifty percent or more of Korean vocabulary of Chinese origin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes. Korean is a language that is heavily influenced", "extracted_entity": null}
{"question": "does \"ketchup\" come from the Minnan pronunciation of \u9bad\u6c41 (koe-tsiap)?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "no. \u9bad\u6c41 is the Hok", "extracted_entity": "Southern_Min"}
{"question": "what is the most common romanization standard for Standard Mandarin today?", "answer": "Hanyu Pinyin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hanyu Pinyin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "pinyin", "extracted_entity": "Standard_Chinese"}
{"question": "what is the official standard language used by the People's Republic of China, the Republic of China, and Singapore (where it is called \"Huayu\")?", "answer": "Putonghua / Guoyu, often called \"Mandarin\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Putonghua / Guoyu, often called \"Mandarin\"", "Standard Mandarin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Chinese (Mandarin)", "extracted_entity": "People's_Republic_of_China;Taiwan;Singapore"}
{"question": "how many head entries for character definitions does the Zhonghua Zihai \u4e2d\u534e\u5b57\u6d77 (1994) contain?", "answer": "85,568 head entries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["85,568 head entries", "54,678"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "47,000.", "extracted_entity": null}
{"question": "what does the PRC government classify as literacy amongst workers?", "answer": "knowledge of 2,000 characters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["knowledge of 2,000 characters", "a knowledge of 2000 characters"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "literacy rate of 95.5", "extracted_entity": "People's_Republic_of_China"}
{"question": "how is South China linguistically different from North China?", "answer": "South China displays more linguistic diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["South China displays more linguistic diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "South China is not linguistically different from North China", "extracted_entity": "South_China;North_China"}
{"question": "\u5348\u9910\u76d2 \u201clunchbox or boxed lunch\u201d (from bento) and \u6599\u7406 \u201cprepared cuisine\u201d are loan words from which language?", "answer": "Japanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japanese", "Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Japanese", "extracted_entity": "Bento"}
{"question": "what is the standardized form of spoken chinese?", "answer": "The standardized form of spoken Chinese is Standard Mandarin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The standardized form of spoken Chinese is Standard Mandarin.", "The standardized from of spoken Chinese is Standard Mandarin."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "standard mandarin chinese", "extracted_entity": "Chinese_language"}
{"question": "is diglossia a common feature in mainland China and Taiwan?", "answer": "Yes, diglossia is a common feature in mainland China and Taiwan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, diglossia is a common feature in mainland China and Taiwan."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes. Mainland China has a diglossic", "extracted_entity": "People's_Republic_of_China;Taiwan"}
{"question": "do linguists often view Chinese as a language family?", "answer": "Yes, linguists often view Chinese as a language family.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, linguists often view Chinese as a language family."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes. The Chinese language family includes Mandarin, Canton", "extracted_entity": "Chinese_language"}
{"question": "when did classical Chinese literature begin?", "answer": "Classical Chinese literature began in the Spring and Autumn period.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Classical Chinese literature began in the Spring and Autumn period.", "Classical Chinese literature began in the 14th to 11th centuires BCE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "the classical period of Chinese literature began in the", "extracted_entity": null}
{"question": "how does the government of the People's Republic of China often refer to all divisions of the Sinitic language(s) besides Standard Mandarin?", "answer": "The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "The PRC commonly refers to all divisons of the Sinitic language(s) beside Standard Mandarin as fangyan, or \"regional tongues.\""], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Chinese", "extracted_entity": "People's_Republic_of_China;Sinitic_languages;Standard_Chinese"}
{"question": "when was Middle Chinese used?", "answer": "Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "Middle Chinese was the language used in the 6th through 1-th centuries CE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Middle Chinese was used from the 7th to", "extracted_entity": "Middle_Chinese"}
{"question": "how is the pronunciation of foreign words determined?", "answer": "The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "Foreign words are transliterated into close-match existing Chinese morphemes."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "the pronunciation of foreign words is determined by the phon", "extracted_entity": null}
{"question": "what is the total number of Chinese words?", "answer": "The Hanyu Da Zidian includes 54,678 head entries for characters.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Hanyu Da Zidian includes 54,678 head entries for characters."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "400,000", "extracted_entity": "Chinese_language"}
{"question": "what language was used in the Zhou and Southern Dynasties?", "answer": "Old Chinese was the language common during the early and middle Zhou Dynasty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old Chinese was the language common during the early and middle Zhou Dynasty."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Old Chinese", "extracted_entity": "Zhou_dynasty;Northern_and_Southern_dynasties"}
{"question": "is a cougar also known as a mountain lion?", "answer": "A cougar is also known as a mountain lion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cougar is also known as a mountain lion.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes, a cougar is also known as a mountain", "extracted_entity": "Cougar"}
{"question": "are cougars larger than jaguars?", "answer": "Cougars are not larger than jaguars.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cougars are not larger than jaguars.", "no"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": "Cougar;Jaguar"}
{"question": "does the World Conservation Union consider the cougar a \"least concern\" species?", "answer": "The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes. The IUCN Red List of Threat", "extracted_entity": "International_Union_for_Conservation_of_Nature;Cougar"}
{"question": "what are some of the cougar's primary food sources?", "answer": "The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses, and sheep"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "deer, elk, moose, and other large", "extracted_entity": null}
{"question": "how long are cougar adult males (from nose to tail)?", "answer": "The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "2.4 meters"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "7.9 ft", "extracted_entity": "Cougar"}
{"question": "how long is an adult cougar's paw print?", "answer": "An adult cougar's paw print is approximately 10 cm (4 inches) long.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An adult cougar's paw print is approximately 10 cm (4 inches) long.", "4 inches"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "4-5 inches", "extracted_entity": "Cougar"}
{"question": "what are the three heaviest cats in the world?", "answer": "The three heaviest cats in the world are the tiger, lion and jaguar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three heaviest cats in the world are the tiger, lion and jaguar.", "tiger, lion, and jaguar"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "lions, tigers, and bears", "extracted_entity": null}
{"question": "on average, are cougar males heavier than females?", "answer": "On average, cougar males are heavier than females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On average, cougar males are heavier than females.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "The average male weighs 80kg and the average", "extracted_entity": "Cougar"}
{"question": "what are the three cat species that are native to Canada?", "answer": "The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "cougar, bobcat, Canadian lynx"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Canada lynx, bobcat, and cougar.", "extracted_entity": "Canada"}
{"question": "is one of the parts of a cymbal called a disc?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cymbal"}
{"question": "are ride cymbals very common?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes, they are very common.", "extracted_entity": null}
{"question": "does Fender make cymbals?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, Fender does not make cymbals", "extracted_entity": "Fender_Musical_Instruments_Corporation"}
{"question": "what part of the cymbal is the bell?", "answer": "The center of a Cymbal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The center of a Cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the part of the cymbal that is raised", "extracted_entity": null}
{"question": "what's the timber of ancient cimbals like? disfluent?", "answer": "like that of small hand-bells or of the notes of the keyed harmonica", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["like that of small hand-bells or of the notes of the keyed harmonica"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes, it's disfluent. and it", "extracted_entity": null}
{"question": "what does it sound like to play cymbals with a cello bow?", "answer": "squealing car brakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["squealing car brakes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "like a cymbal being played with a c", "extracted_entity": null}
{"question": "what are cymbals made of?", "answer": "brass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "thin plates of various alloys", "extracted_entity": "Cymbal"}
{"question": "how many cymbals typically go in a drum kit?", "answer": "at least 3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at least 3"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "1-2 cymbals", "extracted_entity": null}
{"question": "what kinds of musical groups use cymbals?", "answer": "Cymbals are used in modern orchestras and many military, marching, concert and other bands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cymbals are used in modern orchestras and many military, marching, concert and other bands"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "all of them.", "extracted_entity": null}
{"question": "is the dragonfly associated with late summer as well as early autumn?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "no, it is associated with late spring and early", "extracted_entity": null}
{"question": "does Exposure to air cause the larva to begin breathing?", "answer": "Yes (capitalization).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (capitalization)."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "no, but when they are exposed to air they", "extracted_entity": null}
{"question": "is it characterized by large multifaceted eyes, two pairs of strong transparent wings, and an elongated body?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "insects", "extracted_entity": null}
{"question": "is the drum a member of the percussion group?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, the drum is a member of the percussion", "extracted_entity": null}
{"question": "does each type of drum head serve its own musical purpose?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, each type of drum head serves its own", "extracted_entity": "Drum_kit"}
{"question": "is the Aburukuwa a type of drum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "how long has the basic design of the drum remained unchanged?", "answer": "remained virtually unchanged for thousands of years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["remained virtually unchanged for thousands of years", "thousands of years"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "100 years, or thereabouts.", "extracted_entity": null}
{"question": "what is the second biggest factor affecting the sound produced by a drum?", "answer": "tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["tension", "the tension at which the drum head is held against the shell of the drum."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the size of the drum head", "extracted_entity": "Drum_kit"}
{"question": "by what name are drums classified as under the percussion group?", "answer": "membranophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["membranophone", "membranous"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "membranophones", "extracted_entity": null}
{"question": "how do rock and roll drummers like their drumheads?", "answer": "rock and roll drummer may prefer drums that sound loud and deep", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["rock and roll drummer may prefer drums that sound loud and deep", "drums that sound loud and deep"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "They like them very thin, to get a loud", "extracted_entity": "Rock_and_roll"}
{"question": "whare is the name for drums that have a set of wires held across some of all of the drum heads?", "answer": "snares", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["snares"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "These drums are called timpani.", "extracted_entity": null}
{"question": "had the drums been used in Sri Lanka since Before Christ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, the drums had been used in Sri Lanka", "extracted_entity": "Sri_Lanka"}
{"question": "are drums often used in music therapy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums are often used in music therapy, as", "extracted_entity": "Music_therapy"}
{"question": "do drums consist of at least eight membranes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "no, drums consist of at least nine membranes.", "extracted_entity": null}
{"question": "what has the most effect on how a drum sounds?", "answer": "the drumhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the drumhead"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "The drum itself, and the player", "extracted_entity": null}
{"question": "who constructed a kit using diatonic and chromatically tuned drums?", "answer": "Terry Bozzio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Terry Bozzio"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "I did. It is a 7-drum", "extracted_entity": null}
{"question": "drums are usually played by what?", "answer": "the hands, or by one or two sticks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the hands, or by one or two sticks", "the hands"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "a drummer", "extracted_entity": null}
{"question": "has the design of drums changed recently?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes. The design of drums has changed in the", "extracted_entity": null}
{"question": "can some cylindrical shell drums have no drum heads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, because they are used to transport hazardous materials", "extracted_entity": null}
{"question": "does every drumhead make the same sound?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "no. The type of drumhead material, the", "extracted_entity": null}
{"question": "aren't drums often used in music therapy?", "answer": "Yes, drums are often used in music therapy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, drums are often used in music therapy."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": "Music_therapy"}
{"question": "aren't drums usually played by the hands?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "drums are usually played with sticks", "extracted_entity": null}
{"question": "are drums usually played by the hands?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, but I play with my feet", "extracted_entity": null}
{"question": "do many such drums have six to ten tension rods?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, some do, but not all.", "extracted_entity": null}
{"question": "do drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with parts of a player's body, or with some sort of implement such as a drumstick, to produce sound?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Drumhead;Drum_kit"}
{"question": "have other techniques been used to cause drums to make sound, such as the ``Thumb roll''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, but it is very rare. It is", "extracted_entity": null}
{"question": "is the electric eel a true eel?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, the electric eel is not a true eel."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "no, it is not a true eel", "extracted_entity": "Electric_eel"}
{"question": "are most eels predators?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, most eels are predators."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "eels are predators, they eat other animals,", "extracted_entity": null}
{"question": "is eel blood toxic to humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, eel blood is toxic to humans."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No, eel blood is not toxic to humans", "extracted_entity": null}
{"question": "how many species of true eels are there?", "answer": "approximately 800 species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["approximately 800 species", "There are approximately 800 species of true eels."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "20 species of true eels", "extracted_entity": null}
{"question": "where is smoked eel considered a delicacy?", "answer": "Northern Germany, The Netherlands, Denmark, Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Germany, The Netherlands, Denmark, Sweden", "Smoked eel is considered a delicacy in northern Germany, The Netherlands, Denmark and Sweden."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "In France and Germany", "extracted_entity": "Eel"}
{"question": "how do eels begin life?", "answer": "As flat and transparent larvae, called leptocephali", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As flat and transparent larvae, called leptocephali", "Eels begin life as flat and transparent larvae, called leptocephali."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "They begin life as tiny larvae.", "extracted_entity": null}
{"question": "what types of fins do eels have?", "answer": "Dorsal, anal, caudal, tail fins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dorsal, anal, caudal, tail fins", "Eels have dorsal and anal fins fused with a tail fin. Some also have  pectoral fins."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "eels have pectoral fins and anal fins", "extracted_entity": null}
{"question": "what do leptocephali eat?", "answer": "Marine snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Marine snow", "Leptocephali eat small particles called marine snow."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "plankton, detritus, and other fish", "extracted_entity": null}
{"question": "do all eels live in fresh water?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, not all eels live in fresh water."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "no. some species live in the sea, and", "extracted_entity": null}
{"question": "is Finnish an official language of the European Union?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Finnish is an official language of the European Union."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Finnish_language;European_Union"}
{"question": "are the dialects of Finnish divided into two groups?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, the dialects of Finnish are divided into two groups."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Finnish_language"}
{"question": "is Finnish threatened by English?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Finnish is not threatened by English."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Yes, Finnish is threatened by English", "extracted_entity": "Finland"}
{"question": "when did Proto-Uralic language arrive in Finland?", "answer": "Around 1900 BCE", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Around 1900 BCE", "Proto-Uralic language arrived in Finland around 1900 BCE."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "The hypothetical language is thought to have been originally spoken", "extracted_entity": "Proto-Uralic_language;Finland"}
{"question": "what are characteristic features of Finnish?", "answer": "Vowel harmony and an agglutinative morphology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowel harmony and an agglutinative morphology", "Characteristic features of Finnish are vowel harmony and an agglutinative morphology."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Finnish is an agglutinative language", "extracted_entity": "Finland"}
{"question": "where is the Kven language spoken?", "answer": "Norway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norway", "The Kven language is spoken in Northern Norway."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "the northernmost parts of Norway", "extracted_entity": "Kven_language"}
{"question": "for how long has the classification of dialects spoken outside of Finland been a controversial issue?", "answer": "Since Finland's independence in 1917", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Since Finland's independence in 1917", "The classification of dialects spoken outside of Finland has been a controversial issue since Finland's independence in 1917."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "it has been a controversial issue since the 196", "extracted_entity": "Finland"}
{"question": "who wrote the first novel in Finnish?", "answer": "Aleksis Kivi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Aleksis Kivi", "The first novel in Finnish was published by Aleksis Kivi."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "ksis Kivi", "extracted_entity": "Finnish_language"}
{"question": "what book defined the official Finnish language?", "answer": "The Dictionary of Contemporary Finnish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Dictionary of Contemporary Finnish", "The Dictionary of Contemporary Finnish defined the official Finnish language."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "men kielen perussanakirja", "extracted_entity": "Finnish_language"}
{"question": "what is the earliest extant transverse flute?", "answer": "it dates from 433 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it dates from 433 BC", "Chi"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the 5th century BC flute of the Tomb", "extracted_entity": null}
{"question": "when does a flute produce sound?", "answer": "when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "When a stream of air directed across a hole in the instrument creats a vibration of air at the hole."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "when the player blows into it", "extracted_entity": null}
{"question": "how can a flute's volume be increased?", "answer": "a flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute's volume can generally be increased by making its resonator and tone holes larger", "A flute's volume can be increased by making its resonator and tone holes larger."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by using a microphone", "extracted_entity": null}
{"question": "do open-ended flutes have brighter timbres?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, they do. The reason is that the", "extracted_entity": null}
{"question": "where was the oldest flute ever discovered found?", "answer": "Germany", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany", "Various parts of Germany."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The oldest flute ever discovered was found in Hoh", "extracted_entity": null}
{"question": "are Western flutes simpler than Indian flutes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "No. The Indian flute is more complicated than the", "extracted_entity": "Western_world;India"}
{"question": "why is a police whistle very wide for its pitch?", "answer": "A flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute's volume can generally be increased by making its resonator and tone holes larger", "It's volume is high either because of its larger resonator or larger tone holes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "because it is very long.", "extracted_entity": null}
{"question": "what are the two main varieties of Indian flutes?", "answer": "The first, the Bansuri. The second, the Venu or Pullanguzhal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first, the Bansuri. The second, the Venu or Pullanguzhal", "Bansuri and the Venu or Pullanguzhal."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the bamboo flute and the silver flute", "extracted_entity": null}
{"question": "has the flute been dated to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, flutes have been dated back to", "extracted_entity": null}
{"question": "was the pan flute used in Greece?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "No, the pan flute was not used in Greece", "extracted_entity": "Greece"}
{"question": "can a flute be played with several different air sources?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, but the player will have to use a", "extracted_entity": null}
{"question": "when was the pan flute used in Greece?", "answer": "the 7th century BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 7th century BC", "7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "4th century BC", "extracted_entity": "Greece"}
{"question": "when did concert flutes begin appearing in concert ensembles?", "answer": "the 16th-century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 16th-century", ""], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1920s", "extracted_entity": null}
{"question": "how many main varieties of Indian flutes are currently used?", "answer": "two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two", "Two"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "there are four main varieties of Indian flutes currently", "extracted_entity": null}
{"question": "what material is a chi flute fashioned from?", "answer": "lacquered bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lacquered bamboo", "Lacquered bamboo"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "bamboo.", "extracted_entity": null}
{"question": "how does a flute player change the pitch of the sound?", "answer": "by opening and closing holes in the body of the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by opening and closing holes in the body of the instrument", "Opening and closing holes in the body of the instrument"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by changing the length of the tube", "extracted_entity": null}
{"question": "what is the most basic form of the flute?", "answer": "a flute can be an open tube which is blown like a bottle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute can be an open tube which is blown like a bottle", "Open tube which is blown like a bottle"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the flute is a reedless wind instrument,", "extracted_entity": null}
{"question": "has the dragon historically served as China's national emblem?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do pandas hibernate?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "no, they are not bears. bears hibernate", "extracted_entity": null}
{"question": "do giant pandas attack humans?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes, but only when they are hungry", "extracted_entity": null}
{"question": "what family is the panda a part of?", "answer": "Ursidae (bear)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ursidae (bear)", "Ursidae"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "it is part of the bear family", "extracted_entity": null}
{"question": "how long does it take for the panda cubs skin to turn gray?", "answer": "one to two weeks after birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["one to two weeks after birth", "One to two weeks"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2 years", "extracted_entity": null}
{"question": "how many different names did Chinese writings give to the panda?", "answer": "20", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["20"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "People's_Republic_of_China"}
{"question": "what foods do pandas eat?", "answer": "bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bamboo", "bamboo, honeys, eggs, fish, yams, shrub leaves, oranges, and bananas"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "bamboo, bamboo, bamboo, bamboo, bamboo,", "extracted_entity": null}
{"question": "what are the two subspecies of giant pandas?", "answer": "Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "Ailuropoda melanoleuca melanoleuca and The Qinling Panda, Ailuropoda melanoleuca qinlingensis"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The giant panda has two subspecies: the nominate", "extracted_entity": null}
{"question": "what is panda diplomacy?", "answer": "Loans of Giant Pandas to American and Japanese zoos", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loans of Giant Pandas to American and Japanese zoos"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "panda diplomacy is when China gives another country a panda", "extracted_entity": null}
{"question": "is the Giant Panda a mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes, it is a mammal", "extracted_entity": "Giant_panda"}
{"question": "is the Giant Panda a terrestrial animal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, it is not a terrestrial animal. It", "extracted_entity": "Terrestrial_animal"}
{"question": "is the Giant Panda an endangered species?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Endangered_species"}
{"question": "where do Giant Pandas live?", "answer": "a few mountain ranges in central china", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a few mountain ranges in central china", "Sichuan province, Shaanxi and Gansu provinces"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "in China", "extracted_entity": null}
{"question": "when was the Giant Panda's genome sequenced?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2009", "extracted_entity": null}
{"question": "how many subspecies of the Giant Panda are there?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "There are no subspecies of the Giant Panda.", "extracted_entity": "Giant_panda"}
{"question": "is the Giant Panda a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The Giant Panda is a carnivore", "extracted_entity": "Giant_panda;Carnivore"}
{"question": "why is the Giant Panda an endangered species?", "answer": "threatened by habitat loss", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["threatened by habitat loss", "It is threatened by continued habitat loss and by a very low birthrate"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The Giant Panda is an endangered species because they are", "extracted_entity": null}
{"question": "is the population of wild Giant  Pandas growing?", "answer": "according to some reports, yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["according to some reports, yes", "Yes but uncertain"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes, it is growing.  It has grown", "extracted_entity": null}
{"question": "is an acoustic guitar dependent on an external device?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes, it is. an acoustic guitar needs an", "extracted_entity": null}
{"question": "is the battente smaller than a classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes, the battente is smaller than a classical", "extracted_entity": null}
{"question": "is the flamenco guitar similar to the classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The flamenco guitar is similar to the classical guitar", "extracted_entity": null}
{"question": "where is the headstock located?", "answer": "end of the guitar neck furthest from the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["end of the guitar neck furthest from the body", "At the end of the guitar."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "On the top of the guitar, where the tuning", "extracted_entity": null}
{"question": "whom are guitars made and repaired by?", "answer": "luthiers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["luthiers", "Luthiers."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the people who play them", "extracted_entity": null}
{"question": "what are modern guitar strings constructed of?", "answer": "metal, polymers, animal or plant product materials", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["metal, polymers, animal or plant product materials", "Metal, polymers, or animal or plant product materials."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "nylon, steel, nickel, and copper", "extracted_entity": null}
{"question": "why are Harp Guitars difficult to classify?", "answer": "as there are many variations within this type of guitar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["as there are many variations within this type of guitar", "There are many variations within this type."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Because they are difficult to classify!", "extracted_entity": null}
{"question": "what is the bridge used for?", "answer": "transfer the vibration from the strings to the soundboard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["transfer the vibration from the strings to the soundboard", "The transfer of string vibrations."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "it is used to cross a river.", "extracted_entity": null}
{"question": "which guitars use three single-coil pickups?", "answer": "Fender Statocaster type guitars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fender Statocaster type guitars", "Fender Stratocaster type guitars."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "stratocasters and telecasters.", "extracted_entity": null}
{"question": "is a guitar an instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes, it is an instrument.", "extracted_entity": null}
{"question": "can guitars be divided into two broad categories?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes. They can be divided into acoustic and electric", "extracted_entity": null}
{"question": "do picks come in many shapes and sizes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes, picks come in many shapes and sizes,", "extracted_entity": null}
{"question": "how many strings does a guitar typically have?", "answer": "Six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six", "six"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "what is located at the end of the guitar neck furthest from the body?", "answer": "Headstock", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Headstock", "headstock"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The headstock", "extracted_entity": null}
{"question": "what are most electric guitar bodies made of?", "answer": "Wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wood", "wood"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "solid wood, plywood, or a combination of both", "extracted_entity": null}
{"question": "why do some people believe that left-handed people should learn to play guitars as right-handed people do?", "answer": "to standardise the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to standardise the instrument"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "because they are more likely to be successful if they", "extracted_entity": null}
{"question": "how old is the oldest known representation of a guitar-like intrument being played?", "answer": "3,300 years old", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3,300 years old"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "3000 years old. A picture of a", "extracted_entity": null}
{"question": "what is the point called that is bolted or glued to the body of the guitar?", "answer": "Neck Joint", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Neck Joint", "Neck joint or 'Heel'"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "bridge", "extracted_entity": null}
{"question": "did Becquerel study science at the \u00c9cole des Ponts et Chauss\u00e9es?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he studied at the \u00c9cole Polytechn", "extracted_entity": "Frederick_William,_Duke_of_Brunswick-Wolfenb\u00fcttel"}
{"question": "did he become chief engineer in the Department of Bridges and Highways in 1892?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he became chief engineer in the Department of", "extracted_entity": null}
{"question": "was Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Becquerel;France"}
{"question": "whom did he share the Nobel Prize with?", "answer": "Pierre and Marie Curie", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pierre and Marie Curie"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He shared the Nobel Prize with his wife, a", "extracted_entity": "Nobel_Prize"}
{"question": "what is the SI unit for radioactivity called?", "answer": "becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["becquerel", "the becquerel (Bq)"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "becquerel", "extracted_entity": null}
{"question": "when Becquerel discovered radioactivity, whose work did he investigate?", "answer": "Wilhelm Conrad R\u00f6ntgen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilhelm Conrad R\u00f6ntgen"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel discovered radioactivity while investigating the work", "extracted_entity": "Becquerel"}
{"question": "where are there craters named Becquerel?", "answer": "Moon and Mars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moon and Mars", "on the moon and on mars"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "on the moon.", "extracted_entity": null}
{"question": "why did Bequerel win the Nobel Prize in Physics?", "answer": "discovery of spontaneous radioactivity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["discovery of spontaneous radioactivity", "was the discoverer of radioactivity"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Bequerel won the Nobel Prize in Physics because", "extracted_entity": "Becquerel;Nobel_Prize_in_Physics"}
{"question": "was Isaac Newton British?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, he was English."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, he was. He was born in Wool", "extracted_entity": "Isaac_Newton;United_Kingdom"}
{"question": "was Isaac Newton religious?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear.", "Yes, he was highly religious, though an unorthodox Christian."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes, he was a Christian", "extracted_entity": "Isaac_Newton"}
{"question": "did Isaac Newton die in 1898?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, Newton died in his sleep in London on 31 March 1727."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "No, he died in 1726/27", "extracted_entity": "Isaac_Newton"}
{"question": "where was Isaac Newton born?", "answer": "At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "He was born at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire.", "Woolsthorpe Manor in Woolsthorpe-by-Colsterworth"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was born in Woolsthorpe, Lincoln", "extracted_entity": "Isaac_Newton"}
{"question": "who shares credit with Isaac Newton for developing calculus?", "answer": "Gottfried Leibniz.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gottfried Leibniz.", "Newton shares the credit with Gottfried Leibniz for the development of the differential and integral calculus."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Leibniz", "extracted_entity": "Isaac_Newton;Calculus"}
{"question": "when did Isaac Newton discover the generalized binomial theorem?", "answer": "In 1665.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1665."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton discovered the generalized binomial theorem in", "extracted_entity": "Isaac_Newton"}
{"question": "did the Occult influence Newton's theory of gravitation?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "If Newton had not relied on the occult idea of action at a distance, he might not have developed his theory of gravity."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, the Occult was a source of inspiration", "extracted_entity": "Isaac_Newton"}
{"question": "were Newton's religious views consistent with Anglicanism?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, a conflict between Newton's religious views and Anglican orthodoxy was averted."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was a devout believer in the Anglican Church", "extracted_entity": "Anglicanism"}
{"question": "which fruit inspired Isaac Newton's theory of gravitation?", "answer": "The apple.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The apple.", "Isaac Newton's theory of gravitation was inspired by watching the fall of an apple from a tree."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "An apple", "extracted_entity": "Isaac_Newton"}
{"question": "was Sir Isaac Newton an English physicist and mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": "Isaac_Newton;England"}
{"question": "did he build the first practical reflecting telescope?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Reflecting_telescope"}
{"question": "was Isaac Newton educated at The King's Schol, Grantham?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Isaac_Newton;Grantham"}
{"question": "when was the Principia published?", "answer": "1687", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1687"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1687.", "extracted_entity": null}
{"question": "where is there a Newton statue on display?", "answer": "Oxford University Museum of Natural History ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oxford University Museum of Natural History ", "Oxford University Museum of Natural History"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "In the lobby of the Harvard-Smithsonian Center for", "extracted_entity": null}
{"question": "where was Isaac Newton buried?", "answer": "Westminster Abbey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westminster Abbey"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was buried in Westminster Abbey, London, England", "extracted_entity": "Isaac_Newton"}
{"question": "what religion did Isaac Newton follow?", "answer": "he never made a public declaration of his private faith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he never made a public declaration of his private faith", "unorthodox Christian"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was a Christian and he was a deist", "extracted_entity": "Isaac_Newton"}
{"question": "what food gave Isaac Newton clues to his theory of gravity?", "answer": "apple", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apple"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "an apple.", "extracted_entity": "Isaac_Newton"}
{"question": "who was a farmer and later a land surveyor for the government?", "answer": " LeRoy Pollock was a farmer and later a land surveyor for the government.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" LeRoy Pollock was a farmer and later a land surveyor for the government."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Charles Mason", "extracted_entity": null}
{"question": "regarding this topic, what did David Alfaro Siqueiros do?", "answer": "Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "David Alfaro Siqueiros was a Mexican social", "extracted_entity": "David_Alfaro_Siqueiros"}
{"question": "who used hardened brushes, sticks,?", "answer": "Jackson Pollock used hardened brushes, sticks.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jackson Pollock used hardened brushes, sticks."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "the first people to use tools", "extracted_entity": null}
{"question": "when were his papers donated by Lee Krasner?", "answer": "In 1983.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1983."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "The papers of Lee Krasner were donated by", "extracted_entity": "Lee_Krasner"}
{"question": "did Free Jazz feature a Pollock painting as its cover artwork?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": "Free_jazz;Jackson_Pollock"}
{"question": "was Paul Jackson Pollock an influential American painter as well as a major figure in the abstract expressionist movement?", "answer": "Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": "Jackson_Pollock;United_States;Abstract_expressionism"}
{"question": "give an example of the origins of the term action painting. ?", "answer": "Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Action painting was a style of abstract expressionism,", "extracted_entity": "Action_painting"}
{"question": "during his lifetime, did Pollock enjoy considerable fame and notoriety?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jackson_Pollock"}
{"question": "was he regarded as a mostly reclusive artist?", "answer": "Yes, he was regarded as a mostly reclusive artist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, he was regarded as a mostly reclusive artist. "], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes. he was a loner.", "extracted_entity": null}
{"question": "is the Wisma building the tallest building in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes,"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes, it is the tallest building in Indonesia.", "extracted_entity": "Indonesia"}
{"question": "is there a chinese community in jakarta?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "here are many Chinese Indonesians living in Jakarta.", "extracted_entity": "Jakarta"}
{"question": "is Jakarta the 12th largest city in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Jakarta"}
{"question": "what is the capital of Indonesia?", "answer": "Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta", "The capital of Indonesia is Jakarta."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": "Indonesia"}
{"question": "what is the official name of Jakarta?", "answer": "Daerah Khusus Ibukota Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Daerah Khusus Ibukota Jakarta", "Daerah Khusus."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "pecial Capital Region of Jakarta", "extracted_entity": "Jakarta"}
{"question": "how is the climate in the city?", "answer": "Jakarta has a hot and humid equatorial/tropical climate ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta has a hot and humid equatorial/tropical climate ", "The city is hot and humid."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "it is hot in summer and cold in winter.", "extracted_entity": null}
{"question": "where does the name of the city Jakarta come from?", "answer": "The name Jakarta is derived from the Sanskrit word \"Jayakarta.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name Jakarta is derived from the Sanskrit word \"Jayakarta.\""], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is named after the Javanese word Jay", "extracted_entity": "Jakarta"}
{"question": "how is the city's governor chosen?", "answer": "election", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["election", "Through election."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the city's governor is chosen by the city council", "extracted_entity": null}
{"question": "why does Jakarta suffer frequent flooding?", "answer": "Because it is located approximately eight meters above the sea level", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is located approximately eight meters above the sea level", "The fact that the northern part of Jakarta lies on a plain, approximately eight meters above the sea level."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is located in the flood plain of the C", "extracted_entity": "Jakarta"}
{"question": "is the the biggest university in Jakarta the University of Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Jakarta;University_of_Indonesia"}
{"question": "is Jakarta a city?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es, Jakarta is a city.", "extracted_entity": "Jakarta"}
{"question": "is the most important river the Ciliwung River?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes. The most important river is the Cili", "extracted_entity": null}
{"question": "where is Jakarta located?", "answer": "northwest coast of Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["northwest coast of Java", "On the northwest coast of Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is located on the northwest coast of Java,", "extracted_entity": "Jakarta"}
{"question": "who created Monas Park?", "answer": "General Herman Willem Deandels", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General Herman Willem Deandels", "Dutch Governor General Herman Willem Deandels"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Dr. Fathurahman Malik (A", "extracted_entity": null}
{"question": "do mikrolets travel on the main roads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No, they travel on the back roads", "extracted_entity": null}
{"question": "what contributes to frequent flooding in Jakarta?", "answer": "Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "It lies on a plain, approximately eight meters above the sea level"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "he city's rapid expansion", "extracted_entity": "Jakarta"}
{"question": "did the economy improve in Jakarta in 2007?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jakarta"}
{"question": "in AD 39, King Purnawarman established Sunda Pura as a new capital city for the kingdom, located at the northern coast of where?", "answer": "Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sumatra", "extracted_entity": "Pest,_Hungary"}
{"question": "in 1602, the British East India Company's first voyage, commanded by Sir who, arrived in Aceh and sailed on to Banten where they were allowed to build a trading post?", "answer": "James Lancaster", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James Lancaster"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "James Lancaster", "extracted_entity": "East_India_Company;Aceh;Banten"}
{"question": "who began control of migration to the city in order to stem the overcrowding and poverty?", "answer": "Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the city began to control migration to the city in", "extracted_entity": null}
{"question": "did Jayawikarta's soldiers attack the Dutch fortress?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Netherlands"}
{"question": "did Suharto resign as president?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Suharto"}
{"question": "was the Jakarta area part of the fourth century Indianized kingdom of Tarumanagara?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es, the Jakarta area was part of the fourth", "extracted_entity": "Jakarta;Tarumanagara"}
{"question": "do other landmarks include the Istiqlal Mosque as well as Jakarta Cathedral?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Istiqlal_Mosque,_Jakarta;North_Jakarta"}
{"question": "give an example of the many Sukarno era monuments in the city. ?", "answer": "The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. "], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sukarno era monuments in the city, there", "extracted_entity": null}
{"question": "is Jakarta the country's economic, cultural and political center?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es, Jakarta is the country's economic, cultural", "extracted_entity": "Jakarta"}
{"question": "is James Watt French?", "answer": "No, he was Scottish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, he was Scottish.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "No, he is Scottish.", "extracted_entity": "James_Watt"}
{"question": "did James Watt improve an engine?", "answer": "Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "Yes"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Watt"}
{"question": "is there a statue of Watt in the White House?", "answer": "No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": "James_Watt;White_House"}
{"question": "what was James Watt's father's job?", "answer": "His father was a shipwright, ship owner and contractor, and served as the town's chief baillie.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father was a shipwright, ship owner and contractor, and served as the town's chief baillie."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt's father's job was a ship's", "extracted_entity": "James_Watt"}
{"question": "when did Watt retire?", "answer": "In 1800.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800.", "Watt retired in 1800."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "in 1794", "extracted_entity": null}
{"question": "where is the James Watt Memorial College?", "answer": "In Greenock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Greenock.", "The James Watt Memorial  is in Greenock."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt Memorial College is located in Karratha", "extracted_entity": "James_Watt"}
{"question": "why is the watt named after Watt?", "answer": "The watt is named after him for his contributions to the development of the steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The watt is named after him for his contributions to the development of the steam engine.", "The watt is named after James Watt for his contributions to the development of the steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "because he invented the steam engine.", "extracted_entity": "James_Watt"}
{"question": "where did Watt attend university?", "answer": "Watt did not attend school regularly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt did not attend school regularly.", "He did not attend a university."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "University of Glasgow", "extracted_entity": null}
{"question": "what did he invent?", "answer": "He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "he invented the light bulb.", "extracted_entity": null}
{"question": "what happened in the early 20th century?", "answer": "The koalas of South Australia were largely exterminated.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "the first world war", "extracted_entity": null}
{"question": "do Koalas have a slow metabolism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, they do. That's why they sleep", "extracted_entity": null}
{"question": "did eucalypt forests grow in the place of rainforests?", "answer": "Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eucalypt;Rainforest"}
{"question": "does the koala fill the same ecological role as the sloth of South America?", "answer": "The koala fills the same ecological role as the sloth of South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koala fills the same ecological role as the sloth of South America."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "No. The sloth is a herbivore", "extracted_entity": "Koala;South_America"}
{"question": "is the long term viability of the koala therefore threatened by genetic weakness?", "answer": "The long term viability of the koala is therefore threatened by genetic weakness.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The long term viability of the koala is therefore threatened by genetic weakness."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, it is threatened by genetic weakness.", "extracted_entity": null}
{"question": "is the koala found in coastal regions of eastern and southern Australia, from near Adelaide to the southern part of Cape York Peninsula?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes, the koala is found in coastal regions", "extracted_entity": "Koala;Australia;Adelaide;Cape_York_Peninsula"}
{"question": "do populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands?", "answer": "Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, but only in areas with enough moisture to", "extracted_entity": null}
{"question": "were the koalas of South Australia largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock?", "answer": "The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes. The koalas of South Australia were", "extracted_entity": "South_Australia;Victoria_(Australia)"}
{"question": "is Korean the official language of Korea?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es, Korean is the official language of both North", "extracted_entity": "Korean_language;South_Korea"}
{"question": "is the word \"Korean\" derived from Goryeo?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No. The word \"Korean\" is derived", "extracted_entity": "Goryeo"}
{"question": "are all dialects of Korean similar to each other?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es, they are similar to each other.", "extracted_entity": "Korean_language"}
{"question": "about how many Koreans speakers are there?", "answer": "78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["78 million", "78 million Korean speakers"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "75 million", "extracted_entity": null}
{"question": "older English sources used the name \"Korean\" to refer to what?", "answer": "language, country, and people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["language, country, and people"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "the people of the Korean peninsula", "extracted_entity": null}
{"question": "what is the official language of North Korea?", "answer": "Korean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": "North_Korea"}
{"question": "why may English translation of Koreans adjectives misleadingly suggest that they are verbs?", "answer": "English does not have an identical grammatical category", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["English does not have an identical grammatical category"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The Korean language does not have the concept of a", "extracted_entity": null}
{"question": "where do the majority of loanwords other than Sino-Korean come from?", "answer": "modern times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern times", "modern times, 90% of which are from English"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Japanese", "extracted_entity": "Sino-Korean_vocabulary"}
{"question": "how many Korean speakers are there?", "answer": "There are about 78 million Korean speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are about 78 million Korean speakers."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "bout 80 million people, mostly of Korean descent", "extracted_entity": "Korean_language"}
{"question": "how many parts of speech does the Korean language contain?", "answer": "The Korean Language contains nine parts of speech. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Korean Language contains nine parts of speech. "], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Korean_language"}
{"question": "how many verb paradigms are there in Korean?", "answer": "There are seven verb paradigms or speech levels in Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are seven verb paradigms or speech levels in Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Korean_language"}
{"question": "what is the Sino-Korean system of numbers based on?", "answer": "The Sino-Korean system is nearly entirely based on the Chinese numerals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sino-Korean system is nearly entirely based on the Chinese numerals."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "the Chinese number system", "extracted_entity": null}
{"question": "what is another term for Korean adjectives?", "answer": "Adjectives are also known as \"descriptive verbs\" or \"stative verbs\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adjectives are also known as \"descriptive verbs\" or \"stative verbs\"."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "adverbs", "extracted_entity": null}
{"question": "is modern Korean written in columns or rows?", "answer": "It is written in rows.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is written in rows."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "columns", "extracted_entity": null}
{"question": "what percentage of the Korean language does Jeong Jae-do estimate to be Sino-Korean?", "answer": "Sino-Korean makes up 30% of the Korean language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sino-Korean makes up 30% of the Korean language."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "0%", "extracted_entity": "Korean_language;Sino-Korean_vocabulary"}
{"question": "do Chinese and Japanese have spaces between words?", "answer": "No, Chinese and Japanese do not have spaces between words.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese and Japanese do not have spaces between words."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinese_language;Japanese_language"}
{"question": "from which languages is Korean descended?", "answer": "Korean is descended from Old Korean, Middle Korean and Modern Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean is descended from Old Korean, Middle Korean and Modern Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean is descended from Mongolian, Japanese, Chinese", "extracted_entity": null}
{"question": "are there seven verb paradigms or speech levels in Korean?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Korean_language"}
{"question": "is Modern Korean written with spaces between words?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "o. Korean is a spaceless language, like", "extracted_entity": "Korean_language"}
{"question": "are the Korean names for the language based on the names for Korea used in North as well as South Korea?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": ", they are not. The North Korean name", "extracted_entity": "Korean_language;Korea;North_Korea;South_Korea"}
{"question": "is it not also one of the two official languages of the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is one of the two official languages", "extracted_entity": "Yanbian_Korean_Autonomous_Prefecture;People's_Republic_of_China"}
{"question": "is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists?", "answer": "The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "o. It is not a different language. It", "extracted_entity": "Jeju_language;South_Korea"}
{"question": "is it also one of the two official languages in the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No. It is not one of the two official", "extracted_entity": "Yanbian_Korean_Autonomous_Prefecture;People's_Republic_of_China"}
{"question": "is Kuala Lumpur the capitol of Malaysia? disfluent?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "yes, it is. no, it isn't", "extracted_entity": "Kuala_Lumpur;Malaysia"}
{"question": "is Kuala Lumpur in the Selangor state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes, Kuala Lumpur is in the Selangor", "extracted_entity": "Kuala_Lumpur;Selangor"}
{"question": "what's the population of Kuala lumpur?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1.6 million people.", "extracted_entity": null}
{"question": "what language do they speak in Kuala Lumpur?", "answer": "Bahasa Melayu", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bahasa Melayu"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": "Kuala_Lumpur"}
{"question": "what does the name \"Kuala Lumpur\" mean?", "answer": "muddy confluence, \"muddy estuary,\" and \"muddy city\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["muddy confluence, \"muddy estuary,\" and \"muddy city\""], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur means \"muddy confluence\", referring", "extracted_entity": "Kuala_Lumpur"}
{"question": "what airports does Kuala Lumpur have?", "answer": "Kuala Lumpur International Airport and Subang International Airport", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kuala Lumpur International Airport and Subang International Airport"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur has 1 airports: Kuala Lumpur International", "extracted_entity": "Kuala_Lumpur"}
{"question": "how long was Kuala Lumpur occupied by the Japanese?", "answer": "42 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42 months"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "42-1945", "extracted_entity": "Kuala_Lumpur;Japan"}
{"question": "how many seasons does Kuala Lumpur experience?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": "Kuala_Lumpur"}
{"question": "when were the colonial buildings in Kuala Lumpur constructed?", "answer": "toward the end of the 19th and early 20th centuries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["toward the end of the 19th and early 20th centuries"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1880s", "extracted_entity": "Kuala_Lumpur"}
{"question": "is it a host city for the Formula One World Championship?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Formula_One"}
{"question": "is the central market located in the proximity of the pertama complex?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "No, the central market is about 10-", "extracted_entity": null}
{"question": "is it home to the parliament of malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The Parliament of Malaysia is the national legislature of Malaysia", "extracted_entity": "Malaysia"}
{"question": "when did kuala lumpur become a federal territory of malaysia?", "answer": "February 1, 1974", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["February 1, 1974"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1974", "extracted_entity": "Malaysia"}
{"question": "what is the city's population?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1.3 million", "extracted_entity": null}
{"question": "what is the official religion in the country?", "answer": "Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Islam"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Islam is the official religion of the country", "extracted_entity": null}
{"question": "is there a more industrialized region in malaysia?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The more industrialized region in Malaysia is the Pen", "extracted_entity": null}
{"question": "after the 2008 elections, what party had the most parliament seats?", "answer": "DAP", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["DAP"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The Conservative Party.", "extracted_entity": null}
{"question": "what is the name of the event that draws the worlds top riders to malaysia?", "answer": "KL Grand Prix CSI 5*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["KL Grand Prix CSI 5*"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the event is the malaysia supermoto championship", "extracted_entity": "Malaysia"}
{"question": "wasn't Leonardo da Vinci valued as an engineer?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "He was valued as an engineer, but he was", "extracted_entity": "Leonardo_da_Vinci"}
{"question": "wasn't Leonardo da Vinci born on April 15?", "answer": "Yes, Leonardo da Vinci was born on April 15.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Leonardo da Vinci was born on April 15."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Leonardo_da_Vinci"}
{"question": "who left Verroccio's studio?", "answer": "Leonardo da Vinci left Verroccio's studio.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo da Vinci left Verroccio's studio."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo Da Vinci", "extracted_entity": null}
{"question": "when did Baldassare Castiglione, author of Il Cortegiano ( \" The Courtier \" ), write?", "answer": "1528", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1528"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1478", "extracted_entity": "Baldassare_Castiglione"}
{"question": "when did he flee to Venice?", "answer": "In 1499.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1499."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1204", "extracted_entity": "Venice"}
{"question": "was Leonardo a contemporary of Botticelli, Domenico Ghirlandaio as well as Perugino?", "answer": "Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "es, Leonardo was a contemporary of all three.", "extracted_entity": "Leonardo_da_Vinci;Sandro_Botticelli;Domenico_Ghirlandaio;Pietro_Perugino"}
{"question": "is the larger work now almost universally attributed to Leonardo?", "answer": "Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Yes, although it is still sometimes called the \"", "extracted_entity": null}
{"question": "has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Polymath"}
{"question": "is he widely considered to be one of the greatest painters of all time and perhaps the most diversely talented person ever to have lived?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes. Leonardo Da Vinci.", "extracted_entity": null}
{"question": "according to art historian Helen Gardner, were the scope and depth of his interests without precedent and ``his mind and personality seem to us superhuman, the man himself mysterious and remote''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Michelangelo Buonarroti (1475-", "extracted_entity": null}
{"question": "are lobsters invertebrates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no, they are crustaceans", "extracted_entity": null}
{"question": "are lobsters kosher?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Judaism"}
{"question": "do lobsters have blue blood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lobster"}
{"question": "how many legs do lobsters have?", "answer": "10.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10.", "10"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "5 pairs of legs", "extracted_entity": "Lobster"}
{"question": "where are lobsters found?", "answer": "In all oceans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In all oceans.", "In all oceans"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "they are found in the sea", "extracted_entity": null}
{"question": "what is the mean level of mercury in American lobsters?", "answer": "0.31 ppm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["0.31 ppm"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "0.16 parts per million", "extracted_entity": "United_States"}
{"question": "do lobsters feel pain?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no, they are invertebrates and do not", "extracted_entity": null}
{"question": "did the Moche people worship lobsters?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no, the Moche people did not worship lob", "extracted_entity": "Moche_culture"}
{"question": "what is the caridoid escape reaction?", "answer": "Swimming backwards quickly by curling and uncurling their abdomen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swimming backwards quickly by curling and uncurling their abdomen.", "When lobsters swim backwards quickly by curling and uncurling their abdomen"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "the caridoid escape reaction is when a crab", "extracted_entity": null}
{"question": "were the recitations of the Ancient Greeks accompanied by lyre playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ancient_Greece"}
{"question": "does a classical lyre have a hollow body?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Lyre"}
{"question": "is the lyre a stringed musical instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lyre"}
{"question": "which constellation is said to resemble a lyre shape?", "answer": "Lyra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lyra"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "lyra", "extracted_entity": null}
{"question": "how many raised arms are extending from the sound-chest of a classical lyre?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "two", "extracted_entity": null}
{"question": "what were the strings of a classical lyre made of?", "answer": "Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gut"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "catgut", "extracted_entity": null}
{"question": "is the foot pedal framework for a piano called a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No. The foot pedal framework is called a ly", "extracted_entity": "Lyre"}
{"question": "where was the deepest note of the classical lyre in relation to the player's body?", "answer": "It was farthest from the player's body.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was farthest from the player's body."], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "under the left foot", "extracted_entity": null}
{"question": "does a standard piano have fewer strings than a harp?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Piano"}
{"question": "is Malay the official language of Iraq?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Malay_language;Iraq"}
{"question": "could Malay have originated from Sumatra island?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "it is possible. However, the Malay language was", "extracted_entity": "Malay_language;Sumatra"}
{"question": "is Malay an agglutinative language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": "Malay_language"}
{"question": "what family is Malay a member of?", "answer": "Austronesian family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austronesian family", "Austronesian."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a member of the Austronesian family", "extracted_entity": "Malays_(ethnic_group)"}
{"question": "what is the basic word order in Malay?", "answer": "Subject Verb Object", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Subject Verb Object", "Subject Object Verb."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Subject-Verb-Object (SVO)", "extracted_entity": "Malay_language"}
{"question": "what languages has Malay borrowed words from?", "answer": "Sanskrit, Arabic and English", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sanskrit, Arabic and English", "Arabic, Sanskrit, Tamil, Persian, Portuguese, Dutch, certain Chinese dialects, and English."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Arabic, Sanskrit, Persian, Portuguese, Dutch", "extracted_entity": "Malay_language"}
{"question": "are there many words in Malay that use natural gender?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are many words in Malay that use", "extracted_entity": "Malay_language"}
{"question": "how are new words formed in Malay?", "answer": "three methods.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three methods.", "Affixation, composition, and reduplication."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "new words are formed by adding prefixes and suffixes", "extracted_entity": "Malay_language"}
{"question": "is Malay in the Austronesian family of languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, Malay is in the Austronesian family", "extracted_entity": "Malay_language;Austronesian_languages"}
{"question": "is there one negation word in Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Malay_language"}
{"question": "how many parts of speech are there in Malay?", "answer": "Four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Four"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "There are 8 parts of speech in Malay.", "extracted_entity": "Malay_language"}
{"question": "what is the national language in Malaysia?", "answer": "Malay ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malay "], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": "Malaysia"}
{"question": "do linguists consider Malay to be a single language?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malay_language"}
{"question": "can speakers of modern Malay understand Old Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. The language has changed significantly over the last", "extracted_entity": "Malay_language;History_of_the_Malay_language"}
{"question": "what language besides Malay is in the Austronesia language family?", "answer": "Malagasy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malagasy"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "the answer is Austronesian languages", "extracted_entity": "Malay_language;Austronesia"}
{"question": "wasn't Malay language found in Sumatra?", "answer": "The Malay language was found in Sumatra.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Malay language was found in Sumatra."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, it was found in Sumatra.", "extracted_entity": "Malay_language;Sumatra"}
{"question": "wasn't Malay language written using Pallava?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, Malay language was written using Pallava.", "extracted_entity": "Malay_language;Pallava_dynasty"}
{"question": "was Malay language written using Pallava?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, Malay language was written using Pallava.", "extracted_entity": "Malay_language;Pallava_alphabet"}
{"question": "had the language already been used as the lingua franca throughout the archipelago since the 15th century?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no. it was used by only a small number", "extracted_entity": null}
{"question": "do another distinguishing feature of Malay ( include Indonesian Malay )?", "answer": "Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan)."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "The language is spoken in Malaysia and Indonesia, and", "extracted_entity": "Malay_Indonesian"}
{"question": "is Malay spoken as local dialect of ethnic Malays?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Malay_language;Malays_(ethnic_group)"}
{"question": "give an example of the tributary states of the Majapahit kingdom in eastern Java. ?", "answer": "Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "ajapahit was a Javanese Hindu", "extracted_entity": "Majapahit;Java"}
{"question": "is Malay the one of the most widely spoken languages in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is the 4th most spoken", "extracted_entity": "Malay_language"}
{"question": "was it derived from Latin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Latin"}
{"question": "is Melbourne the capital and most populous city in the state of Victoria?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Yes, Melbourne is the capital and most populous city", "extracted_entity": "Melbourne;Victoria_(Australia)"}
{"question": "is Melbourne home to Australia's busiest seaport?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Yes, the Port of Melbourne is the busiest se", "extracted_entity": "Melbourne"}
{"question": "does Melbourne have a large international student community?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": "Melbourne"}
{"question": "what is a resident of Melbourne known as?", "answer": "Melburnian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Melburnian", "A \"Melburnian\"."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melburnian", "extracted_entity": "Melbourne"}
{"question": "when was Melbourne officially declared as a city?", "answer": "1847", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1847", "In 1847."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "30 April 1847", "extracted_entity": "Melbourne"}
{"question": "how many sister cities does the City of Melbourne have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six", "Six."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": "City_of_Melbourne"}
{"question": "where is Melbourne situated?", "answer": "boundary of the very hot inland areas and the cold southern ocean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["boundary of the very hot inland areas and the cold southern ocean"], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "in the state of Victoria, Australia", "extracted_entity": "Melbourne"}
{"question": "how many domestic tourists visit Melbourne?", "answer": "7.6 million domestic visitors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7.6 million domestic visitors", "7.6 million."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "30 million", "extracted_entity": "Melbourne"}
{"question": "what religion holds majority in Melbourne?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "Christianity."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": "Melbourne"}
{"question": "did Albert Einstein keep a photograph of Faraday on his study wall?", "answer": "Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Albert_Einstein;Michael_Faraday"}
{"question": "did Faraday marry Sarah Barnard?", "answer": "Yes, Faraday married Sarah Barnard.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday married Sarah Barnard.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday also discover the laws of electrolysis?", "answer": "Yes, Faraday also discovered the laws of  electrolysis.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday also discovered the laws of  electrolysis.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No. Faraday discovered the laws of electrolysis", "extracted_entity": null}
{"question": "who discovered benzene?", "answer": "Michael Faraday discovered benzene.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday discovered benzene.", "Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Kekule", "extracted_entity": "Benzene"}
{"question": "where was Michael Faraday born?", "answer": "Michael Faraday was born in Newington Butts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday was born in Newington Butts.", "Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London, England", "extracted_entity": "Michael_Faraday"}
{"question": "what is the Faraday effect?", "answer": "The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "The plane of polarisation of linearly polarised light can be rotated by the application of an exernal magnetic field aligned in the direction the light is moving."], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "It is a physical magneto-optical phenomenon.", "extracted_entity": "Faraday_effect"}
{"question": "did Faraday discover diamagnetism?", "answer": "Yes, Faraday discovered diamagnetism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday discovered diamagnetism.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Michael_Faraday"}
{"question": "how many siblings did Faraday have?", "answer": "Faraday had three siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday had three siblings.", "two"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "4: 2 brothers and 2 sisters", "extracted_entity": null}
{"question": "where is Michael Faraday buried?", "answer": "Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "in the dissenters' section of Highgate Cemetery"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Highgate Cemetery", "extracted_entity": "Michael_Faraday"}
{"question": "is faraday best known for his work with kinematics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes. Faraday's work with kinematics was", "extracted_entity": null}
{"question": "was he active in the area now called environmental science?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No. He was a geographer.", "extracted_entity": "Environmental_science"}
{"question": "did he discover benzene?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "who was his mentor?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "His mentor was his father.", "extracted_entity": null}
{"question": "who was Faraday's wife?", "answer": "Sarah Barnard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sarah Barnard"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did he advise the british government on the production of chemical weapons for use in the crimean war?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "He advised the British Government on the production of chemical", "extracted_entity": "United_Kingdom"}
{"question": "did faraday have strong maritime interests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he was a strong mariner", "extracted_entity": null}
{"question": "does the charge reside on the interior of a charged conductor?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, it resides on the surface of the conductor", "extracted_entity": null}
{"question": "wasn't Michelangelo abstemious?", "answer": "Michelangelo was abstemious.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michelangelo was abstemious."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Michelangelo abstemious", "extracted_entity": "Michelangelo"}
{"question": "wasn't Michelangelo a homosexual?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Michelangelo"}
{"question": "when did Filippo Strozzi sell it to Francis I?", "answer": "In 1529.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1529."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1530", "extracted_entity": "Francis_I_of_France"}
{"question": "when was his house demolished?", "answer": "1874", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1874"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "when was his house demolished?", "extracted_entity": null}
{"question": "was Michelangelo's mother Francesca di Neri del Miniato di Siena?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": "Michelangelo;Siena"}
{"question": "give an example of the most renowned works of the Renaissance. ?", "answer": "The Statue of David, completed by Michelangelo in 1504", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Statue of David, completed by Michelangelo in 1504"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "ichelangelo's David, Botticelli's Birth", "extracted_entity": "Renaissance"}
{"question": "was Michelangelo's output in every field during his long life prodigious?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michelangelo"}
{"question": "is English the language most often spoken in Montreal?", "answer": "The language most spoken at home in the city is French.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The language most spoken at home in the city is French.", "No, the language most spoken in Montreal is French."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes, it is. English is the most common", "extracted_entity": "English_language;Montreal"}
{"question": "is Montreal's economy the third largest of all cities in Canada?", "answer": "No, Montreal's economy is the second largest of all cities in Canada.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Montreal's economy is the second largest of all cities in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "No, it's the second largest.", "extracted_entity": "Montreal;Canada"}
{"question": "is the most popular sport in Montreal ice hockey?", "answer": "yes, The most popular sport in Montreal is ice hockey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, The most popular sport in Montreal is ice hockey.", "Yes, the most popular sport in Montreal is ice hockey."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Montreal"}
{"question": "how many international airports does Montreal have?", "answer": "Montreal has two international airports.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montreal has two international airports."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": "Montreal"}
{"question": "who is the Mayor of Montreal?", "answer": "The mayor is G\u00c3\u00a9rald Tremblay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mayor is G\u00c3\u00a9rald Tremblay."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Val\u00e9rie Plante", "extracted_entity": "Mayor_of_Montreal"}
{"question": "what is the name of the largest church in Montreal?", "answer": "The largest church in Montreal is named Saint Joseph's Oratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The largest church in Montreal is named Saint Joseph's Oratory.", "Saint Joseph's Oratory is the largest church in Montreal."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Notre-Dame Basilica", "extracted_entity": "Montreal"}
{"question": "where was much of Montreal's industry during the late 19th and early-to-mid 20th century?", "answer": "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal's industry was located in the East End", "extracted_entity": "Montreal"}
{"question": "does Montreal contain the largest church in Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Montreal contains the largest church in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes. It is the Basilica of Notre-D", "extracted_entity": "Montreal;Canada"}
{"question": "what is the largest primarily French-speaking city in the western world?", "answer": "Paris is the largest primarily French-speaking city in the western world.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris is the largest primarily French-speaking city in the western world.", "The largest primarily French-speaking city in the western world is Paris."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "what is the largest primarily French-speaking city in the", "extracted_entity": "Western_world"}
{"question": "is Nairobi the capital of Kenya?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes, Nairobi is the capital of Kenya.", "extracted_entity": "Nairobi;Kenya"}
{"question": "was Nairobi founded in 1899?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi"}
{"question": "is the current estimated population of Nairobi about 6 milion?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes, the city proper had a population of", "extracted_entity": "Nairobi"}
{"question": "what is the current estimated population of Nairobi?", "answer": "About 3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["About 3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4,397,073", "extracted_entity": "Nairobi"}
{"question": "in what year was Nairobi founded?", "answer": "1899", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1899"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1899", "extracted_entity": "Nairobi"}
{"question": "which embassy in Nairobi was bombed in August 1998?", "answer": "The United States Embassy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The United States Embassy"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "US embassy", "extracted_entity": "Nairobi"}
{"question": "how many civilians died in the 1998 U.S. embassy bombing?", "answer": "Over two hundred", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over two hundred"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "224", "extracted_entity": "United_States"}
{"question": "how many trades can the NSE make per day?", "answer": "10 million trades", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10 million trades"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1000 trades per day", "extracted_entity": "National_Stock_Exchange_of_India"}
{"question": "about how many people are estimated to live in Nairobi?", "answer": "3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4.4 million", "extracted_entity": "Nairobi"}
{"question": "the district is bordered to the southwest by Uhuru Park and where?", "answer": "Central Park", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "The district is bordered to the southwest by Uhuru", "extracted_entity": null}
{"question": "when did the Catholic University of Eastern Africa which obtain its Letter of Interim Authority?", "answer": "In 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1989."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "12th September 1984.", "extracted_entity": null}
{"question": "is Central Park adjacent to Uhuru Park?", "answer": "Central Park is adjacent to Uhuru Park.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park is adjacent to Uhuru Park."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": "Central_Park"}
{"question": "give an example of the highest growth rates of any city in Africa. ?", "answer": " The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "he highest growth rate of any city in Africa is", "extracted_entity": "Africa"}
{"question": "give an example of the most prominent cities in Africa politically and financially. ?", "answer": "Nairobi is now one of the most prominent cities in Africa politically and financially", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is now one of the most prominent cities in Africa politically and financially"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "he most prominent cities in Africa politically and financially are", "extracted_entity": "Africa"}
{"question": "is Nairobi not the capital as well as largest city of Kenya?", "answer": "Nairobi is the capital and largest city of Kenya.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is the capital and largest city of Kenya."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi is the capital as well as largest city of", "extracted_entity": "Nairobi;Kenya"}
{"question": "does the city and its surrounding area also form the Nairobi Province?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "no. The city is not a province. The", "extracted_entity": null}
{"question": "does the name ``Nairobi'' come from the Maasai phrase Enkare Nyirobi, which translates to ``the place of cool waters''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi;Maasai_language"}
{"question": "did Nikola Tesla die alone?", "answer": "Nikola Tesla died alone.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla died alone.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes, Nikola Tesla died alone", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla close friends with Mark Twain?", "answer": "Nikola Tesla was close friends with Mark Twain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla was close friends with Mark Twain.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "They were close friends and they both lived in the", "extracted_entity": "Nikola_Tesla;Mark_Twain"}
{"question": "did Nikola Tesla use a technique called picture thinking?", "answer": "Nikola Tesla used a technique called picture thinking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla used a technique called picture thinking.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nikola_Tesla"}
{"question": "when did Nikola Tesla appear on the cover of Time Magazine?", "answer": "On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "1931"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1943", "extracted_entity": "Nikola_Tesla;Time_(magazine)"}
{"question": "who became adversaries with Nikola Tesla?", "answer": "Nikola Tesla became adversaries with Edison.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla became adversaries with Edison.", "Edison"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison", "extracted_entity": "Nikola_Tesla"}
{"question": "where did Nikola Tesla move to in 1880?", "answer": "In 1880, Nikola Tesla moved to Budapest.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1880, Nikola Tesla moved to Budapest.", "Budapest"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla moved to the United States in 1880", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla's mother illiterate?", "answer": "Nikola Tesla's mother never learned to read.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla's mother never learned to read.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nikola_Tesla"}
{"question": "how many siblings did Nikola Tesla have?", "answer": "Nikola Tesla had four siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla had four siblings.", "4"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla a vegetarian?", "answer": " In his later years Tesla became a vegetarian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" In his later years Tesla became a vegetarian.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, Nikola Tesla was a vegetarian.", "extracted_entity": "Nikola_Tesla"}
{"question": "is there a monument to Tesla at Niagara Falls, New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "There is a statue of Tesla at the Niagara Falls", "extracted_entity": "Nikola_Tesla;Niagara_Falls,_New_York;New_York"}
{"question": "was Tesla an American citizen?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Croatia but he became", "extracted_entity": "United_States"}
{"question": "was Tesla on the cover of Time magazine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No. Tesla was on the cover of Time magazine", "extracted_entity": "Tesla_Motors;Time_(magazine)"}
{"question": "when did Tesla receive his last patent?", "answer": "1928", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1928"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1914", "extracted_entity": "Nikola_Tesla"}
{"question": "what languages did Tesla speak?", "answer": "Serbian, Czech, English, French, German, Hungarian, Italian, and Latin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Serbian, Czech, English, French, German, Hungarian, Italian, and Latin"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "English, German, and French", "extracted_entity": "Nikola_Tesla"}
{"question": "did Tesla win the Nobel Prize?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tesla_Motors;Nobel_Prize"}
{"question": "was Tesla rich at the time of his death?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes, he was a millionaire.", "extracted_entity": null}
{"question": "what music group was named after Tesla?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": "Nikola_Tesla"}
{"question": "consequently, what is not considered a \"serious painter\" by some contemporary artists, who often regard his work as bourgeois and kitsch?", "answer": "Rockwell ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Consequently, what is not considered a \"serious painter", "extracted_entity": "Bourgeoisie;Kitsch"}
{"question": "who spent the next 10 years painting for Look magazine?", "answer": "Norman Rockwell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norman Rockwell."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Norman Rockwell", "extracted_entity": "Look_(American_magazine)"}
{"question": "when was his last painting for the Post published?", "answer": "1963", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1963"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "2008, the year of his death", "extracted_entity": null}
{"question": "when did Rockwell marry his first wife, Irene O'Connor,?", "answer": "In 1916.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1916."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "1932", "extracted_entity": null}
{"question": "did Norman Rockwell spend the winter months as artist-in-residence at Otis College of Art as well as Design?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": "Norman_Rockwell;Otis_College_of_Art_and_Design"}
{"question": "did Norman Rockwell publish a total of 321 original covers for The Saturday Evening Post over 47 years?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Norman_Rockwell;The_Saturday_Evening_Post"}
{"question": "was Rockwell also commissioned to illustrate over 40 books including Tom Sawyer as well as Huckleberry Finn?", "answer": "Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, he was. His illustrations were used in", "extracted_entity": "Tom_Sawyer;Huckleberry_Finn"}
{"question": "do his works enjoy a broad popular appeal in the United States, where Rockwell is most famous for the cover illustrations of everyday life scenarios he created for The Saturday Evening Post magazine over more than four decades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, they do. He is most famous for", "extracted_entity": "United_States;The_Saturday_Evening_Post"}
{"question": "are among the best-known of Rockwell's works the Willie Gillis series, Rosie the Riveter (although his Rosie was reproduced less than others of the day), Saying Grace (1951), and the Four Freedoms series?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "are among the best-known of Rockwell's works", "extracted_entity": "Rosie_the_Riveter;Four_Freedoms"}
{"question": "is he also noted for his work for the Boy Scouts of America (BSA)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Boy_Scouts_of_America"}
{"question": "is the octopus a cephalopod?", "answer": "Yes, the octopus is a cephalopod.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus is a cephalopod."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no, the octopus is a mollusc.", "extracted_entity": null}
{"question": "does the octopus have two eyes?", "answer": "Yes, the octopus has two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has two eyes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, the octopus has two eyes", "extracted_entity": null}
{"question": "does the octopus have a hard beak?", "answer": "Yes, the octopus has a hard beak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has a hard beak."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no, it has a beak made of cart", "extracted_entity": null}
{"question": "where is the mouth of an octopus?", "answer": "The mouth of an octopus is at the center point of the arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mouth of an octopus is at the center point of the arms.", "The octopus' mouth is at the center point of the arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "the mouth of an octopus is on the bottom", "extracted_entity": "Octopus"}
{"question": "around how many recognized octopus species are there?", "answer": "There are around 300 recognized octopus species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are around 300 recognized octopus species."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "300", "extracted_entity": "Octopus"}
{"question": "how many arms does an octopus have?", "answer": "An octopus has four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An octopus has four pairs of arms.", "The octopus has four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "8", "extracted_entity": "Octopus"}
{"question": "what is the range of lifespans of the octopus?", "answer": "Octopuses can live from six months to five years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Octopuses can live from six months to five years.", "The octopus has a short lifespan."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "1-4 years", "extracted_entity": "Octopus"}
{"question": "what is an example of something an octopus learned in a research experiment?", "answer": "In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "An octopus learned to solve mazes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "An octopus learned to distinguish between different shapes of", "extracted_entity": null}
{"question": "how can an octopus defend against predators?", "answer": "To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "They can hide, flee, expel ink, or use camouflage."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "The octopus has a venomous bite, and", "extracted_entity": "Octopus"}
{"question": "are Octopus highly intelligent?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes. In fact, they are so smart that", "extracted_entity": null}
{"question": "do males use a specialized arm called a hectocotylus to insert spermatophores?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, males use a specialized arm called a hect", "extracted_entity": "Spermatophore"}
{"question": "do the octopuses in the less familiar Cirrina suborder have two fins and an internal shell?", "answer": "Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no, they have four fins and an external shell", "extracted_entity": null}
{"question": "do octopuses have two eyes and four pairs of arms?", "answer": "Yes, octopuses have two eyes and four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, octopuses have two eyes and four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no, octopuses have two eyes and three", "extracted_entity": null}
{"question": "is octopus a common food in Mediterranean cuisine as well as Portuguese cuisine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": "Mediterranean_cuisine;Portuguese_cuisine"}
{"question": "is the collective form octopus usually reserved for animals consumed for food?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "The plural of octopus is octopuses.", "extracted_entity": "Octopus"}
{"question": "has an octopus a hard beak, with its mouth at the center point of the arms?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no. octopi have a soft beak,", "extracted_entity": null}
{"question": "what happened in 1968 through 1971?", "answer": "Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "I was born.", "extracted_entity": null}
{"question": "what happened in 1838?", "answer": "Don Jos\u00e9 Ruiz y Blasco was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Don Jos\u00e9 Ruiz y Blasco was born"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "the Cherokee were forced to march to Oklahoma", "extracted_entity": null}
{"question": "who was not long in finding another lover, Jacqueline Roque?", "answer": "Picasso.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Picasso."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "when did Picasso make his first trip to Paris?", "answer": "1900", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1900"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "In 1900, at the age of", "extracted_entity": "Pablo_Picasso;Paris"}
{"question": "give an example of the most comprehensive records extant of any major artists beginnings. ?", "answer": "Museu Picasso", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Museu Picasso"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The most comprehensive records extant of any major artists", "extracted_entity": null}
{"question": "is in the 1996 movie Surviving Picasso Picasso played by actor Anthony Hopkins?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Anthony_Hopkins"}
{"question": "is the U.S. copyright representative for the Picasso Administration the Artists Rights Society?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Yes, the U.S. copyright representative for the", "extracted_entity": "United_States;Artists_Rights_Society"}
{"question": "is he one of the most recognized figures in 20th-century art?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "20th-century_art"}
{"question": "is he best known for co-founding the Cubist movement and for the wide variety of styles embodied in his work?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": "Cubism"}
{"question": "are among his most famous works the proto-Cubist Les Demoiselles d'Avignon (1907) and Guernica (1937), his portrayal of the German bombing of Guernica during the Spanish Civil War?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The two most famous works of Pablo Picasso are the", "extracted_entity": "Proto-Cubism;Guernica_(Picasso);Bombing_of_Guernica;Spanish_Civil_War"}
{"question": "are pianos used in Western music?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes. pianos are used in Western music.", "extracted_entity": null}
{"question": "are \"upright pianos\" called \"vertical pianos\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes, upright pianos are also called vertical pian", "extracted_entity": null}
{"question": "did Bartolomeo Cristofori invent the modern piano?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "no, he invented a keyboard instrument that was a", "extracted_entity": "Bartolomeo_Cristofori;Piano"}
{"question": "what is the middle pedal called on grand pianos?", "answer": "the sostenuto pedal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the sostenuto pedal.", "sostenuto pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Sustain Pedal", "extracted_entity": null}
{"question": "how many black keys do modern pianos have?", "answer": "36.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["36.", "36"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "36", "extracted_entity": null}
{"question": "what is the sustain pedal called?", "answer": "The pedal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pedal", "damper pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "the damper pedal", "extracted_entity": null}
{"question": "where is Irving Berlin's piano located?", "answer": "In the Smithsonian Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Smithsonian Museum.", "in the Smithsonian Museum"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Irving Berlin's piano is located in the United States", "extracted_entity": "Irving_Berlin"}
{"question": "what kind of piano did Irving Berlin play?", "answer": "The transposing piano.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The transposing piano.", "transposing piano"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "He played a piano that was not a concert grand", "extracted_entity": "Irving_Berlin"}
{"question": "is the left-most pedal on a grand piano called the una corda?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes, the left-most pedal on a grand piano is called the una corda."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Piano"}
{"question": "is it advantageous for a grand piano's metal plate to be quite massive?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "It is advantageous for the plate to be quite massive."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes, it is advantageous for a grand piano's", "extracted_entity": null}
{"question": "is the mechanism in an upright piano perpendicular to its keys?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "The mechanism in upright pianos is perpendicular to the keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "No, the mechanism is angled.", "extracted_entity": null}
{"question": "about how tall is a typical studio piano?", "answer": "Studio pianos are around 42 to 45 inches tall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Studio pianos are around 42 to 45 inches tall."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "about 4 feet 6 inches tall", "extracted_entity": null}
{"question": "what are a piano's keys generally made of?", "answer": "Piano keys are generally made of spruce or basswood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano keys are generally made of spruce or basswood."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "ivory", "extracted_entity": null}
{"question": "how many total keys does a typical modern piano have?", "answer": "88 keys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88 keys.", "A modern piano has 88 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "88, but the modern piano has 7", "extracted_entity": null}
{"question": "why are upright pianos more compact?", "answer": "Upright pianos are more compact because the frame and strings are vertical.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Upright pianos are more compact because the frame and strings are vertical."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "because the strings are shorter.", "extracted_entity": null}
{"question": "do older pianos have more keys than modern pianos?", "answer": "no.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no.", "Many older pianos only have 85 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "no, there are 88 keys on a piano", "extracted_entity": null}
{"question": "what are the names of a piano's pedals?", "answer": "Piano pedals from left to right: una corda, sostenuto, and damper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano pedals from left to right: una corda, sostenuto, and damper.", "The names of a piano's pedals are una corda, sostenuto, and damper."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "soft pedal, sostenuto pedal, and dam", "extracted_entity": null}
{"question": "what happened in the January 15, 1882?", "answer": "On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "the first official flight of the \"Eole\"", "extracted_entity": null}
{"question": "what happened in 1887?", "answer": "In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "the first national convention of women's clubs was held", "extracted_entity": null}
{"question": "is one of the best known Impressionist works Renoir's 1876 Dance at Le Moulin de la Galette?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Impressionism;Pierre-Auguste_Renoir"}
{"question": "was Pierre-Auguste Renoir born in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Pierre-Auguste_Renoir;Limoges;Haute-Vienne;France"}
{"question": "have two of Renoir's paintings sold for more than US0 million?", "answer": "Yes (any painting sells for more than $0 million though...)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (any painting sells for more than $0 million though...)"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Yes, Renoir's painting Bal au Moulin", "extracted_entity": "Pierre-Auguste_Renoir"}
{"question": "as a celebrator of beauty, and especially feminine sensuality, has it been said that ``Renoir is the final representative of a tradition which runs directly from Rubens to Watteau''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Pierre-Auguste_Renoir;Peter_Paul_Rubens;Jean-Antoine_Watteau"}
{"question": "did Pierre-Auguste Renoir die in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": ", he died in Cagnes-sur-M", "extracted_entity": "Limoges;Haute-Vienne;France"}
{"question": "as a boy, did he work in a porcelain factory where his drawing talents led to him being chosen to paint designs on fine china?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Portuguese an official language of Andorra?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Portuguese is not an official language of Andorra."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Portuguese is an official language of Andorra", "extracted_entity": "Portuguese_language;Andorra"}
{"question": "does the Portuguese language have its roots in the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese is derived from Latin."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes. It does.", "extracted_entity": "Portuguese_language;Latin"}
{"question": "does Portuguese contain words from the Arabic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese contains words from the Arabic language."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Portuguese_language;Arabic_language"}
{"question": "where are there small Portuguese-speaking communities?", "answer": "Former overseas colonies of Portugal such as Macau and East Timor ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Former overseas colonies of Portugal such as Macau and East Timor ", "There are small Portuguese-speaking communities in Macau and East Timor."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "there are Portuguese-speaking communities in:", "extracted_entity": null}
{"question": "what are the two main groups of Portuguese dialects?", "answer": "Those of Brazil and those of the Old World", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Those of Brazil and those of the Old World", "The two main groups of dialects are Brazil and the Old World."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "European Portuguese and Brazilian Portuguese.", "extracted_entity": "Portugal"}
{"question": "what event marked the end of the Old Portuguese period?", "answer": "The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "The end of the Old Portuguese period was marked by the publication of the Cancioneiro Geral by Garcia de Resende."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "the conquest of the Algarve", "extracted_entity": null}
{"question": "which government established Portuguese as it's third official language in order to meet the requirements to apply for full membership in the CPLP?", "answer": "Equatorial Guinea", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Equatorial Guinea", "The government of Equatorial Guinea established Portuguese as its third official language in order to meet the requirements to apply for full membership in the CPLP."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese is the official language of the African country of", "extracted_entity": "Portuguese_language;Community_of_Portuguese_Language_Countries"}
{"question": "by 2050, what will the total population of Portuguese speakers in the world be?", "answer": "335 million people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["335 million people", "By 2050, Portuguese-speaking countries will have a total population of 335 million people."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "240 million people", "extracted_entity": "Portuguese_language"}
{"question": "in which country is Portuguese the co-official language with Cantonese Chinese?", "answer": "Macau", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Macau", "Portuguese is the co-official language with Cantonese Chinese in Macau."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Brazil", "extracted_entity": "Portuguese_language;Cantonese"}
{"question": "wasn't Portuguese language spread by arriving Roman soldiers?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Portuguese_language"}
{"question": "(where French has a similar phenomenon, with alveolar affricates instead of postalveolars?", "answer": "Quebec.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Alveolar affricates in French are not", "extracted_entity": "France;Postalveolar_consonant"}
{"question": "give an example of the official languages of the special administrative region of Macau and East Timor, . ?", "answer": "Portuguese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Portuguese"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "inese and Portuguese", "extracted_entity": "Macau;East_Timor"}
{"question": "is Portuguese an official language of several international organizations?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes. Portuguese is an official language of the United", "extracted_entity": null}
{"question": "is it derived from the Latin spoken by the romanized pre-Roman peoples of the Iberian Peninsula (namely the Gallaeci, the Lusitanians, the Celtici and the Conii) around 2000 years ago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "No, it is not. The language spoken by", "extracted_entity": "Latin;Iberian_Peninsula;Gallaeci;Lusitanians"}
{"question": "was it used as the exclusive lingua franca on the island of Sri Lanka for almost 350 years?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "No, it wasn't. The Dutch East India", "extracted_entity": "Sri_Lanka"}
{"question": "is the city named after the apostle Saint Peter?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The city of Rome is named after the apostle", "extracted_entity": "Saint_Peter"}
{"question": "has the terrain in the city been artificially raised?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes, the terrain in the city has been artificially", "extracted_entity": null}
{"question": "what were some other names of the city?", "answer": "Petrograd, Leningrad, and Piter", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Petrograd, Leningrad, and Piter", "Petrograd and Leningrad."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "the city was also called the \"city of the", "extracted_entity": null}
{"question": "what festival features fireworks  celebrating the end of the school year?", "answer": "The White Nights Festival in Saint Petersburg", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The White Nights Festival in Saint Petersburg", "The White Nights Festival."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The Fourth of July", "extracted_entity": null}
{"question": "where do most people in urban Saint Petersburg live?", "answer": "apartments", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apartments", "Apartments."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "in apartments", "extracted_entity": "Saint_Petersburg"}
{"question": "what was made in Russia's largest foundry?", "answer": "thousands of sculptures and statues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["thousands of sculptures and statues", "Thousands of sculptures and statues."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "a 30-tonne bronze statue of the country", "extracted_entity": "Russia"}
{"question": "when did the crime level become higher?", "answer": "After the October revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the October revolution."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "when the police force was cut down", "extracted_entity": null}
{"question": "how to travel to work from the city to Moscow?", "answer": "railway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["railway", "One way is through the Moscow-Saint Petersburg Railway."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "by metro", "extracted_entity": "Moscow"}
{"question": "who laid down the Peter and Paul Fortress?", "answer": "Peter the Great ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Peter the Great "], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Peter the Great.", "extracted_entity": null}
{"question": "when was the 7th symphony premiered?", "answer": "1942", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1942"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": null}
{"question": "has Equestrianism been a long tradition, popular among the Tsars and aristocracy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "s, it has been a long tradition, popular", "extracted_entity": "Equestrianism;Tsar"}
{"question": "give an example of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995. ?", "answer": "Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "In 1950-1993 and 199", "extracted_entity": "FC_Zenit_Saint_Petersburg"}
{"question": "give an example of the best known symphony orchestras in the world under the leadership of conductors Yevgeny Mravinsky and Yuri Temirkanov. ?", "answer": "The Leningrad Philharmonic Orchestra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Leningrad Philharmonic Orchestra"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The best known symphony orchestras in the world", "extracted_entity": "Yevgeny_Mravinsky;Yuri_Temirkanov"}
{"question": "give an example of the longest, most destructive, and most lethal sieges of major cities in modern history. ?", "answer": "The Siege of Leningrad", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Siege of Leningrad"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1. The siege of Stalingrad was", "extracted_entity": null}
{"question": "were the city's other names Petrograd (, 1914 -- 1924) and Leningrad (, 1924 -- 1991)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Petrograd (, 1914 -- 192", "extracted_entity": null}
{"question": "is it often called just Petersburg (-RRB- and is informally known as Piter (-RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes, but the city is officially named Saint Petersburg", "extracted_entity": null}
{"question": "founded by Tsar Peter I of Russia on 27 May 1703, was it the capital of the Russian Empire for more than two hundred years (1713 -- 1728, 1732 -- 1918)?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "St. Petersburg was founded by Tsar Peter I", "extracted_entity": "Peter_the_Great;Russian_Empire"}
{"question": "is the standard of living in San Franciscio high?", "answer": "Yes, San Francisco is characterized by a high standard of living.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, San Francisco is characterized by a high standard of living.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, it is. San Francisco is a very", "extracted_entity": "San_Francisco;San_Francisco_Giants"}
{"question": "does San Francisco have a high percentage of gay and lesbian individuals?", "answer": "Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, San Francisco has a high percentage of gay", "extracted_entity": "San_Francisco"}
{"question": "is San Franscico a popular tourist destination?", "answer": "Yes. San Francisco is a popular international tourist destination.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. San Francisco is a popular international tourist destination.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes. It is one of the most popular tourist", "extracted_entity": null}
{"question": "how large is the population of San Francisco?", "answer": "The estimated population of San Francisco in the year 2008 was 808,976.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The estimated population of San Francisco in the year 2008 was 808,976.", "San Francisco has an estimated population of 808,976."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The city proper is the fourth most populous city in", "extracted_entity": "San_Francisco"}
{"question": "how many state parks are in San Francisco?", "answer": "There is only one park managed by the California State Park system: Candlestick Point.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There is only one park managed by the California State Park system: Candlestick Point.", "There is one state park in San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "0", "extracted_entity": "San_Francisco"}
{"question": "where is San Francisco?", "answer": "San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "San Francisco is in California."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is in California.", "extracted_entity": "San_Francisco"}
{"question": "what is the cause of local earthquakes?", "answer": "It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "The nearby San Andreas and Hayward Faults are responsible for much earthquake activity."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the movement of tectonic plates", "extracted_entity": null}
{"question": "are more residents employed by small businesses than in 1977?", "answer": "The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, by a wide margin.", "extracted_entity": null}
{"question": "how does poverty in San Francisco compare to the nation-wide average?", "answer": "The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "San Francisco's poverty rate is lower than the national average."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco has a poverty rate of 7.", "extracted_entity": "San_Francisco"}
{"question": "was Mission Bay campus opened in 2003?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, Mission Bay campus was opened in 200", "extracted_entity": null}
{"question": "was the University of San Fransisco founded in 1855?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University_of_San_Francisco"}
{"question": "is Golden Gate Park the largest city park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": "Golden_Gate_Park"}
{"question": "what is Northern California's most widely circulated newspaper?", "answer": "San Francisco Chronicle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco Chronicle"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco Chronicle", "extracted_entity": "Northern_California"}
{"question": "what makes San Francisco among the top-ten North American destinations for conventions and conferences?", "answer": "San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "a large hotel infrastructure and a world-class convention facility in the Moscone Center"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is the only city on the West Coast", "extracted_entity": "San_Francisco;North_America"}
{"question": "who manages Candlestick point?", "answer": "California State Park system", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["California State Park system", "the California State Park system"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The East Bay Regional Park District", "extracted_entity": null}
{"question": "is Crissy field an airfield?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, it is an airfield", "extracted_entity": null}
{"question": "does the de Young museum house the Asian Art Museum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No, they are separate museums", "extracted_entity": null}
{"question": "what prompted the city to upgrade its building codes?", "answer": "earthquakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["earthquakes", "The threat of major earthquakes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the city is required to update its building codes every", "extracted_entity": null}
{"question": "like many larger where cities, San Francisco is a minority-majority city, as non-Hispanic whites comprise less than half of the population?", "answer": "U.S.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["U.S."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, like many larger cities, San Francisco is", "extracted_entity": "San_Francisco;Non-Hispanic_whites"}
{"question": "who laid out a street plan for the expanded settlement?", "answer": "William Richardson and Alcalde Francisco de Haro.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["William Richardson and Alcalde Francisco de Haro."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "John Smith", "extracted_entity": null}
{"question": "when was a 43-acre Mission Bay campus opened?", "answer": "2003", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2003"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "2008", "extracted_entity": null}
{"question": "when did it sign on the air?", "answer": "In 1941.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1941."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "9:00am, 10/1", "extracted_entity": null}
{"question": "give an example of the largest two-year community colleges in the country. ?", "answer": "The City College of San Francisco.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The City College of San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The largest two-year community colleges in the country are", "extracted_entity": null}
{"question": "give an example of the ten Beta World Cities. ?", "answer": "San Trancisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Trancisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The ten Beta World Cities are: London, Paris", "extracted_entity": null}
{"question": "the only consolidated city-county in California, does it encompass a land area of on the northern end of the San Francisco Peninsula, making it the second most densely populated city in the United States?", "answer": "San Francisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco, California, the only consolidated city-count", "extracted_entity": "California;San_Francisco_Peninsula;United_States"}
{"question": "is San Francisco also the financial, cultural, and transportation center of the larger San Francisco Bay Area, a region of 7.4 million people?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "San_Francisco;San_Francisco_Bay_Area"}
{"question": "in what year did the Spanish establish a fort at the Golden Gate and a mission named for Francis of Assisi on the site?", "answer": "In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": "Spain;Golden_Gate;Francis_of_Assisi"}
{"question": "does Swahili have dipthongs?", "answer": "Yes, Swahili has diphthongs. (Typo)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili has diphthongs. (Typo)"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes, it has the diphthong \"", "extracted_entity": "Swahili_language"}
{"question": "is Swahili a Bantu language?", "answer": "Yes, Swahili is a Bantu language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is a Bantu language.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es, Swahili is a Bantu language", "extracted_entity": "Swahili_language;Bantu_languages"}
{"question": "did Uganda make Swahili a required subject in primary schools?", "answer": "Yes, Uganda made Swahili a required subject in primary schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Uganda made Swahili a required subject in primary schools.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "no, it did not", "extracted_entity": "Uganda;Swahili_language"}
{"question": "what is the Swahili word for the Swahili language?", "answer": "Kiswahili is the Swahili word for the Swahili language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kiswahili is the Swahili word for the Swahili language.", "Kiswahili"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Kiswahili", "extracted_entity": "Swahili_language"}
{"question": "is Swahili spoken in Mozambique?", "answer": "Yes, Swahili is spoken in parts of northern Mozambique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is spoken in parts of northern Mozambique.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es, Swahili is spoken in Mozambique", "extracted_entity": "Swahili_language;Mozambique"}
{"question": "how does Swahili time run?", "answer": "Swahili time runs from dawn to dusk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili time runs from dawn to dusk.", "from dawn to dusk"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "in the direction of the sun", "extracted_entity": "Swahili_language"}
{"question": "how many noun classes does Swahili have?", "answer": "Swahili has sixteen noun classes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili has sixteen noun classes.", "sixteen"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "18", "extracted_entity": "Swahili_language"}
{"question": "in what year did Germany seize Tanganyika?", "answer": "Germany seized Tanganyika in 1886.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany seized Tanganyika in 1886.", "1886"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": "Germany;Tanganyika_(territory)"}
{"question": "which is the sub-Saharan indigenous language with the greatest number of speakers?", "answer": "Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "Hausa"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili, with over 50 million speakers", "extracted_entity": null}
{"question": "is Swedish a descendant of Old Norse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": "Sweden;Old_Norse"}
{"question": "does Swedish use the perfect participle to form the present perfect tense?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "Yes, the perfect participle is used to form", "extracted_entity": "Sweden"}
{"question": "how many Swedish speakers were reported in Canada in 2001?", "answer": "16,915.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["16,915.", "There are 16,915 reported Swedish speakers in Canada."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "325,785 (1.1% of", "extracted_entity": "Swedish_language;Canada"}
{"question": "how many letters are in the basic Latin alphabet?", "answer": "26.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["26.", "26"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "26.", "extracted_entity": null}
{"question": "when did a standard and common language become available to all Swedes?", "answer": "In the 20th Century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the 20th Century.", "It was during the 20th century that a common, standardized national language became available to all Swedes."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": "Sweden"}
{"question": "when did the Soviet military forces occupy Estonia?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "In 1944."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "17 June 1940", "extracted_entity": "Soviet_Union;Estonia"}
{"question": "what are the differences between English and Swedish pronouns?", "answer": "Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "The differences between English and Swedish pronouns are that", "extracted_entity": "English_language;Sweden"}
{"question": "how does Swedish language counterbalance the absence of cases?", "answer": "The lack of cases in Swedish is compensated by a wide variety of prepositions.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lack of cases in Swedish is compensated by a wide variety of prepositions."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "The Swedish language has a system of declension,", "extracted_entity": "Swedish_language"}
{"question": "is Taipei in a valley?", "answer": "Taipei is in the valleys of the Keelung and Xindian Rivers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei is in the valleys of the Keelung and Xindian Rivers", "Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "no, it\u2019s in a basin", "extracted_entity": null}
{"question": "was Taipei founded in the 18th century?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "No, it was founded in the 17th", "extracted_entity": "Taipei"}
{"question": "is the National Palace Museum in Taipei?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "yes, the National Palace Museum is in Taipei.", "extracted_entity": "National_Palace_Museum;Taipei"}
{"question": "what is along the city's western border?", "answer": "The Danshui River borders Taipei to the west.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Danshui River borders Taipei to the west.", "Taipei has the Danshui River along its western border."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "the Pacific Ocean", "extracted_entity": null}
{"question": "what is Taipei 101?", "answer": "Taipei 101 is a 101-floor landmark skyscraper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei 101 is a 101-floor landmark skyscraper."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei 101 is a skyscraper in the capital", "extracted_entity": "Taipei_101"}
{"question": "what are some annual festivals in Taipei?", "answer": "The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "Some annual festivals include Tomb-Sweping Day, the Dragon Boat Festival, the Ghost Festival, and the Mid-Autumn Festival."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "there are many festivals in Taipei, some of them", "extracted_entity": "Taipei"}
{"question": "when is the Pacific typhoon season?", "answer": "Between June and October.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Between June and October.", "It's between June and October."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "June to November", "extracted_entity": null}
{"question": "who sets up shrines?", "answer": "Many homes and business set up shrines.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many homes and business set up shrines.", "Many homes and businesses would set up shrines."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "people who believe in the god they are worshiping", "extracted_entity": null}
{"question": "what does Taipei produce?", "answer": "Taipei produces high technology and hi-tech components", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei produces high technology and hi-tech components", "Taipei produces textiles, apparel, electronics, machinery, printed materials, precision equipment, food, and beverages."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei produces a lot of things. It is the", "extracted_entity": "Taipei"}
{"question": "is the trumpet the musical instrument with the highest register in the brass family?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Trumpet"}
{"question": "is a trumpet played by blowing air through closed lips?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Trumpet"}
{"question": "is the most common type of trumpet the B trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, the most common type of trumpet is the", "extracted_entity": null}
{"question": "what is the earliest date of the trumpet?", "answer": "1500 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1500 BC", "around 1500 BC"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the trumpet is one of the oldest musical instruments known", "extracted_entity": null}
{"question": "what is a trumpet constructed of?", "answer": "brass tubing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass tubing", "brass tubing bent twice into an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "brass.", "extracted_entity": null}
{"question": "what are the smallest trumpets referred to as?", "answer": "Piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piccolo trumpets", "piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "piccolos", "extracted_entity": null}
{"question": "why do trumpets have a bright, loud sound?", "answer": "The trumpet and trombone share a roughly cylindrical bore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The trumpet and trombone share a roughly cylindrical bore.", "Due to the cylindrical bore"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "because the sound waves are compressed and the frequency is", "extracted_entity": null}
{"question": "why does the cornet have a slightly mellower tone than the trumpet?", "answer": "The cornet and flugelhorn have conical bores", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cornet and flugelhorn have conical bores", "because it has conical bores"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the cornet has a conical bore, while", "extracted_entity": "Cornet;Trumpet"}
{"question": "what makes a trumpet fully chromatic?", "answer": "able to play all twelve pitches of Western music", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["able to play all twelve pitches of Western music"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the ability to play all the notes of the chrom", "extracted_entity": null}
{"question": "were trumpet players heavily guarded?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, because they were a threat to the music", "extracted_entity": null}
{"question": "are pocket trumpets compact B trumpets?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "was Dizzy Gillespie a famous trumpeter in 1998?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Dizzy_Gillespie"}
{"question": "what shape is a trumpet bent into?", "answer": "oblong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["oblong", "an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a helix.", "extracted_entity": null}
{"question": "what trumpet was the first to be allowed in the Christian Church?", "answer": "slide trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slide trumpets", "Slide trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the trumpet was the first to be allowed in the", "extracted_entity": "Christian_Church"}
{"question": "the trumpet can be confused with what close relative?", "answer": "the cornet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cornet", "the coronet"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the bugle", "extracted_entity": "Trumpet"}
{"question": "what can be augmented with a fourth valve?", "answer": "the flugelhorn", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the flugelhorn", "flugelhorn"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the euphonium, the baritone, and", "extracted_entity": null}
{"question": "what was Maynard Ferguson noted for being able to play accurately?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "He was noted for his bands, which often served", "extracted_entity": "Maynard_Ferguson"}
{"question": "what was built to play in both B and A keys?", "answer": "piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "A B flat scale", "extracted_entity": null}
{"question": "what happened in 1894?", "answer": "Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\""], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the first modern Olympic games were held in Athens", "extracted_entity": null}
{"question": "what happened in nineteen fifty seven?", "answer": "Maynard Ferguson formed his own band.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maynard Ferguson formed his own band."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "in nineteen fifty seven the first submarine was invented", "extracted_entity": null}
{"question": "is the Breeze Eazy' ' method sometimes used to teach younger students?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "es, the Breeze Eazy' method is", "extracted_entity": null}
{"question": "do many players use a smaller mouthpiece on the piccolo trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "I have never seen a player use a smaller mouth", "extracted_entity": null}
{"question": "is the trumpet constructed of brass tubing bent twice into an oblong shape?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No. The trumpet is constructed of three sections of", "extracted_entity": null}
{"question": "give an example of the most influential musicians of the 20th century. ?", "answer": "Miles Davis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Miles Davis"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The most influential musicians of the 20th century", "extracted_entity": null}
{"question": "were Slide trumpets the first trumpets allowed in the Christian church?", "answer": "Slide trumpets were the first trumpets allowed in the Christian church. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Slide trumpets were the first trumpets allowed in the Christian church. "], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No. The trumpets were used in the church", "extracted_entity": "Christianity"}
{"question": "are they constructed of brass tubing bent twice into an oblong shape, and are played by blowing air through closed lips, producing a ``buzzing'' sound which starts a standing wave vibration in the air column inside the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Standing_wave;Trumpet"}
{"question": "are there several types of trumpet?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, there are. There are the straight trumpet", "extracted_entity": null}
{"question": "was the Ottoman script replaced with a variant of the Latin alphabet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes. It was replaced with the Latin alphabet in", "extracted_entity": "Latin_alphabet"}
{"question": "is there a definite article in Turkish language?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes, the definite article in Turkish is the same", "extracted_entity": "Turkish_language"}
{"question": "is it possible to alter the word order to stress the importance of a certain phrase?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "it's possible to do that with a comma,", "extracted_entity": null}
{"question": "what is the official language of Turkey?", "answer": "Turkish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turkish.", "Turkish is the official language of Turkey."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish", "extracted_entity": null}
{"question": "when was the Turkish Language Association founded?", "answer": "In 1928.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1928.", "The Turkish Language Association was founded in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "1932.", "extracted_entity": null}
{"question": "what are the most significant foreign contributors to Turkish vocabulary?", "answer": "Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. "], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Persian, Arabic, and French.", "extracted_entity": "Turkish_language"}
{"question": "who were the Orkhon inscriptions built for?", "answer": "For the prince Kul Tigin and his brother Emperor Bilge Khan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For the prince Kul Tigin and his brother Emperor Bilge Khan.", "The Orkhon inscriptions were erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Orkhon inscriptions were built for the Turk", "extracted_entity": "Orkhon_River"}
{"question": "what was one of the results of the introduction of the new Turkish alphabet?", "answer": "A dramatic increase in literacy from Third World levels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A dramatic increase in literacy from Third World levels.", "There was a dramatic increase in literacy from its original Third World levels."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the new alphabet was introduced in 1928.", "extracted_entity": null}
{"question": "why does Lewis use \"absolute case\" instead of \"nominative\"?", "answer": "Because it is also used for the indefinite accusative.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is also used for the indefinite accusative."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "\"Nominative\" is a bit of a", "extracted_entity": null}
{"question": "how many people speak Turkish worldwide?", "answer": "There are roughly another 10 million native speakers worldwide.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are roughly another 10 million native speakers worldwide."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "80-90 million", "extracted_entity": "Turkish_language"}
{"question": "in Turkish, which syllable usually has the stress?", "answer": "Stress is usually on the last syllable", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stress is usually on the last syllable"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the first syllable", "extracted_entity": null}
{"question": "is the spelling of Turkish phonetic?", "answer": "yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "No, Turkish is a phonetic language, so", "extracted_entity": null}
{"question": "what percentage of the words in Turkish vocabulary have Turkish origins?", "answer": "86% of the Turkish vocabulary have Turkish origins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["86% of the Turkish vocabulary have Turkish origins."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "50%", "extracted_entity": "Turkish_language"}
{"question": "after what event was the Turkish Language Association established?", "answer": "After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "after the war of independence", "extracted_entity": null}
{"question": "what exception to the rules of vowel harmony do compound words have?", "answer": "In compound words, the vowels need not harmonize between the constituent words of the compound.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In compound words, the vowels need not harmonize between the constituent words of the compound."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the first vowel of a compound word takes on the", "extracted_entity": null}
{"question": "what are the patterns of vowel harmony?", "answer": "Vowels are characterised by two features: front/back and rounded/unrounded.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowels are characterised by two features: front/back and rounded/unrounded."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "vowel harmony is a process in which vowels in a", "extracted_entity": null}
{"question": "what are the ways to join groups of nouns?", "answer": "Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "1. by means of conjunctions", "extracted_entity": null}
{"question": "does Vietnamese have a comparatively large number of vowels?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": "Vietnamese_language"}
{"question": "is Vietnamese the mother tongue of the Vietnamese people?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnamese_language;Vietnamese_people"}
{"question": "is Vietnamese the official administrative language of Vietnam?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnamese is the official administrative language of Vietnam", "extracted_entity": "Vietnamese_language;Vietnam"}
{"question": "was Vietnamese influenced by Chinese?", "answer": "Much vocabulary has been borrowed from Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much vocabulary has been borrowed from Chinese.", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, it was influenced by Chinese.", "extracted_entity": null}
{"question": "when did written Vietnamese become the official administrative language?", "answer": "the 20th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 20th century", "Vietnamese became the official administrative language by the 20th century."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1919", "extracted_entity": "Vietnamese_language"}
{"question": "have consonant clusters been lost?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no, they haven\u2019t. the only consonant", "extracted_entity": null}
{"question": "is the Red River Delta traditionally poor?", "answer": "Not compared to Nghe An, Ha Tinh, or Quang Binh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Not compared to Nghe An, Ha Tinh, or Quang Binh", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": "Red_River_Delta"}
{"question": "what kind of words have been borrowed from Chinese?", "answer": "Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "Vietnamese adopts words from Chinese which express abstract ideas."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "The words \"ketchup\" and \"tea\"", "extracted_entity": "Chinese_language"}
{"question": "is Vietnam independent from France?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnam is independent from France.", "extracted_entity": "Vietnam;France"}
{"question": "was Vietnamese formally written using the Chinese writing system?", "answer": "Yes, Vietnamese was *formerly* written using the Chinese writing system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese was *formerly* written using the Chinese writing system."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vietnamese_language"}
{"question": "does Vietnamese have a large number of vowels?", "answer": "Yes, Vietnamese has a comparatively large number of vowels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese has a comparatively large number of vowels."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnamese has 11 vowels.", "extracted_entity": "Vietnamese_language"}
{"question": "what are the three dialect regions of Vietnamese?", "answer": "The three regions are North, Central, and South.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three regions are North, Central, and South."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "North, Central, and South", "extracted_entity": "Vietnamese_language"}
{"question": "when was the reunification of Vietnam?", "answer": "The reunification of Vietnam occurred in 1975-76.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The reunification of Vietnam occurred in 1975-76."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "in 1975", "extracted_entity": "Vietnam"}
{"question": "how many tones do Northern varieties have?", "answer": "Northern Vietnamese has six tones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Vietnamese has six tones."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "4 (i.e. 4 tones)", "extracted_entity": null}
{"question": "does Vietnamese borrow from Latin and Greek?", "answer": "No, Vietnamese does not borrow from Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Vietnamese does not borrow from Latin and Greek."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnamese borrow a lot of words from Latin", "extracted_entity": "Vietnamese_language;Latin;Greek_language"}
{"question": "did Vietnamese come to predominate politically in 2nd century BC?", "answer": "No, Chinese came to predominate politically in the 2nd century B.C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese came to predominate politically in the 2nd century B.C."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnam"}
{"question": "do more northern speakers speak the southern regional dialect than southern speakers speak the northern dialect?", "answer": "More southern speakers speak the northern dialect than northerners speak the suthern dialect.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More southern speakers speak the northern dialect than northerners speak the suthern dialect."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, since there are more northern speakers than southern", "extracted_entity": null}
{"question": "what happened in 1860?", "answer": "Vincent van Gogh attended the Zundert village school from 1860.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vincent van Gogh attended the Zundert village school from 1860."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Lincoln was elected president.", "extracted_entity": null}
{"question": "who frequented the circle of the British-Australian artist John Peter Russell?", "answer": "Van Gogh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Van Gogh"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Paul Gauguin, Vincent van Gogh,", "extracted_entity": null}
{"question": "were the letters first annotated in 1913 by Theo's widow Johanna van Gogh-Bonger?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Theo_van_Gogh_(art_dealer)"}
{"question": "was Kee seven years older than Van Gogh as well as had an eight-year-old son?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "No, she was eight years more youthful than Van", "extracted_entity": null}
{"question": "did he suffer from anxiety and increasingly frequent bouts of mental illness throughout his life, and died largely unknown, at the age of 37, from a self-inflicted gunshot wound?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes, he did suffer from anxiety and increasingly frequent", "extracted_entity": "Anxiety;Mental_disorder"}
{"question": "appreciated during his lifetime, did his fame grow in the years after his death?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes, his fame grew in the years after his", "extracted_entity": null}
{"question": "is today, he widely regarded as one of history's greatest painters and an important contributor to the foundations of modern art?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "The correct answer is: Pablo Picasso", "extracted_entity": "Modern_art"}
{"question": "does a violin have four strings?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it has three strings.", "extracted_entity": null}
{"question": "is the violin supported by the left shoulder?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No. It is supported by the left elbow.", "extracted_entity": null}
{"question": "is the violin shaped like an hourglass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no. it's shaped like a violin.", "extracted_entity": null}
{"question": "what were strings first made out of?", "answer": "Sheep Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sheep Gut", "Sheep gut."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "they were made out of a type of animal skin", "extracted_entity": null}
{"question": "what is a violin called informally?", "answer": "Fiddle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fiddle", "A fiddle."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a fiddle", "extracted_entity": null}
{"question": "what does vitula mean?", "answer": "Stringed Instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stringed Instrument", "Stringed instrument."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "something that is very attractive", "extracted_entity": null}
{"question": "what is a person that makes or repairs violins called?", "answer": "Luthier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luthier", "A luthier, or simply a violin maker."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a violin maker or a violin repairer.", "extracted_entity": null}
{"question": "what does the \"voice\" of a violin depend upon?", "answer": "Shape", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shape", "Its shape."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the quality of the wood, the length of the", "extracted_entity": null}
{"question": "is the violin a percussion instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it is a string instrument", "extracted_entity": null}
{"question": "what happened in 1980s?", "answer": "The use of traditional (non-synthesized) orchestras declined.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The use of traditional (non-synthesized) orchestras declined."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the rise of the internet", "extracted_entity": null}
{"question": "who features violinist Boyd Tinsley?", "answer": "Dave Matthews Band features violinist Boyd Tinsley.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dave Matthews Band features violinist Boyd Tinsley."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Dave Matthews Band", "extracted_entity": null}
{"question": "regarding this topic, what did Antonio Stradivari do?", "answer": "Antonio Stradivari made violins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antonio Stradivari made violins."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Antonio Stradivari was an Italian luth", "extracted_entity": "Antonio_Stradivari"}
{"question": "when did Charles IX order Amati to construct 24 violins for him?", "answer": "1560", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1560"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1566", "extracted_entity": "Charles_IX_of_France;Amati"}
{"question": "is a string first tuned to a standard pitch?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes, a string is first tuned to a standard", "extracted_entity": null}
{"question": "is each node at an integer division of the string?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the tuning G-D-A-E used for most violin music?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it is G-D-A-E for most", "extracted_entity": null}
{"question": "is it the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello?", "answer": "Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "s it the smallest and highest-pitched member of", "extracted_entity": "Violin_family;Viola;Cello"}
{"question": "is a violin sometimes informally called a fiddle, regardless of the type of music played on it?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes, a violin is sometimes informally called a", "extracted_entity": "Violin;Fiddle"}
{"question": "does the word ``violin'' come from the Middle Latin word vitula, meaning ``stringed instrument''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No, the word ``violin'' is a", "extracted_entity": "Medieval_Latin"}
{"question": "is the xylophone a musical instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, the xylophone is a musical instrument", "extracted_entity": "Xylophone"}
{"question": "was the xylophone associated with the folk music of the United States by the 19th century?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The xylophone was associated with the folk music", "extracted_entity": "Xylophone;American_folk_music;United_States"}
{"question": "was it likely that the xylophone reached Europe during the Crusades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Xylophone;Europe;Crusades"}
{"question": "the xylophone is an instrument from what family?", "answer": "the percussion family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the percussion family", "Percussion"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "percussion family", "extracted_entity": "Xylophone"}
{"question": "how do the Java and Bali use xylophones?", "answer": "In gamelan ensembles", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In gamelan ensembles"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the xylophone is used as a musical instrument", "extracted_entity": "Java;Bali"}
{"question": "some versions of xylophones have resonators made of what?", "answer": "gourds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["gourds", "Gourds."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "metal", "extracted_entity": "Xylophone"}
{"question": "which came first, the xylophone or the vibraphone?", "answer": "xylophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["xylophone", "The xylophone."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The xylophone.", "extracted_entity": "Xylophone"}
{"question": "did the golden age of xylophones come before or after the first usage of the European-derived orchestral?", "answer": "after", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["after", "After."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "after.", "extracted_entity": null}
{"question": "does kelon increase or decrease tone quality of the xylophone?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Decrease."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "decrease, because it decreases the amount of air hitting", "extracted_entity": null}
{"question": "do the different species of zebras interbreed?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "no, they don't interbreed. they", "extracted_entity": null}
{"question": "do zebras sleep standing up?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "no, they sleep lying down", "extracted_entity": null}
{"question": "have plains zebras been crossed with mountain zebras?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "no, mountain zebras are a different species", "extracted_entity": null}
{"question": "how many species of zebra are there?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three", "Three"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "three", "extracted_entity": "Zebra"}
{"question": "what do zebras eat?", "answer": "mainly grass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly grass", "Grasses, shrubs, herbs, twigs, leaves, and bark"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Zebras are herbivores and they eat grass", "extracted_entity": "Zebra"}
{"question": "what are zebras hunted for?", "answer": "mainly for their skins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly for their skins", "Skins"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Zebra meat is a source of food for lions", "extracted_entity": "Zebra"}
{"question": "what areas do the Grevy's Zebras inhabit?", "answer": "semi-arid grasslands of Ethiopia and northern Kenya", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["semi-arid grasslands of Ethiopia and northern Kenya", ""], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "they are found in the Horn of Africa", "extracted_entity": null}
{"question": "which species of zebra is known as the common zebra?", "answer": "Plains Zebra (Equus quagga, formerly Equus burchelli)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plains Zebra (Equus quagga, formerly Equus burchelli)", "Plains Zebra"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "The common zebra is the plains zebra (", "extracted_entity": null}
{"question": "at what age can a zebra breed?", "answer": "five or six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["five or six", "5 or 6"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "1 year", "extracted_entity": "Zebra"}
