{"question": "was Abraham Lincoln the sixteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "did Lincoln sign the National Banking Act of 1863?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The National Banking Act of 186", "extracted_entity": null}
{"question": "did his mother die of pneumonia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many long was Lincoln's formal education?", "answer": "18 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["18 months", "18 months."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Lincoln begin his political career?", "answer": "1832", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1832", "1832."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did The Legal Tender Act of 1862 establish?", "answer": "the United States Note, the first paper currency in United States history", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United States Note, the first paper currency in United States history", "The United States Note, the first paper currency in United States history."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The Legal Tender Act of 18", "extracted_entity": null}
{"question": "who suggested Lincoln grow a beard?", "answer": "11-year-old Grace Bedell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11-year-old Grace Bedell", "Grace Bedell."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Grace Bedell", "extracted_entity": null}
{"question": "when did the Gettysburg address argue that America was born?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776", "1776."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Lincoln beat John C. Breckinridge in the 1860 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "was Abraham Lincoln the first President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was the 16th President of", "extracted_entity": null}
{"question": "did Lincoln start his political career in 1832?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "did Lincoln ever represent Alton & Sangamon Railroad?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Alton & Sangamon Railroad", "extracted_entity": null}
{"question": "which county was Lincoln born in?", "answer": "Hardin County", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hardin County", "Southeast Hardin County, Kentucky"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Lincoln first serve as President?", "answer": "March 4, 1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1861"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who assassinated Lincoln?", "answer": "John Wilkes Booth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Wilkes Booth"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "John Wilkes Booth", "extracted_entity": null}
{"question": "did Lincoln win the election of 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who was the general in charge at the Battle of Antietam?", "answer": "General McClellan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General McClellan", "John Alexander McClernand"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "McClellan", "extracted_entity": null}
{"question": "why did Lincoln issue the Emancipation Proclamation?", "answer": "To free slaves ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To free slaves ", "The goal was to weaken the rebellion, which was led and controlled by slave owners."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do scholars rank lincoln among the top three presidents?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did lincoln have 18 months of schooling?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "was Lincoln chosen as a presidential candidate in 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "how old was Lincoln in 1816?", "answer": "seven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["seven"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "16 years old", "extracted_entity": null}
{"question": "when was the first photgraph of lincoln taken?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1846", "extracted_entity": null}
{"question": "how long was Lincoln's legal Career?", "answer": "23 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["23 years"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1837-1865", "extracted_entity": null}
{"question": "what trail did Lincoln use a Farmers' Almanac in? ?", "answer": "he defended William \"Duff\" Armstrong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he defended William \"Duff\" Armstrong"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The Lincoln Highway", "extracted_entity": null}
{"question": "did Abraham Lincoln live in the Frontier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln lived in the Frontier", "extracted_entity": null}
{"question": "did Lincoln's Wife's Family support slavery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "who is most noted for his contributions to the theory of molarity and molecular weight?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "who graduated in ecclesiastical law at the early age of 20 and began to practice?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro", "blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he publish another memoria?", "answer": "1821", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1821"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1947", "extracted_entity": null}
{"question": "when did he become a professor?", "answer": "1820", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1820"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "is it true that he became a professor in 1820?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, he became a professor in 18", "extracted_entity": null}
{"question": "was Lorenzo Romano Amedeo Carlo Avogadro an Italian savant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in Turin August 9th 1776 to a noble ancient family of Piedmont, Italy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in", "extracted_entity": null}
{"question": "is he most noted for his contributions to the theory of molarity and molecular weight?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was King Victor Emmanuel III there to pay homage to Avogadro ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Avogadro 's number commonly used to compute the results of chemical reactions ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did the scientific community not reserve great attention to his theory ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, the scientific community reserved great attention to his", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", ""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in french?", "answer": "", "dataset": "factoid_qa", "split": "train", "answer_aliases": [""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1833?", "answer": "Avogadro had been recalled to Turin university", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro had been recalled to Turin university", "blah blah blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the cholera epidemic in Europe was", "extracted_entity": null}
{"question": "who determined the dependence of the boiling of water with atmospheric pressure?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is named after him?", "answer": "The Celsius crater on the Moon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Celsius crater on the Moon"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he publish a collection?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": null}
{"question": "is it true that he published a collection in 1738?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that thermometer had 100 for the freezing point?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Anders Celsius (November 27, 1701 April 25, 1744) a Swedish astronomer?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is The Celsius crater on the Moon named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the Celsius crater on the Moon is what?", "answer": "named after him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["named after him"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the Celsius crater on the Moon named after him ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "had his thermometer 100 for the freezing point of water and 0 for the boiling point ?", "answer": "Yes it had", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes it had"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he not determine the dependence of the boiling of water with atmospheric pressure -LRB- in excellent agreement with modern data -RRB- ?", "answer": "Yes he did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he did"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "boiling point of water at 101", "extracted_entity": null}
{"question": "what happened from 1730 to 1744?", "answer": "He was professor of astronomy at Uppsala University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was professor of astronomy at Uppsala University"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The War of the Polish Succession", "extracted_entity": null}
{"question": "what happened in 1745?", "answer": "The scale was reversed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Jacobite rising of 1745", "extracted_entity": null}
{"question": "are beetles insects?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can beetles be found in polar regions?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "do beetles antennae function primarily as organs of smell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the three sections of a beetle?", "answer": "the head, the thorax, and the abdomen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the head, the thorax, and the abdomen", "The head, the thorax, and the abdomen"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "head, thorax, and abdomen.", "extracted_entity": null}
{"question": "which defense mechanism uses colour or shape to deceive potential enemies?", "answer": "mimicry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mimicry", "Mimicry"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "camouflage", "extracted_entity": null}
{"question": "which type of beetle is a pest of potato plants?", "answer": "Colorado potato beetle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colorado potato beetle"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how can beetle larvae be differentiated from other insect larvae?", "answer": "their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "By their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "by their distinctive head capsules.", "extracted_entity": null}
{"question": "what do beetles eat?", "answer": "Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "beetles eat a variety of foods,", "extracted_entity": null}
{"question": "what are the similarities between beetles and grasshoppers?", "answer": "mouthparts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mouthparts", "Beetles have mouthparts similar to those of grasshoppers"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are certain species of beetles considered pests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, certain species of beetles are considered", "extracted_entity": null}
{"question": "is a beetle's general anatomy uniform?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Beetles are insects that belong to the", "extracted_entity": null}
{"question": "are beetles endopterygotes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many species of beetles are there?", "answer": "350,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["350,000", "about 350,000"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "500,000", "extracted_entity": null}
{"question": "when are sperm cells transferred to the female?", "answer": "during pairing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["during pairing", "During pairing"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the study of beetles called?", "answer": "coleopterology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["coleopterology"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleopterology", "extracted_entity": null}
{"question": "is it possible that there are more than 350,000 species of beetles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, it is possible that there are more than", "extracted_entity": null}
{"question": "is the Adephaga suborder larger than the Polyphaga suborder?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do carrion beetles eat dung?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are prey of various animals including birds and mammals?", "answer": "Beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "various animals including birds and mammals", "extracted_entity": null}
{"question": "what was given by Aristotle for the hardened shield like forewings?", "answer": "The name \"Coleoptera\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name \"Coleoptera\"."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "ailerons", "extracted_entity": null}
{"question": "who or what vary greatly in form within the coleoptera?", "answer": "Antennae.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antennae."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleoptera (the beetles) is the", "extracted_entity": null}
{"question": "are many beetles territorial?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are beetles endopterygotes with complete metamorphosis?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "oxygen is what?", "answer": "One kind of gas obtained via a tracheal system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One kind of gas obtained via a tracheal system."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Oxygen is an element.", "extracted_entity": null}
{"question": "is there a thriving industry in the collection of beetle specimens for amateur and professional collectors ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "[beetle](https://www.google", "extracted_entity": null}
{"question": "have coleopterists formed organisations to facilitate the study of beetles ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleopterists", "extracted_entity": null}
{"question": "is the study of beetles called coleopterology , and its practitioners are coleopterists ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "may a single female lay from several dozen to several thousand eggs during her lifetime ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can beetles be found in almost all habitats?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened on plants and fungi?", "answer": "They are food to beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are food to beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "plants and fungi", "extracted_entity": null}
{"question": "was Calvin Coolidge the twenty-ninth vice president?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge", "extracted_entity": null}
{"question": "was Calvin Coolidge born in Plymouth, Windsor County, Vermont?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge", "extracted_entity": null}
{"question": "did Coolidge graduate from Black River Academy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "when did he drop John from his name?", "answer": "upon graduating from college", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon graduating from college", "Upon graduating from college"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1988", "extracted_entity": null}
{"question": "when did Coolidge meet and marry Grace Anna Goodhue?", "answer": "1905", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1905", "In 1905"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1905", "extracted_entity": null}
{"question": "what period of rapid economic growth did the United States experience during Coolidge's presidency?", "answer": "the Roaring Twenties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Roaring Twenties", "the \"Roaring Twenties\""], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "The Great Depression", "extracted_entity": null}
{"question": "what did Coolidge do after graduating from Amherst?", "answer": "Coolidge moved to Northampton, Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge moved to Northampton, Massachusetts", "Coolidge moved to Northampton, Massachusetts to take up the practice of law"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "of what state was Coolidge governor?", "answer": "Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Massachusetts"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who appointed Harlan Fiske Stone to the Supreme Court?", "answer": "Coolidge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "President Calvin Coolidge", "extracted_entity": null}
{"question": "was Coolidge the thirteenth President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "was Calvin Coolidge Republican?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge was a Republican.", "extracted_entity": null}
{"question": "was Calvin Coolidge a governor of Massachusetts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "when was Coolidge born?", "answer": "July 4 1872", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4 1872", "Plymouth, Windsor County, Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge was born on July", "extracted_entity": null}
{"question": "where did Coolidge's grandfather had government offices?", "answer": "Plymouth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth", "The town of Plymouth Notch"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "The White House", "extracted_entity": null}
{"question": "which state were Coolidge born in?", "answer": "Plymouth, Windsor County, Vermont", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth, Windsor County, Vermont", "Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Vermont", "extracted_entity": null}
{"question": "is Calvin Jr. older than John Coolidge?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Coolidge get in Amherst College?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "John Calvin Coolidge", "extracted_entity": null}
{"question": "what caused Calvin Jr.'s death?", "answer": "heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["heart attack", "Heart attack"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Jr. was killed by his father,", "extracted_entity": null}
{"question": "was John Calvin Coolidge Jr. was born in Las Vegas?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No, he was born in Northampton,", "extracted_entity": null}
{"question": "was Coolidge opposed in the Republican nomination for Governor of Massachusetts in 1918?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Charles G. Dawes", "extracted_entity": null}
{"question": "did Coolidge meet and marry Grace Anna Goodhue?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Anna Goodhue", "extracted_entity": null}
{"question": "what year did Coolidge open his own law office?", "answer": "1898", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1898"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": null}
{"question": "what fraternity was Coolidge a member of?", "answer": "Phi Gamma Delta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Phi Gamma Delta", "phi gamma delta"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Beta Theta Pi", "extracted_entity": null}
{"question": "in 1905 Coolidge met and married whom?", "answer": "Grace Anna Goodhue", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grace Anna Goodhue"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Goodhue Coolidge", "extracted_entity": null}
{"question": "why did Coolidge not attend law school?", "answer": "It was too expensive", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was too expensive", "cost"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge did not attend law school because he", "extracted_entity": null}
{"question": "have there been other US presidents that have visited Cuba?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what years were Coolidge's two sons born in?", "answer": "1906 and 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1906 and 1908"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1906 and 190", "extracted_entity": null}
{"question": "did France cede nearly all of its colonies in Europe in 1763?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is Canada a member of the OECD?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada is a member of the OECD.", "extracted_entity": null}
{"question": "is Canada's official language Zulu?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what is one significant non-official language?", "answer": "Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese.", "Chinese"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in addition to Port Royal, where else did Samuel de Champlain establish a settlement?", "answer": "Quebec City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec City"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Chateau de Champlain, Quebec", "extracted_entity": null}
{"question": "what was the Consitution Act formerly called?", "answer": "The British North America Act.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British North America Act.", "British North America Act"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Constitution Act, 1867", "extracted_entity": null}
{"question": "which countries established colonies in Canada?", "answer": "France and Britain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France and Britain.", "Fance and Britain"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The United States, Russia, and Great Britain.", "extracted_entity": null}
{"question": "how has Canada helped UN peacekeeping efforts?", "answer": "During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "Canada has played a leading role in UN peacekeeping efforts. During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada has been a strong supporter of UN peace", "extracted_entity": null}
{"question": "what percentage of the population don't speak English or French at home?", "answer": "28%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28%", "1.5%"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "78%", "extracted_entity": null}
{"question": "is Canada bilingual?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, it is."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Canadian soldiers win the Battle of Vimy Ridge in 1917?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No, it did not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canadian soldiers won the Battle of Vimy R", "extracted_entity": null}
{"question": "is an official language of Canada German?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, it is not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many provinces and territories does Canada have?", "answer": "A federation now comprising ten provinces and three territories", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A federation now comprising ten provinces and three territories", "Ten provinces and three territories"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "13", "extracted_entity": null}
{"question": "what is Canada's national unemployment rate?", "answer": "While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "In October 2007, Canada's national unemployment rate is 5.9%."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada's national unemployment rate is", "extracted_entity": null}
{"question": "where is the most densely populated part of Canada?", "answer": "The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Toronto, Ontario", "extracted_entity": null}
{"question": "what is the largest country in the world?", "answer": "Canada is the second largest country in the world, after Russia, and largest on the continent.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada is the second largest country in the world, after Russia, and largest on the continent.", "Russia."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "China", "extracted_entity": null}
{"question": "what is the largest religious group in Canada?", "answer": "According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "Christian"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Canada have one of the largest armed forces in the world?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "World War II."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": null}
{"question": "what have been inhabited for millennia by aboriginal peoples?", "answer": "The lands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lands"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "european books and maps began referring to this region as Canada in what year?", "answer": "1545", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1545"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1535", "extracted_entity": null}
{"question": "what was Canada's most important industry until the 1800s?", "answer": "The fur trade", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The fur trade"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that it angered many residents of the thirteen colonies?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes it did", "extracted_entity": null}
{"question": "how do cuba and declining participation relate?", "answer": "Canada maintains full relations with both.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada maintains full relations with both."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Cuba and declining participation relate because Cuba is a", "extracted_entity": null}
{"question": "is it true that provinces have a large degree of autonomy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what did Canada adopt in 1965?", "answer": "The Maple Leaf Flag", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Maple Leaf Flag"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada adopted the Canadian Charter of Rights and Fre", "extracted_entity": null}
{"question": "what arrived in Britain?", "answer": "The first Canadian Army units in WW II", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first Canadian Army units in WW II"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the first group of British people to arrive in Australia", "extracted_entity": null}
{"question": "are Canada 's two official languages English and French ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "French", "extracted_entity": null}
{"question": "are Canada 's official national sports ice hockey ( winter ) and lacrosse ( summer ) ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Lacrosse", "extracted_entity": null}
{"question": "is Canada a geographically vast and ethnically diverse country ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ?", "answer": "no!", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no!"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Nova Scotia", "extracted_entity": null}
{"question": "did Cartier not use the word ` Canada ' to refer to not only that village , but the entire area subject to Donnacona , Chief at Stadacona ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did continental European immigrants not settle the prairies ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it the world 's second largest country by total area?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened in 1867?", "answer": "Canada was formed as a federal, semi-autonomous polity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada was formed as a federal, semi-autonomous polity"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1867 was the year of the", "extracted_entity": null}
{"question": "are ducks in the Arctic Northern Hemisphere migratory?", "answer": "Some are", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is a drake a male?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do all ducks \"quack\"?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the name of a specialized species of duck adapted to catch large fish?", "answer": "the smew", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the smew"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what animal attracts the most humor and silliness?", "answer": "The duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The duck", "the duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a donkey", "extracted_entity": null}
{"question": "what is an economic use of a duck?", "answer": "Meat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat", "being farmed for their meat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Ducks are hunted for sport and for", "extracted_entity": null}
{"question": "who is Daffy Duck?", "answer": "A silly cartoon character", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A silly cartoon character", "a silly cartoon character"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Daffy Duck", "extracted_entity": null}
{"question": "what makes it more difficult for a diving duck to fly?", "answer": "They are heavier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are heavier", "they are heavier"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The Diving Duck is more difficult to fly", "extracted_entity": null}
{"question": "what are the reasons for hunting wild ducks?", "answer": "Meat, eggs, and feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do ducks forage underwater?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Diving ducks and sea ducks forage underwater"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is foie gras often made from ducks?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "ducks", "extracted_entity": null}
{"question": "what economic uses to ducks have?", "answer": "They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "They can be farmed for meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "ducks are used to make foie gras", "extracted_entity": null}
{"question": "what kind of ducks feed on land?", "answer": "Dabbling ducks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dabbling ducks", "Dabbling ducks feed on land"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "\"Feral\" ducks.", "extracted_entity": null}
{"question": "what unrelated water birds are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "cormorants, gulls, and", "extracted_entity": null}
{"question": "what does the word duck mean?", "answer": "It is the common name for a number of species in the Anatidae family of birds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the common name for a number of species in the Anatidae family of birds.", "to bend down low as if to get under something"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are some common predators of ducks?", "answer": "Pike, crocodilians, herons, hawks and eagles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pike, crocodilians, herons, hawks and eagles.", "pike, crocodilians, and other aquatic hunters"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does a Mallard's tongue work?", "answer": "It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "It uses short spikes to push struggling prey and other food down its throat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The tongue of a Mallard duck is a", "extracted_entity": null}
{"question": "are diving ducks heavier tha dabbling ducks?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are adult ducks fast fliers?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are ducks an accepted presence in some populated areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what types of unrelated water birds with similar forms are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why are ducklings particularly vulnerable?", "answer": "Their inability to fly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their inability to fly.", "their inability to fly makes them easy prey"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The ducklings are particularly vulnerable because they", "extracted_entity": null}
{"question": "what are some economic uses for duck?", "answer": "Meat,eggs,feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat,eggs,feathers", "being farmed for their meat, eggs, feathers, (particularly their down)"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "eggs", "extracted_entity": null}
{"question": "what allows a duck to filter water out of the side of their beaks and keep food inside?", "answer": "Tiny rows of plates called lamellae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tiny rows of plates called lamellae", "tiny rows of plates called lamellae like a whale's baleen"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what expression is part of a conceptual framework for testing computer systems?", "answer": "Quacks like a duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quacks like a duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did the expression \"a sitting duck\" come from?", "answer": "In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "In many areas, wild ducks of various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys."], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "A sitting duck is a person or object that", "extracted_entity": null}
{"question": "is Egypt bordered by the Gaza Strip?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Egypt the most populated country in Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt is the most populated country in Africa", "extracted_entity": null}
{"question": "does Egypt have political influence in the Middle East?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was the Six Day War?", "answer": "1967", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1967"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1967", "extracted_entity": null}
{"question": "what religions has Egypt outlawed?", "answer": "All but Christianity, Islam, and Judaism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["All but Christianity, Islam, and Judaism", "all religions and belief except Islam, Christianity and Judaism"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Christianity, Judaism, and Islam", "extracted_entity": null}
{"question": "what is the poulation of Egypt?", "answer": "more than 78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["more than 78 million", "78 million people"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "100,000,0", "extracted_entity": null}
{"question": "why does most of Egypt's population live near the Nile?", "answer": "the only arable agricultural land is found there,", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the only arable agricultural land is found there,", "The only arable agricultural land is found there"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Because the Nile River is the only source of", "extracted_entity": null}
{"question": "are there a large number of Jews living in Egypt today?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "during what period was Akhenaten a Pharaoh?", "answer": "The New Kingdom (c.1550\u22121070 BC) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The New Kingdom (c.1550\u22121070 BC) ", "The New Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Akhenaten was a Pharaoh from", "extracted_entity": null}
{"question": "is Egypt in Asia?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Egypt receive the least rainfall in the world?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt", "extracted_entity": null}
{"question": "does Egypt's foreign policy operates along moderate lines?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "since when has Egypt been a republic?", "answer": "June 18 1953.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 18 1953.", "June 18 1953"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when were the Great Sphinx and the Pyramids of Giza built?", "answer": "During the Old Kingdom.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Old Kingdom.", "During the Old Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "2551 BC", "extracted_entity": null}
{"question": "what countries border Egypt?", "answer": "Libya, Sudan, the Gaza Strip and Israel.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Libya, Sudan, the Gaza Strip and Israel.", "Libya, Sudan, the Gaza Strip and Israel"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Libya, Sudan, Israel, Gaza", "extracted_entity": null}
{"question": "what is the estimated population of Egypt?", "answer": "More than 78 million.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 78 million.", "78 million"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "100000000", "extracted_entity": null}
{"question": "does snow fall in Egypt?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did organized agriculture appear in the Nile Valley?", "answer": "6000 BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6000 BC.", "About 6000 BC"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "10000 BC", "extracted_entity": null}
{"question": "are elephants the largest land animals alive today?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can an elephant kill a rhinoceros?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are elephants good swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "elephants are good swimmers.", "extracted_entity": null}
{"question": "how many living species of African Elephants are there?", "answer": "2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2", "The African Elephant genus contains two (or, arguably, three) living species."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "how long may elephants live?", "answer": "70 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70 years", "As long as 70 years, sometimes longer."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why does the phrase 'elephants never forget' have no metaphorical meaning?", "answer": "it refers literally to elephants supposedly having an excellent memory", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it refers literally to elephants supposedly having an excellent memory", "Because it refers literally to elephants supposedly having an excellent memory."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The phrase 'elephants never forget' has", "extracted_entity": null}
{"question": "how many living species of Asian Elephants are there?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1", "one"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "how much do elephants weight at birth?", "answer": "120 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["120 kilograms", "At birth it is common for an elephant calf to weigh 120 kilograms (265 lb)."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "120kg", "extracted_entity": null}
{"question": "what did Aristotle say about elephants?", "answer": "the beast which passeth all others in wit and mind", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the beast which passeth all others in wit and mind", "Aristotle once said the elephant was \"the beast which passeth all others in wit and mind.\""], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "elephants are large, grayish, and", "extracted_entity": null}
{"question": "are elephant populations in West Africa generally small and fragmented?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Asian elephant larger than the African?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Sometimes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, the Asian elephant is larger than", "extracted_entity": null}
{"question": "is an elephant's skin tough?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "tough", "extracted_entity": null}
{"question": "how do elephants communicate over long distances?", "answer": "by producing and receiving low frequency sound", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by producing and receiving low frequency sound", "By producing and receiving low-frequency sound (infrasound)"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when do African elephants lie down?", "answer": "when they are sick or wounded", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when they are sick or wounded"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "at night", "extracted_entity": null}
{"question": "how long is the elephant's gestation period?", "answer": "22 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["22 months"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "22 months", "extracted_entity": null}
{"question": "how many species of African elephants have been proposed?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "what are the elephant's ears important for?", "answer": "temperature regulation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["temperature regulation"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "They help to cool down the animal by evap", "extracted_entity": null}
{"question": "what land animal has the largest brain?", "answer": "elephant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["elephant", "The elephant"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "a human", "extracted_entity": null}
{"question": "are elephants mammals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have elephants been used as working animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "elephants", "extracted_entity": null}
{"question": "do they prefer forested areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what is the world population of Asian elephants?", "answer": "60000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60000", "The world population of Asian elephants \u2013 also called Indian Elephants or Elephas maximus \u2013 is estimated to be around 60,000"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "450000", "extracted_entity": null}
{"question": "what part of an elephant is very important for temperature regulation?", "answer": "Ears", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ears", "The large flapping ears of an elephant are also very important for temperature regulation."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the ears", "extracted_entity": null}
{"question": "where was the largest elephant ever recorded shot?", "answer": "Angola", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Angola", "The largest elephant ever recorded was shot in Angola in 1956."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "in 1936, the largest ele", "extracted_entity": null}
{"question": "is an Asian elephant smaller than an African elephant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what land animal is larger than an elephant?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "None, the elephant is the largest land animal."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "a blue whale", "extracted_entity": null}
{"question": "has tusklessness become a rare abnormality?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, tusklessness has become a widespread hereditary trait. "], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were the treaties signed in 1947 and 1948 with the Ukraine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is Finnish a member of the Baltic-Finnic subgroup of the Uralic languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finnish", "extracted_entity": null}
{"question": "is the most read newspaper in Finland Taloussanomat?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "is the most read newspaper in Finland Taloussan", "extracted_entity": null}
{"question": "which Russian army general conquered Finland in 1809?", "answer": "Alexander I", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alexander I"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Alexander Vasilyevich Suvorov", "extracted_entity": null}
{"question": "what is the life expectancy for men in Finland?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "79 years", "extracted_entity": null}
{"question": "in what year were the \"Games of the XV Olympiad\" held?", "answer": "1952", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1952"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what body of water lies to the south of Finland?", "answer": "the Gulf of Finland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Gulf of Finland", "Archipelago Sea"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Baltic Sea", "extracted_entity": null}
{"question": "name an animal that is growing in number due to recent conservation efforts?", "answer": "Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "Golden Eagle"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The black-footed ferret", "extracted_entity": null}
{"question": "today, are there more cell phone subscriptions than people in Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Matti Vanhanen the Prime Minister of Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No, Matti Vanhanen is the President", "extracted_entity": null}
{"question": "is the capital city Oslo?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Oslo", "extracted_entity": null}
{"question": "does it have a border with Norway?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the life expectancy for men?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "80 years", "extracted_entity": null}
{"question": "what is the biggest city in Finland?", "answer": "Greater Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greater Helsinki", "the cities of the Greater Helsinki metropolitan area"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki", "extracted_entity": null}
{"question": "who is the most popular rock group in Finland?", "answer": "CMX", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["CMX"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "HIM", "extracted_entity": null}
{"question": "what is Finland's economy like?", "answer": "a highly industrialised, free-market economy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a highly industrialised, free-market economy"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is an advanced economy with a highly industrialized", "extracted_entity": null}
{"question": "where is Finland located?", "answer": "Northern Europe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Europe"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland", "extracted_entity": null}
{"question": "what is the weather like in summer?", "answer": "relatively warm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["relatively warm"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "It is hot.", "extracted_entity": null}
{"question": "does salmon remain the favorite of fly rod enthusiasts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there cathedrals scattered all across Finland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki Cathedral", "extracted_entity": null}
{"question": "according to Reader's Digest, is Finland best for living?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is the best country to live in.", "extracted_entity": null}
{"question": "when did the first verifiable written documents appear?", "answer": "Twelfth Century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twelfth Century", " The first verifiable written documents appeared in the twelfth century. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "3100 BCE", "extracted_entity": null}
{"question": "what is the capital city?", "answer": "Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helsinki", "The capital city is Helsinki. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the most extensively celebrated holiday?", "answer": "Christmas", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas", "Christmas is the most extensively celebrated holiday."], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Christmas", "extracted_entity": null}
{"question": "does a large proportion of the population speak Swedish as its mother tongue?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do women live longer than men?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "women", "extracted_entity": null}
{"question": "what is a country with which Finland is involved in an international conflict?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "Finland is not involved in international conflicts or border disputes. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland", "extracted_entity": null}
{"question": "was Ford a member of the House of Representatives?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "for how long was Ford a member of the House of Representatives?", "answer": "Over eight years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over eight years.", "Ford served for over eight years as the Republican Minority Leader of the United States House of Representatives"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "25 years", "extracted_entity": null}
{"question": "was Gerald Ford a member of Delta Kappa Epsilon?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did ford get an award called \"Congressman's congressman\"?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, the award is called \"Golden", "extracted_entity": null}
{"question": "did ford attend the University of Michigan?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what positions Ford played in the school football team?", "answer": "Center and linebacker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Center and linebacker", "Captain"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Running back", "extracted_entity": null}
{"question": "what did Ford say about his biological father?", "answer": "He was abusive and had a history of hitting his mother.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was abusive and had a history of hitting his mother.", "his biological father was abusive and had a history of hitting his mother"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "I am not a bastard.", "extracted_entity": null}
{"question": "who did Ford nominate for Vice President?", "answer": "Bob Dole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bob Dole", "Nelson Rockefeller"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Ford active about Vietnamese affairs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Nixon", "extracted_entity": null}
{"question": "had Ford's wife married before?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Ford related with the assassination of John F. Kennedy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford Motor Company", "extracted_entity": null}
{"question": "was Gerald Ford the 38th President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Gerald Ford", "extracted_entity": null}
{"question": "did Gerald Ford serve as the Republican Minority Leader of the House of Representatives?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Gerald Ford", "extracted_entity": null}
{"question": "was Ford an Eagle Scout?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what district was Ford elected from?", "answer": "Michigan's 5th congressional district", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michigan's 5th congressional district"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "how many Eagle Scouts were involved in Ford's funeral procession?", "answer": "400", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["400", "About 400"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "14", "extracted_entity": null}
{"question": "which future Heisman Trophy winner did Ford tackle?", "answer": "Jay Berwanger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jay Berwanger"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has another US President been an Eagle Scout?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Ford need to do extra work to pay for college?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford Motor Company", "extracted_entity": null}
{"question": "have more than five Presidents lived past the age of 90?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln, John F. Kennedy, Gerald", "extracted_entity": null}
{"question": "how do kwajalein and eniwetok relate?", "answer": "The Monterey supported landings at both locations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monterey supported landings at both locations."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Ford receive on April 13, 1942?", "answer": "Ford received a commission as ensign in the US Naval Reserve.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ford received a commission as ensign in the US Naval Reserve."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "The Ford Motor Company received a contract from the U", "extracted_entity": null}
{"question": "was Ford released from the hospital?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford was released from the hospital.", "extracted_entity": null}
{"question": "was Ford interred at his Presidential Museum in Grand Rapids, Michigan?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Ford one of only four former Presidents to live to 90 or more years of age ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "George H. W. Bush", "extracted_entity": null}
{"question": "was Ford the last surviving member of the Warren Commission ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Grand Rapids supporters not urge him to take on Bartel J. Jonkman , the incumbent Republican congressman ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did they not claim Ford 's pardon was quid pro quo in exchange for Nixon 's resignation ?", "answer": "They did claim Ford's pardon was quid pro quo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They did claim Ford's pardon was quid pro quo."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Ghana have twice the per capita output of the poorer countries in West Africa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is English Ghana's official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "did the Dutch build the Elmina Castle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Elmina Castle", "extracted_entity": null}
{"question": "when did Ghana achieve independence from the United Kingdom?", "answer": "1957", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1957"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "6th March 1957", "extracted_entity": null}
{"question": "how many regions is Ghana divided into?", "answer": "10", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "six", "extracted_entity": null}
{"question": "who is the head of state of Ghana?", "answer": "President John Agyekum Kofuor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["President John Agyekum Kofuor", "John Agyekum Kufuor"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the dominant religion in Ghana?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "christian"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": null}
{"question": "what are the two subfamilies of the Native Ghanaian languages?", "answer": "Kwa and Gur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kwa and Gur"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the weather like at Lake Volta?", "answer": "Warm and comparatively dry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Warm and comparatively dry", "warm and comparatively dry"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Ghana in Asia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No, Ghana is in Africa", "extracted_entity": null}
{"question": "is English the official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "is Ghana's head of state John Agyekum Kufuor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "John Agyekum Kufuor is", "extracted_entity": null}
{"question": "what does the word Ghana mean?", "answer": "warrior king", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["warrior king", "Warrior King"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the word Ghana means \"warrior king\"", "extracted_entity": null}
{"question": "who was Kwame Nkrumah?", "answer": "founder and first president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["founder and first president", "founder and first president of the modern Ghanaian state"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Kwame Nkrumah was a Gh", "extracted_entity": null}
{"question": "where is old Ghana in relation to present Ghana?", "answer": "500 miles north", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500 miles north", "500 miles north of the present Ghana, and occupied the area between Rivers Senegal and Niger"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who makes up Ghana's Parliament?", "answer": "the New Patriotic Party and National Democratic Congress", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the New Patriotic Party and National Democratic Congress"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the House of Parliament", "extracted_entity": null}
{"question": "what European countries established states in Ghana?", "answer": "The UK", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The UK", "Portugual, France, Britain, Netherlands"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Gray Wolf share an ancestry with the domestic dog?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do wolf pups tend to have darker fur than adults?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, They do tend to have darker furs"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do wolves leave their pack?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "They leave their birth pack when they reach sexual maturity"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes, wolves leave their pack", "extracted_entity": null}
{"question": "what kinds of coats do wolves have?", "answer": "bulky coats with two layers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bulky coats with two layers", "Tough guard hairs that repel water and dirt, and a dense, water-resistant layer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do wolves have scent glands?", "answer": "Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "between a wolf's toes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolves have scent glands in their p", "extracted_entity": null}
{"question": "how much do wolves weigh?", "answer": "typically varies between 32 and 62 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["typically varies between 32 and 62 kilograms", "between 32 and 68 kilograms "], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "150kg", "extracted_entity": null}
{"question": "why do wolves howl?", "answer": "Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "To pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is surplus killing?", "answer": "Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "the killing of several prey animals too numerous to eat at one sitting"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "the process by which the surplus of a comm", "extracted_entity": null}
{"question": "are Gray Wolves native to North America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No. Current theory suggests that it's from Eurasia"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Gray Wolf a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are a wolf's teeth its main weapons?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolverines", "extracted_entity": null}
{"question": "are wolves built for stamina?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is the largest gray wolf population thought to be found?", "answer": "Kazakhstan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kazakhstan"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "to what genus does the gray wolf belong?", "answer": "Canis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canis"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when do wolves molt?", "answer": "Late Spring or Early Summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Late Spring or Early Summer", "late spring or early summer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is it beneficial for alpha males and females to forcefully prevent other wolves from mating?", "answer": "A pack can only suppport one littler per year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A pack can only suppport one littler per year", "The pack tension rises as each mature wolf feels urged to mate"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what type of tools do biologists use to capture wolves for tagging?", "answer": "Darting and Foot hold traps.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Darting and Foot hold traps.", "Darting and foot hold traps"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "forward, erect ears and slightly bristle hackles are a sign of what in wolves?", "answer": "Dominance", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dominance"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "fear", "extracted_entity": null}
{"question": "was Grover Cleveland born in New York?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland was born in New York.", "extracted_entity": null}
{"question": "did Grover Cleveland win the 1884 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland won the 1884", "extracted_entity": null}
{"question": "did Grover Cleveland support women's suffrage?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland did not support women's suff", "extracted_entity": null}
{"question": "where was Grover Cleveland married?", "answer": "In the White House", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the White House", "in the Blue Room in the White House"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "White House", "extracted_entity": null}
{"question": "what did Cleveland die from?", "answer": "A heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A heart attack", "a heart attack"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Cancer", "extracted_entity": null}
{"question": "how many years after Cleveland left office did the U.S. win the Spanish-American War?", "answer": "One Year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One Year", "one"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1898", "extracted_entity": null}
{"question": "what did Cleveland's opponents say in 1884 to counter his innocent image?", "answer": "That he had fathered an illegitimate child", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["That he had fathered an illegitimate child", "that he had fathered an illegitimate child while he was a lawyer in Buffalo"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Cleveland's opponents said that he was cor", "extracted_entity": null}
{"question": "who did Cleveland run against in 1884?", "answer": "former Senator James G. Blaine of Maine", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["former Senator James G. Blaine of Maine", "James G. Blaine"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "James G. Blaine", "extracted_entity": null}
{"question": "why did Cleveland want to hide his cancer surgery from the public?", "answer": "To avoid further market panic", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To avoid further market panic", "because of the financial depression of the country"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Grover Cleveland the twenty-seventh president of the United States?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "is Grover Cleveland honest?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland is not honest.", "extracted_entity": null}
{"question": "was Grover Cleveland married in the whitehouse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "when was he elected sheriff of Erire County, New York?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "1870"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "2019", "extracted_entity": null}
{"question": "when did he die?", "answer": "June 24 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 24 1908", "1908 ", "1908"], "entity_annotations": ["Henri_Becquerel", "Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1922", "extracted_entity": null}
{"question": "which election did Grover Cleveland win?", "answer": "1884 and 1892 presidential elections", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1884 and 1892 presidential elections"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1884", "extracted_entity": null}
{"question": "who were Grover Cleveland's parents?", "answer": "Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "Reverend Richard Cleveland and Anne Neal."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many children did Grover Cleveland have?", "answer": "5", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5", "Six."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "who lost control of his party to the agrarians and silverites in 1896?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "William Jennings Bryan", "extracted_entity": null}
{"question": "is it true that he sent in federal troops to chicago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, it is true that he sent in federal", "extracted_entity": null}
{"question": "was Cleveland born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Grover Cleveland elected Sheriff of Erie County, New York?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland", "extracted_entity": null}
{"question": "was Cleveland 's portrait on the U.S. $ 1000 bill from 1928 to 1946 ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is he buried in the Princeton Cemetery of the Nassau Presbyterian Church ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he is buried in the Princeton", "extracted_entity": null}
{"question": "did the Department of the Interior not charge that the rights of way for this land must be returned to the public because the railroads failed to extend their lines according to agreements ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "The Department of the Interior did not charge that", "extracted_entity": null}
{"question": "did the team , sedating Cleveland with nitrous oxide -LRB- laughing gas -RRB- , not remove his upper left jaw and portions of his hard palate ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "The team sedated Cleveland with nitrous ox", "extracted_entity": null}
{"question": "he became chief engineer in the Department of Bridges and Highways in what year?", "answer": "in 1894", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 1894"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "what led Becquerel to investigate the spontaneous emission of nuclear radiation?", "answer": "phographic plates being fully exposed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["phographic plates being fully exposed"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel was a physicist who", "extracted_entity": null}
{"question": "is it true that he married louise d\u00e9sir\u00e9e lorieux in 1890?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he shared the nobel prize in physics?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did he marry louise d\u00e9sir\u00e9e lorieux?", "answer": "1890", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1890"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1696", "extracted_entity": null}
{"question": "was Becquerel elected Permanent Secretary of the Acad\u00e9mie des Sciences?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb", "extracted_entity": null}
{"question": "was Becquerel born in Paris into a family which, including he and his son Jean, produced four generations of scientists?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Jean Baptiste Becquerel", "extracted_entity": null}
{"question": "who won the 1903 Nobel Prize in Physics for discovering radioactivity ?", "answer": "Henri Becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Marie Curie", "extracted_entity": null}
{"question": "did he not die at the age of 55 in Le Croisic ?", "answer": "Yes he died at the age of 55", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he died at the age of 55"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he did not die at the age of", "extracted_entity": null}
{"question": "did one places between the phosphorescent substance and the paper a piece of money or a metal screen not pierce with a cut-out design ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1896?", "answer": " Becquerel accidentally discovered radioactivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Becquerel accidentally discovered radioactivity."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "A tornado struck the town of Goliad", "extracted_entity": null}
{"question": "what happened with a bromide emulsion in two sheets of very thick black paper?", "answer": "the plate does not become clouded upon being exposed to the sun for a day", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the plate does not become clouded upon being exposed to the sun for a day"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is there a becquerel crater on the moon and a becquerel crater on mars?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel crater on the Moon is", "extracted_entity": null}
{"question": "are the Javanese the largest and politically dominant ethnic group in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the Javanese are the largest and politically", "extracted_entity": null}
{"question": "is Indonesia a monarchy with a presidential system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is a republic with a presidential system", "extracted_entity": null}
{"question": "did the Japanese invasion and subsequent occupation during WWII end Dutch rule?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "in what year did East Timor secede from Indonesia?", "answer": "1999", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1999"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "2002", "extracted_entity": null}
{"question": "when did Islam become the dominant religion in Java and Sumatra?", "answer": "the end of the 16th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the end of the 16th century", "by the end of the 16th century"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Islam", "extracted_entity": null}
{"question": "does Indonesia have the world's hightest level of biodiversity?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No, Brazil has the highest level of biod", "extracted_entity": null}
{"question": "was Indonesia named after it became an independent country?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Republic of Indonesia", "extracted_entity": null}
{"question": "why did Indonesia break off relations with the People's Republic of China?", "answer": "because of anti-communist purges early in the Suharto era", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["because of anti-communist purges early in the Suharto era", "anti-communist purges early in the Suharto era"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which spice originally attracted Europeans to Indonesia?", "answer": "nutmeg, cloves, or cubeb pepper", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nutmeg, cloves, or cubeb pepper", "nutmeg, cloves, and cubeb pepper"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Jakarta?", "answer": "Indonesia's special capital region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia's special capital region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Jakarta is the capital and largest city of Indones", "extracted_entity": null}
{"question": "what shares land borders with Papua New Guinea, East Timor and Malaysia?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "what are the largest and politically dominant ethnic group?", "answer": "the Javanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Javanese"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happens when the srivijaya kingdom formed trade links?", "answer": "The indonesian archipelago became an important trade region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The indonesian archipelago became an important trade region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "it made trade easier and more efficient", "extracted_entity": null}
{"question": "is it true that indonesia has vast areas of wilderness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Java", "extracted_entity": null}
{"question": "is Indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Sports in Indonesia generally male-orientated?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Indonesia the world's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "are sports in Indonesia generally male-orientated and spectator sports are often associated with illegal gambling ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "soccer", "extracted_entity": null}
{"question": "is Pencak Silat an Indonesian martial art ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Indonesia the world 's largest Muslim population ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "who has its own political legislature and governor ?", "answer": "each province", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["each province"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "California", "extracted_entity": null}
{"question": "is it the world 's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "is it the world 's fourth most populous country and the most populous muslim-majority nation?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia is a republic.", "extracted_entity": null}
{"question": "is James Monrow the fifth president of US?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the first word on the page?", "answer": "James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James", "James_Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": null}
{"question": "what is the first number on the page?", "answer": "28", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "when did Monroe's presidency expired?", "answer": "March 4, 1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1825", "March 4, 1825."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1825", "extracted_entity": null}
{"question": "what is Monroe's father's name?", "answer": "Spence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spence", "Spence Monroe."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Monroe's daughter's name?", "answer": "Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does \"Era of Good Feelings\" refers to?", "answer": "Reduced tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Reduced tension", "Monroe allowed his political base to decay, which reduced tensions and led to the naming of his era as the \"Era of Good Feelings\"."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Monroe' wedding happen at the Trinity Church in New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Monroe Doctrine?", "answer": "A doctrine declaring U.S. opposition to European interference in the Americas.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A doctrine declaring U.S. opposition to European interference in the Americas.", "In it, Monroe proclaimed the Americas should be free from future European colonization and free from European interference in sovereign countries' affairs. It further stated the United States' intention to stay neutral in European wars and wars between European powers and their colonies, but to consider any new colonies or interference with independent countries in the Americas as hostile acts toward the United States. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "It was a foreign policy of the United States that", "extracted_entity": null}
{"question": "was James Monroe President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe", "extracted_entity": null}
{"question": "did James Monroe attend the College of William and Mary?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe did not attend the College of William", "extracted_entity": null}
{"question": "did James Monroe fight in the Continental Army?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe did fight in the Continental Army", "extracted_entity": null}
{"question": "where was James Monroe born?", "answer": "Westmoreland County, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westmoreland County, Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Virginia", "extracted_entity": null}
{"question": "where was James Monroe shot?", "answer": "at the Battle of Trenton, in his left shoulder", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the Battle of Trenton, in his left shoulder", "The Battle of Trenton"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who did James Monroe marry?", "answer": "Elizabeth Kortright", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Elizabeth Kortright"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did James Monroe do before being elected governor?", "answer": "He practiced law in Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He practiced law in Virginia.", "He practiced law"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "He was a lawyer.", "extracted_entity": null}
{"question": "in which years were John Monroe elected as President?", "answer": "1817-1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1817-1825"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1816 and 182", "extracted_entity": null}
{"question": "who was John Monroe standing behind in the painting of  Washington Crossing the Delaware?", "answer": "George Washington", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["George Washington"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was James Monroe appointed to Secretary of War?", "answer": "1814", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1814"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1815", "extracted_entity": null}
{"question": "when did James Monroe die?", "answer": "July 4, 1831", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4, 1831"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1831", "extracted_entity": null}
{"question": "when did James Monroe graduate from William and Mary?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": null}
{"question": "when was James Monroe elected president?", "answer": "Monroe was elected president in the election of 1816, and re-elected in 1820.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president in the election of 1816, and re-elected in 1820."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1817", "extracted_entity": null}
{"question": "which property did James Monroe sell in 1817?", "answer": "Monroe Hill on the grounds of the University of Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe Hill on the grounds of the University of Virginia."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The property that James Monroe sold in 1", "extracted_entity": null}
{"question": "when did James Monroe introduce the Monroe Doctrine?", "answer": "December 2, 1823", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 2, 1823"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who did James Monroe live with in New York City?", "answer": "His daughter Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His daughter Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe", "extracted_entity": null}
{"question": "what did James Monroe's letters not contain?", "answer": "No letters survive in which he might have discussed his religious beliefs.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No letters survive in which he might have discussed his religious beliefs."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was the result of the rejection of the Jay Treaty?", "answer": "As a result, the two nations moved closer toward the War of 1812. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As a result, the two nations moved closer toward the War of 1812. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who is depicted holding the flag in the famous painting of Washington Crossing the Delaware?", "answer": "Monroe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what dwindled and eventually died out, starting with the Hartford Convention?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Federalist Party", "extracted_entity": null}
{"question": "is it true that he practiced law in fredericksburg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what did James Monroe make in 1817?", "answer": "two long tours", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two long tours"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Monroe Doctrine", "extracted_entity": null}
{"question": "was Monroe anticlerical?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "was Monroe appointed Minister to France from 1794 to 1796?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what expired on March?", "answer": "Monroe's presidency", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe's presidency"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "2015", "extracted_entity": null}
{"question": "had Monroe racked up many debts during his years of public life ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "who dismantled partisan and sectional coalitions ?", "answer": "many congressmen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["many congressmen"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1811?", "answer": "Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the Battle of Tippecanoe", "extracted_entity": null}
{"question": "what happened in the election of 1816?", "answer": "Monroe was elected president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe won.", "extracted_entity": null}
{"question": "who died in childbirth in 1772?", "answer": "Margaret Miller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who died in 1832?", "answer": "Margaret Miller.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is watt commemorated by statuary in George Square, Glasgow and Princes Street, Edinburgh?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was watt a gentleman, greatly respected by other prominent men of the Industrial Revolution?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was watt a fellow of the Royal Society of Edinburgh and the Royal Society of London?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "four years after opening his shop , Watt began what?", "answer": "Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Watt ranked 22nd in Michael H. Hart 's list of the most influential figures in history ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Watt", "extracted_entity": null}
{"question": "did John Adams represent the Continental Congress in Europe?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Adams raised Congregationalist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Adams an opponent of the Stamp Act?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams was not an opponent of the St", "extracted_entity": null}
{"question": "when did Adams graduate from college?", "answer": "1755.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1755.", "1755"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1982", "extracted_entity": null}
{"question": "who was on the committee with Adams to draft  a Declaration of Independence?", "answer": "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Jefferson call John Adams?", "answer": "The \"Colossus of Independence\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The \"Colossus of Independence\".", "Colossus of Independence"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was Adams' political party?", "answer": "The Federalist Party.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party.", "Federalist"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Adams the first to introduce a bicameral legislature?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The United States Congress", "extracted_entity": null}
{"question": "did John Adams get along with Alexander Hamilton?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams and Alexander Hamilton did not get along with", "extracted_entity": null}
{"question": "did John Adams go to Harvard? ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams did not go to Harvard.", "extracted_entity": null}
{"question": "did John Adams support the Stamp Act of 1765?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Adams' birthplace part of a national park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams National Historical Park", "extracted_entity": null}
{"question": "when did John Adams serve as Vice President?", "answer": "1789-1797", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1789-1797"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1797", "extracted_entity": null}
{"question": "with what party did Adams run for presidency?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Adams buried?", "answer": "United First Parish Church", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["United First Parish Church"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams is buried in the United States Capitol Rot", "extracted_entity": null}
{"question": "who were the midnight judges?", "answer": "They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "They were judges formally appointed days before Adams term expired"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The Midnight Judges were the judges appointed", "extracted_entity": null}
{"question": "in what ways was Adams opposed by Anderw Hamilton?", "answer": "Hamilton wanted to control the army differently than Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hamilton wanted to control the army differently than Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what information did he record in his diary?", "answer": "Descriptions of events and ompressions of men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Descriptions of events and ompressions of men", "He wrote descriptions of events and impressions of men"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was defeated for re-election in the`` Revolution of 1800'' by Thomas Jefferson?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who represented the Continental Congress in Europe?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the Continental Congress sent Benjamin Franklin to Europe to", "extracted_entity": null}
{"question": "what is now part of Adams National Historical Park?", "answer": "John Adams' birthplace", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams' birthplace"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams National Historical Park", "extracted_entity": null}
{"question": "is it true that adams had spent some time as the ambassador?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that massachusetts sent him in 1774?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Massachusetts sent him in 1774.", "extracted_entity": null}
{"question": "who did Massachusetts send in 1774?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the first Continental Congress", "extracted_entity": null}
{"question": "are his last words often quoted as \" Thomas Jefferson survives \" . ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson survives", "extracted_entity": null}
{"question": "the John Adams Library , housed at the Boston Public Library , contains what?", "answer": "Adams's personal collection of more than 3,500 volumes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams's personal collection of more than 3,500 volumes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "adams ' opponents were what?", "answer": "Democratic Republicans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Democratic Republicans"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the election of 1800 not become a bitter and volatile battle , with each side expressing extraordinary fear of the other party and its policies ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1764?", "answer": "Adams married Abigail Smith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams married Abigail Smith"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1764 was the year of the", "extracted_entity": null}
{"question": "is a kangaroo a reptile?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is a kangaroo on the Australian coat of arms?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was James Cook the first to record the name \"Kangooroo?\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "James Cook was the first to record the name \"", "extracted_entity": null}
{"question": "what is a collective noun for kangaroos?", "answer": "mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mob, troop, or court", "Mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "mob", "extracted_entity": null}
{"question": "where do joeys complete postnatal development?", "answer": "marsupium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["marsupium", "A pouch called a marsupium"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do kangaroos use for \"crawl-walking?\"?", "answer": "its hind feet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["its hind feet", "Their hind feet and their tails"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Forelimbs", "extracted_entity": null}
{"question": "why do kangaroos have a wide bite?", "answer": "The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "Because of grazing"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos have a wide bite because", "extracted_entity": null}
{"question": "what is responsible for converting the hydrogen byproduct of fermentation into acetate?", "answer": "bacteria", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bacteria", "The digestive system of a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Hydrogenase", "extracted_entity": null}
{"question": "are wild kangaroos shot for meat?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Wild kangaroos are not shot for meat", "extracted_entity": null}
{"question": "have kangaroos fared well since European settlement?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, they have fared well", "extracted_entity": null}
{"question": "do kangaroos have many natural predators?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is the average life expectancy of a kangaroo?", "answer": "4.6 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["4.6 years", "about 4.6 years", "4 to 6 years"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did the first official report of kangaroo blindness take place?", "answer": "1994", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1994"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "1923", "extracted_entity": null}
{"question": "what are vehicles that frequent isolated roads often fitted with?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "4x4", "extracted_entity": null}
{"question": "do kangaroos eat plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroo", "extracted_entity": null}
{"question": "what are some extinct predators of the kangaroo?", "answer": "Thylacine, Marsupial Lion, Maagalania, and Wonambi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thylacine, Marsupial Lion, Maagalania, and Wonambi", "The Thylacine"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Tiger Quoll", "extracted_entity": null}
{"question": "is a Kangaroo a Marsupial?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the kangaroo an herbivour?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Kangaroos Shy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what method of locomotion do Kangaroos Use?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Hopping", "extracted_entity": null}
{"question": "what is a collective noun for a kangaroo?", "answer": "a mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a mob", "extracted_entity": null}
{"question": "what is a roo?", "answer": "a kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a kangaroo", "extracted_entity": null}
{"question": "what is used to protect a vehicale from a Kangaroo?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a Kangaroo", "extracted_entity": null}
{"question": "what method is used by Kangaroos to travel?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "who asked a nearby local what the creatures were called?", "answer": "Captain James Cook and naturalist Sir Joseph Banks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Captain James Cook and naturalist Sir Joseph Banks"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are four species that are commonly referred to as kangaroos?", "answer": "The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroo, Wallaroo, Wall", "extracted_entity": null}
{"question": "are kangaroos and wallabies adept swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroos and wallabies are ade", "extracted_entity": null}
{"question": "are kangaroos farmed to any extent?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos are not farmed, but", "extracted_entity": null}
{"question": "who also discovered that less than three percent of kangaroos exposed to the virus developed blindness ?", "answer": "Veterinarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Veterinarians"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "different species of kangaroos eat what?", "answer": "different diets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different diets"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "grass", "extracted_entity": null}
{"question": "have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a collision with a vehicle capable of killing a kangaroo ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the game of Marn grook played using a ball made from kangaroo by the Kurnai people ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have kangaroos large , powerful hind legs , large feet adapted for leaping , a long muscular tail for balance , and a small head ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos have large, powerful hind", "extracted_entity": null}
{"question": "is a kangaroo a marsupial from the family macropodidae -LRB- macropods , meaning (`` ` large foot ('' ' -RRB-?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what kinds of changes have larger kangaroos adapted much better to?", "answer": "Changes wrought to the Australian landscape by humans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Changes wrought to the Australian landscape by humans."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have larger kangaroos adapted much better to changes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "wallabies", "extracted_entity": null}
{"question": "is the leopard smaller than the other members of Panthera?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is a leopard larger and less lanky than a cheetah?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are large pythons potential prey for leopards?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what may a leopard be mistaken for?", "answer": "A cheetah or a jaguar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cheetah or a jaguar"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is a hybrid animal resulting from a union between a leopard and a puma?", "answer": "a pumapard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pumapard", "A pumapard"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do leopards often hide their kills?", "answer": "in dense vegetation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in dense vegetation", "in dense vegetation or in trees"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "in trees", "extracted_entity": null}
{"question": "how did early naturalists distinguish between leopards and panthers?", "answer": "by the length of their tails", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by the length of their tails", "By the length of the tail, panthers having longer tails than leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are dark leopards known as colloquially?", "answer": "Black Panthers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Black Panthers", "black panthers"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "black panthers.", "extracted_entity": null}
{"question": "what shape are a leopard's black rosettes in East Africa?", "answer": "circular", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["circular"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The shape of a leopard's black", "extracted_entity": null}
{"question": "is the leopard an Old World mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the leopard of the Felidae family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the leopard one of the four 'big cats'?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopard", "extracted_entity": null}
{"question": "where is the leopard distributed?", "answer": "southern Eurasia and Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["southern Eurasia and Africa", "southern Eurasia and Africa, from Korea to South Africa and Spain"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how long is a leopard's tail?", "answer": "60 to 110cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 to 110cm", "60-110cm"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how much does a leopard weigh?", "answer": "Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "(because of ambiguity) - 28-90kg "], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopard weighs about 200", "extracted_entity": null}
{"question": "does a male leopard weigh more than a female leopard?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the difference between leopards and cheetahs?", "answer": "The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "The leopard has rosettes rather than cheetah's simple spots, but they lack internal spots, unlike the jaguar. The leopard is larger and less lanky than the cheetah but smaller than the jaguar."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopards are more agile and have a wider", "extracted_entity": null}
{"question": "how does the distribution size of the leopard compare to the distribution of other wild cats?", "answer": "As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "the leopard had the largest distribution of any wild cat."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The leopard is the most widely distributed cat", "extracted_entity": null}
{"question": "what resembles that of the similarly-sized cougar in the Americas?", "answer": "The leopard's ecological role", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard's ecological role"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was one of the many species described in Linnaeus's 18th-century work, Systema Naturae?", "answer": "Felis pardus", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felis pardus"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what sort of cats are solitary?", "answer": "Leopards", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the lion", "extracted_entity": null}
{"question": "are leopards circular?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the leopard solitary?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The leopard is solitary", "extracted_entity": null}
{"question": "what centred in Sierra?", "answer": "The leopard men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard men"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Sierra Nevada", "extracted_entity": null}
{"question": "felis pardus was what?", "answer": "One of the many species described in Linnaeus's 18th-century work, Systema Naturae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of the many species described in Linnaeus's 18th-century work, Systema Naturae"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the name of the Roman god of the underworld", "extracted_entity": null}
{"question": "has the New Zealand Rugby League featured the Otahuhu Leopards and then the Tamaki Leopards ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Tamaki Leopards", "extracted_entity": null}
{"question": "were the Leopard men a West African secret society who practised cannibalism ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopard Society", "extracted_entity": null}
{"question": "was the leopard tank a German designed tank which entered service in 1965 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopard 1", "extracted_entity": null}
{"question": "is the black color heritable and caused by only one recessive gene locus ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the leopard -LRB- panthera pardus -RRB- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can a panther be any of several species of large felid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the leopard an agile and graceful predator?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard", "extracted_entity": null}
{"question": "is Liechtenstein bordered by Switzerland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is bordered by Switzerland.", "extracted_entity": null}
{"question": "is Liechtenstein the smallest German-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is the smallest German-speaking", "extracted_entity": null}
{"question": "was Liechtenstein part of the ancient Roman province of Raetia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein was not part of the ancient Roman", "extracted_entity": null}
{"question": "when was the first factory opened?", "answer": "1836", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1836"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1769", "extracted_entity": null}
{"question": "how many municipalities is Liechtenstein divided into?", "answer": "11", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the national currency of Liechtenstein?", "answer": "Swiss franc", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swiss franc"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Swiss Franc", "extracted_entity": null}
{"question": "has Leichtenstein worked to promote the county's image by prosecuting international money-laundering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what company administers Leichtenstein's railways?", "answer": "Austrian Federal Railways", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austrian Federal Railways"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what percentage of Liechtenstein's population is foreign-born?", "answer": "two-thirds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two-thirds", "67%"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "23%", "extracted_entity": null}
{"question": "is there an airport in Liechtenstein?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "there is no airport in Liechtenstein", "extracted_entity": null}
{"question": "is Liechtenstein heavily urbanized?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is heavily urbanized.", "extracted_entity": null}
{"question": "is Liechtenstein doubly landlocked?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is doubly landlocked.", "extracted_entity": null}
{"question": "does Liechtenstein have an army?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "when was Liechtenstein's current constitution adopted?", "answer": "October 1921.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["October 1921.", "October 1921"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1921", "extracted_entity": null}
{"question": "what is the official language of Liechtenstein?", "answer": "German.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German.", "German"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "German", "extracted_entity": null}
{"question": "what countries border Liechtenstein?", "answer": "Switzerland and Austria.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Switzerland and Austria.", "Switzerland and Austria"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Switzerland, Austria", "extracted_entity": null}
{"question": "what Roman province was Liechtenstein part of?", "answer": "Raetia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Raetia.", "Raetia"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Rhaetia", "extracted_entity": null}
{"question": "how many municipalities are within Oberland?", "answer": "6.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.", "6"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the smallest German-speaking country in the world?", "answer": "Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "the Savings and Loans Bank was founded, as was the first cotton-weaving mill in what year?", "answer": "1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1861"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1870", "extracted_entity": null}
{"question": "what enjoys one of the world's highest standards of living?", "answer": "Liechtenstein's population", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein's population"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "United States", "extracted_entity": null}
{"question": "does the state court rule on the conformity of laws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what do most recognizable international company and largest employer have in common?", "answer": "They are Hilti.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are Hilti."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the answer is \"BIG\"", "extracted_entity": null}
{"question": "was Liechtenstein completed in November 2000?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what completed in November?", "answer": "the Kunstmuseum Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Kunstmuseum Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the book", "extracted_entity": null}
{"question": "the State Court rules what?", "answer": "on the conformity of laws with the constitution", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["on the conformity of laws with the constitution"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the State Court rules on the matter", "extracted_entity": null}
{"question": "the Historical Society of the Principality of Liechtenstein plays what?", "answer": "a role in preserving the culture and history of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a role in preserving the culture and history of the country."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The Historical Society of the Principality of Lie", "extracted_entity": null}
{"question": "is the museum collection also the national art collection of Liechtenstein ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is there a small heliport at Balzers in Liechtenstein available for charter helicopter flights ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Heliport Balzers", "extracted_entity": null}
{"question": "are nationals referred to by the plural : Liechtensteiners ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtensteiners", "extracted_entity": null}
{"question": "is it a winter sports resort , although it is perhaps best known as a tax haven ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it a winter sports resort?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it the smallest german-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": null}
{"question": "what happened in moravia , lower austria , silesia , and styria , though in all cases , these territories were held in fief under other more senior feudal lords?", "answer": "The Liechtenstein Dynasty acquired vast swaths of land", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Liechtenstein Dynasty acquired vast swaths of land"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Webster write, \"I can now sleep of nights\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Sleep of nights", "extracted_entity": null}
{"question": "was the arrival of Louis Kossuth (exiled leader of a failed Hungarian revolution) another issue that presented itself during Fillmore's presidency?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Kossuth was an exiled leader of a", "extracted_entity": null}
{"question": "did he die at 11:10 p.m. on March 8, 1874, of the after-effects of a stroke?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "11:10 p.m.", "extracted_entity": null}
{"question": "who got Seward elected to the senate?", "answer": "Weed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Weed", "Weed ultimately got Seward elected to the senate"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "William H. Seward", "extracted_entity": null}
{"question": "where was Commodore Matthew C. Perry sent to open Japan to Western trade?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "Commodore Matthew C. Perry send to Japan"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": null}
{"question": "what is The Remarkable Millard Fillmore?", "answer": "A fake biography based on real events that happened in Fillmore's life.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A fake biography based on real events that happened in Fillmore's life.", "In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events that happened in Fillmore's life"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The Remarkable Millard Fillmore", "extracted_entity": null}
{"question": "did Fillmore form a law partnership before or after he founded the private University of Buffalo?", "answer": "Before.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Before.", "Law partnership founded before University of Buffalo"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Fillmore run for President a second time?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "I don't know (semantic ambiguity)"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how long after the death of his first wife did Fillmore marry Caroline McIntosh?", "answer": "where is the death date of his first wife?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where is the death date of his first wife?"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "3 years", "extracted_entity": null}
{"question": "was Millard Fillmore the thirteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore", "extracted_entity": null}
{"question": "was Millard Fillmore born on January 7, 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore was born on January", "extracted_entity": null}
{"question": "did Millard Fillmore die on March 8, 1974?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore died on March 8", "extracted_entity": null}
{"question": "where was Millard Fillmore born?", "answer": "in a log cabin in Summerhill, New York", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in a log cabin in Summerhill, New York"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "to whom was Millard Fillmore born to?", "answer": "to Nathaniel and Phoebe Millard Fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to Nathaniel and Phoebe Millard Fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "to Abigail Powers Fillmore", "extracted_entity": null}
{"question": "who did Millard Fillmore fall in love with?", "answer": "He fell in love with Abigail Powers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He fell in love with Abigail Powers", "Abigail Powers"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Taylor and Fillmore disagree upon?", "answer": "slavory issues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slavory issues", "Taylor and Fillmore disagreed on the slavery issue in the new western territories taken from Mexico in the Mexican-American War"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "the idea that the United States should expand its borders", "extracted_entity": null}
{"question": "how did Fillmore ascend to the presidency?", "answer": "upon the death of the sitting president, Taylor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon the death of the sitting president, Taylor", "Official White House portrait of Millard FillmoreFillmore ascended to the presidency upon the sudden and unexpected death of President Taylor in July 1850."], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "_____________", "extracted_entity": null}
{"question": "how did the supporters of Henry Clay feel about Fillmore in 1848?", "answer": "they were ok with him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they were ok with him", "angry"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was first apprenticed to a fuller to learn the cloth-making trade?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "he founded the private university of buffalo on what date?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "1846", "extracted_entity": null}
{"question": "who or what fell in love with abigail powers?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Abigail's Mother", "extracted_entity": null}
{"question": "was Fillmore one of the founders of the University of Buffalo?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Another primary objective of Fillmore to preserve the Union from the intensifying slavery debate?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The Union", "extracted_entity": null}
{"question": "was Fillmore the second Chancellor, a position he maintained while both Vice President and President?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Fillmore the first of two presidents to have been an indentured servant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore was the first of two presidents", "extracted_entity": null}
{"question": "is the comic strip Mallard Fillmore named after the president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The comic strip Mallard Fillmore is", "extracted_entity": null}
{"question": "was Fillmore the first U.S. President born after the death of a former president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes, Fillmore was the first U.", "extracted_entity": null}
{"question": "did Fillmore not turn down the honor , explaining that he had neither the `` literary nor scientific attainment '' to justify the degree ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in a log cabin in summerhill , new york?", "answer": "Fillmore was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fillmore was born"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "the 3rd floor of the log cabin in", "extracted_entity": null}
{"question": "who was born precisely at midnight during an electrical storm , to a Serbian family in the village of Smiljan near Gospi\u0107 , in the Lika region of the Croatian Krajina in Military Frontier ( part of the Austrian Empire ) , in the present-day Croatia . ?", "answer": "Nikola Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla", "extracted_entity": null}
{"question": "are there at least two films describing Tesla 's life ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have a number of live theatrical plays based on Tesla 's life been produced and staged worldwide ?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "\"The Secret Life of Nikola Tesla", "extracted_entity": null}
{"question": "do sea otters have long muscular tails?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea otter", "extracted_entity": null}
{"question": "can otters survive in cold water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the giant otter inhabit South Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many species of otter are there?", "answer": "13", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "13", "extracted_entity": null}
{"question": "what do river otters eat?", "answer": "a variety of fish and shellfish, as well as small land mammals and birds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a variety of fish and shellfish, as well as small land mammals and birds", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds. ", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish", "extracted_entity": null}
{"question": "how much do sea otters weigh?", "answer": "30 kg (about 65 pounds)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["30 kg (about 65 pounds)"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do sea otters insulate themselves?", "answer": "a layer of air trapped in their fur(!)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a layer of air trapped in their fur(!)", "As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they have a layer of fur", "extracted_entity": null}
{"question": "why are sea otters nearly extinct?", "answer": "They've been hunted for their fur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They've been hunted for their fur", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is one of the challenges of re-establishing a population of Eurasian otters in the UK?", "answer": "roadkill deaths", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roadkill deaths", "Roadkill deaths have become one of the significant threats to the success of their re-establishment. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is otter a kind of mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many species and genera does otter have?", "answer": "13 species and 7 genera", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13 species and 7 genera", "13 species in 7 genera."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otter", "extracted_entity": null}
{"question": "do otters live in water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters live in water.", "extracted_entity": null}
{"question": "where do sea otters live?", "answer": "Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "The sea otter lives actually in the sea. ", "Beside water", "Pacific coast of North America"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where does the word \"otter\" derive from?", "answer": "The word \"otter\" derives from the Old English word otr, otor or oter. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The word \"otter\" derives from the Old English word otr, otor or oter. ", "The word \"otter\" derives from the Old English word otr, otor or oter. This and cognate words in other Indo-European languages ultimately stem from a root which also gave rise to the English words \"water\", \"wet\" and \"winter\". "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otter derives from the Old English word \"", "extracted_entity": null}
{"question": "does otter give birth or lay egg?", "answer": "give birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["give birth"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otter give birth", "extracted_entity": null}
{"question": "what drives sea otter almost to extinction?", "answer": "Humans hunted them almost to extinction. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Humans hunted them almost to extinction. ", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them almost to extinction. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why otters are considered as totem animals?", "answer": "The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are considered totem animals because they are", "extracted_entity": null}
{"question": "do sea otters live along the Pacific coast?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea otters", "extracted_entity": null}
{"question": "are otters totem animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are otters herbivores?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are carnivores.", "extracted_entity": null}
{"question": "what is the primary item in an otter's diet?", "answer": "Fish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fish.", "fish"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish", "extracted_entity": null}
{"question": "what is an otter's den called?", "answer": "Holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holt", "holt", "A holt"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "a holt", "extracted_entity": null}
{"question": "why is the giant otter becoming increasingly rare?", "answer": "Poaching, habitat loss, and toxins in gold mining.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Poaching, habitat loss, and toxins in gold mining.  ", "poaching, habitat loss, and the use of mercury in illegal alluvial gold mining"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do otters keep themselves warm without blubber?", "answer": "A layer of air trapped in their fur.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A layer of air trapped in their fur.  ", "a layer of air trapped in their fur"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they have a thick layer of fur", "extracted_entity": null}
{"question": "how are otters playful animals?", "answer": "The slide down snowy slopes, apparently for sheer enjoyment.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The slide down snowy slopes, apparently for sheer enjoyment.  ", "They slide repeatedly down snowy slopes for sheer enjoyment."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are playful animals", "extracted_entity": null}
{"question": "what animals are related to otters?", "answer": "weasels, polecats, and badgers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["weasels, polecats, and badgers"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "the sea otter and the North American river ot", "extracted_entity": null}
{"question": "what traps a layer of air, and keeps them dry and warm under water?", "answer": "Long guard hair", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Long guard hair"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "a bubble", "extracted_entity": null}
{"question": "the collective noun romp is sometimes used for a group of what?", "answer": "Otters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Otters"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that otters eat a variety of fish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are otters playful animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are playful animals. They are known", "extracted_entity": null}
{"question": "are otters very active?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otters are very active", "extracted_entity": null}
{"question": "are male otters dog-otters?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "an otter 's den is what?", "answer": "A holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A holt"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "a den", "extracted_entity": null}
{"question": "sea otters eat what?", "answer": "Shellfish and other invertebrates", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shellfish and other invertebrates"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea urchins", "extracted_entity": null}
{"question": "is the myth of Otter 's Ransom the starting point of the Volsunga saga ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Male otters dog-otters , females are bitches and babies are cubs or pups ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "cubs", "extracted_entity": null}
{"question": "is an otter 's den called a holt ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have most otters fish as the primary item in their diet , supplemented by frogs , crayfish and crabs ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do penguins feed on krill?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is the largest living species of penguin?", "answer": "Emperor Penguin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Emperor Penguin", "the Emperor Penguin (Aptenodytes forsteri)", "Emperor"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do penguins live almost exclusively in the Southern Hemisphere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "what is \"tobogganing\"?", "answer": "when penguins slide on their bellies across the snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when penguins slide on their bellies across the snow", "Tobogganing is when penguins slide on their bellies across the snow."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "tobogganing is a form of sl", "extracted_entity": null}
{"question": "why are penguins countershaded?", "answer": "for camouflage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["for camouflage"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are penguins afraid of humans?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguins are not afraid of humans.", "extracted_entity": null}
{"question": "how much time to penguins spend on land?", "answer": "half of their life", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["half of their life", "They spend half of their life on land."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "23 hours", "extracted_entity": null}
{"question": "how many species of penguins are there?", "answer": "between 17 and 20 living species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["between 17 and 20 living species", "Penguin biodiversity varies between 17 and 20 living species, all in the subfamily Spheniscinae."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "18", "extracted_entity": null}
{"question": "how are Isabelline penguins different from most penguins?", "answer": "they have brown rather than black plumage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they have brown rather than black plumage", "Because they are born with brown rather than black plumage."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Isabelline penguins are different from", "extracted_entity": null}
{"question": "are penguins birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are not birds. They are flight", "extracted_entity": null}
{"question": "do penguins have a better than average sense of hearing for birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are penguins considered \"higher waterbirds\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what order do penguins belong to?", "answer": "Sphenisciformes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sphenisciformes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Sphenisciformes", "extracted_entity": null}
{"question": "how tall were the tallest prehistoric penguins?", "answer": "1.80 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.80 meters", "as tall as an adult human"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The tallest prehistoric penguins were", "extracted_entity": null}
{"question": "what makes penguins so agile in the water?", "answer": "Their wings have become flippers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their wings have become flippers", "Smooth plumage preserves a layer of air, ensuring buoyancy, wings are flippers"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are aquatic birds that are well", "extracted_entity": null}
{"question": "why do penguins \"tobaggan\"?", "answer": "It conserves energy while moving quickly", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It conserves energy while moving quickly", "Conserves energy while moving quickly"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "They're sliding on their bellies.", "extracted_entity": null}
{"question": "who spend half of their life on land and half in the oceans?", "answer": "Penguins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is also the distance that Antarctic tourists are told to keep from penguins?", "answer": "3 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 meters"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what have become flippers, useless for flight in the air?", "answer": "Penguins' wings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins' wings"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "wings", "extracted_entity": null}
{"question": "what happens when mothers lose a chick?", "answer": "They sometimes attempt to \"steal\" another chick.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes attempt to \"steal\" another chick."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The chick dies.", "extracted_entity": null}
{"question": "is it true that each penguin gets a turn in the center?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The penguins are sorted by age.", "extracted_entity": null}
{"question": "are all penguins countershaded?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are penguins astonishingly agile?", "answer": "In the water they are.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the water they are."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are penguins at risk?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it also possible that penguin comes from the Latin pinguis , fat ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, penguin does not come from the", "extracted_entity": null}
{"question": "have penguins an average sense of hearing for birds ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can larger penguins dive deep in case of need ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the smallest penguin species the Little Blue Penguin -LRB- also known as the Fairy Penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2.2 lb -RRB- ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Little Blue Penguin -LRB- also", "extracted_entity": null}
{"question": "is it not even known whether the gigantic palaeeudyptines constitute a monophyletic lineage , or whether gigantism was evolved independently in a much restricted Palaeeudyptinae and the Anthropornithinae ?", "answer": "It is not even known.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not even known."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the largest living species the emperor penguin -LRB- aptenodytes forsteri -RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "emperor penguin", "extracted_entity": null}
{"question": "is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The little blue penguin is the smallest p", "extracted_entity": null}
{"question": "what happened in a region not quite 2000 km south of the equator 35 mya?", "answer": "At least one giant penguin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At least one giant penguin."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the last word on the page?", "answer": "Connecticut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Connecticut", "(SSN-22)"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is polar bear a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is polar bear a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is largest polar bear on record?", "answer": "2200 lb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2200 lb", "A huge male, allegedly weighing 1002 kg (2200 lb) shot at Kotzebue Sound in northwestern Alaska in 1960."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The largest polar bear on record is a male polar", "extracted_entity": null}
{"question": "what is polar bear's skin color?", "answer": "white or cream", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["white or cream", "Black"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "white", "extracted_entity": null}
{"question": "how long is polar bear's guard hair?", "answer": "5-15 cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5-15 cm"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is cause of polar bear's skin diseases?", "answer": "mites or other parasites", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mites or other parasites", "Mites or other parasites"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the cause of polar bear's skin diseases", "extracted_entity": null}
{"question": "why polar bear is a special species?", "answer": "The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears are a special species because they", "extracted_entity": null}
{"question": "what do fossil and DNA evidence tell us?", "answer": "The polar bear diverged from the brown bear about 200 thousand years ago.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The polar bear diverged from the brown bear about 200 thousand years ago.", "The polar bear diverged from the brown bear roughly 200 thousand years ago."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Fossil evidence is not as reliable as DNA", "extracted_entity": null}
{"question": "does a polar bear live in the Arctic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears live in the Arctic.", "extracted_entity": null}
{"question": "is a polar bear white in color?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a polar bear at high risk of extinction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how heavy is a male polar bear?", "answer": "300-600 kg (660-1320 lb)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300-600 kg (660-1320 lb)", "Most adult males weigh 350-650 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how heavy was the largest polar bear on record?", "answer": "1002 kg (2200 lb) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1002 kg (2200 lb) ", "He allegedly weighed 1002 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "2,000 pounds", "extracted_entity": null}
{"question": "what does a polar bear's fur provide?", "answer": "It provides the animal with effective camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It provides the animal with effective camouflage.", "A polar bear's fur provides camouflage and insulation"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a warm coat", "extracted_entity": null}
{"question": "do female polar bears weight more than the male?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears are the largest carniv", "extracted_entity": null}
{"question": "how much weight do female polar bears gain during pregnancy?", "answer": "They gain double their weight.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They gain double their weight.", "They double their weight during pregnancy"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can polar bears be seen under infrared photography?", "answer": "Polar bears are nearly invisible under infrared photography.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Polar bears are nearly invisible under infrared photography.", "Only their breath and muzzles can be easily seen"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bears are most active at night.", "extracted_entity": null}
{"question": "what is actually black in color?", "answer": "A polar bear's skin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A polar bear's skin."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Black", "extracted_entity": null}
{"question": "what insulate it against the cold?", "answer": "Its think blubber and fur.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its think blubber and fur."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what includes a lengthy justification of why this species is listed as vulnerable?", "answer": "Database entry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Database entry."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do with greenpeace and the natural resources defense council have in common?", "answer": "They filed lawsuits in California.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They filed lawsuits in California."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "They both have a website.", "extracted_entity": null}
{"question": "are studies insufficient evidence for global protection?", "answer": "It is arguable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is arguable."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are polar bears excellent swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "a semi-aquatic marine mammal , the polar bear has what?", "answer": "It has adapted for life on a combination of land, sea, and ice.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It has adapted for life on a combination of land, sea, and ice."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Ursus maritimus", "extracted_entity": null}
{"question": "garbage is what?", "answer": "Garbage is now recycled or transported to Thompson, Manitoba.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Garbage is now recycled or transported to Thompson, Manitoba."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "A type of waste", "extracted_entity": null}
{"question": "the bears sometimes what?", "answer": "They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the bears sometimes what?", "extracted_entity": null}
{"question": "is the Polar Bear the mascot of Bowdoin college ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "have thumbpolar bears been made both controversial and famous for their distinctive white fur and their habitat ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bear", "extracted_entity": null}
{"question": "did Mitchell Taylor , the Nunavut Government Manager of Wildlife Research , not write a letter to the U.S. Fish and Wildlife Service arguing that local studies are insufficient evidence for global protection at this time ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "are Cubs born in December without awakening the mother ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Qatar rank as the eighth richest country in the world per capita?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar", "extracted_entity": null}
{"question": "do nearly all Qataris profess Islam?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can women legally drive in Qatar?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "when did Qatar become an independent state?", "answer": "September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 3, 1971.", "September 3, 1971"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": null}
{"question": "where is Qatar's telecommunication system centered?", "answer": "Doha.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Doha.", "Doha"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar's telecommunication system is centered", "extracted_entity": null}
{"question": "where did a suicide-bombing kill a teacher in 2005?", "answer": "At the Doha Players Theatre.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At the Doha Players Theatre.", "Doha Players Theatre"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the native pronunciation of Qatar sound like 'cutter'?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is the Arabic word for municipalities?", "answer": "baladiyah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["baladiyah.", "baladiyah"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "\u0645\u062f\u064a\u0646\u0629", "extracted_entity": null}
{"question": "what universities are in Education City?", "answer": "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Georgetown University, Northwestern University,", "extracted_entity": null}
{"question": "what carried with it a tacit recognition of Qatar's status as distinct from Bahrain?", "answer": "The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "qatar became an independent sovereign state in what year?", "answer": "On September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On September 3, 1971."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": null}
{"question": "what consists of a low, barren plain, covered with sand?", "answer": "Much of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much of the country."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do qatar and the wider region relate?", "answer": "Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "The Gulf of Oman is an extension of", "extracted_entity": null}
{"question": "was Qatar University founded in 1973?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Qatar bordered by Saudi Arabia to the south?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what bordered by Saudi?", "answer": "Qatar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Iraq", "extracted_entity": null}
{"question": "who served as the headquarters and one of the main launching sites of the US invasion of Iraq in 2003 ?", "answer": "Qatar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Camp Victory", "extracted_entity": null}
{"question": "has Qatar a modern Telecommunication system centered in Doha ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar has a modern Telecommunication system centered", "extracted_entity": null}
{"question": "was Qatar University founded in 1973 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar University", "extracted_entity": null}
{"question": "did the Qataris not choose as their negotiator the respected entrepreneur and long-time resident of Doha , Muhammed bin Thani ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Muhammed bin Thani", "extracted_entity": null}
{"question": "will oil and gas probably remain the backbone of Qatar 's economy for some time to come ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, probably.", "extracted_entity": null}
{"question": "has it been created with a long term perspective to support the development of Qatar and the wider region , develop local and regional markets , and strengthen the links between the energy based economies and global financial markets ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar Petroleum", "extracted_entity": null}
{"question": "what happened in these positions in english?", "answer": "These allophones cannot occur there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["These allophones cannot occur there."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "A, B, C, D, E,", "extracted_entity": null}
{"question": "can these allophones not occur in these positions in english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is -RRB- , officially the state of qatar -LRB- arabic (: : transliterated as dawlat qatar -RRB- , an arab emirate in southwest asia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Romania border Hungary?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania borders Hungary", "extracted_entity": null}
{"question": "is Romania a secular state?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the president elected by popular vote?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "how many counties is Romania divided into?", "answer": "41.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["41.", "forty-one"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "41", "extracted_entity": null}
{"question": "what is the highest mountain in Romania?", "answer": "Moldoveanu Peak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moldoveanu Peak.", "Moldoveanu Peak"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The highest mountain in Romania is the Moldove", "extracted_entity": null}
{"question": "what is the official language of Romania?", "answer": "Romanian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romanian.", "Romanian"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanian", "extracted_entity": null}
{"question": "is the Romanian economy doing well?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "the Romanian economy is doing well", "extracted_entity": null}
{"question": "are there many Roma in Romania?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many territories joined to form Romania?", "answer": "2.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2.", "2"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "4", "extracted_entity": null}
{"question": "does Romania share a border with Ukraine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Romania declare neutrality during World War I?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania did not declare neutrality during World War", "extracted_entity": null}
{"question": "does Romania share the same language with Moldova?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Practically"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "who was the first gymnast to score a perfect \"ten\"?", "answer": "Nadia Com\u0103neci", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nadia Com\u0103neci"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Nadia Com\u0103neci", "extracted_entity": null}
{"question": "what is the largest city in Romania?", "answer": "Bucharest", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bucharest"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest", "extracted_entity": null}
{"question": "what is the largest ethnic minority in Romania?", "answer": "Hungarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hungarians"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Hungarians", "extracted_entity": null}
{"question": "how many countries in Europe are bigger than Romania?  ?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven", "Eleven"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Italy, France, Germany, Spain, Poland,", "extracted_entity": null}
{"question": "why doesn't Romania have a state religion?  ?", "answer": "Romania is a secular state", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romania is a secular state"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania doesn't have a state religion because", "extracted_entity": null}
{"question": "why wasn't Romania neutral during World War II?", "answer": "it received a Soviet ultimatum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it received a Soviet ultimatum", "The Soviets threatened invasion."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania was not neutral during World War II,", "extracted_entity": null}
{"question": "sibiu, a large city in Transylvania, was chosen as European Capital of Culture in what year?", "answer": "2007", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2007"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "2007", "extracted_entity": null}
{"question": "what are approximately 42,000 years old and have been nicknamed`` John of Anina''?", "answer": "the remains (the lower jaw) of the oldest modern human", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the remains (the lower jaw) of the oldest modern human"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that romania has a population of 21,698,181?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what do economy and law have in common?", "answer": "(not sure how to answer this)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(not sure how to answer this)"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Tourism in Romania attract in 2005?", "answer": "investments worth 400 million euros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["investments worth 400 million euros"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Romania a semi-presidential democratic republic where executive functions are shared between the president and the prime minister ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The president of Romania is the head of state", "extracted_entity": null}
{"question": "was Gellu Naum the leader of the surrealist movement in Romania ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the Bucharest Metro only opened in 1979 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest Metro", "extracted_entity": null}
{"question": "is romania -LRB- , -RRB- a country in southeastern europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania", "extracted_entity": null}
{"question": "is romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania is a semi-presidential unit", "extracted_entity": null}
{"question": "do singaporeans enjoy squid?", "answer": "Yes, they do.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they do.", "yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Singapore's standard of living decrease since independence?", "answer": "No, the standard living was raised.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the standard living was raised.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The Economist Intelligence Unit (EIU", "extracted_entity": null}
{"question": "did the media contribute 15.6% to Singapore's GDP in 2001?", "answer": "No, it contributed 1.56% to GDP in 2001.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, it contributed 1.56% to GDP in 2001.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many people use the bus network daily?", "answer": "More than 2.78 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 2.78 million people.", "2.78 million people"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1250000", "extracted_entity": null}
{"question": "where did Orang Laut people live?", "answer": "Orang Laut people lived around the coast, rivers and smaller islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Orang Laut people lived around the coast, rivers and smaller islands.", "Several hundred indigenous Orang Laut people also lived around the coast, rivers and smaller islands."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Orang Laut people lived in the coastal", "extracted_entity": null}
{"question": "what is the MRT?", "answer": "One of three heavy rail passenger transport lines in Singapore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of three heavy rail passenger transport lines in Singapore.", "Mass Rapid Transit system"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The MRT is a mass rapid transit system", "extracted_entity": null}
{"question": "how is Singapore's climate characterized?", "answer": "Singapore has a tropical rainforest climate with no distinctive seasons.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore has a tropical rainforest climate with no distinctive seasons.", "Its climate is characterized by uniform temperature and pressure, high humidity, and abundant rainfall"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore's climate is characterized by a warm", "extracted_entity": null}
{"question": "how much increase in the media's contribution to GDP is the government seeking?", "answer": "The government seeks to increase its GDP contribution to 3% by 2012. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government seeks to increase its GDP contribution to 3% by 2012. ", "the government seeks to increase its GDP contribution to 3% by 2012. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many days did it take the Imperial Japanese Army to win the Battle of Singapore?", "answer": "Six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six days", "6 days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "3 days", "extracted_entity": null}
{"question": "is Singapore located at the southern tip of the Korean Penisula?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "is Singapore a city-state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore is a city-state.", "extracted_entity": null}
{"question": "has Sang Nila Utama visited Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "what does \"singa\" mean?", "answer": "lion", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lion"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when is the first record of settlement in Singapore?", "answer": "second century AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["second century AD", "The first records of settlement in Singapore are from the second century AD."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1819", "extracted_entity": null}
{"question": "which nation invaded Singapore during World War II?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "During World War II, the Imperial Japanese Army invaded Malaya, culminating in the Battle of Singapore."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": null}
{"question": "why Churchill called the occupation of Singapor by Janpan during WWII \"Britain's greatest defeat\"?", "answer": "The British were defeated in six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British were defeated in six days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Lee Kuan Yew a successful leader of Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Goh Chok Tong succeed Lee as Prime Minister?", "answer": "In 1990, Goh Chok Tong succeeded Lee as Prime Minister.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1990, Goh Chok Tong succeeded Lee as Prime Minister."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "23 September 1990", "extracted_entity": null}
{"question": "what is the primary domestic source of water supply?", "answer": "The primary domestic source of water supply in Singapore is rainfall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The primary domestic source of water supply in Singapore is rainfall."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the national language of Singapore?", "answer": "The national language of Singapore is Malay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The national language of Singapore is Malay."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": null}
{"question": "where is the National Orchid Garden?", "answer": "Singapore Botanic Gardens.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore Botanic Gardens."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The National Orchid Garden is located within the", "extracted_entity": null}
{"question": "why are relations with Malaysia and Indonesia important?", "answer": "Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the most densely populated independent country in the world?", "answer": "Monaco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monaco"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The most densely populated independent country in the world", "extracted_entity": null}
{"question": "who did Sir Thomas Stamford Raffles work for?", "answer": "British East India Company", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The British East India Company", "extracted_entity": null}
{"question": "when was Lee Kuan Yew prime minister of Singapore?", "answer": "1959", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1959"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Lee Kuan Yew was prime minister of Singapore", "extracted_entity": null}
{"question": "what is the punishment for first-degree murder?", "answer": "There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what established a trading post on the island in 1819?", "answer": "British East India Company ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The British.", "extracted_entity": null}
{"question": "what used Singapore as a strategic trading post along the spice route?", "answer": "British", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "what became one of the most important commercial and military centres of the British Empire?", "answer": "Singapore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The City of London", "extracted_entity": null}
{"question": "who boasted a sizeable community of 100,000 by 1869?", "answer": "British India office", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British India office"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do reservoirs and water relate?", "answer": "By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "water reservoirs", "extracted_entity": null}
{"question": "is it true that singapore has several long - standing disputes with malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has Singapore Changi Airport a network of 81 airlines connecting Singapore to 185 cities in 58 countries ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Changi Airport", "extracted_entity": null}
{"question": "has Singapore a network of reservoirs and water catchment areas ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has Urbanization eliminated many areas of former primary rainforest , with the only remaining area of primary rainforest being Bukit Timah Nature Reserve ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Bukit Timah Nature Reserve", "extracted_entity": null}
{"question": "does the economy not depend heavily on exports refining imported goods , especially in manufacturing ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is singapore -LRB- ; (: ; , -RRB- , officially the republic of singapore -LRB- ; (: ; , -RRB- , an island nation located at the southern tip of the malay peninsula?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "singapore", "extracted_entity": null}
{"question": "is singapore the 17th wealthiest country in the world in terms of gdp per capita?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Thedore Roosevelt  a member of the Republican Party?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt was a member of", "extracted_entity": null}
{"question": "does Theodore Roosevelt have a brother?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt has a brother named", "extracted_entity": null}
{"question": "did Theodore Roosevelt go to Harvard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt", "extracted_entity": null}
{"question": "where was Theodore Roosevelt born?", "answer": "28 East 20th Street in the modern-day Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28 East 20th Street in the modern-day Gramercy section of New York City", "a four-story brownstone at 28 East 20th Street, in the modern-day Gramercy section of New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York City, New York", "extracted_entity": null}
{"question": "what did Roosevelt do to improve his physical condition?", "answer": "Roosevelt took up exercise", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roosevelt took up exercise", "He started boxing"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did Roosevelt start Boxing lessons?", "answer": "To deal with bullies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To deal with bullies"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "The answer is that Roosevelt started Box", "extracted_entity": null}
{"question": "what was the nickname of Theodore Roosevelt's sister Anna?", "answer": "Bamie as a child and Bye as an adult", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bamie as a child and Bye as an adult", "Bamie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Bamie", "extracted_entity": null}
{"question": "what was Roosevelt's justification for supporting desegregation of schools in New York?", "answer": "Noting that his children had been educated with other races and there was nothing wrong with it", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Noting that his children had been educated with other races and there was nothing wrong with it", "that his children had been educated with other races and there was nothing wrong with it"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how did newspapers respond to Roosevelt inviting Brooker T. Washington to dinner at the White House?", "answer": "mixing of the races on social occassions", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mixing of the races on social occassions", "They outraged."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Theodore Roosevelt study Judo?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No clue"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Roosevelt receive a Medal of Honor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "nein"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Roosevelt study biology?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He was strong in biology."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where was Theodore Roosevelt from?", "answer": "Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gramercy section of New York City", "New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York", "extracted_entity": null}
{"question": "who was Roosevelt's presidential hero?", "answer": "Lincoln was Roosevelt's presidential hero.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lincoln was Roosevelt's presidential hero.", "Lincoln was Roosevelt's presidential hero"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln", "extracted_entity": null}
{"question": "when did Roosevelt die?", "answer": "On January 6, 1919, Roosevelt died in his sleep.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 6, 1919, Roosevelt died in his sleep."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": null}
{"question": "who helped to fund Roosevelt's African safari?", "answer": "Financed by Andrew Carnegie and his own proposed writings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Financed by Andrew Carnegie and his own proposed writings", "Andrew Carnegie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "The Smithsonian Institution", "extracted_entity": null}
{"question": "did Roosevelt support racial integration in schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He supported a bill allowing school desegregation in New York State."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Roosevelt's family rich?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt's family was rich.", "extracted_entity": null}
{"question": "are turtles ectothermic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles are ectothermic.", "extracted_entity": null}
{"question": "does the mother care for the young?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the mother is the female parent of a child.", "extracted_entity": null}
{"question": "are turtles pets?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles are pets.", "extracted_entity": null}
{"question": "what shape are the eggs of the larest species of turtle?", "answer": "Spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spherical", "spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The eggs of the largest species of turtle", "extracted_entity": null}
{"question": "how often do turtles breed?", "answer": "Every few years or more", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Every few years or more"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles breed on a regular basis,", "extracted_entity": null}
{"question": "what do all turtles and tortoises breathe?", "answer": "air", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["air"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "air", "extracted_entity": null}
{"question": "how do turtles reproduce?", "answer": "They lay eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They lay eggs", "they lay eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles reproduce by laying eggs.", "extracted_entity": null}
{"question": "what has been discovered about turtles organs?", "answer": "They do not gradually break down or become less efficient over time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They do not gradually break down or become less efficient over time", "they do not break down"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The turtle's shell is a b", "extracted_entity": null}
{"question": "what suborder of turtle draws its head into its shell?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Pleurodira", "extracted_entity": null}
{"question": "do sea turtles lay eggs on dry sandy beaches?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "are Pleurodira known as side-necked turtles?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are turtles a part of the Flying Spaghetti Monster?", "answer": "*shrug*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["*shrug*"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No, turtles are not a part of", "extracted_entity": null}
{"question": "how many suborders are turtles divided into?", "answer": "Three.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three.", "three"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "two", "extracted_entity": null}
{"question": "name a turtle popularly kept as a pet.?", "answer": "Russian Tortoises", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Russian Tortoises"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "name four related articles for the article on turtles.?", "answer": "Triassic, Ernst Haeckel, Kunstformen der Natur, Animal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Triassic, Ernst Haeckel, Kunstformen der Natur, Animal"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "if a turtle egg was kept warm, what would likely hatch?", "answer": "A female turtle.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A female turtle.", "A female turtle"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "a female turtle", "extracted_entity": null}
{"question": "what do turles use to breathe in the water?", "answer": "Papillae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Papillae"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "A turle's lungs are connected to", "extracted_entity": null}
{"question": "why are side-necked turtles called side-necked turtles?", "answer": "Because of the way they withdraw their heads into their shells.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because of the way they withdraw their heads into their shells.", "The way they withdraw their heads into their shells."], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "They are called side-necked turt", "extracted_entity": null}
{"question": "do turtles lay eggs underwater?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are turtle eggs leathery and soft?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles do not lay eggs.", "extracted_entity": null}
{"question": "are turtles being examined for longevity genes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what suborder of turtles is extinct?", "answer": "Paracryptodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paracryptodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do turtle eggs preared to eat consist mainly of?", "answer": "Yolk", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yolk"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "protein", "extracted_entity": null}
{"question": "where do sea turtles lay their eggs?", "answer": "Holes Dug into the Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holes Dug into the Mud or Sand", "holes dug into mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "sea turtles lay their eggs on the beach", "extracted_entity": null}
{"question": "what is the smallest suborder of turtles?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are turtle eggs covered in when they incubate?", "answer": "Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Mud or Sand", "mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what type of creatures breathe air and don't lay eggs underwater?", "answer": "Amniotes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amniotes", "amnoites"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Fish", "extracted_entity": null}
{"question": "what are deposited in holes dug into mud or sand?", "answer": "eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what sort of turtles are ectothermic?", "answer": "all of them", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["all of them"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Ectothermic", "extracted_entity": null}
{"question": "are the largest turtles aquatic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The largest turtles are aquatic.", "extracted_entity": null}
{"question": "is aquatic respiration in Australian freshwater turtles being studied?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "these projections , called papillae , have what?", "answer": "a rich blood supply", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a rich blood supply"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "papillae", "extracted_entity": null}
{"question": "the eggs of the largest species are what?", "answer": "spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the eggs of the largest species are the largest eggs", "extracted_entity": null}
{"question": "has this inspired genetic researchers to begin examining the turtle genome for longevity genes ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtle", "extracted_entity": null}
{"question": "can turtles take many years to reach breeding age ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The answer is no, they do not.", "extracted_entity": null}
{"question": "are Immature sea turtles not cared for by the adults ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the Pleurodira sometimes known as the side-necked turtles , a reference to the way they withdraw their heads into their shells ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does the Order Testudines not include both extant -LRB- living -RRB- and extinct species , the earliest known turtles being from around 215 million years ago ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Order Testudines", "extracted_entity": null}
{"question": "was Ulysses Grant a general in the American Civil War?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "General Ulysses Grant", "extracted_entity": null}
{"question": "was Grant's father-in-law a Democrat?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yup"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "No, he was a Republican.", "extracted_entity": null}
{"question": "did Ulysses win the Battle of Champion Hill?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "you betcha"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was Grant's brother in law?", "answer": "Fred Dent", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fred Dent"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Grant born?", "answer": "Point Pleasant, Ohio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Point Pleasant, Ohio", "A log cabin in Point Pleasant, Clermont County, Ohio"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was Grant's political affiliation?", "answer": "Republican", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Republican"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Republican", "extracted_entity": null}
{"question": "why did Grant say \"Damn, I had nothing to do with this batte.\"?", "answer": "It went well.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It went well.", "Because the generals under him acted on their own."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant was referring to the Battle of Vicksburg", "extracted_entity": null}
{"question": "did Horace Greenley lose in the presidential elections of 1872?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "si"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did Grant think that war was unjust?", "answer": "He thought it was designed to gain land open to slavery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He thought it was designed to gain land open to slavery.", "He accepted the theory that it was designed to gain land open to slavery."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "He did not think war was unjust.", "extracted_entity": null}
{"question": "who achieved international fame as the leading Union general in the American Civil War?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "grant was elected president as a Republican in what year?", "answer": "1868", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1868"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1868", "extracted_entity": null}
{"question": "who took a hard line that reduced violence by groups like the Ku Klux Klan?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "The Black Codes", "extracted_entity": null}
{"question": "when did he vote for democrat james buchanan?", "answer": "1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1856"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1856", "extracted_entity": null}
{"question": "who did President Lincoln promote of major general in the regular army, effective July 4?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "was Grant elected president as a Republican?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant", "extracted_entity": null}
{"question": "what was from Pennsylvania?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "The Lenape.", "extracted_entity": null}
{"question": "was it a two-sentence description that completely caught the essence of Ulysses S. Grant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Grant 's favorite brand of bourbon whiskey Old Crow ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Old Crow", "extracted_entity": null}
{"question": "had he smoked only sporadically ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Grant & Perkins not sell harnesses , saddles , and other leather goods and purchase hides from farmers in the prosperous Galena area ?", "answer": "they did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they did"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the famous dragon gate at the entrance to the district at the corner of Grant and Bush Street ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what happened in 1865?", "answer": "he accepted the surrender of Robert E. Lee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he accepted the surrender of Robert E. Lee"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in recent years?", "answer": "his reputation as president has improved", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["his reputation as president has improved"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the earthquake and the subsequent flooding", "extracted_entity": null}
{"question": "is Uruguay located in the northwesten part of Africa?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "is Uruguay's capital Montevideo?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Montevideo", "extracted_entity": null}
{"question": "does Uruguay have cold summers?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who founded Montevideo?", "answer": "The Spanish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Spanish.", "the Spanish", "By the Spanish, in the early 18th century", "Montevideo was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Uruguay's oldest church?", "answer": "San Carlos, Maldonado.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Carlos, Maldonado.", "San Carlos, Maldonado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The church of San Pedro de Mina in Monte", "extracted_entity": null}
{"question": "who heavily influenced the architecture and culture of Montevideo?", "answer": "European immigrants.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["European immigrants.", "European immigrants"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are poor neighborhoods called informally?", "answer": "Cantegriles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cantegriles.", "Cantegriles"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is uruguay's landscape mountainous?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "not really?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay's landscape is not mountainous.", "extracted_entity": null}
{"question": "what religion do most Uruguayans profess?", "answer": "None.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None.", "Roman Catholic"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Catholic", "extracted_entity": null}
{"question": "is Uruguay located in South America?", "answer": "Yes. It's located in the southeastern part", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. It's located in the southeastern part", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay is located in South America.", "extracted_entity": null}
{"question": "can citizens propose changes to the Constitution?", "answer": "Yes. People are allowed to challenge laws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. People are allowed to challenge laws.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Uruguay host the first ever World Cup?", "answer": "Yes, in 1930.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, in 1930.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "how many square kilometres of continental land is Uruguay?", "answer": "176.215 km\u00b2 ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["176.215 km\u00b2 ", "176,214 square kilometres"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "176,215 km\u00b2", "extracted_entity": null}
{"question": "how much of the population is of white European descent?", "answer": "88%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88%", "Approximately 88% of its population are of prevalently white European descent."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "White European", "extracted_entity": null}
{"question": "why was the capital of Uruguay founded?", "answer": "For a military stronghold.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For a military stronghold.", "Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Montevideo", "extracted_entity": null}
{"question": "how many times has Uruguay won the World Cup?", "answer": "Twice. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twice. ", "On two or more occasions."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "how much is the average income of an African woman compared to a European man?", "answer": "African women earns 0.65 * 0.718  = 46.67% of a European man earns in average", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["African women earns 0.65 * 0.718  = 46.67% of a European man earns in average"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The average income of an African woman is 5", "extracted_entity": null}
{"question": "does Uruguay recognize same-sex civil unions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Uruguay border French Guiana?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "is Uruguay's warmest month June?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "June", "extracted_entity": null}
{"question": "what does a citizen use to propose changes to the Constitution?", "answer": "Referendum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Referendum", "Plebiscite"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a constitutional convention", "extracted_entity": null}
{"question": "what religions are found in Uruguay?", "answer": "Roman Catholic, Protestant, Jewish, and nonprofessing.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roman Catholic, Protestant, Jewish, and nonprofessing.", "Roman Catholic, Protestant, Jewish"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholicism", "extracted_entity": null}
{"question": "what has Uruguay done to be competitive in agriculture?", "answer": "Labeling as \"Natural\" or \"Ecological\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Labeling as \"Natural\" or \"Ecological\"", "Use low inputs of labour, technology, and capital, which results in lower yields but also opens the door for Uruguay to market its products as \"natural\" or \"ecological\""], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the names of Uruguay's political parties?", "answer": "Partido Colorado and Partido Blanco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Partido Colorado and Partido Blanco", "Partido Blanco and Partido Colorado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Uruguay the smallest soverign nation in South America?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": null}
{"question": "what was founded by the Spanish in the early 18th century as a military stronghold?", "answer": "Montevideo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montevideo"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Fort San Felipe del Morro", "extracted_entity": null}
{"question": "what has many possible meanings?", "answer": "(What?)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(What?)"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The meaning of life", "extracted_entity": null}
{"question": "how do india and latin america relate?", "answer": "Uruguay exports architectural services to India and Latin America", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uruguay exports architectural services to India and Latin America"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The Indian government has been actively promoting Indian", "extracted_entity": null}
{"question": "what did Uruguay win in 1828?", "answer": "Its independence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its independence"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The Uruguayan War", "extracted_entity": null}
{"question": "is Uruguay very common?", "answer": "No?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is The climate in Uruguay temperate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the main sport in Uruguay football ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Uruguay 's oldest church in San Carlos , Maldonado ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Uruguay a member of the Cairns Group of exporters of agricultural products ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay is a member of the Cairns Group", "extracted_entity": null}
{"question": "is it widely considered the most secular nation in Latin America ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it the second smallest independent country in south america , larger only than suriname and the french overseas department of french guiana?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it a constitutional democracy , where the president fulfills the roles of both head of state and head of government?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The United States of America", "extracted_entity": null}
{"question": "was Woodrow Wilson the thirtieth President of the United States?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "did Woodrow Wilson create the League of Nations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the U.S. join the League of Nations?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was the League of Nations created?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Geneva, Switzerland", "extracted_entity": null}
{"question": "when was Woodrow Wilson born?", "answer": "December 28, 1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 28, 1856"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "1856", "extracted_entity": null}
{"question": "who was President when Wilson finished Congressional Government?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "what field did Woodrow Wilson leave law practice to study?", "answer": "history and political science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["history and political science", "Government"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Political Science", "extracted_entity": null}
{"question": "what caused Wilson to ask Congress to declare war on the Central Powers?", "answer": "German began unrestricted submarine warfare", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German began unrestricted submarine warfare", "World War I"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The Zimmerman Telegram", "extracted_entity": null}
{"question": "what was more damaging than moving students into colleges?", "answer": "His confrontation with Andrew Fleming West", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His confrontation with Andrew Fleming West"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson a member of the Phi Kappa Psi fraternity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Totally"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson an automobile enthusiast?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "ouai"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Wilson's father own slaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yeah"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where is Wilson buried?", "answer": "He was buried in Washington National Cathedral", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was buried in Washington National Cathedral", "Washington National Cathedral"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson is buried in the cemetery of St.", "extracted_entity": null}
{"question": "where did Wilson attend law school?", "answer": "Wilson attended law school at University of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson attended law school at University of Virginia", "University of Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Woodrow Wilson born?", "answer": "Woodrow Wilson was born in Staunton, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Woodrow Wilson was born in Staunton, Virginia", "Staunton, Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Thomas Woodrow Wilson", "extracted_entity": null}
{"question": "did Wilson support desegregation?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Not in the slightest."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Wilson support the committee system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson was not in favor of the committee system.", "extracted_entity": null}
{"question": "did Wilson have any siblings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "what was Scots-Irish and Scottish?", "answer": "His ancestry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His ancestry"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Scots-Irish and Scottish are two", "extracted_entity": null}
{"question": "what defended slavery, owned slaves and set up a Sunday school for them?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the Church of England", "extracted_entity": null}
{"question": "who did Wilson win in 1917?", "answer": "Irish Americans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish Americans"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson", "extracted_entity": null}
{"question": "was Wilson awarded the 1919 Nobel Peace Prize?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Wilson a remarkably effective writer and thinker?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what lived in Columbia?", "answer": "Wilson", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson president of the American Political Science Association in 1910 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he not cast his ballot for John M. Palmer , the presidential candidate of the National Democratic Party , or Gold Democrats , a short-lived party that supported a gold standard , low tariffs , and limited government ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Wilson not spend 1914 through the beginning of 1917 trying to keep America out of the war in Europe ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Wilson , a staunch opponent of antisemitism , sympathetic to the plight of Jews , especially in Poland and in France ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened in 1917?", "answer": "raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Volta an Italian physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Volta buried in the city of Pittsburgh?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta is buried in the city of Pittsburgh", "extracted_entity": null}
{"question": "did Volta have a passion for the study of electricity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta did not have a passion for the study", "extracted_entity": null}
{"question": "what is the battery made by Volta credited to be?", "answer": "the first cell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the first cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "voltaic pile", "extracted_entity": null}
{"question": "what important electrical unit was named in honor of Volta?", "answer": "the volt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the volt", "volt"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "volt", "extracted_entity": null}
{"question": "where did Volta enter retirement?", "answer": "Spain", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spain"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it a disadvantage for something to be unsafe to handle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Lombardy under Napoleon's rule in 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Lombardy", "extracted_entity": null}
{"question": "was the Italian 10.000 lira banknote created before the euro?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "10.000 lira bank", "extracted_entity": null}
{"question": "for how many years did Alessandro Volta live?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta lived 88 years.", "extracted_entity": null}
{"question": "did Alessandro Volta live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta lived to be 80", "extracted_entity": null}
{"question": "what was Alessandro Volta`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "physicist", "extracted_entity": null}
{"question": "how old was Alessandro Volta when he died?", "answer": " 82", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" 82"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82 years old.", "extracted_entity": null}
{"question": "how many years ago was it when Volta married the daughter of Count Ludovico Peregrini , Teresa , with whom he raised three sons?", "answer": "215", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["215"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1816", "extracted_entity": null}
{"question": "is the electrolyte sulphuric acid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is volta buried in the city  of Como?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Como is a city and comune in Lombardy", "extracted_entity": null}
{"question": "was his 1800  paper written in French?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, it was written in English.", "extracted_entity": null}
{"question": "before 1796, was Lombardy ruled by Austria?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he receive the Society`s 1794 Copley Medal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he experiment with individual cells?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, he experimented with the whole body.", "extracted_entity": null}
{"question": "when did lombardy come under Napoleon`s rule?", "answer": "From 1796 to 1815", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1796 to 1815"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did he publish his invention of the Voltaic pile battery?", "answer": "the Philosophical Transactions of the Royal Society", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Philosophical Transactions of the Royal Society"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1799", "extracted_entity": null}
{"question": "did he become professor of experimental physics at the University of Pavia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it true that his passion been always the study of electricity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "It was always his passion.", "extracted_entity": null}
{"question": "his passion been always the study of what?", "answer": "Electricity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Electricity"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "his passion been always the study of the human mind", "extracted_entity": null}
{"question": "is it true that Volta married the daughter of Count Ludovico Peregrini?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta married the daughter of Count Ludovico", "extracted_entity": null}
{"question": "is it true that he published his invention of the Voltaic pile battery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Amedeo Avogadro graduate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he graduated from the University of Turin", "extracted_entity": null}
{"question": "did Avogadro live in England?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where was Avogadro a professor of physics?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "University of Turin", "extracted_entity": null}
{"question": "how many children did Avogadro have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "21", "extracted_entity": null}
{"question": "what is Avogadro most noted for?", "answer": "contributions to the theory of molarity and molecular weight", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["contributions to the theory of molarity and molecular weight"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "what is Avogadro's number?", "answer": "6.02214199x10 23", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.02214199x10 23"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number is 6", "extracted_entity": null}
{"question": "in what year did Avogadro stop teaching at Turin University?", "answer": "1853", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1853", "1823"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1826", "extracted_entity": null}
{"question": "what subject did Avogadro study first?", "answer": "ecclesiastical law", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ecclesiastical law"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Physics", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s birthplace?", "answer": "Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s profession?", "answer": "professor of physics", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of physics"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many years ago was it when he became a professor of physics at the University of Turin?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s first name?", "answer": "Amedeo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro.", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s last name?", "answer": "Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in North America?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was King Victor  Emmanuel  III there to pay  homage  to Avogadro?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in fact, was Avogadro `s  famous  1811  paper written in French . )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "is avogadro `s  number commonly used to compute  the results  of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "hence, can relative molecular  masses be calculated from the masses  of gas samples?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did he publish his work?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he did not publish his work", "extracted_entity": null}
{"question": "did Avogadro submit his poem?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that loschmidt calculated first the value of Avogadro number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, it is true.", "extracted_entity": null}
{"question": "loschmidt calculated first the value of what?", "answer": "Avogadro's number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro's number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "hence , can relative molecular masses be calculated from the masses of gas samples ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into English as \"essay on determining the Relative Masses of the Elementary Molecules of Bodies\" ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "\"On the Relative Masses of the Element", "extracted_entity": null}
{"question": "what is Amedeo Avogadro?", "answer": "A person.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A person."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is Amedeo Avogadro from?", "answer": "Italy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Italy."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Italy", "extracted_entity": null}
{"question": "when did Anders Celcius publish his observations on the aurora borealis?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733", "1733 "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1742", "extracted_entity": null}
{"question": "what did Anders Celsius determine about the boiling of water?", "answer": "He determined the dependence of the boiling of water with atmospheric pressure.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He determined the dependence of the boiling of water with atmospheric pressure.", "dependence with atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which temperature scale did Celsius propose?", "answer": "Celcius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celcius", " the Celsius temperature scale "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": null}
{"question": "what is the boiling point of water dependent on?", "answer": "atmospheric pressure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the temperature", "extracted_entity": null}
{"question": "where was Celsius born?", "answer": "Uppsala in Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uppsala in Sweden", "Uppsala, Sweden"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "whad did Celsius report in his paper \"Observations of two persistent degrees on a thermometer\"?", "answer": "the freezing point is independent of latitude", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the freezing point is independent of latitude"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "\"I have observed two degrees on a thermometer", "extracted_entity": null}
{"question": "how old was Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "32", "extracted_entity": null}
{"question": "what is temperature would water have to be to be halfway between its standard boiling and freezing point?", "answer": "50", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["50"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "90.15 \u00b0C", "extracted_entity": null}
{"question": "what is the difference between Celsius' original temperature scale and the one we use today?", "answer": "The scale was reversed by Carolus Linnaeus. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed by Carolus Linnaeus. ", "scale was reversed "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Celsius scale is now called the Kel", "extracted_entity": null}
{"question": "what was Anders Celsius`s profession?", "answer": "professor of astronomy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of astronomy"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how old was Anders Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "80", "extracted_entity": null}
{"question": "what is Anders Celsius`s last name?", "answer": "Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was celsius born in Uppsala  in Sweden?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is The Celsius  crater  on the Moon named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Anders Celsius  ( November 27 ,  1701  April 25 ,  1744  ) a Swedish  astronomer?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Anders Celsius the first  to perform  and  publish  careful experiments  aiming at the definition  of an international  temperature  scale  on scientific grounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was he professor at Uppsala University?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he published a collection of 316 observations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he published a collection of 31", "extracted_entity": null}
{"question": "he published a collection of what?", "answer": "observations", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["observations"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can syllables begin with a vowel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are calligraphers held in great esteem?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have many European languages borrowed numerous words from it?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, many European languages borrowed numerous words from", "extracted_entity": null}
{"question": "are the most active in Damascus and Cairo?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do arabs consider literary Arabic?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "the sociolinguistic situation of Arabic provides a prime example of what?", "answer": "the linguistic phenomenon of diglossia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the linguistic phenomenon of diglossia"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are pronouns in Literary Arabic marked for person , number and gender ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "however , are non-human plural nouns grammatically considered to be feminine singular ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a bee an insect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "No, it is not an insect.", "extracted_entity": null}
{"question": "are bees related to ants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "bees are not related to ants.", "extracted_entity": null}
{"question": "have managed populations of European honey bees experienced substantial declines?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where do bees live?", "answer": "in colonies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in colonies"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Honey Bees live in Hives.", "extracted_entity": null}
{"question": "when do bumblebee colonies reach peak population?", "answer": "mid to late summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mid to late summer"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "peak population occurs in the summer", "extracted_entity": null}
{"question": "what do people commonly call cleptoparasitic bees?", "answer": "cuckoo bees", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cuckoo bees"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "cleptoparasitic bees", "extracted_entity": null}
{"question": "how did vespoid wasps evolve?", "answer": "from predatory ancestors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from predatory ancestors"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do humans use bees for?", "answer": "honey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["honey"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Honey, pollination, and bees", "extracted_entity": null}
{"question": "why do honey bee queens continue to lay eggs during the foraging season?", "answer": "to replace daily casualties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to replace daily casualties"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "honey bee queens continue to lay eggs", "extracted_entity": null}
{"question": "does Beijing mean \"northern capital\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing", "extracted_entity": null}
{"question": "are the caves of Dragon Bone Hill near the village of Zhoukoudian in Fangshan District?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Dragon Bone Hill", "extracted_entity": null}
{"question": "are hutongs disappearing?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did Beijing host the Olympic Games?", "answer": "2008", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2008"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "2008", "extracted_entity": null}
{"question": "when was the An Shi Rebellion launched?", "answer": "in 755 AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 755 AD"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "755", "extracted_entity": null}
{"question": "in 1949, where did Communist forces enter without a fight?", "answer": "Beiping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beiping"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Yuan Shikai die?", "answer": "1916", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1916"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": null}
{"question": "where does air pollution in Beijing come from?", "answer": "surrounding cities and provinces", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["surrounding cities and provinces"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "as of August 1st, 2006, how many trains stop daily at the Beijing Railway Station or the Beijing West Railway Station?", "answer": "167", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["167"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing Railway Station, 230", "extracted_entity": null}
{"question": "what is the name of a university (or similar institution for imparting higher education) in Beijing?", "answer": "Tsinghua University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tsinghua University"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing University", "extracted_entity": null}
{"question": "are famous middle  schools  in Beijing :?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Blaise Pascal a mathematician of the first order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal's triangle", "extracted_entity": null}
{"question": "could Blaise Pascal move without crutches?", "answer": "He could move without crutches until a paralytic attack in 1647.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He could move without crutches until a paralytic attack in 1647.", "no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Blaise Pascal", "extracted_entity": null}
{"question": "has the name Pascal been given to the SI unit of pressure?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "from what did Pascal suffer throughout his life?", "answer": "ill health", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ill health", "poor health"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal suffered from stomach problems.", "extracted_entity": null}
{"question": "what did Pascal argue was as perfect as possible?", "answer": "the procedure used in geometry ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the procedure used in geometry ", "the procedure used in geometry"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The perfect circle.", "extracted_entity": null}
{"question": "what is the best physician?", "answer": "Time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Time"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The best physician is the one who has a", "extracted_entity": null}
{"question": "who was the eldest sibling?", "answer": "Gilberte", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gilberte"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "aside from the Provincial Letters' religious influence, were they popular as a literary work?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "of whose continual poor health was the cause never precisely determined?", "answer": "Blaise Pascal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Blaise Pascal"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the violin cello a bowed string instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes, the violin is a bowed string", "extracted_entity": null}
{"question": "is the purfling just for decoration?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are the z-holes used as access points to the interior of the cello? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does violincello mean?", "answer": "Violincello means \"little violene\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Violincello means \"little violene\"."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the two materials that bows are made of traditionally? ?", "answer": "Traditionally, bows are made from pernambuco or brazilwood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Traditionally, bows are made from pernambuco or brazilwood."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is used to tune a cello?", "answer": "Pegs are used to tune a cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pegs are used to tune a cello.", "the pegs"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "pegs", "extracted_entity": null}
{"question": "which harmonics can produce any notes above middle C? ?", "answer": "Artificial harmonics can produce any notes above middle C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Artificial harmonics can produce any notes above middle C.", "Artificial harmonics"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "C5, C6, C7, C", "extracted_entity": null}
{"question": "who referred to the cello as \"basso de viola da braccio\"?", "answer": "Monteverdi referred to the cello as \"basso de viola da braccio\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monteverdi referred to the cello as \"basso de viola da braccio\".", "Monteverdi"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Giovanni Bottesini", "extracted_entity": null}
{"question": "where is the violincello held?", "answer": "The violincello is held on the shoulder.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The violincello is held on the shoulder.", "on the shoulder"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "it is held in the cello section", "extracted_entity": null}
{"question": "is there cello in performances by Rihanna?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are cellos constructed with glue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the cello a stringed instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what is cello an abbreviation of?", "answer": "violoncello", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["violoncello"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "cello is an abbreviation of violoncel", "extracted_entity": null}
{"question": "what position is used to play the cello?", "answer": "a gamba", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a gamba", "seated"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Cello Position", "extracted_entity": null}
{"question": "what is a person who plays the cello called?", "answer": "cellist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cellist"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a cellist", "extracted_entity": null}
{"question": "why is there purling on a cello?", "answer": "to stop cracks from forming ****misspelled****", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to stop cracks from forming ****misspelled****"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The cello is a string instrument played with a", "extracted_entity": null}
{"question": "when did the first educational works appear for the cello?", "answer": "the 18th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 18th century", "18th century"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "1720", "extracted_entity": null}
{"question": "what cello manufacturer should I buy from if I want to play outside?", "answer": "Luis & Clark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luis & Clark"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a member of the National Institute?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb find any relationship between electric charges and magnetic poles? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb did find a relationship between electric charges", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb's father's family in Montpellier? ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb was a", "extracted_entity": null}
{"question": "where did Charles-Augustin de Coulomb die?", "answer": "Charles-Augustin de Coulomb died in Paris.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb died in Paris.", "Paris, France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Paris, France", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb permanently stationed in Paris?", "answer": "Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1780-1800", "extracted_entity": null}
{"question": "what contribution did Charles-Augustin de Coulomb make to the field of geotechnical engineering?", "answer": "Retaining wall design", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Retaining wall design"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The Coulomb Theory of friction is a", "extracted_entity": null}
{"question": "where did the construction of Fort Bourbon take place?", "answer": "The construction of Fort Bourbon took place in Martinique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The construction of Fort Bourbon took place in Martinique.", "Martinique "], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Fort Bourbon", "extracted_entity": null}
{"question": "what is the definition of the electrostatic force of attraction and repulsion? ?", "answer": "Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "Coulomb's Law"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "electric force", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb retire to a small estate he possessed at Blois?", "answer": "Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "1789"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1770", "extracted_entity": null}
{"question": "if Charles-Augustin de Coulomb was alive today, how old would he have been?", "answer": "273", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["273"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "265 years old.", "extracted_entity": null}
{"question": "for how many years did Charles-Augustin de Coulomb live?", "answer": "70", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736-1806", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb", "extracted_entity": null}
{"question": "how old was Charles-Augustin de Coulomb when he died?", "answer": "1806", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1806"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "66 years old", "extracted_entity": null}
{"question": "how many years ago did he resign his appointment as intendant de eaux et fontaine ?", "answer": "220", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["220"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1792", "extracted_entity": null}
{"question": "how many years ago did Charles-Augustin de Coulomb die?", "answer": "203", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["203"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "207 years ago", "extracted_entity": null}
{"question": "was The SI  unit  of charge , the  coulomb , named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was coulomb born in Angoul\u00eame,  France ,  to a well  to do  family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "was his father ,  Henri  Coulomb , inspector of the Royal  Fields  in Montpellier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Henri Coulomb was his father", "extracted_entity": null}
{"question": "is coulomb distinguished in the history  of mechanics  and  of electricity and  magnetism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Charles Augustin  de Coulomb  ( born  June  14 ,  1736 ,  Angoul\u00eame ,  France  -  died August 23 ,  1806 ,  Paris ,  France  ) a French  physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles Augustin de Coulomb", "extracted_entity": null}
{"question": "does Coulomb leave a legacy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb born?", "answer": "June 14, 1736", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 14, 1736"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1736", "extracted_entity": null}
{"question": "did he publish an important investigation of the laws of friction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The Mechanical Properties of Fluids", "extracted_entity": null}
{"question": "was Coulomb born in Angoul\u00eame, France, to a well to do family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was his father inspector of the Royal Fields?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "was his father inspector of the Royal Fields", "extracted_entity": null}
{"question": "he discovered an inverse relationship of what?", "answer": "distance and electric force", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["distance and electric force"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the relationship between the rate of change of the distance", "extracted_entity": null}
{"question": "is it true that coulomb leaves a legacy as a hero?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "where is Charles-Augustin de Coulomb from?", "answer": "France", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "France", "extracted_entity": null}
{"question": "was Old Chinese wholly uninflected?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are The modern Chinese dialects more like a family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that one-fifth of world population speak some form of Chinese?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "one-fifth of world population speak some form of what?", "answer": "Chinese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is mandarin spoken Chinese distinguished by its high level?", "answer": "no, it's distinguished by diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no, it's distinguished by diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "copenhagen is the capital of what country?", "answer": "Denmark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Denmark"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Denmark", "extracted_entity": null}
{"question": "what is the population of Copenhagen?", "answer": "1,161,063", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,161,063", "1,153,615"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "1,027,181", "extracted_entity": null}
{"question": "what transnational bridge was completed in 2000?", "answer": "Oresund Bridge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oresund Bridge"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "the Hangzhou Bay Bridge", "extracted_entity": null}
{"question": "why is Copenhagen a regional hub?", "answer": "Its strategic location and excellent infrastructure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its strategic location and excellent infrastructure", "Its strategic location and excellent infrastructure with the largest airport in Scandinavia[6] located 14 minutes by train from the city centre"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Copenhagen is a regional hub because of its", "extracted_entity": null}
{"question": "what happened to Copenhagen between 1251 and 1255?", "answer": "a bunch of things", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a bunch of things"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "The city was destroyed by fire in 12", "extracted_entity": null}
{"question": "is Copenhagen completely surrounded with water? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Copenhagen is not completely surrounded with water.", "extracted_entity": null}
{"question": "copenhagen is ranked number one worldwide for which things?", "answer": "Most Livable City in the World, ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most Livable City in the World, ", "Most Livable City, Location Ranking Survey"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did German troops occupy Copenhagen?", "answer": "Because it was WW2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it was WW2"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "The Battle of Copenhagen was a naval battle", "extracted_entity": null}
{"question": "have cymbals been used historically to suggest bacchanal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals", "extracted_entity": null}
{"question": "can the origins of cymbals be traced to prehistoric times?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbals", "extracted_entity": null}
{"question": "are cymbals used in moden orchestras?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the most common Cymbals?", "answer": " The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "Hi-Hats, Crash, Splash, Ride, and China"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals", "extracted_entity": null}
{"question": "what is the second main orchestral use of cymbals?", "answer": "The suspended cymbal is the second main orchestral use of symbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The suspended cymbal is the second main orchestral use of symbals.", "The suspended cymbal", "the suspended cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what have clash cymbals traditionally been accompanied by?", "answer": "Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "The bass drum playing an identical part"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "a gong", "extracted_entity": null}
{"question": "what happens when the center of a Cymbal is hit?", "answer": "It causes a sound which is in a higher register than the rest of the symbal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It causes a sound which is in a higher register than the rest of the symbal.", "It causes a sound which is in a higher register than the rest of the cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The sound of a cymbal is produced by", "extracted_entity": null}
{"question": "what is an effective way to accentuate a note?", "answer": "Playing a bass drum playing an identical part is an effective way to accentuate a note.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Playing a bass drum playing an identical part is an effective way to accentuate a note.", "The combination of a clash cymbol being accompanied by a bass drum playing an identical part, played loudly"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "from what type of Cymbals can a expert player obtain an enormous dynamic range?", "answer": "An expert player can obtain an enormous dynamic range from crash cymbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An expert player can obtain an enormous dynamic range from crash cymbals.", "Crash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can the origins of cymbals be traced back to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the modern hi-hat evolve from clash cymbals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the word cymbal derived from a Germanic word?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the center of a cymbal called?", "answer": "Bell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bell", "bell"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "center", "extracted_entity": null}
{"question": "what part of the cymbal gets the best \"crash\"?", "answer": "Edge or rim", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Edge or rim", "edge or rim"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what two instruments form the modern drum kit?", "answer": "the cymbals and the drum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cymbals and the drum"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what Greek mythical creatures were depicted holding cymbals?", "answer": "fauns and satyrs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["fauns and satyrs"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are orchestral crash cymbals technically called?", "answer": "a pair of clash cymbals", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pair of clash cymbals", "clash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Dhaka the capital of Bangladesh?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Dhaka is the capital of Bangladesh", "extracted_entity": null}
{"question": "is Dhaka located on the eastern banks of the Buriganga River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Dhaka is located on the eastern banks of", "extracted_entity": null}
{"question": "is University of Dhaka the largest public university in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how much area does Dhaka cover?", "answer": "1462.60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1462.60 square kilometers"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "1455 sq km", "extracted_entity": null}
{"question": "how many people did the 1970 Bhola cyclone kill?", "answer": "500,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500,000"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "300,000", "extracted_entity": null}
{"question": "when is the end of the independence war of Bangladesh?", "answer": "December 16, 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 16, 1971"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "16 December 1971", "extracted_entity": null}
{"question": "did the Mughals arrive in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "the Mughals arrived in Dhaka", "extracted_entity": null}
{"question": "is University of Dhaka older than the Dhaka College?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "University of Dhaka", "extracted_entity": null}
{"question": "what is given for the number of native speakers?", "answer": "No figure is given for the number of native speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No figure is given for the number of native speakers."], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "400,000,0", "extracted_entity": null}
{"question": "is english an intonation  language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is english a de  jure  official  language  of Israel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the stressed  syllable called the nuclear  syllable?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does english grammar have minimal inflection?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did old English develop into Middle English?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes, Old English developed into Middle English.", "extracted_entity": null}
{"question": "are books, magazines, and newspapers written in English available?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the syntax of German different with different rules?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that semantics causes a number of false friends?", "answer": "true", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["true"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "it is true that semantics causes a number of false", "extracted_entity": null}
{"question": "here , are all syllables unstressed , except the syllables/words best and done , which are stressed ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are many words describing the navy , types of ships , and other objects or activities on the water of dutch origin ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are local variations in the formal written version of the language quite limited , being restricted largely to the spelling differences between British and American English ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "American English", "extracted_entity": null}
{"question": "is the flute a musical instrument?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it possible to open flutes at one or both ends?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Indian concert flutes available in standard pitches?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do we refer musicians who play flute?", "answer": "A flute player, a flautist or a flutist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute player, a flautist or a flutist. ", "A flute player, a flautist, or a flutist"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Flute Player", "extracted_entity": null}
{"question": "when was a three-holed flute made from a mammoth tusk discovered?", "answer": "2004.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2004.", "In 2004"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did the tin whistle first appear?", "answer": "12th century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["12th century.", "In the 12th century"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "1840", "extracted_entity": null}
{"question": "what are the oldest known musical instruments?", "answer": "A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "A three-holed flute made from a mammoth tusk and two flutes made from swans' bones are among the oldest known musical instruments"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "flutes", "extracted_entity": null}
{"question": "when was the pan flute spread to other parts of Europe?", "answer": "7th century BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7th century BC.", "After the 7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The pan flute was spread to other parts of", "extracted_entity": null}
{"question": "what does the air stream across this hole create?", "answer": "A Bernoulli, or siphon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A Bernoulli, or siphon."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "a breeze", "extracted_entity": null}
{"question": "are foxes wary of humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are fennec foxes endangered?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the diet of foxes include reptiles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how long do most foxes live?", "answer": "Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "2 to 3 years"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "15 years", "extracted_entity": null}
{"question": "in what country did fox hunting originate?", "answer": "the United Kingdom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United Kingdom", "in the United Kingdom"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "England", "extracted_entity": null}
{"question": "what is the smallest species of fox?", "answer": "The Fennec Fox is the smallest species of fox. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Fennec Fox is the smallest species of fox. ", "the Fennec Fox"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are female foxes called?", "answer": "vixens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["vixens"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is caching food?", "answer": "Caching is burying it for later consumption, usually under leaves, snow, or soil.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Caching is burying it for later consumption, usually under leaves, snow, or soil.", "burying it for later consumption"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do foxes damage fruit on farms?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "how many years ago did African people spread across 31 francophone African countries can speak French either as a first or second language?", "answer": "2 years ago", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2 years ago"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "50 years ago", "extracted_entity": null}
{"question": "in Old French, was the plural  for animal animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is catalan the only  official  language  of Andorra?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Catalan", "extracted_entity": null}
{"question": "in Belgium, however, is quatre-vingts universally used?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is french taught in many schools as a primary language along with Arabic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is widely used by the Lebanese, especially for administrative purposes?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is french descendant of the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there also a variety of regional languages In addition?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes, there are also a variety of regional languages", "extracted_entity": null}
{"question": "is it true that france signed the European Charter for Regional Languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are the prefixes en- and em- always nasalized ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "en-", "extracted_entity": null}
{"question": "how many years ago was the Luther Bible by Martin Luther printed?", "answer": "475", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["475"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "500 years ago", "extracted_entity": null}
{"question": "is german an inflected  language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "German is an inflected language.", "extracted_entity": null}
{"question": "in Switzerland, is \u00df used at all?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there two common  word  orders?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are two common word orders.", "extracted_entity": null}
{"question": "are feminines declined in the singular?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is 9 %  of the Internet  population German?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "9 %  of the Internet  population German", "extracted_entity": null}
{"question": "is german written using the Latin  alphabet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the Luther Bible by Martin Luther printed?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a  tz indicate that the preceding vowel is short?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there 622,650 speakers of German In Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "German speakers in Canada", "extracted_entity": null}
{"question": "are there also large populations of German ancestry In Mexico?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "German Mexicans", "extracted_entity": null}
{"question": "would Bundesl\u00e4nder not accept North Rhine Westphalia and Bavaria ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "have adverbs of time to appear in the third place in the sentence , just after the predicate ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do both sexes of giraffe have horns?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do giraffes give birth standing up?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are giraffes hunted for their hides?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Giraffe hide is used to make leather", "extracted_entity": null}
{"question": "what areas can giraffes inhabit?", "answer": "savannas, grasslands, or open woodlands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["savannas, grasslands, or open woodlands", "inhabit savannas, grasslands, or open woodlands"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Africa", "extracted_entity": null}
{"question": "what will a giraffe use to clean off any bugs that appear on its face?", "answer": "extremely long tongue ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["extremely long tongue ", "It's extremely long tongue"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Soap", "extracted_entity": null}
{"question": "how do giraffes defend themselves?", "answer": "with a powerful kick", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["with a powerful kick", "With powerful kicks"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do male giraffes weigh more than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "male giraffes weigh more than female", "extracted_entity": null}
{"question": "do male giraffes have larger horns than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are male females generally taller than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Henri Becquerel one of the discoverers of radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the SI unit for radioactivity named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "the SI unit for radioactivity is named after him", "extracted_entity": null}
{"question": "was Henri Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in what year did Henri Becquerel die?", "answer": "1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1908", "August 24, 1908"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": null}
{"question": "where was Henri Becquerel born?", "answer": " Paris ", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Paris ", "Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "in Paris, France.", "extracted_entity": null}
{"question": "in what year did Henri Becquerel win the Nobel Prize in Physics?", "answer": "1903", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1903"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": null}
{"question": "was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Henri Becquerel the sole winner of the 1903 Nobel Prize in Physics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Henri Becquerel intentionally discover radioactivity?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "He discovered it accidentally"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel", "extracted_entity": null}
{"question": "if Henri Becquerel was alive today, how old would he have been?", "answer": "157", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["157"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "151 years old.", "extracted_entity": null}
{"question": "for how many years did Henri Becquerel live?", "answer": "56", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["56"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "81 years", "extracted_entity": null}
{"question": "did Henri Becquerel live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was Henri Becquerel`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "physicist", "extracted_entity": null}
{"question": "how old was Henri Becquerel when he died?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "49", "extracted_entity": null}
{"question": "how many years ago was it when he became the third in his family to occupy the physics chair at the Musum National d`Histoire Naturelle?", "answer": "117", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["117"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1882", "extracted_entity": null}
{"question": "in 1908, the year  of his death, was Becquerel elected Permanent Secretary  of the Acad\u00e9mie  des  Sciences?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Jean Baptiste Perronet", "extracted_entity": null}
{"question": "when was Henri Becquerel born?", "answer": "December 15, 1852", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 15, 1852"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1852", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he share the Nobel Prize in Physics?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it true that Becquerel wrapped a fluorescent substance in photographic plates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel wrapped a fluorescent subst", "extracted_entity": null}
{"question": "becquerel wrapped a fluorescent substance in what?", "answer": "photographic plates and black material", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["photographic plates and black material"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is there a Becquerel crater on the Moon for radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, there is a Becquerel cr", "extracted_entity": null}
{"question": "must \u2026 One conclude from these experiments that the phosphorescent substance in question emits rays which pass through the opaque paper and reduces silver salts ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "silver", "extracted_entity": null}
{"question": "what is Henri Becquerel?", "answer": "Henri Becquerel was a famous physicist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel was a famous physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was a French physic", "extracted_entity": null}
{"question": "where is Henri Becquerel from?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": null}
{"question": "what does Henri Becquerel do?", "answer": "was a physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["was a physisist", "Henri Becquerel was a physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he was a French physicist who discovered radio", "extracted_entity": null}
{"question": "was Newton a English physicist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton", "extracted_entity": null}
{"question": "did Newton reject the church's doctrine of the Trinity?", "answer": "Newton may have rejected the church's doctrine of the Trinity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newton may have rejected the church's doctrine of the Trinity.", "Maybe"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "The Church of England", "extracted_entity": null}
{"question": "was Newton admitted into Havard College?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "Maybe, but it doesn't say so in the article"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "when was Newton's date of birth recorded?", "answer": "Christmas Day, Decembeer 25, 1642.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas Day, Decembeer 25, 1642.", "his date of birth was recorded as Christmas Day, December 25, 1642"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "4 January 1643", "extracted_entity": null}
{"question": "how many laws of motion did Netwon have?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "who did Newton see as the master creator?", "answer": "God", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["God", "Newton saw God as the master creator whose existence could not be denied in the face of the grandeur of all creation"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "The Supreme God", "extracted_entity": null}
{"question": "when did Netwon investigate the refraction of light?", "answer": "From 1670 to 1672", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1670 to 1672", "1670-1672"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1665", "extracted_entity": null}
{"question": "what principles did Newton explain for mechanics?", "answer": "The principles of conservation of momentum and angular momentum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The principles of conservation of momentum and angular momentum", "In mechanics, Newton enunciated the principles of conservation of momentum and angular momentum"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton's Laws of Motion", "extracted_entity": null}
{"question": "interaction with what man stirred up Newton's interest in alchemy?", "answer": "Henry More", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henry More", "The contact with the theosophist Henry More, revived his interest in alchemy"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "james Watt's improvements of what were fundamental to the changes wrought by the Industrial Revolution?", "answer": "The steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The steam engine.", "steam engine"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "james Watt was born where?", "answer": "Greenock, a seaport on the Firth of Clyde", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greenock, a seaport on the Firth of Clyde"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Glasgow, Scotland", "extracted_entity": null}
{"question": "what was the name of James Watt's mother?", "answer": "Agnus Muirhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Agnus Muirhead", "Agnes Muirhead"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Agnes Watt", "extracted_entity": null}
{"question": "how many of James Watt's children did not live to adulthood?", "answer": "3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "james Watt ranked first among how many people in Charles Murray's survey of historiometry?", "answer": "229", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["229"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "in what year did James travel to Lodon to study instrument-making?", "answer": "1753", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1753"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1663", "extracted_entity": null}
{"question": "for how many years of James Watt's life was his mother alive?", "answer": "17 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["17 years"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is James Watt most famous for?", "answer": "Steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt is most famous for inventing the", "extracted_entity": null}
{"question": "do most Japanese people employ politeness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that Japanese borrowed a considerable number of words?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "it is true that Japanese borrowed a considerable number", "extracted_entity": null}
{"question": "are the several dialects of Kagoshima famous to speakers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "Kagoshima dialect", "extracted_entity": null}
{"question": "is Lima the capital of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima", "extracted_entity": null}
{"question": "is the Museum of Gold in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Museum of Gold", "extracted_entity": null}
{"question": "does Lima have volleyball venues?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima has volleyball venues", "extracted_entity": null}
{"question": "when was Lima founded?", "answer": "January 18,1535.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["January 18,1535."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima was founded in 1535.", "extracted_entity": null}
{"question": "where is the Park of the Reserve located?", "answer": "Near the downtown area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Near the downtown area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "the Park of the Reserve is located in the city", "extracted_entity": null}
{"question": "what is a colectivo?", "answer": "Automobiles that renders express service on some major roads of the Lima Metropolitan Area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Automobiles that renders express service on some major roads of the Lima Metropolitan Area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "A colectivo is a social movement organization.", "extracted_entity": null}
{"question": "is the National University of San Marcos in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has the San Lorenzo Megaport Project been completed?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "how many years ago was the Lima Stock Exchange the most profitable in the world?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "1990", "extracted_entity": null}
{"question": "is rainfall very low?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "rainfall is very low", "extracted_entity": null}
{"question": "is lima the capital  and  largest  city  of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima", "extracted_entity": null}
{"question": "is The GDP  per capita in Lima $ 7,600  /  ref )?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "GDP  per capita in Lima is $", "extracted_entity": null}
{"question": "the Spanish Crown named Pizarro governor of what?", "answer": "Hemming.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hemming."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "New Castile", "extracted_entity": null}
{"question": "is it true that the ideas of the Enlightenment shaped the development of the city?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the ideas of the Enlightenment shaped the development of what?", "answer": "the city, Lima", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the city, Lima"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Enlightenment shaped the development of", "extracted_entity": null}
{"question": "is London the capital of the United Kingdom?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "does London's population draw from a wide range of religions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Caunte take control of the English throne in 101?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Edward the Confessor", "extracted_entity": null}
{"question": "over how many languages are spoken in London?", "answer": "Over 300 languages are spoken in london", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over 300 languages are spoken in london", "Over 300"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "333", "extracted_entity": null}
{"question": "what had the Anglo-Saxons created by the 600s?", "answer": "By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "a new settlement called Lundenwic"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "a unified kingdom of England.", "extracted_entity": null}
{"question": "what city in the UK has been subjected to bouts of terrorism?", "answer": "London has been subjected to bouts of terrorism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London has been subjected to bouts of terrorism.", "London"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what countries did James VI of Scotland unite?", "answer": "James VI of Scotland united Scotland and England.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James VI of Scotland united Scotland and England.", "England and Scotland"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "England and Scotland", "extracted_entity": null}
{"question": "how did civil wars affect England during the Middle Ages?", "answer": "London remained relatively untouched", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London remained relatively untouched"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "The English Civil War (1642\u2013", "extracted_entity": null}
{"question": "are the strings of a classical lyre made of gut?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does a classical lyre have a sound-chest?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The lyre is a string instrument, a member", "extracted_entity": null}
{"question": "is lyre played in mordern Greece?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "which part of the strings does the left hand touch?", "answer": "The lower strings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lower strings"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the fingerboard", "extracted_entity": null}
{"question": "in which place is lyre still played?", "answer": "North-eastern parts of Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North-eastern parts of Africa"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Greece", "extracted_entity": null}
{"question": "does a classical lyre have a fingerboard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a guitar played with a plectrum like a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "guitar", "extracted_entity": null}
{"question": "was Michael Faraday an English chemist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday", "extracted_entity": null}
{"question": "was Faraday a devout Christian?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday", "extracted_entity": null}
{"question": "was Faraday considered a gentleman?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was considered a gentleman.", "extracted_entity": null}
{"question": "who was a member of the Sandemanian sect of Christianity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "John Glas", "extracted_entity": null}
{"question": "what did Joseph Henry likely discover?", "answer": "self-induction", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["self-induction"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did the University of Oxford grant Faraday?", "answer": "a Doctor of Civil Law degree (honorary)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Doctor of Civil Law degree (honorary)", "Doctor of Civil Law degree (honorary)"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was made to travel outside the coach?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Faraday construct the ancestor of modern power generators?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the dynamo", "extracted_entity": null}
{"question": "did Faraday lecture on education in 1854?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Education", "extracted_entity": null}
{"question": "did Faraday receive an extensive formal education?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Faraday discover the chemical substance benzene?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Benzene", "extracted_entity": null}
{"question": "was Faraday born in England?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what do some historians of science refer to Faraday as?", "answer": "the best experimentalist in the history of science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the best experimentalist in the history of science", "best experimentalist in the history of science"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Father of Electricity.", "extracted_entity": null}
{"question": "who was the sponsor and mentor of Faraday?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Faraday invent that is used almost universally in science laboratories?", "answer": "the Bunsen burner", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Bunsen burner", "Bunsen burner"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Voltaic pile, an early form", "extracted_entity": null}
{"question": "does the production of chemical weapons raise an ethical issue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "what was the Faraday effect first called?", "answer": "diamagnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["diamagnetism"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday effect", "extracted_entity": null}
{"question": "can you meet someone through attending a church?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what was Michael Faraday`s birthplace?", "answer": "Newington Butts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what was Michael Faraday`s profession?", "answer": "chemist and physicist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["chemist and physicist"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "scientist", "extracted_entity": null}
{"question": "how many years ago did Faraday report on the first synthesis of compound make from carbon and chlorine , c 2 h 6 and c 2 h 4 ?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1821", "extracted_entity": null}
{"question": "how many years ago was it when he discovered the phenomenon that he named diamagnetism , and what is now called the Faraday effect?", "answer": "164", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["164"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1845", "extracted_entity": null}
{"question": "how many years ago was it when Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames , which resulted in an oft-reprinted cartoon in Punch?", "answer": "154", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["154"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1851", "extracted_entity": null}
{"question": "how many years ago was it when he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal?", "answer": "162", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["162"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "150 years", "extracted_entity": null}
{"question": "was education another area  of service  for Faraday?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was a teacher at the Royal Institution", "extracted_entity": null}
{"question": "was faraday `s  earliest  chemical  work as an assistant  to Davy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "who died at his house at Hampton Court on August 25, 1867?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "George Washington.", "extracted_entity": null}
{"question": "when was Michael Faraday born?", "answer": "September 22, 1791", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 22, 1791"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "22 September 1791", "extracted_entity": null}
{"question": "where did he serve two terms?", "answer": "the Church of Scotland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Church of Scotland"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "President of the United States", "extracted_entity": null}
{"question": "who was born in Newington Butts, near present-day South London, England?", "answer": "Michael Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that institution formed the foundation of electric motor technology?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The first electrical motor was invented in", "extracted_entity": null}
{"question": "institution formed the foundation of what?", "answer": "modern electromagnetic technology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern electromagnetic technology"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was his father member of the Sandemanian sect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was faraday Christian of the small Sandemanian denomination?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is Nassau Sound a body of water in Maine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Sound is a body of water in", "extracted_entity": null}
{"question": "was Nassau placed in operation by Robert Fulton?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau", "extracted_entity": null}
{"question": "who was Fort Nassau built by?", "answer": "the Dutch", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Dutch"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The Dutch", "extracted_entity": null}
{"question": "what is Nassau Coliseum?", "answer": "an arena in Uniondale, New York, USA", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["an arena in Uniondale, New York, USA"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Veterans Memorial Coliseum", "extracted_entity": null}
{"question": "what was the Dead or Alive 4 fighting arena modeled after?", "answer": "a Magnetic Accelerator Cannon station from Halo 2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Magnetic Accelerator Cannon station from Halo 2"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Nassau Range the highest mountain range in the world?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Mount Everest", "extracted_entity": null}
{"question": "is Nassau County named after a German town?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau County, New York", "extracted_entity": null}
{"question": "does the United States have a base near Glasgow?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Tesla study electrical engineering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla studied electrical engineering.", "extracted_entity": null}
{"question": "was Tesla born in the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was born in the United States.", "extracted_entity": null}
{"question": "was Tesla hired by Edison?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Tesla demonstrate wireless communication (radio)?", "answer": "1893", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1893"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1893", "extracted_entity": null}
{"question": "what is the SI unit measuring magnetic flux density or magnetic induction?", "answer": "the tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Weber per meter squared", "extracted_entity": null}
{"question": "was Tesla regarded as a mad scientist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla", "extracted_entity": null}
{"question": "what type of current did Tesla invent?", "answer": "AC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["AC"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "alternating current", "extracted_entity": null}
{"question": "who was the victor of the \"War of Currents\"?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla", "Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where did Tesla study electrical engineering?", "answer": "the Austrian Polytechnic in Graz ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Austrian Polytechnic in Graz ", "Austrian Polytechnic in Graz", "The Austrian Polytechnic in Graz"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "the University of Prague.", "extracted_entity": null}
{"question": "is Ottawa the capital of Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa is the capital of Canada.", "extracted_entity": null}
{"question": "does Ottawa sit on the Ottowa River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa", "extracted_entity": null}
{"question": "do small tremors occur in Ottawa?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is Ottawa's population?", "answer": "1,190,982", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,190,982"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa's population is 1,2", "extracted_entity": null}
{"question": "what are Ottawa's primary industries?", "answer": "Logging and lumber", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Logging and lumber"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Ottawa's junior ice hockey team?", "answer": "the Ottawa 67's", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa 67's"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa 67's", "extracted_entity": null}
{"question": "what is Ottawa's major league hockey team?", "answer": "the Ottawa Senators", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa Senators"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa Senators", "extracted_entity": null}
{"question": "what are Carleton University's athletic teams called?", "answer": "Carleton Ravens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carleton Ravens"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "the Ravens", "extracted_entity": null}
{"question": "is Ottawa colder than Moscow in January?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa is colder than Moscow in January.", "extracted_entity": null}
{"question": "is Ottawa on a river?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does it snow in Ottawa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa", "extracted_entity": null}
{"question": "was there a cholera outbreak in 1832?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who is the mayor of Ottawa?", "answer": "Larry O'Brien", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Larry O'Brien"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many townships are in Ottawa?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what's the main highway in Ottawa?", "answer": "Highway 417, The Queensway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Highway 417, The Queensway", "provinical Highway 417"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Highway 417", "extracted_entity": null}
{"question": "what was Ottawa's name in 1850?", "answer": "Bytown", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bytown"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa", "extracted_entity": null}
{"question": "based on the average, in what year will the next small tremor occur in Ottawa?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "2024", "extracted_entity": null}
{"question": "where were immigrants from in the 1800s?", "answer": "Irish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish", "Ireland"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "English, Scottish, Irish, German, French,", "extracted_entity": null}
{"question": "are there also discussions of having OC Transpo Light Rail and STO Transitway networks linking on the Gatineau side of the Ottawa River ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The O-Train Confederation Line and the", "extracted_entity": null}
{"question": "does the giant otter live in South America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "the giant otter lives in South America.", "extracted_entity": null}
{"question": "do otters have claws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do otters eat fish?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why are otters vulnerable to prey depletion?", "answer": "Prey-dependence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Prey-dependence", "prey-dependency"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "because they are not the most efficient predator", "extracted_entity": null}
{"question": "do otters enjoy playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do sea otters have a layer of fat like whales?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the otter in the same family as the badger?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are there any cities named Santiago in the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Santiago the name of an indie/punk band?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Santiago a name in Spanish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "what is the origin of the name Santiago?", "answer": "Spanish for St. James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spanish for St. James", "sanit + iago, James"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the name of the patron saint of Spain", "extracted_entity": null}
{"question": "who has released an album or song named Santiago?", "answer": "The Chieftains; Loreena McKennitt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Chieftains; Loreena McKennitt", "The Chieftains and Loreena McKennitt"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the Order of Santiago?", "answer": "a Spanish knightly order", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Spanish knightly order", "a Spanish Knightly order"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Order of Santiago", "extracted_entity": null}
{"question": "what notable people have been named Santiago?", "answer": "Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "Benito, Hugo, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what fictional stories include a main character named Santiago?", "answer": "Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "Interview with the Vampire, The Alchemist, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Santiago the national capital of a country?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago", "extracted_entity": null}
{"question": "is santiago Spanish for St. James  ( from santo ,  saint  +  iago ,  James  )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "santiago is Spanish for St. James", "extracted_entity": null}
{"question": "is santiago Spanish for St. James?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what is Santiago?", "answer": "Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry."], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago", "extracted_entity": null}
{"question": "has swahili no diphthongs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a third  prefix the object  prefix?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is      / ref Swahili unusual among sub-Saharan languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are vowels never reduced , regardless of stress ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Australian Black Swan have white feathers on its wings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do swans belong to the family Anatidae?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "can black swans swim with only one leg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what are young swans known as?", "answer": "cygnets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cygnets", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "what is the Irish legend of the Children of Lir about?", "answer": "a stepmother transforming her children into swans for 900 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a stepmother transforming her children into swans for 900 years", "A stepmother transforming her children into swans for 900 years"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Children of Lir is a story from Irish", "extracted_entity": null}
{"question": "what is the Sanskrit word for swan?", "answer": "hamsa or hansa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hamsa or hansa", "Hamsa or hansa"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "\u0939\u0902\u0938 (Hamsa)", "extracted_entity": null}
{"question": "which album was the song \"The Bonny Swans\" from?", "answer": "The Mask and Mirror", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Mask and Mirror"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the Polish word for swan?", "answer": "\u0141ab\u0119d\u017a", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["\u0141ab\u0119d\u017a"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cyganka", "extracted_entity": null}
{"question": "which ballet by Pyotr Tchaikovsky is partially based on an Ancient German legend about a princess who was turned into a swan?", "answer": "Swan Lake", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swan Lake"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swan Lake", "extracted_entity": null}
{"question": "are swan birds of the family Anatidae?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do swans feature strongly in mythology?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swans are a prominent figure in many myths", "extracted_entity": null}
{"question": "is the Trumpeter Swan the largest bird?", "answer": "It is the largest bird of North America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the largest bird of North America.", "The Trumpeter Swan is the largest bird of North America"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Trumpeter Swan", "extracted_entity": null}
{"question": "what is the name of young swans?", "answer": "Cygnets.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cygnets.", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Cygnets", "extracted_entity": null}
{"question": "where is the word \"swan\" derived from?", "answer": "Old English swan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old English swan.", "The Old English word \"swan,\" derived from the Indo-European root \"*swen\", meaning to sound or to sing"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The word \"swan\" is derived from the", "extracted_entity": null}
{"question": "what is the best known story about a swan?", "answer": "The Ugly Duckling fable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Ugly Duckling fable.", "The Ugly Duckling"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The best known story about a swan is the", "extracted_entity": null}
{"question": "are swans and ducks birds of the same family?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Anatidae", "extracted_entity": null}
{"question": "what is the name of an adult female?", "answer": "Pen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pen.", "An adult female swan is called a pen."], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "woman", "extracted_entity": null}
{"question": "what is the color of the Australian Black Swan?", "answer": "Completely black except for the white flight feathers on its wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Completely black except for the white flight feathers on its wings.", "Completely black except for the white feathers on its wings"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Black", "extracted_entity": null}
{"question": "is the Tiger a member of the Felidar family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are tigers solitary animals?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are panthers a type of tiger?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "what do tigers' powerfully built legs and shoulders allow tigers to do?", "answer": "They have the ability to pull down prey substantially heavier than themselves.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They have the ability to pull down prey substantially heavier than themselves.", "have the ability to pull down prey substantially heavier than themselves"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "jump", "extracted_entity": null}
{"question": "what would a tiger do when seized by a crocodile?", "answer": "A tiger will strike at the reptile's eyes with its paws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A tiger will strike at the reptile's eyes with its paws.", "strike at the reptile's eyes with its paws"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "The tiger would fight back with its claws", "extracted_entity": null}
{"question": "what are the typical features of a tiger country?", "answer": "It will always have good cover, it will always be close to water and plenty of prey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It will always have good cover, it will always be close to water and plenty of prey.", "Good cover, close to water, and plenty of prey "], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "The tiger is a large cat. It has", "extracted_entity": null}
{"question": "would it be common for tigers to mate in January?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "about how many square kilometers would be needed to house 3 female tigers?", "answer": "60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 square kilometers", "60"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "would a tiger be likely to feedon smaller animals, such as mice?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Tiger", "extracted_entity": null}
{"question": "did the predecessors to trumpets have valves?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are trumpets constructed of brass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the bass trumpet the smallest trumpet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what musical styles is the trumpet used in?", "answer": "Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Jazz, classical, and pop music.", "extracted_entity": null}
{"question": "what is the most common type of trumpet?", "answer": "The B trumpet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The B trumpet."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Bb trumpet", "extracted_entity": null}
{"question": "how does a trumpet produce sound?", "answer": "By blowing air through closed lips.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By blowing air through closed lips."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the player blows air through the mouthpiece", "extracted_entity": null}
{"question": "how do trumpets compensate for wolf tones?", "answer": "Somce trumpetes have a slide mechanism built in to compensate.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Somce trumpetes have a slide mechanism built in to compensate."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Miles Davis play the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "does a trumpet have a mellower tone than a cornet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the name of the upper shell of a turtle called the plastron?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do scutes fall away from the turtle's shell?", "answer": "Some of the scutes eventually fall away from the shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some of the scutes eventually fall away from the shell.", "yes, eventually"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Terrestrial tortoises have short feet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Tortoise", "extracted_entity": null}
{"question": "how many groups are turtles broken down into?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two", "two"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "where are the only surviving giant tortoises?", "answer": "They are on the Seychelles and Gal\u00e1pagos Islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are on the Seychelles and Gal\u00e1pagos Islands.", "Seychelles and Gal\u00e1pagos Islands"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do turtles chew food?", "answer": "Turtles use their jaws to cut and chew food.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles use their jaws to cut and chew food.", "Turtles use their jaws to cut and chew food"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are tortoises land based?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Tortoises are land based.", "extracted_entity": null}
{"question": "where are turtle eggs layed?", "answer": "Turtles lay eggs on land.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles lay eggs on land.", "on land"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "turtle eggs are laid on the beach", "extracted_entity": null}
{"question": "is turtle soup considered a delicacy?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Testudines the crown group of the superorder Chelonia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Testudines", "extracted_entity": null}
{"question": "do turtles breathe air?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "are harvesting wild turtles legal anywhere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "approximately how many species of Testudines are alive today?", "answer": "300", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is harvesting wild turtles legal?", "answer": "Florida", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Florida"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "[Turtle Island Restoration Network](https", "extracted_entity": null}
{"question": "what was the largest ever chelonian?", "answer": "Archelon ischyros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Archelon ischyros", "The great letherback sea tutrtle"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Titanoboa", "extracted_entity": null}
{"question": "is there a way to approximate the age of a turtle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can turtles spend all their time underwater?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are violinists and fiddlers the same thing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violinist", "extracted_entity": null}
{"question": "are violins a single size?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are violas and cellos in the same family of instruments as violins?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violas and cellos are in the same family", "extracted_entity": null}
{"question": "where does the word \"violin\" come from?", "answer": "the Middle Latin word vitula, meaning \"stringed instrument\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Middle Latin word vitula, meaning \"stringed instrument\"", "the Middle Latin word vitula"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is someone who makes violins called?", "answer": "a luthier, or simply a violin maker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a luthier, or simply a violin maker", "a luthier"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violin maker", "extracted_entity": null}
{"question": "how many strings does a violin usually have?", "answer": "four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["four", "4"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "four", "extracted_entity": null}
{"question": "what are violins made of?", "answer": "different types of wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different types of wood", "maple, ebony, sheep gut"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how long have people been making instruments like violins?", "answer": "since ancient times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["since ancient times", "since 1555"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1765", "extracted_entity": null}
{"question": "whate is the usual pitch range of a violin?", "answer": "from G3 to C8", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from G3 to C8"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "415 Hz", "extracted_entity": null}
{"question": "are xylophone bars made of rosewood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the xylophone a precursor to the vibraphone?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were ancient mallets made of copper?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Copper mallet", "extracted_entity": null}
{"question": "where did the xylophone originate?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the earliest historical reference in Europe?", "answer": "Arnold Schlick's Spiegel der Orgelmacher und Organisten", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arnold Schlick's Spiegel der Orgelmacher und Organisten"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The earliest historical reference in Europe is the Roman Empire", "extracted_entity": null}
{"question": "how are western-style xylophones characterised?", "answer": "by a bright, sharp tone and high register", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by a bright, sharp tone and high register"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can a xylophone be 3 octaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a short bar follow a long bar?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did vibraphones exist in 1930?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Alessandro Volta a professor of chemistry?", "answer": "Alessandro Volta was not a professor of chemistry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was not a professor of chemistry.", "No"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was a professor of chemistry.", "extracted_entity": null}
{"question": "did Alessandro Volta invent the remotely operated pistol?", "answer": "Alessandro Volta did invent the remotely operated pistol.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta did invent the remotely operated pistol.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta invented the remotely", "extracted_entity": null}
{"question": "was Alessandro Volta taught in public schools?", "answer": "Volta was taught in public schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was taught in public schools.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was a professor of physics at", "extracted_entity": null}
{"question": "who did Alessandro Volta marry?", "answer": "Alessandro Volta married Teresa Peregrini.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta married Teresa Peregrini.", "Teresa Peregrini"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did Alessandro Volta invent in 1800?", "answer": "In 1800, Alessandro Volta invented the voltaic pile.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800, Alessandro Volta invented the voltaic pile.", "voltaic pile"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The Voltaic Pile", "extracted_entity": null}
{"question": "what is the battery made by Alessandro Volta credited as?", "answer": "The battery made by Volta is credited as the first electrochemical cell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The battery made by Volta is credited as the first electrochemical cell.", "the first electrochemical cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "voltaic pile", "extracted_entity": null}
{"question": "did Alessandro Volta die and retire in the same place?", "answer": "Alessandro Volta retired and died in the same place.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta retired and died in the same place.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did Alessandro Volta improve  and popularize the electrophorus?", "answer": "Alessandro Volta improved and popularized the electrophorus in 1775.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta improved and popularized the electrophorus in 1775.", "1775"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1775", "extracted_entity": null}
{"question": "how long was Alessandro Volta a professor at the University of Pavia?", "answer": "Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "almost 25 years"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1779 - 181", "extracted_entity": null}
{"question": "was Alessandro Volta an Egyptian?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta (1745 \u2013", "extracted_entity": null}
{"question": "was Volta taught in public schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Volta made a count in 1810?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "who made Volta a count?", "answer": "Napoleon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Napoleon"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Volta born?", "answer": "Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Como", "Como, Italy"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Volta retire?", "answer": "1819", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1819", "In 1819."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1827", "extracted_entity": null}
{"question": "a year before improving and popularizing the electrophorus, what did Volta become?", "answer": "A professor of physics at the Royal School in Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A professor of physics at the Royal School in Como"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "a year before improving and popularizing the elect", "extracted_entity": null}
{"question": "was Volta buried where he died or was he buried someplace else?", "answer": "where he died", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where he died", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was buried at the Panth\u00e9on in", "extracted_entity": null}
{"question": "did Volta marry before he became professor of experimental physics at the University of Pavia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta married in 179", "extracted_entity": null}
{"question": "what happened in 1810?", "answer": "Volta was made a count by Napoleon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Alessandro Volta born in Como?", "answer": "Yes, Volta was born in Como, Italy and was taught in the public schools there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was born in Como, Italy and was taught in the public schools there."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was born in Como.", "extracted_entity": null}
{"question": "was Alessandro Volta born in Como?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who became a professor of physics at the Royal School in Como?", "answer": "Volta.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was Volta made a count by Napoleon?", "answer": "Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1809", "extracted_entity": null}
{"question": "is Volta's legacy celebrated by a Temple on the shore of Lake Como in the center of the town?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the battery made by Volta credited as the first electrochemical cell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "was Volta made a count by Napoleon in 1810?", "answer": "Yes, Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in what year did he become a professor of physics at the Royal School in Como?", "answer": "1774", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1774"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1715", "extracted_entity": null}
{"question": "was Avogadro a  professor at the University of Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avogadro was a professor at the University of Turin."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was he a member of the Royal Superior Council on Public Instruction?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes,  Avogadro was a member of the Royal Superior Council on Public Instruction."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Avogadro's number used to compute the results of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avagadro's number is used to compute the results of chemical reactions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number is used to", "extracted_entity": null}
{"question": "who first calculated the value of Avogadro's number?", "answer": "Johann Josef Loschmidt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Johann Josef Loschmidt", "Johann Josef Loschmidt first calculated the value of Avogadro's number."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Johann Josef Loschmidt", "extracted_entity": null}
{"question": "what does Avogadro's Law state?", "answer": "The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "Avogadro's Law states that the relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The number of atoms in one mole of a subst", "extracted_entity": null}
{"question": "who showed that Avogadro's theory held in dilute solutions?", "answer": "Jacobus Henricus van Hoff", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacobus Henricus van Hoff", "Jacobus Henricus van 't Hoff showed that Avogadro's theory holds in dilute solutions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Cannizzaro", "extracted_entity": null}
{"question": "in what language was his 1811 paper published?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French", "Avogadro's 1811 paper was published in French."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "who was Avogadro's wife?", "answer": "Felicita Mazz", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felicita Mazz", "Felicita Mazz\u00e9 was Avogadro's wife."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why did Avogadro lose his chair at the University of Turin?", "answer": "He was active in the revolutionary movements of 1821 against the king of Sardinia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was active in the revolutionary movements of 1821 against the king of Sardinia", "Avogadro lost his chair at the University of Turin because he was active in the revolutionary movements of 1821 against the king of Sardinia."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Amedeo Avogadro was born in Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is Avogadro hailed as a founder of the atomic-molecular theory?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro was hailed as a founder", "extracted_entity": null}
{"question": "did Johann Josef Loschmidt first calculate the value of Avogadro's number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "in 1820, Avogadro became a professor of physics where?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "University of Turin", "extracted_entity": null}
{"question": "avogadro did not actually use what word?", "answer": "atom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atom"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The answer is:", "extracted_entity": null}
{"question": "the number of elementary entities in 1 mole of a substance is known as what?", "answer": "Avogadro constant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro constant", "Avogadro's constant"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what would a German call Avogadro's number?", "answer": "Loschmidt number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loschmidt number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "is Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did he become a professor before the revolutionary movements against the king of Sardinia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do ants belong to the Hymenoptera order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are ants used in cuisine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does an ant's head contain sensory organs?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do most ants travel?", "answer": "most ants travel by walking", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["most ants travel by walking", "by walking"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in ant colonies, what are the fertile female ants called?", "answer": "queens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["queens"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Queens", "extracted_entity": null}
{"question": "who wrote about ants in A Tramp Abroad?", "answer": "mark twain wrote about ants", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mark twain wrote about ants", "Mark Twain"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Mark Twain", "extracted_entity": null}
{"question": "do the ants eat plants, meats, or both?", "answer": "both", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["both"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Meats", "extracted_entity": null}
{"question": "what organs gives a bull ant its good sight?", "answer": "compound eyes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["compound eyes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The eyes.", "extracted_entity": null}
{"question": "what may happen to red fire ants if we use boiling water on the queen?", "answer": "nests of red fire ants may be destroyed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nests of red fire ants may be destroyed", "die"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The queen may die and the colony may die", "extracted_entity": null}
{"question": "are ants found in Antartica?", "answer": "No, there are no ants in antarctica.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, there are no ants in antarctica."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do male ants take flight before females?", "answer": "Yes, male ants take flight before females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, male ants take flight before females."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do worker ants have wings?", "answer": "No, worker ants do not have wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, worker ants do not have wings."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what are the three segments of an ant?", "answer": "The head, mesosoma and metasoma are the three distinct body segments.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The head, mesosoma and metasoma are the three distinct body segments."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "head, thorax, abdomen", "extracted_entity": null}
{"question": "what are the ant colonies that lack queens called?", "answer": "Colonies that lack queens are called gamergate colonies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colonies that lack queens are called gamergate colonies."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where are bullet ants located?", "answer": "Bullet ants are located in Central and South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bullet ants are located in Central and South America."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "\"Bullet ant\" is a common name", "extracted_entity": null}
{"question": "do ants belong to the same order as bees?", "answer": "Yes, ants belong to the same order as bees.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, ants belong to the same order as bees."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "what is the most common color of ants?", "answer": "Most ants are red or black.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most ants are red or black."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "black", "extracted_entity": null}
{"question": "do all ants build nests?", "answer": "No, not all ants build nests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, not all ants build nests."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "ultraviolet vision was first discovered in ants by Sir who in 1881?", "answer": "John Lubbok", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Lubbok"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do ants thrive in most ecosystems?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "ants thrive in most ecosystems", "extracted_entity": null}
{"question": "are several species of ants able to use the Earth's magnetic field?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "are termites actually more closely related to cockroaches as well as mantids?", "answer": "Yes, termites are actually more closely related to cockroaches as well as mantids.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, termites are actually more closely related to cockroaches as well as mantids."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Cockroaches are more closely related to term", "extracted_entity": null}
{"question": "do some caterpillars produce vibrations as well as sounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, some caterpillars do produce", "extracted_entity": null}
{"question": "did ants evolve from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Ants evolved from wasp-like ancest", "extracted_entity": null}
{"question": "is the ant a marsupial?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are they easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist?", "answer": "Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Antwerp a city?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Antwerp a municipality?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a municipality.", "extracted_entity": null}
{"question": "is Antwerp in Belgium?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is in Belgium.", "extracted_entity": null}
{"question": "what is Antwerp?", "answer": "Antwerp is a city and municipality in Belgium.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is a city and municipality in Belgium."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a city in Belgium", "extracted_entity": null}
{"question": "what is the population of the city of Antwerp?", "answer": "Antwerp's population is 472,071.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp's population is 472,071."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a city in Belgium, and", "extracted_entity": null}
{"question": "where is the city of Antwerp?", "answer": "Antwerp is in Belgium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is in Belgium"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a city in Belgium, and", "extracted_entity": null}
{"question": "why is Antwerp important to fashion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "antwerp is to the east of what river?", "answer": "Antwerp is to the east of the Scheldt river", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is to the east of the Scheldt river"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Scheldt", "extracted_entity": null}
{"question": "how many municipalities does Antwerp have?", "answer": "None, Antwerp is a municipality.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None, Antwerp is a municipality."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "19", "extracted_entity": null}
{"question": "is Arabic a Central Semitic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Arabic a major vehicle of culture in Europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, the language was used for literature, philosophy", "extracted_entity": null}
{"question": "is Arabic classified alongside Semitic languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is a Semitic language.", "extracted_entity": null}
{"question": "how many people speak the Arabic language?", "answer": "280 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["280 million people."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "285 million", "extracted_entity": null}
{"question": "when was Arabic calligraphy invented?", "answer": "Many styles were developed after 786.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many styles were developed after 786."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "700", "extracted_entity": null}
{"question": "where is Arabic spoken?", "answer": "The Middle East and North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Middle East and North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is spoken in the Arabian Penins", "extracted_entity": null}
{"question": "why is Arabic related to Islam?", "answer": "Arabic is the liturgical language of Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic is the liturgical language of Islam"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic is not related to Islam.", "extracted_entity": null}
{"question": "why does Arabic heavily influence European languages?", "answer": "Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Arabic the largest member of the Semitic language family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Modern Standard Arabic continue to evolve like other languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Hassaniya Arabic spoken in Mauritania?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "what is the only variety of modern Arabic that has acquired official language status?", "answer": "maltese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["maltese", "Maltese"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "egyptian Arabic is spoken by how many in Egypt?", "answer": "76 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["76 million"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "58 million.", "extracted_entity": null}
{"question": "where are the Western Arabic numerals used?", "answer": "North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North Africa", "present-day North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "In most of the world, the Arabic numer", "extracted_entity": null}
{"question": "the most active Academies of Arabic Language are found where?", "answer": "damascus and cairo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["damascus and cairo"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Arab world.", "extracted_entity": null}
{"question": "hassan Massoudy is a master of what genre?", "answer": "Hassan Massoudy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hassan Massoudy", "Arabic calligraphy"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Calligraphy", "extracted_entity": null}
{"question": "is Berlin the capital city of Germany?", "answer": "Berlin is the capital city of Germany.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is the capital city of Germany.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin", "extracted_entity": null}
{"question": "is Berlin the largest city in Germany?", "answer": "Berlin is Germany's largest city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is Germany's largest city.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin", "extracted_entity": null}
{"question": "is Schloss Charlottenburg the largest existing palace in Berlin?", "answer": "Schloss Charlottenburg is the largest existing palace in Berlin. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Schloss Charlottenburg is the largest existing palace in Berlin. ", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "which building is the site of the German parliament?", "answer": "The Reichstag building is the site of the German parliament. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building is the site of the German parliament. ", "The Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Reichstag", "extracted_entity": null}
{"question": "which two sports events did the Olympiastadion host?", "answer": "The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "the 1936 Summer Olympics and the 2006 FIFA World Cup final"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is the Berliner Dom located?", "answer": "The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "on the Spree Island"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berliner Dom", "extracted_entity": null}
{"question": "which Berlin building did architect Norman Foster remodel in the 1990s?", "answer": "The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Palace", "extracted_entity": null}
{"question": "why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?", "answer": "Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "Because burning lignite produces harmful emissions"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the names of the two zoos in Berlin?", "answer": "The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "Zoologischer Garten Berlin and Tierpark Friedrichsfelde"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Tierpark Berlin and Zoo Berlin", "extracted_entity": null}
{"question": "is Berlin the headquarters of Springer?", "answer": "Yes, Berlin is the headquarters of Springer.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Berlin is the headquarters of Springer."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does the Gendarmenmarkt border the French Cathedral?", "answer": "Yes, the Gendarmenmarkt borders the French Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the Gendarmenmarkt borders the French Cathedral."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Nauen plain north of Berlin?", "answer": "No, the Nauen plain stretches to the west of Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the Nauen plain stretches to the west of Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Nauen Plateau", "extracted_entity": null}
{"question": "where is the bust of Queen Nefertiti?", "answer": "The bust of Queen Nefertiti is in the Altes Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The bust of Queen Nefertiti is in the Altes Museum."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where does the German President live?", "answer": "The German President lives west of the center, Schloss Bellevue.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The German President lives west of the center, Schloss Bellevue."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Berlin give up its status as a free Hanseatic city?", "answer": "In 1451 Berlin gave up its status as a free Hanseatic city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1451 Berlin gave up its status as a free Hanseatic city."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": null}
{"question": "which is the busiest airport in Berlin?", "answer": "Tegel International Airport is the busiest airport in Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tegel International Airport is the busiest airport in Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Tegel Airport", "extracted_entity": null}
{"question": "how many occupation zones was Germany divided into after the war?", "answer": "Germany was divided into four occupation zones after the war.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany was divided into four occupation zones after the war."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "4", "extracted_entity": null}
{"question": "where was Popkomm held before moving to Berlin?", "answer": "Popkomm was held in Cologne before moving to Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Popkomm was held in Cologne before moving to Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "D\u00fcsseldorf", "extracted_entity": null}
{"question": "who was Blaise Pascal's father?", "answer": "His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "\u00c3\u2030tienne Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "\u00c9tienne Pascal", "extracted_entity": null}
{"question": "was Pascal a French mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Pascal have poor health throughout his life?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal had poor health throughout his life", "extracted_entity": null}
{"question": "how old was Pascal when he lost his mother?", "answer": "at the age of three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the age of three", "3."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "7", "extracted_entity": null}
{"question": "who was Pascal's younger sister?", "answer": "Jacqueline", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacqueline", "Jacqueline."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Marie", "extracted_entity": null}
{"question": "what led Pascal to his religious conversion?", "answer": "Two basic influences led him to his conversion: sickness and Jansenism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two basic influences led him to his conversion: sickness and Jansenism", "sickness and Jansenism"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The Pascal's Wager.", "extracted_entity": null}
{"question": "how old was Pascal when he died?", "answer": "39", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["39", "1662-08-19."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "39", "extracted_entity": null}
{"question": "did Pascal write about cycloid before 1658?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The cycloid was first studied by Eucl", "extracted_entity": null}
{"question": "is there a programming language called Pascal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "crowds of believers came to see and kiss the thorn; all of Catholic where acclaimed a miracle?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The Church of the Holy Sepulchre", "extracted_entity": null}
{"question": "wasn't Blaise Pascal a work of Desargues on conic sections?", "answer": "Yes, it was", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it was"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Blaise Pascal was a French mathematician,", "extracted_entity": null}
{"question": "who died the next morning?", "answer": "Pascal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did his father die?", "answer": "1651", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1651", "His father died in 1651."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "1911", "extracted_entity": null}
{"question": "was he a child prodigy who was educated by his father, a civil servant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "he was a child prodigy who was educated", "extracted_entity": null}
{"question": "was Pascal's earliest work in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Pascal's Wager", "extracted_entity": null}
{"question": "did Pascal also write in defense of the scientific method?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do butterflies make sounds?", "answer": "Some butterflies make sounds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some butterflies make sounds."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do butterflies have two eyes?", "answer": "Yes, butterflies have two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, butterflies have two eyes."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "butterflies have four eyes.", "extracted_entity": null}
{"question": "does a black moth mean that someone has died?", "answer": "In the Philippines a black butterfly or moth mean that someone has died.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Philippines a black butterfly or moth mean that someone has died."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "a black moth means that someone has died.", "extracted_entity": null}
{"question": "what is the outer layer of the cuticle made of?", "answer": "The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was there a vast swarm of butterflies?", "answer": "In Kyoto there was a vast swarm of butterflies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Kyoto there was a vast swarm of butterflies."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "A swarm of butterflies is a", "extracted_entity": null}
{"question": "what butterfly is migratory?", "answer": "The Monarch butterfly is migratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monarch butterfly is migratory."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do butterflies carry more pollen than Hymenoptera?", "answer": "No, butterflies carry less pollen than Hymenoptera.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, butterflies carry less pollen than Hymenoptera."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what allows the butterfly to evade predators?", "answer": "A butterfly's hind wings allow the butterfly to evade predators.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A butterfly's hind wings allow the butterfly to evade predators."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "The butterfly is able to evade", "extracted_entity": null}
{"question": "what happens if a wing is removed?", "answer": "If a ring is removed the other three will grow to a larger size.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["If a ring is removed the other three will grow to a larger size."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "The wing will not be able to fly", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a Spanish Biologist?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, he was a French physicist."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was the SI unit of charge named after Charles-Augustin de Coulomb?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, the SI unit of charge, the coulomb, was named after him."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb ever employed at La Rochelle?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, upon his return to France, with the rank of Captain, he was employed at La Rochelle."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb", "extracted_entity": null}
{"question": "what is Charles-Augustin de Coulomb best known for?", "answer": "He is best known for developing Coulomb's law.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He is best known for developing Coulomb's law."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "electricity", "extracted_entity": null}
{"question": "whose ideas inspired Charles-Augustin de Coulomb's experiments on the resistance of masonries?", "answer": "Pieter van Musschenbroek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pieter van Musschenbroek.", "He carried out several experiments on the resistance of masonries which were inspired by the ideas of Pieter van Musschenbroek on friction."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb join his father's family in Montpeillier?", "answer": "From 1757 to 1759 he joined his father's family in Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1757 to 1759 he joined his father's family in Montpellier."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1744", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb come from a poor family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, his mother came from a wealthy family in the wool trade."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is Coulomb's law?", "answer": "The definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The definition of the electrostatic force of attraction and repulsion."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb's law states that the magnitude", "extracted_entity": null}
{"question": "where is Coll\u00e8ge des Quatre-Nations located?", "answer": "In Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Montpellier.", "In Paris."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1785?", "answer": "Coulomb presented his three reports on Electricity and Magnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb presented his three reports on Electricity and Magnetism"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1757 to 1759?", "answer": "Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "regarding this topic, what did the Revolution do?", "answer": "The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who determined to pursue mathematics?", "answer": "Charles-Augustin de Coulomb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Pythagoras", "extracted_entity": null}
{"question": "when did Coulomb return to Paris?", "answer": "In 1759.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1759."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1771", "extracted_entity": null}
{"question": "does Coulomb leave a legacy as a pioneer in the field of geotechnical engineering for his contribution to retaining wall design?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the family move to Paris as well as there Coulomb studied at the prestigious Collge des Quatre-Nations?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, the family moved to Paris as well as", "extracted_entity": null}
{"question": "was Coulomb born in Angoulme, France to a well-to-do family?", "answer": "Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb was born in Angoul\u00eame,", "extracted_entity": null}
{"question": "is he best known for developing Coulomb's law, the definition of the electrostatic force of attraction and repulsion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "the SI unit of charge, the coulomb, was named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the SI unit of charge, the coulomb", "extracted_entity": null}
{"question": "did Coulomb die in Angoul\u00c3\u00aame, France, to a well-to-do family?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are all spoken varieties of Chinese tonal and analytical?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is fifty percent or more of Korean vocabulary of Chinese origin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does \"ketchup\" come from the Minnan pronunciation of \u9bad\u6c41 (koe-tsiap)?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the most common romanization standard for Standard Mandarin today?", "answer": "Hanyu Pinyin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hanyu Pinyin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "pinyin", "extracted_entity": null}
{"question": "what is the official standard language used by the People's Republic of China, the Republic of China, and Singapore (where it is called \"Huayu\")?", "answer": "Putonghua / Guoyu, often called \"Mandarin\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Putonghua / Guoyu, often called \"Mandarin\"", "Standard Mandarin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Standard Chinese", "extracted_entity": null}
{"question": "how many head entries for character definitions does the Zhonghua Zihai \u4e2d\u534e\u5b57\u6d77 (1994) contain?", "answer": "85,568 head entries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["85,568 head entries", "54,678"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "11,825", "extracted_entity": null}
{"question": "what does the PRC government classify as literacy amongst workers?", "answer": "knowledge of 2,000 characters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["knowledge of 2,000 characters", "a knowledge of 2000 characters"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how is South China linguistically different from North China?", "answer": "South China displays more linguistic diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["South China displays more linguistic diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "\u5348\u9910\u76d2 \u201clunchbox or boxed lunch\u201d (from bento) and \u6599\u7406 \u201cprepared cuisine\u201d are loan words from which language?", "answer": "Japanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japanese", "Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "\u82f1\u8a9e \u201cEnglish\u201d", "extracted_entity": null}
{"question": "what is the standardized form of spoken chinese?", "answer": "The standardized form of spoken Chinese is Standard Mandarin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The standardized form of spoken Chinese is Standard Mandarin.", "The standardized from of spoken Chinese is Standard Mandarin."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Mandarin", "extracted_entity": null}
{"question": "is diglossia a common feature in mainland China and Taiwan?", "answer": "Yes, diglossia is a common feature in mainland China and Taiwan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, diglossia is a common feature in mainland China and Taiwan."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do linguists often view Chinese as a language family?", "answer": "Yes, linguists often view Chinese as a language family.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, linguists often view Chinese as a language family."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did classical Chinese literature begin?", "answer": "Classical Chinese literature began in the Spring and Autumn period.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Classical Chinese literature began in the Spring and Autumn period.", "Classical Chinese literature began in the 14th to 11th centuires BCE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "1000 BCE", "extracted_entity": null}
{"question": "how does the government of the People's Republic of China often refer to all divisions of the Sinitic language(s) besides Standard Mandarin?", "answer": "The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "The PRC commonly refers to all divisons of the Sinitic language(s) beside Standard Mandarin as fangyan, or \"regional tongues.\""], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was Middle Chinese used?", "answer": "Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "Middle Chinese was the language used in the 6th through 1-th centuries CE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how is the pronunciation of foreign words determined?", "answer": "The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "Foreign words are transliterated into close-match existing Chinese morphemes."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The pronunciation of foreign words is determined by", "extracted_entity": null}
{"question": "what is the total number of Chinese words?", "answer": "The Hanyu Da Zidian includes 54,678 head entries for characters.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Hanyu Da Zidian includes 54,678 head entries for characters."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "22,450,92", "extracted_entity": null}
{"question": "what language was used in the Zhou and Southern Dynasties?", "answer": "Old Chinese was the language common during the early and middle Zhou Dynasty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old Chinese was the language common during the early and middle Zhou Dynasty."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a cougar also known as a mountain lion?", "answer": "A cougar is also known as a mountain lion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cougar is also known as a mountain lion.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are cougars larger than jaguars?", "answer": "Cougars are not larger than jaguars.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cougars are not larger than jaguars.", "no"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the World Conservation Union consider the cougar a \"least concern\" species?", "answer": "The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are some of the cougar's primary food sources?", "answer": "The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses, and sheep"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "cougars eat deer, elk,", "extracted_entity": null}
{"question": "how long are cougar adult males (from nose to tail)?", "answer": "The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "2.4 meters"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "2.25m", "extracted_entity": null}
{"question": "how long is an adult cougar's paw print?", "answer": "An adult cougar's paw print is approximately 10 cm (4 inches) long.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An adult cougar's paw print is approximately 10 cm (4 inches) long.", "4 inches"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the three heaviest cats in the world?", "answer": "The three heaviest cats in the world are the tiger, lion and jaguar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three heaviest cats in the world are the tiger, lion and jaguar.", "tiger, lion, and jaguar"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "on average, are cougar males heavier than females?", "answer": "On average, cougar males are heavier than females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On average, cougar males are heavier than females.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "cougar", "extracted_entity": null}
{"question": "what are the three cat species that are native to Canada?", "answer": "The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "cougar, bobcat, Canadian lynx"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is one of the parts of a cymbal called a disc?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are ride cymbals very common?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Fender make cymbals?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No. Fender does not make cymb", "extracted_entity": null}
{"question": "what part of the cymbal is the bell?", "answer": "The center of a Cymbal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The center of a Cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The bell is the part of the cymbal", "extracted_entity": null}
{"question": "what's the timber of ancient cimbals like? disfluent?", "answer": "like that of small hand-bells or of the notes of the keyed harmonica", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["like that of small hand-bells or of the notes of the keyed harmonica"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what does it sound like to play cymbals with a cello bow?", "answer": "squealing car brakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["squealing car brakes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals", "extracted_entity": null}
{"question": "what are cymbals made of?", "answer": "brass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Aluminum", "extracted_entity": null}
{"question": "how many cymbals typically go in a drum kit?", "answer": "at least 3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at least 3"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbals", "extracted_entity": null}
{"question": "what kinds of musical groups use cymbals?", "answer": "Cymbals are used in modern orchestras and many military, marching, concert and other bands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cymbals are used in modern orchestras and many military, marching, concert and other bands"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the dragonfly associated with late summer as well as early autumn?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "The dragonfly is associated with late summer as", "extracted_entity": null}
{"question": "does Exposure to air cause the larva to begin breathing?", "answer": "Yes (capitalization).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (capitalization)."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "Yes, exposure to air causes the larva", "extracted_entity": null}
{"question": "is it characterized by large multifaceted eyes, two pairs of strong transparent wings, and an elongated body?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the drum a member of the percussion group?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, the drum is a member of the per", "extracted_entity": null}
{"question": "does each type of drum head serve its own musical purpose?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Aburukuwa a type of drum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Aburukuwa", "extracted_entity": null}
{"question": "how long has the basic design of the drum remained unchanged?", "answer": "remained virtually unchanged for thousands of years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["remained virtually unchanged for thousands of years", "thousands of years"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "4000 years", "extracted_entity": null}
{"question": "what is the second biggest factor affecting the sound produced by a drum?", "answer": "tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["tension", "the tension at which the drum head is held against the shell of the drum."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "by what name are drums classified as under the percussion group?", "answer": "membranophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["membranophone", "membranous"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do rock and roll drummers like their drumheads?", "answer": "rock and roll drummer may prefer drums that sound loud and deep", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["rock and roll drummer may prefer drums that sound loud and deep", "drums that sound loud and deep"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Rock drummers like their drumheads with a", "extracted_entity": null}
{"question": "whare is the name for drums that have a set of wires held across some of all of the drum heads?", "answer": "snares", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["snares"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Timpani", "extracted_entity": null}
{"question": "had the drums been used in Sri Lanka since Before Christ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are drums often used in music therapy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums are often used in music therapy.", "extracted_entity": null}
{"question": "do drums consist of at least eight membranes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what has the most effect on how a drum sounds?", "answer": "the drumhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the drumhead"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "drumheads", "extracted_entity": null}
{"question": "who constructed a kit using diatonic and chromatically tuned drums?", "answer": "Terry Bozzio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Terry Bozzio"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "drums are usually played by what?", "answer": "the hands, or by one or two sticks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the hands, or by one or two sticks", "the hands"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drummers", "extracted_entity": null}
{"question": "has the design of drums changed recently?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can some cylindrical shell drums have no drum heads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "does every drumhead make the same sound?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the drumheads make different sounds", "extracted_entity": null}
{"question": "aren't drums often used in music therapy?", "answer": "Yes, drums are often used in music therapy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, drums are often used in music therapy."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums", "extracted_entity": null}
{"question": "aren't drums usually played by the hands?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are drums usually played by the hands?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do many such drums have six to ten tension rods?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with parts of a player's body, or with some sort of implement such as a drumstick, to produce sound?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Drums consist of at least one membrane,", "extracted_entity": null}
{"question": "have other techniques been used to cause drums to make sound, such as the ``Thumb roll''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the electric eel a true eel?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, the electric eel is not a true eel."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "The electric eel is not a true eel", "extracted_entity": null}
{"question": "are most eels predators?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, most eels are predators."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is eel blood toxic to humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, eel blood is toxic to humans."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many species of true eels are there?", "answer": "approximately 800 species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["approximately 800 species", "There are approximately 800 species of true eels."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is smoked eel considered a delicacy?", "answer": "Northern Germany, The Netherlands, Denmark, Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Germany, The Netherlands, Denmark, Sweden", "Smoked eel is considered a delicacy in northern Germany, The Netherlands, Denmark and Sweden."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how do eels begin life?", "answer": "As flat and transparent larvae, called leptocephali", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As flat and transparent larvae, called leptocephali", "Eels begin life as flat and transparent larvae, called leptocephali."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "_______", "extracted_entity": null}
{"question": "what types of fins do eels have?", "answer": "Dorsal, anal, caudal, tail fins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dorsal, anal, caudal, tail fins", "Eels have dorsal and anal fins fused with a tail fin. Some also have  pectoral fins."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Dorsal and anal fins", "extracted_entity": null}
{"question": "what do leptocephali eat?", "answer": "Marine snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Marine snow", "Leptocephali eat small particles called marine snow."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Leptocephali eat plankton.", "extracted_entity": null}
{"question": "do all eels live in fresh water?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, not all eels live in fresh water."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No, eels do not live in fresh water", "extracted_entity": null}
{"question": "is Finnish an official language of the European Union?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Finnish is an official language of the European Union."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are the dialects of Finnish divided into two groups?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, the dialects of Finnish are divided into two groups."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Finnish threatened by English?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Finnish is not threatened by English."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Finnish is not threatened by English", "extracted_entity": null}
{"question": "when did Proto-Uralic language arrive in Finland?", "answer": "Around 1900 BCE", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Around 1900 BCE", "Proto-Uralic language arrived in Finland around 1900 BCE."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "5000 BC", "extracted_entity": null}
{"question": "what are characteristic features of Finnish?", "answer": "Vowel harmony and an agglutinative morphology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowel harmony and an agglutinative morphology", "Characteristic features of Finnish are vowel harmony and an agglutinative morphology."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is the Kven language spoken?", "answer": "Norway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norway", "The Kven language is spoken in Northern Norway."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Kven is spoken in Norway", "extracted_entity": null}
{"question": "for how long has the classification of dialects spoken outside of Finland been a controversial issue?", "answer": "Since Finland's independence in 1917", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Since Finland's independence in 1917", "The classification of dialects spoken outside of Finland has been a controversial issue since Finland's independence in 1917."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who wrote the first novel in Finnish?", "answer": "Aleksis Kivi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Aleksis Kivi", "The first novel in Finnish was published by Aleksis Kivi."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Juhani Aho", "extracted_entity": null}
{"question": "what book defined the official Finnish language?", "answer": "The Dictionary of Contemporary Finnish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Dictionary of Contemporary Finnish", "The Dictionary of Contemporary Finnish defined the official Finnish language."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the earliest extant transverse flute?", "answer": "it dates from 433 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it dates from 433 BC", "Chi"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when does a flute produce sound?", "answer": "when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "When a stream of air directed across a hole in the instrument creats a vibration of air at the hole."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "When the air is blown through the hole.", "extracted_entity": null}
{"question": "how can a flute's volume be increased?", "answer": "a flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute's volume can generally be increased by making its resonator and tone holes larger", "A flute's volume can be increased by making its resonator and tone holes larger."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by increasing the size of the air column.", "extracted_entity": null}
{"question": "do open-ended flutes have brighter timbres?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where was the oldest flute ever discovered found?", "answer": "Germany", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany", "Various parts of Germany."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Western flutes simpler than Indian flutes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is a police whistle very wide for its pitch?", "answer": "A flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute's volume can generally be increased by making its resonator and tone holes larger", "It's volume is high either because of its larger resonator or larger tone holes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "A police whistle is very wide for its", "extracted_entity": null}
{"question": "what are the two main varieties of Indian flutes?", "answer": "The first, the Bansuri. The second, the Venu or Pullanguzhal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first, the Bansuri. The second, the Venu or Pullanguzhal", "Bansuri and the Venu or Pullanguzhal."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has the flute been dated to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, the flute has been dated to pre", "extracted_entity": null}
{"question": "was the pan flute used in Greece?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Syrinx", "extracted_entity": null}
{"question": "can a flute be played with several different air sources?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when was the pan flute used in Greece?", "answer": "the 7th century BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 7th century BC", "7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did concert flutes begin appearing in concert ensembles?", "answer": "the 16th-century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 16th-century", ""], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "in the 1800s", "extracted_entity": null}
{"question": "how many main varieties of Indian flutes are currently used?", "answer": "two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two", "Two"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "bansuri", "extracted_entity": null}
{"question": "what material is a chi flute fashioned from?", "answer": "lacquered bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lacquered bamboo", "Lacquered bamboo"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Bamboo", "extracted_entity": null}
{"question": "how does a flute player change the pitch of the sound?", "answer": "by opening and closing holes in the body of the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by opening and closing holes in the body of the instrument", "Opening and closing holes in the body of the instrument"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by opening and closing the holes in the flute", "extracted_entity": null}
{"question": "what is the most basic form of the flute?", "answer": "a flute can be an open tube which is blown like a bottle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute can be an open tube which is blown like a bottle", "Open tube which is blown like a bottle"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The most basic form of the flute is the", "extracted_entity": null}
{"question": "has the dragon historically served as China's national emblem?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do pandas hibernate?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do giant pandas attack humans?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, they do not attack humans.", "extracted_entity": null}
{"question": "what family is the panda a part of?", "answer": "Ursidae (bear)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ursidae (bear)", "Ursidae"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Ursidae", "extracted_entity": null}
{"question": "how long does it take for the panda cubs skin to turn gray?", "answer": "one to two weeks after birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["one to two weeks after birth", "One to two weeks"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many different names did Chinese writings give to the panda?", "answer": "20", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["20"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what foods do pandas eat?", "answer": "bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bamboo", "bamboo, honeys, eggs, fish, yams, shrub leaves, oranges, and bananas"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Bamboo", "extracted_entity": null}
{"question": "what are the two subspecies of giant pandas?", "answer": "Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "Ailuropoda melanoleuca melanoleuca and The Qinling Panda, Ailuropoda melanoleuca qinlingensis"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is panda diplomacy?", "answer": "Loans of Giant Pandas to American and Japanese zoos", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loans of Giant Pandas to American and Japanese zoos"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "China's diplomatic efforts to promote goodwill", "extracted_entity": null}
{"question": "is the Giant Panda a mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Giant Panda a terrestrial animal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the Giant Panda an endangered species?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The giant panda is an endangered species", "extracted_entity": null}
{"question": "where do Giant Pandas live?", "answer": "a few mountain ranges in central china", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a few mountain ranges in central china", "Sichuan province, Shaanxi and Gansu provinces"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "China", "extracted_entity": null}
{"question": "when was the Giant Panda's genome sequenced?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2009", "extracted_entity": null}
{"question": "how many subspecies of the Giant Panda are there?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "is the Giant Panda a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, it is a herbivore.", "extracted_entity": null}
{"question": "why is the Giant Panda an endangered species?", "answer": "threatened by habitat loss", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["threatened by habitat loss", "It is threatened by continued habitat loss and by a very low birthrate"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Pandas are endangered due to habitat loss", "extracted_entity": null}
{"question": "is the population of wild Giant  Pandas growing?", "answer": "according to some reports, yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["according to some reports, yes", "Yes but uncertain"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is an acoustic guitar dependent on an external device?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the battente smaller than a classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the battente is smaller than a classical guitar", "extracted_entity": null}
{"question": "is the flamenco guitar similar to the classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "where is the headstock located?", "answer": "end of the guitar neck furthest from the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["end of the guitar neck furthest from the body", "At the end of the guitar."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The headstock is located on the upper portion of", "extracted_entity": null}
{"question": "whom are guitars made and repaired by?", "answer": "luthiers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["luthiers", "Luthiers."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Guitars are made and repaired by", "extracted_entity": null}
{"question": "what are modern guitar strings constructed of?", "answer": "metal, polymers, animal or plant product materials", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["metal, polymers, animal or plant product materials", "Metal, polymers, or animal or plant product materials."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Steel-string acoustic guitar strings are", "extracted_entity": null}
{"question": "why are Harp Guitars difficult to classify?", "answer": "as there are many variations within this type of guitar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["as there are many variations within this type of guitar", "There are many variations within this type."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Harp Guitars are difficult to classify", "extracted_entity": null}
{"question": "what is the bridge used for?", "answer": "transfer the vibration from the strings to the soundboard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["transfer the vibration from the strings to the soundboard", "The transfer of string vibrations."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which guitars use three single-coil pickups?", "answer": "Fender Statocaster type guitars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fender Statocaster type guitars", "Fender Stratocaster type guitars."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is a guitar an instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "guitar", "extracted_entity": null}
{"question": "can guitars be divided into two broad categories?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "acoustic and electric", "extracted_entity": null}
{"question": "do picks come in many shapes and sizes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many strings does a guitar typically have?", "answer": "Six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six", "six"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "what is located at the end of the guitar neck furthest from the body?", "answer": "Headstock", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Headstock", "headstock"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The heel.", "extracted_entity": null}
{"question": "what are most electric guitar bodies made of?", "answer": "Wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wood", "wood"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "wood", "extracted_entity": null}
{"question": "why do some people believe that left-handed people should learn to play guitars as right-handed people do?", "answer": "to standardise the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to standardise the instrument"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Guitars are played with the left hand.", "extracted_entity": null}
{"question": "how old is the oldest known representation of a guitar-like intrument being played?", "answer": "3,300 years old", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3,300 years old"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the point called that is bolted or glued to the body of the guitar?", "answer": "Neck Joint", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Neck Joint", "Neck joint or 'Heel'"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Becquerel study science at the \u00c9cole des Ponts et Chauss\u00e9es?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways in 1892?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Chief Engineer in the Department of Bridges", "extracted_entity": null}
{"question": "was Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "whom did he share the Nobel Prize with?", "answer": "Pierre and Marie Curie", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pierre and Marie Curie"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the SI unit for radioactivity called?", "answer": "becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["becquerel", "the becquerel (Bq)"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when Becquerel discovered radioactivity, whose work did he investigate?", "answer": "Wilhelm Conrad R\u00f6ntgen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilhelm Conrad R\u00f6ntgen"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Marie Sk\u0142odowska-Curie", "extracted_entity": null}
{"question": "where are there craters named Becquerel?", "answer": "Moon and Mars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moon and Mars", "on the moon and on mars"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "there are 4 craters named Becquer", "extracted_entity": null}
{"question": "why did Bequerel win the Nobel Prize in Physics?", "answer": "discovery of spontaneous radioactivity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["discovery of spontaneous radioactivity", "was the discoverer of radioactivity"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Isaac Newton British?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, he was English."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton British", "extracted_entity": null}
{"question": "was Isaac Newton religious?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear.", "Yes, he was highly religious, though an unorthodox Christian."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton was a Christian.", "extracted_entity": null}
{"question": "did Isaac Newton die in 1898?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, Newton died in his sleep in London on 31 March 1727."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton died in 1727.", "extracted_entity": null}
{"question": "where was Isaac Newton born?", "answer": "At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "He was born at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire.", "Woolsthorpe Manor in Woolsthorpe-by-Colsterworth"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Woolsthorpe-by-Colster", "extracted_entity": null}
{"question": "who shares credit with Isaac Newton for developing calculus?", "answer": "Gottfried Leibniz.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gottfried Leibniz.", "Newton shares the credit with Gottfried Leibniz for the development of the differential and integral calculus."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Gottfried Wilhelm Leibniz", "extracted_entity": null}
{"question": "when did Isaac Newton discover the generalized binomial theorem?", "answer": "In 1665.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1665."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the Occult influence Newton's theory of gravitation?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "If Newton had not relied on the occult idea of action at a distance, he might not have developed his theory of gravity."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton", "extracted_entity": null}
{"question": "were Newton's religious views consistent with Anglicanism?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, a conflict between Newton's religious views and Anglican orthodoxy was averted."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "which fruit inspired Isaac Newton's theory of gravitation?", "answer": "The apple.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The apple.", "Isaac Newton's theory of gravitation was inspired by watching the fall of an apple from a tree."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Apricot", "extracted_entity": null}
{"question": "was Sir Isaac Newton an English physicist and mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Sir Isaac Newton", "extracted_entity": null}
{"question": "did he build the first practical reflecting telescope?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Isaac Newton educated at The King's Schol, Grantham?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "when was the Principia published?", "answer": "1687", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1687"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1687", "extracted_entity": null}
{"question": "where is there a Newton statue on display?", "answer": "Oxford University Museum of Natural History ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oxford University Museum of Natural History ", "Oxford University Museum of Natural History"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where was Isaac Newton buried?", "answer": "Westminster Abbey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westminster Abbey"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "in Westminster Abbey.", "extracted_entity": null}
{"question": "what religion did Isaac Newton follow?", "answer": "he never made a public declaration of his private faith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he never made a public declaration of his private faith", "unorthodox Christian"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Anglican", "extracted_entity": null}
{"question": "what food gave Isaac Newton clues to his theory of gravity?", "answer": "apple", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apple"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "apple", "extracted_entity": null}
{"question": "who was a farmer and later a land surveyor for the government?", "answer": " LeRoy Pollock was a farmer and later a land surveyor for the government.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" LeRoy Pollock was a farmer and later a land surveyor for the government."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "John McDouall Stuart", "extracted_entity": null}
{"question": "regarding this topic, what did David Alfaro Siqueiros do?", "answer": "Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who used hardened brushes, sticks,?", "answer": "Jackson Pollock used hardened brushes, sticks.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jackson Pollock used hardened brushes, sticks."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Horses", "extracted_entity": null}
{"question": "when were his papers donated by Lee Krasner?", "answer": "In 1983.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1983."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "2000", "extracted_entity": null}
{"question": "did Free Jazz feature a Pollock painting as its cover artwork?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Paul Jackson Pollock an influential American painter as well as a major figure in the abstract expressionist movement?", "answer": "Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the origins of the term action painting. ?", "answer": "Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "during his lifetime, did Pollock enjoy considerable fame and notoriety?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was he regarded as a mostly reclusive artist?", "answer": "Yes, he was regarded as a mostly reclusive artist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, he was regarded as a mostly reclusive artist. "], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Wisma building the tallest building in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes,"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is there a chinese community in jakarta?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is the capital city of Indonesia and", "extracted_entity": null}
{"question": "is Jakarta the 12th largest city in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is the largest city in the world.", "extracted_entity": null}
{"question": "what is the capital of Indonesia?", "answer": "Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta", "The capital of Indonesia is Jakarta."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "what is the official name of Jakarta?", "answer": "Daerah Khusus Ibukota Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Daerah Khusus Ibukota Jakarta", "Daerah Khusus."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "how is the climate in the city?", "answer": "Jakarta has a hot and humid equatorial/tropical climate ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta has a hot and humid equatorial/tropical climate ", "The city is hot and humid."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the city has a warm climate.", "extracted_entity": null}
{"question": "where does the name of the city Jakarta come from?", "answer": "The name Jakarta is derived from the Sanskrit word \"Jayakarta.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name Jakarta is derived from the Sanskrit word \"Jayakarta.\""], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how is the city's governor chosen?", "answer": "election", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["election", "Through election."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the governor is chosen by the people of the city", "extracted_entity": null}
{"question": "why does Jakarta suffer frequent flooding?", "answer": "Because it is located approximately eight meters above the sea level", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is located approximately eight meters above the sea level", "The fact that the northern part of Jakarta lies on a plain, approximately eight meters above the sea level."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The Ciliwung River is the main river", "extracted_entity": null}
{"question": "is the the biggest university in Jakarta the University of Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Jakarta a city?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is a city.", "extracted_entity": null}
{"question": "is the most important river the Ciliwung River?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Ciliwung River", "extracted_entity": null}
{"question": "where is Jakarta located?", "answer": "northwest coast of Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["northwest coast of Java", "On the northwest coast of Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "who created Monas Park?", "answer": "General Herman Willem Deandels", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General Herman Willem Deandels", "Dutch Governor General Herman Willem Deandels"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do mikrolets travel on the main roads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what contributes to frequent flooding in Jakarta?", "answer": "Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "It lies on a plain, approximately eight meters above the sea level"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did the economy improve in Jakarta in 2007?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Bank Central Asia", "extracted_entity": null}
{"question": "in AD 39, King Purnawarman established Sunda Pura as a new capital city for the kingdom, located at the northern coast of where?", "answer": "Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Java", "extracted_entity": null}
{"question": "in 1602, the British East India Company's first voyage, commanded by Sir who, arrived in Aceh and sailed on to Banten where they were allowed to build a trading post?", "answer": "James Lancaster", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James Lancaster"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sir Henry Middleton", "extracted_entity": null}
{"question": "who began control of migration to the city in order to stem the overcrowding and poverty?", "answer": "Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The British government", "extracted_entity": null}
{"question": "did Jayawikarta's soldiers attack the Dutch fortress?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The Dutch garrison was overwhelmed by", "extracted_entity": null}
{"question": "did Suharto resign as president?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Suharto", "extracted_entity": null}
{"question": "was the Jakarta area part of the fourth century Indianized kingdom of Tarumanagara?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "do other landmarks include the Istiqlal Mosque as well as Jakarta Cathedral?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Istiqlal Mosque", "extracted_entity": null}
{"question": "give an example of the many Sukarno era monuments in the city. ?", "answer": "The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. "], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Mendong", "extracted_entity": null}
{"question": "is Jakarta the country's economic, cultural and political center?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is James Watt French?", "answer": "No, he was Scottish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, he was Scottish.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt", "extracted_entity": null}
{"question": "did James Watt improve an engine?", "answer": "Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "Yes"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a statue of Watt in the White House?", "answer": "No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what was James Watt's father's job?", "answer": "His father was a shipwright, ship owner and contractor, and served as the town's chief baillie.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father was a shipwright, ship owner and contractor, and served as the town's chief baillie."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "a shipwright", "extracted_entity": null}
{"question": "when did Watt retire?", "answer": "In 1800.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800.", "Watt retired in 1800."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "2016", "extracted_entity": null}
{"question": "where is the James Watt Memorial College?", "answer": "In Greenock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Greenock.", "The James Watt Memorial  is in Greenock."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why is the watt named after Watt?", "answer": "The watt is named after him for his contributions to the development of the steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The watt is named after him for his contributions to the development of the steam engine.", "The watt is named after James Watt for his contributions to the development of the steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "The watt is named after James Watt because", "extracted_entity": null}
{"question": "where did Watt attend university?", "answer": "Watt did not attend school regularly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt did not attend school regularly.", "He did not attend a university."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what did he invent?", "answer": "He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Aeolipile", "extracted_entity": null}
{"question": "what happened in the early 20th century?", "answer": "The koalas of South Australia were largely exterminated.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "World War I", "extracted_entity": null}
{"question": "do Koalas have a slow metabolism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did eucalypt forests grow in the place of rainforests?", "answer": "Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the koala fill the same ecological role as the sloth of South America?", "answer": "The koala fills the same ecological role as the sloth of South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koala fills the same ecological role as the sloth of South America."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "koala", "extracted_entity": null}
{"question": "is the long term viability of the koala therefore threatened by genetic weakness?", "answer": "The long term viability of the koala is therefore threatened by genetic weakness.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The long term viability of the koala is therefore threatened by genetic weakness."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the koala found in coastal regions of eastern and southern Australia, from near Adelaide to the southern part of Cape York Peninsula?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands?", "answer": "Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were the koalas of South Australia largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock?", "answer": "The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "koalas", "extracted_entity": null}
{"question": "is Korean the official language of Korea?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": null}
{"question": "is the word \"Korean\" derived from Goryeo?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Goryeo", "extracted_entity": null}
{"question": "are all dialects of Korean similar to each other?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "about how many Koreans speakers are there?", "answer": "78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["78 million", "78 million Korean speakers"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "older English sources used the name \"Korean\" to refer to what?", "answer": "language, country, and people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["language, country, and people"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean language", "extracted_entity": null}
{"question": "what is the official language of North Korea?", "answer": "Korean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": null}
{"question": "why may English translation of Koreans adjectives misleadingly suggest that they are verbs?", "answer": "English does not have an identical grammatical category", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["English does not have an identical grammatical category"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The Korean adjective \uc774\uac8c(", "extracted_entity": null}
{"question": "where do the majority of loanwords other than Sino-Korean come from?", "answer": "modern times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern times", "modern times, 90% of which are from English"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "how many Korean speakers are there?", "answer": "There are about 78 million Korean speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are about 78 million Korean speakers."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "70 million", "extracted_entity": null}
{"question": "how many parts of speech does the Korean language contain?", "answer": "The Korean Language contains nine parts of speech. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Korean Language contains nine parts of speech. "], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Nouns, verbs, adjectives", "extracted_entity": null}
{"question": "how many verb paradigms are there in Korean?", "answer": "There are seven verb paradigms or speech levels in Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are seven verb paradigms or speech levels in Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "what is the Sino-Korean system of numbers based on?", "answer": "The Sino-Korean system is nearly entirely based on the Chinese numerals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sino-Korean system is nearly entirely based on the Chinese numerals."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is another term for Korean adjectives?", "answer": "Adjectives are also known as \"descriptive verbs\" or \"stative verbs\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adjectives are also known as \"descriptive verbs\" or \"stative verbs\"."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is modern Korean written in columns or rows?", "answer": "It is written in rows.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is written in rows."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "columns", "extracted_entity": null}
{"question": "what percentage of the Korean language does Jeong Jae-do estimate to be Sino-Korean?", "answer": "Sino-Korean makes up 30% of the Korean language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sino-Korean makes up 30% of the Korean language."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do Chinese and Japanese have spaces between words?", "answer": "No, Chinese and Japanese do not have spaces between words.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese and Japanese do not have spaces between words."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "from which languages is Korean descended?", "answer": "Korean is descended from Old Korean, Middle Korean and Modern Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean is descended from Old Korean, Middle Korean and Modern Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean is descended from Old Korean.", "extracted_entity": null}
{"question": "are there seven verb paradigms or speech levels in Korean?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Modern Korean written with spaces between words?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are the Korean names for the language based on the names for Korea used in North as well as South Korea?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it not also one of the two official languages of the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists?", "answer": "The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it also one of the two official languages in the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Kuala Lumpur the capitol of Malaysia? disfluent?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Kuala Lumpur in the Selangor state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur is in the Sel", "extracted_entity": null}
{"question": "what's the population of Kuala lumpur?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what language do they speak in Kuala Lumpur?", "answer": "Bahasa Melayu", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bahasa Melayu"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": null}
{"question": "what does the name \"Kuala Lumpur\" mean?", "answer": "muddy confluence, \"muddy estuary,\" and \"muddy city\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["muddy confluence, \"muddy estuary,\" and \"muddy city\""], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the confluence of the Gombak and", "extracted_entity": null}
{"question": "what airports does Kuala Lumpur have?", "answer": "Kuala Lumpur International Airport and Subang International Airport", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kuala Lumpur International Airport and Subang International Airport"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "KLIA, KLIA2, K", "extracted_entity": null}
{"question": "how long was Kuala Lumpur occupied by the Japanese?", "answer": "42 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42 months"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1942-1945", "extracted_entity": null}
{"question": "how many seasons does Kuala Lumpur experience?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the rainy season", "extracted_entity": null}
{"question": "when were the colonial buildings in Kuala Lumpur constructed?", "answer": "toward the end of the 19th and early 20th centuries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["toward the end of the 19th and early 20th centuries"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1896-1907", "extracted_entity": null}
{"question": "is it a host city for the Formula One World Championship?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the central market located in the proximity of the pertama complex?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it home to the parliament of malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the Parliament of Malaysia", "extracted_entity": null}
{"question": "when did kuala lumpur become a federal territory of malaysia?", "answer": "February 1, 1974", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["February 1, 1974"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1974", "extracted_entity": null}
{"question": "what is the city's population?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the official religion in the country?", "answer": "Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Islam"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Islam", "extracted_entity": null}
{"question": "is there a more industrialized region in malaysia?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Sabah", "extracted_entity": null}
{"question": "after the 2008 elections, what party had the most parliament seats?", "answer": "DAP", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["DAP"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the name of the event that draws the worlds top riders to malaysia?", "answer": "KL Grand Prix CSI 5*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["KL Grand Prix CSI 5*"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Tour de Langkawi", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci valued as an engineer?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci was valued as an", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci born on April 15?", "answer": "Yes, Leonardo da Vinci was born on April 15.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Leonardo da Vinci was born on April 15."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci", "extracted_entity": null}
{"question": "who left Verroccio's studio?", "answer": "Leonardo da Vinci left Verroccio's studio.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo da Vinci left Verroccio's studio."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "The ghost of the Count of Cagliost", "extracted_entity": null}
{"question": "when did Baldassare Castiglione, author of Il Cortegiano ( \" The Courtier \" ), write?", "answer": "1528", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1528"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1528", "extracted_entity": null}
{"question": "when did he flee to Venice?", "answer": "In 1499.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1499."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Leonardo a contemporary of Botticelli, Domenico Ghirlandaio as well as Perugino?", "answer": "Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo was a contemporary of Perugino.", "extracted_entity": null}
{"question": "is the larger work now almost universally attributed to Leonardo?", "answer": "Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "The Last Supper", "extracted_entity": null}
{"question": "has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo", "extracted_entity": null}
{"question": "is he widely considered to be one of the greatest painters of all time and perhaps the most diversely talented person ever to have lived?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "according to art historian Helen Gardner, were the scope and depth of his interests without precedent and ``his mind and personality seem to us superhuman, the man himself mysterious and remote''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are lobsters invertebrates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are lobsters kosher?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do lobsters have blue blood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many legs do lobsters have?", "answer": "10.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10.", "10"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "where are lobsters found?", "answer": "In all oceans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In all oceans.", "In all oceans"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the mean level of mercury in American lobsters?", "answer": "0.31 ppm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["0.31 ppm"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "0.00001", "extracted_entity": null}
{"question": "do lobsters feel pain?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Lobsters feel pain.", "extracted_entity": null}
{"question": "did the Moche people worship lobsters?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is the caridoid escape reaction?", "answer": "Swimming backwards quickly by curling and uncurling their abdomen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swimming backwards quickly by curling and uncurling their abdomen.", "When lobsters swim backwards quickly by curling and uncurling their abdomen"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were the recitations of the Ancient Greeks accompanied by lyre playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a classical lyre have a hollow body?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the lyre a stringed musical instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "which constellation is said to resemble a lyre shape?", "answer": "Lyra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lyra"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Aquila", "extracted_entity": null}
{"question": "how many raised arms are extending from the sound-chest of a classical lyre?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Two", "extracted_entity": null}
{"question": "what were the strings of a classical lyre made of?", "answer": "Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gut"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the foot pedal framework for a piano called a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes, the foot pedal framework for a piano", "extracted_entity": null}
{"question": "where was the deepest note of the classical lyre in relation to the player's body?", "answer": "It was farthest from the player's body.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was farthest from the player's body."], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a standard piano have fewer strings than a harp?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Malay the official language of Iraq?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "could Malay have originated from Sumatra island?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay", "extracted_entity": null}
{"question": "is Malay an agglutinative language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what family is Malay a member of?", "answer": "Austronesian family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austronesian family", "Austronesian."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a member of the Malay family", "extracted_entity": null}
{"question": "what is the basic word order in Malay?", "answer": "Subject Verb Object", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Subject Verb Object", "Subject Object Verb."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "The basic word order is SOV, subject-", "extracted_entity": null}
{"question": "what languages has Malay borrowed words from?", "answer": "Sanskrit, Arabic and English", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sanskrit, Arabic and English", "Arabic, Sanskrit, Tamil, Persian, Portuguese, Dutch, certain Chinese dialects, and English."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Arabic, Sanskrit, Portuguese, Dutch", "extracted_entity": null}
{"question": "are there many words in Malay that use natural gender?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how are new words formed in Malay?", "answer": "three methods.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three methods.", "Affixation, composition, and reduplication."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Malay in the Austronesian family of languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a member of the Austronesian", "extracted_entity": null}
{"question": "is there one negation word in Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many parts of speech are there in Malay?", "answer": "Four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Four"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "there are 4 parts of speech in Malay", "extracted_entity": null}
{"question": "what is the national language in Malaysia?", "answer": "Malay ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malay "], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Bahasa Malaysia", "extracted_entity": null}
{"question": "do linguists consider Malay to be a single language?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. Malay is a language with a number", "extracted_entity": null}
{"question": "can speakers of modern Malay understand Old Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "what language besides Malay is in the Austronesia language family?", "answer": "Malagasy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malagasy"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Malay language found in Sumatra?", "answer": "The Malay language was found in Sumatra.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Malay language was found in Sumatra."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "wasn't Malay language written using Pallava?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Malay language written using Pallava?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay language was written using Jawi script", "extracted_entity": null}
{"question": "had the language already been used as the lingua franca throughout the archipelago since the 15th century?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do another distinguishing feature of Malay ( include Indonesian Malay )?", "answer": "Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan)."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Malay spoken as local dialect of ethnic Malays?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is a language spoken in Brunei", "extracted_entity": null}
{"question": "give an example of the tributary states of the Majapahit kingdom in eastern Java. ?", "answer": "Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Malay the one of the most widely spoken languages in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is the most widely spoken language in the", "extracted_entity": null}
{"question": "was it derived from Latin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Melbourne the capital and most populous city in the state of Victoria?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne", "extracted_entity": null}
{"question": "is Melbourne home to Australia's busiest seaport?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Melbourne have a large international student community?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "The University of Melbourne", "extracted_entity": null}
{"question": "what is a resident of Melbourne known as?", "answer": "Melburnian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Melburnian", "A \"Melburnian\"."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbournian", "extracted_entity": null}
{"question": "when was Melbourne officially declared as a city?", "answer": "1847", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1847", "In 1847."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "1847", "extracted_entity": null}
{"question": "how many sister cities does the City of Melbourne have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six", "Six."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "12", "extracted_entity": null}
{"question": "where is Melbourne situated?", "answer": "boundary of the very hot inland areas and the cold southern ocean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["boundary of the very hot inland areas and the cold southern ocean"], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne, Victoria", "extracted_entity": null}
{"question": "how many domestic tourists visit Melbourne?", "answer": "7.6 million domestic visitors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7.6 million domestic visitors", "7.6 million."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "9.3 million", "extracted_entity": null}
{"question": "what religion holds majority in Melbourne?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "Christianity."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": null}
{"question": "did Albert Einstein keep a photograph of Faraday on his study wall?", "answer": "Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday", "extracted_entity": null}
{"question": "did Faraday marry Sarah Barnard?", "answer": "Yes, Faraday married Sarah Barnard.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday married Sarah Barnard.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did Faraday also discover the laws of electrolysis?", "answer": "Yes, Faraday also discovered the laws of  electrolysis.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday also discovered the laws of  electrolysis.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday discovered the laws of electroly", "extracted_entity": null}
{"question": "who discovered benzene?", "answer": "Michael Faraday discovered benzene.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday discovered benzene.", "Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Friedrich W\u00f6hler", "extracted_entity": null}
{"question": "where was Michael Faraday born?", "answer": "Michael Faraday was born in Newington Butts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday was born in Newington Butts.", "Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the Faraday effect?", "answer": "The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "The plane of polarisation of linearly polarised light can be rotated by the application of an exernal magnetic field aligned in the direction the light is moving."], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Faraday discover diamagnetism?", "answer": "Yes, Faraday discovered diamagnetism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday discovered diamagnetism.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday discovered diamagnetism", "extracted_entity": null}
{"question": "how many siblings did Faraday have?", "answer": "Faraday had three siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday had three siblings.", "two"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "where is Michael Faraday buried?", "answer": "Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "in the dissenters' section of Highgate Cemetery"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is faraday best known for his work with kinematics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday is best known for his work with", "extracted_entity": null}
{"question": "was he active in the area now called environmental science?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "did he discover benzene?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was his mentor?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was Faraday's wife?", "answer": "Sarah Barnard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sarah Barnard"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he advise the british government on the production of chemical weapons for use in the crimean war?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did faraday have strong maritime interests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sir Humphry Davy", "extracted_entity": null}
{"question": "does the charge reside on the interior of a charged conductor?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the charge resides on the exterior of a charged", "extracted_entity": null}
{"question": "wasn't Michelangelo abstemious?", "answer": "Michelangelo was abstemious.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michelangelo was abstemious."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Michelangelo", "extracted_entity": null}
{"question": "wasn't Michelangelo a homosexual?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Michelangelo Buonarroti was an", "extracted_entity": null}
{"question": "when did Filippo Strozzi sell it to Francis I?", "answer": "In 1529.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1529."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1538", "extracted_entity": null}
{"question": "when was his house demolished?", "answer": "1874", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1874"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "2016", "extracted_entity": null}
{"question": "was Michelangelo's mother Francesca di Neri del Miniato di Siena?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Francesca del Miniato", "extracted_entity": null}
{"question": "give an example of the most renowned works of the Renaissance. ?", "answer": "The Statue of David, completed by Michelangelo in 1504", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Statue of David, completed by Michelangelo in 1504"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Michelangelo's output in every field during his long life prodigious?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is English the language most often spoken in Montreal?", "answer": "The language most spoken at home in the city is French.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The language most spoken at home in the city is French.", "No, the language most spoken in Montreal is French."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "is Montreal's economy the third largest of all cities in Canada?", "answer": "No, Montreal's economy is the second largest of all cities in Canada.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Montreal's economy is the second largest of all cities in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal's economy is the third largest of all", "extracted_entity": null}
{"question": "is the most popular sport in Montreal ice hockey?", "answer": "yes, The most popular sport in Montreal is ice hockey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, The most popular sport in Montreal is ice hockey.", "Yes, the most popular sport in Montreal is ice hockey."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "The most popular sport in Montreal is ice hockey.", "extracted_entity": null}
{"question": "how many international airports does Montreal have?", "answer": "Montreal has two international airports.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montreal has two international airports."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal-Pierre Elliott Trudeau International Airport", "extracted_entity": null}
{"question": "who is the Mayor of Montreal?", "answer": "The mayor is G\u00c3\u00a9rald Tremblay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mayor is G\u00c3\u00a9rald Tremblay."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Val\u00e9rie Plante", "extracted_entity": null}
{"question": "what is the name of the largest church in Montreal?", "answer": "The largest church in Montreal is named Saint Joseph's Oratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The largest church in Montreal is named Saint Joseph's Oratory.", "Saint Joseph's Oratory is the largest church in Montreal."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Saint Joseph's Oratory", "extracted_entity": null}
{"question": "where was much of Montreal's industry during the late 19th and early-to-mid 20th century?", "answer": "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Montreal contain the largest church in Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Montreal contains the largest church in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the largest primarily French-speaking city in the western world?", "answer": "Paris is the largest primarily French-speaking city in the western world.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris is the largest primarily French-speaking city in the western world.", "The largest primarily French-speaking city in the western world is Paris."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": null}
{"question": "is Nairobi the capital of Kenya?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi is the capital of Kenya", "extracted_entity": null}
{"question": "was Nairobi founded in 1899?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "is the current estimated population of Nairobi about 6 milion?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "what is the current estimated population of Nairobi?", "answer": "About 3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["About 3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4,000,000", "extracted_entity": null}
{"question": "in what year was Nairobi founded?", "answer": "1899", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1899"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1899", "extracted_entity": null}
{"question": "which embassy in Nairobi was bombed in August 1998?", "answer": "The United States Embassy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The United States Embassy"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "US Embassy in Nairobi", "extracted_entity": null}
{"question": "how many civilians died in the 1998 U.S. embassy bombing?", "answer": "Over two hundred", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over two hundred"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "213", "extracted_entity": null}
{"question": "how many trades can the NSE make per day?", "answer": "10 million trades", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10 million trades"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "5", "extracted_entity": null}
{"question": "about how many people are estimated to live in Nairobi?", "answer": "3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi has a population of 3,", "extracted_entity": null}
{"question": "the district is bordered to the southwest by Uhuru Park and where?", "answer": "Central Park", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Kampala", "extracted_entity": null}
{"question": "when did the Catholic University of Eastern Africa which obtain its Letter of Interim Authority?", "answer": "In 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1989."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1963", "extracted_entity": null}
{"question": "is Central Park adjacent to Uhuru Park?", "answer": "Central Park is adjacent to Uhuru Park.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park is adjacent to Uhuru Park."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Central Park is adjacent to Uhuru Park", "extracted_entity": null}
{"question": "give an example of the highest growth rates of any city in Africa. ?", "answer": " The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi, Kenya", "extracted_entity": null}
{"question": "give an example of the most prominent cities in Africa politically and financially. ?", "answer": "Nairobi is now one of the most prominent cities in Africa politically and financially", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is now one of the most prominent cities in Africa politically and financially"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Lagos, Nigeria", "extracted_entity": null}
{"question": "is Nairobi not the capital as well as largest city of Kenya?", "answer": "Nairobi is the capital and largest city of Kenya.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is the capital and largest city of Kenya."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the city and its surrounding area also form the Nairobi Province?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi Province", "extracted_entity": null}
{"question": "does the name ``Nairobi'' come from the Maasai phrase Enkare Nyirobi, which translates to ``the place of cool waters''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi", "extracted_entity": null}
{"question": "did Nikola Tesla die alone?", "answer": "Nikola Tesla died alone.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla died alone.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Nikola Tesla close friends with Mark Twain?", "answer": "Nikola Tesla was close friends with Mark Twain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla was close friends with Mark Twain.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla was close friends with Mark", "extracted_entity": null}
{"question": "did Nikola Tesla use a technique called picture thinking?", "answer": "Nikola Tesla used a technique called picture thinking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla used a technique called picture thinking.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Nikola Tesla appear on the cover of Time Magazine?", "answer": "On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "1931"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1931", "extracted_entity": null}
{"question": "who became adversaries with Nikola Tesla?", "answer": "Nikola Tesla became adversaries with Edison.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla became adversaries with Edison.", "Edison"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison", "extracted_entity": null}
{"question": "where did Nikola Tesla move to in 1880?", "answer": "In 1880, Nikola Tesla moved to Budapest.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1880, Nikola Tesla moved to Budapest.", "Budapest"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "to the United States of America", "extracted_entity": null}
{"question": "was Nikola Tesla's mother illiterate?", "answer": "Nikola Tesla's mother never learned to read.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla's mother never learned to read.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "how many siblings did Nikola Tesla have?", "answer": "Nikola Tesla had four siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla had four siblings.", "4"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": null}
{"question": "was Nikola Tesla a vegetarian?", "answer": " In his later years Tesla became a vegetarian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" In his later years Tesla became a vegetarian.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he was a vegetarian.", "extracted_entity": null}
{"question": "is there a monument to Tesla at Niagara Falls, New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "The Tesla Monument is a monument in honor", "extracted_entity": null}
{"question": "was Tesla an American citizen?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla was not an American citizen.", "extracted_entity": null}
{"question": "was Tesla on the cover of Time magazine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Tesla receive his last patent?", "answer": "1928", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1928"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1928", "extracted_entity": null}
{"question": "what languages did Tesla speak?", "answer": "Serbian, Czech, English, French, German, Hungarian, Italian, and Latin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Serbian, Czech, English, French, German, Hungarian, Italian, and Latin"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "English, French, German, Italian, and Ser", "extracted_entity": null}
{"question": "did Tesla win the Nobel Prize?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Tesla rich at the time of his death?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what music group was named after Tesla?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": null}
{"question": "consequently, what is not considered a \"serious painter\" by some contemporary artists, who often regard his work as bourgeois and kitsch?", "answer": "Rockwell ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who spent the next 10 years painting for Look magazine?", "answer": "Norman Rockwell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norman Rockwell."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was his last painting for the Post published?", "answer": "1963", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1963"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "1999", "extracted_entity": null}
{"question": "when did Rockwell marry his first wife, Irene O'Connor,?", "answer": "In 1916.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1916."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "1923", "extracted_entity": null}
{"question": "did Norman Rockwell spend the winter months as artist-in-residence at Otis College of Art as well as Design?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Norman Rockwell publish a total of 321 original covers for The Saturday Evening Post over 47 years?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Norman Rockwell", "extracted_entity": null}
{"question": "was Rockwell also commissioned to illustrate over 40 books including Tom Sawyer as well as Huckleberry Finn?", "answer": "Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do his works enjoy a broad popular appeal in the United States, where Rockwell is most famous for the cover illustrations of everyday life scenarios he created for The Saturday Evening Post magazine over more than four decades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "The Saturday Evening Post", "extracted_entity": null}
{"question": "are among the best-known of Rockwell's works the Willie Gillis series, Rosie the Riveter (although his Rosie was reproduced less than others of the day), Saying Grace (1951), and the Four Freedoms series?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Rosie the Riveter", "extracted_entity": null}
{"question": "is he also noted for his work for the Boy Scouts of America (BSA)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the octopus a cephalopod?", "answer": "Yes, the octopus is a cephalopod.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus is a cephalopod."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "does the octopus have two eyes?", "answer": "Yes, the octopus has two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has two eyes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "The octopus has two eyes.", "extracted_entity": null}
{"question": "does the octopus have a hard beak?", "answer": "Yes, the octopus has a hard beak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has a hard beak."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "where is the mouth of an octopus?", "answer": "The mouth of an octopus is at the center point of the arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mouth of an octopus is at the center point of the arms.", "The octopus' mouth is at the center point of the arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "the mouth of an octopus is in the center", "extracted_entity": null}
{"question": "around how many recognized octopus species are there?", "answer": "There are around 300 recognized octopus species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are around 300 recognized octopus species."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "300", "extracted_entity": null}
{"question": "how many arms does an octopus have?", "answer": "An octopus has four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An octopus has four pairs of arms.", "The octopus has four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "8", "extracted_entity": null}
{"question": "what is the range of lifespans of the octopus?", "answer": "Octopuses can live from six months to five years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Octopuses can live from six months to five years.", "The octopus has a short lifespan."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "1 to 5 years", "extracted_entity": null}
{"question": "what is an example of something an octopus learned in a research experiment?", "answer": "In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "An octopus learned to solve mazes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "It is a good example of learning.", "extracted_entity": null}
{"question": "how can an octopus defend against predators?", "answer": "To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "They can hide, flee, expel ink, or use camouflage."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are Octopus highly intelligent?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do males use a specialized arm called a hectocotylus to insert spermatophores?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do the octopuses in the less familiar Cirrina suborder have two fins and an internal shell?", "answer": "Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "do octopuses have two eyes and four pairs of arms?", "answer": "Yes, octopuses have two eyes and four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, octopuses have two eyes and four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is octopus a common food in Mediterranean cuisine as well as Portuguese cuisine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the collective form octopus usually reserved for animals consumed for food?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "has an octopus a hard beak, with its mouth at the center point of the arms?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Octopus", "extracted_entity": null}
{"question": "what happened in 1968 through 1971?", "answer": "Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "the United States was at war with Vietnam.", "extracted_entity": null}
{"question": "what happened in 1838?", "answer": "Don Jos\u00e9 Ruiz y Blasco was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Don Jos\u00e9 Ruiz y Blasco was born"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The Mormons were driven from Missouri", "extracted_entity": null}
{"question": "who was not long in finding another lover, Jacqueline Roque?", "answer": "Picasso.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Picasso."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Picasso make his first trip to Paris?", "answer": "1900", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1900"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "give an example of the most comprehensive records extant of any major artists beginnings. ?", "answer": "Museu Picasso", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Museu Picasso"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is in the 1996 movie Surviving Picasso Picasso played by actor Anthony Hopkins?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Anthony Hopkins", "extracted_entity": null}
{"question": "is the U.S. copyright representative for the Picasso Administration the Artists Rights Society?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Artists Rights Society (ARS)", "extracted_entity": null}
{"question": "is he one of the most recognized figures in 20th-century art?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is he best known for co-founding the Cubist movement and for the wide variety of styles embodied in his work?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "are among his most famous works the proto-Cubist Les Demoiselles d'Avignon (1907) and Guernica (1937), his portrayal of the German bombing of Guernica during the Spanish Civil War?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are pianos used in Western music?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are \"upright pianos\" called \"vertical pianos\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "upright pianos\" are called", "extracted_entity": null}
{"question": "did Bartolomeo Cristofori invent the modern piano?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what is the middle pedal called on grand pianos?", "answer": "the sostenuto pedal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the sostenuto pedal.", "sostenuto pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "sostenuto", "extracted_entity": null}
{"question": "how many black keys do modern pianos have?", "answer": "36.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["36.", "36"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "88", "extracted_entity": null}
{"question": "what is the sustain pedal called?", "answer": "The pedal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pedal", "damper pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "the sustain pedal", "extracted_entity": null}
{"question": "where is Irving Berlin's piano located?", "answer": "In the Smithsonian Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Smithsonian Museum.", "in the Smithsonian Museum"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what kind of piano did Irving Berlin play?", "answer": "The transposing piano.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The transposing piano.", "transposing piano"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "A piano.", "extracted_entity": null}
{"question": "is the left-most pedal on a grand piano called the una corda?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes, the left-most pedal on a grand piano is called the una corda."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it advantageous for a grand piano's metal plate to be quite massive?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "It is advantageous for the plate to be quite massive."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the mechanism in an upright piano perpendicular to its keys?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "The mechanism in upright pianos is perpendicular to the keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "The mechanism in an upright piano is per", "extracted_entity": null}
{"question": "about how tall is a typical studio piano?", "answer": "Studio pianos are around 42 to 45 inches tall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Studio pianos are around 42 to 45 inches tall."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "5 feet", "extracted_entity": null}
{"question": "what are a piano's keys generally made of?", "answer": "Piano keys are generally made of spruce or basswood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano keys are generally made of spruce or basswood."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "ivory or plastic", "extracted_entity": null}
{"question": "how many total keys does a typical modern piano have?", "answer": "88 keys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88 keys.", "A modern piano has 88 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "88", "extracted_entity": null}
{"question": "why are upright pianos more compact?", "answer": "Upright pianos are more compact because the frame and strings are vertical.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Upright pianos are more compact because the frame and strings are vertical."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Because they have a smaller frame and smaller strings.", "extracted_entity": null}
{"question": "do older pianos have more keys than modern pianos?", "answer": "no.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no.", "Many older pianos only have 85 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "The piano has 88 keys, the same", "extracted_entity": null}
{"question": "what are the names of a piano's pedals?", "answer": "Piano pedals from left to right: una corda, sostenuto, and damper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano pedals from left to right: una corda, sostenuto, and damper.", "The names of a piano's pedals are una corda, sostenuto, and damper."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in the January 15, 1882?", "answer": "On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what happened in 1887?", "answer": "In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "1887 was a year of the", "extracted_entity": null}
{"question": "is one of the best known Impressionist works Renoir's 1876 Dance at Le Moulin de la Galette?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Dance at Le Moulin de la Galette", "extracted_entity": null}
{"question": "was Pierre-Auguste Renoir born in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Pierre-Auguste Renoir", "extracted_entity": null}
{"question": "have two of Renoir's paintings sold for more than US0 million?", "answer": "Yes (any painting sells for more than $0 million though...)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (any painting sells for more than $0 million though...)"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "as a celebrator of beauty, and especially feminine sensuality, has it been said that ``Renoir is the final representative of a tradition which runs directly from Rubens to Watteau''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Renoir", "extracted_entity": null}
{"question": "did Pierre-Auguste Renoir die in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Pierre-Auguste Renoir", "extracted_entity": null}
{"question": "as a boy, did he work in a porcelain factory where his drawing talents led to him being chosen to paint designs on fine china?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Portuguese an official language of Andorra?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Portuguese is not an official language of Andorra."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does the Portuguese language have its roots in the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese is derived from Latin."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Portuguese contain words from the Arabic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese contains words from the Arabic language."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "where are there small Portuguese-speaking communities?", "answer": "Former overseas colonies of Portugal such as Macau and East Timor ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Former overseas colonies of Portugal such as Macau and East Timor ", "There are small Portuguese-speaking communities in Macau and East Timor."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portugal", "extracted_entity": null}
{"question": "what are the two main groups of Portuguese dialects?", "answer": "Those of Brazil and those of the Old World", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Those of Brazil and those of the Old World", "The two main groups of dialects are Brazil and the Old World."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Northern and Southern", "extracted_entity": null}
{"question": "what event marked the end of the Old Portuguese period?", "answer": "The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "The end of the Old Portuguese period was marked by the publication of the Cancioneiro Geral by Garcia de Resende."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "the Carnation Revolution of 25 April", "extracted_entity": null}
{"question": "which government established Portuguese as it's third official language in order to meet the requirements to apply for full membership in the CPLP?", "answer": "Equatorial Guinea", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Equatorial Guinea", "The government of Equatorial Guinea established Portuguese as its third official language in order to meet the requirements to apply for full membership in the CPLP."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "by 2050, what will the total population of Portuguese speakers in the world be?", "answer": "335 million people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["335 million people", "By 2050, Portuguese-speaking countries will have a total population of 335 million people."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "288,000,0", "extracted_entity": null}
{"question": "in which country is Portuguese the co-official language with Cantonese Chinese?", "answer": "Macau", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Macau", "Portuguese is the co-official language with Cantonese Chinese in Macau."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Macau", "extracted_entity": null}
{"question": "wasn't Portuguese language spread by arriving Roman soldiers?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Roman soldiers didn't speak Portuguese. They spoke", "extracted_entity": null}
{"question": "(where French has a similar phenomenon, with alveolar affricates instead of postalveolars?", "answer": "Quebec.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "(where French has a similar phenomenon, with", "extracted_entity": null}
{"question": "give an example of the official languages of the special administrative region of Macau and East Timor, . ?", "answer": "Portuguese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Portuguese"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese", "extracted_entity": null}
{"question": "is Portuguese an official language of several international organizations?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is an official language of the United", "extracted_entity": null}
{"question": "is it derived from the Latin spoken by the romanized pre-Roman peoples of the Iberian Peninsula (namely the Gallaeci, the Lusitanians, the Celtici and the Conii) around 2000 years ago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was it used as the exclusive lingua franca on the island of Sri Lanka for almost 350 years?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the city named after the apostle Saint Peter?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Rome", "extracted_entity": null}
{"question": "has the terrain in the city been artificially raised?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes, it has been.", "extracted_entity": null}
{"question": "what were some other names of the city?", "answer": "Petrograd, Leningrad, and Piter", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Petrograd, Leningrad, and Piter", "Petrograd and Leningrad."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what festival features fireworks  celebrating the end of the school year?", "answer": "The White Nights Festival in Saint Petersburg", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The White Nights Festival in Saint Petersburg", "The White Nights Festival."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "the end of the school year", "extracted_entity": null}
{"question": "where do most people in urban Saint Petersburg live?", "answer": "apartments", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apartments", "Apartments."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "apartments", "extracted_entity": null}
{"question": "what was made in Russia's largest foundry?", "answer": "thousands of sculptures and statues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["thousands of sculptures and statues", "Thousands of sculptures and statues."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Cannons.", "extracted_entity": null}
{"question": "when did the crime level become higher?", "answer": "After the October revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the October revolution."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1970s", "extracted_entity": null}
{"question": "how to travel to work from the city to Moscow?", "answer": "railway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["railway", "One way is through the Moscow-Saint Petersburg Railway."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who laid down the Peter and Paul Fortress?", "answer": "Peter the Great ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Peter the Great "], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Saint Peter and Saint Paul", "extracted_entity": null}
{"question": "when was the 7th symphony premiered?", "answer": "1942", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1942"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1998", "extracted_entity": null}
{"question": "has Equestrianism been a long tradition, popular among the Tsars and aristocracy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Equestrianism has been a long tradition,", "extracted_entity": null}
{"question": "give an example of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995. ?", "answer": "Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "give an example of the best known symphony orchestras in the world under the leadership of conductors Yevgeny Mravinsky and Yuri Temirkanov. ?", "answer": "The Leningrad Philharmonic Orchestra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Leningrad Philharmonic Orchestra"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Saint Petersburg Philharmonic Orchestra", "extracted_entity": null}
{"question": "give an example of the longest, most destructive, and most lethal sieges of major cities in modern history. ?", "answer": "The Siege of Leningrad", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Siege of Leningrad"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were the city's other names Petrograd (, 1914 -- 1924) and Leningrad (, 1924 -- 1991)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Saint Petersburg", "extracted_entity": null}
{"question": "is it often called just Petersburg (-RRB- and is informally known as Piter (-RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "founded by Tsar Peter I of Russia on 27 May 1703, was it the capital of the Russian Empire for more than two hundred years (1713 -- 1728, 1732 -- 1918)?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the standard of living in San Franciscio high?", "answer": "Yes, San Francisco is characterized by a high standard of living.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, San Francisco is characterized by a high standard of living.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Franciscio is a high standard of living.", "extracted_entity": null}
{"question": "does San Francisco have a high percentage of gay and lesbian individuals?", "answer": "Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco has a high percentage of gay and les", "extracted_entity": null}
{"question": "is San Franscico a popular tourist destination?", "answer": "Yes. San Francisco is a popular international tourist destination.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. San Francisco is a popular international tourist destination.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco", "extracted_entity": null}
{"question": "how large is the population of San Francisco?", "answer": "The estimated population of San Francisco in the year 2008 was 808,976.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The estimated population of San Francisco in the year 2008 was 808,976.", "San Francisco has an estimated population of 808,976."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "850,000", "extracted_entity": null}
{"question": "how many state parks are in San Francisco?", "answer": "There is only one park managed by the California State Park system: Candlestick Point.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There is only one park managed by the California State Park system: Candlestick Point.", "There is one state park in San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Golden Gate Park", "extracted_entity": null}
{"question": "where is San Francisco?", "answer": "San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "San Francisco is in California."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco, California", "extracted_entity": null}
{"question": "what is the cause of local earthquakes?", "answer": "It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "The nearby San Andreas and Hayward Faults are responsible for much earthquake activity."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The cause of local earthquakes is a result", "extracted_entity": null}
{"question": "are more residents employed by small businesses than in 1977?", "answer": "The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does poverty in San Francisco compare to the nation-wide average?", "answer": "The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "San Francisco's poverty rate is lower than the national average."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Mission Bay campus opened in 2003?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Mission Bay campus opened in 2003", "extracted_entity": null}
{"question": "was the University of San Fransisco founded in 1855?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "University of San Fransisco", "extracted_entity": null}
{"question": "is Golden Gate Park the largest city park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Golden Gate Park is the largest city park", "extracted_entity": null}
{"question": "what is Northern California's most widely circulated newspaper?", "answer": "San Francisco Chronicle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco Chronicle"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the _Sacramento Bee_", "extracted_entity": null}
{"question": "what makes San Francisco among the top-ten North American destinations for conventions and conferences?", "answer": "San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "a large hotel infrastructure and a world-class convention facility in the Moscone Center"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The convention center.", "extracted_entity": null}
{"question": "who manages Candlestick point?", "answer": "California State Park system", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["California State Park system", "the California State Park system"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Crissy field an airfield?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Crissy field is an airfield.", "extracted_entity": null}
{"question": "does the de Young museum house the Asian Art Museum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "what prompted the city to upgrade its building codes?", "answer": "earthquakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["earthquakes", "The threat of major earthquakes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the city was concerned about the safety of its residents", "extracted_entity": null}
{"question": "like many larger where cities, San Francisco is a minority-majority city, as non-Hispanic whites comprise less than half of the population?", "answer": "U.S.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["U.S."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who laid out a street plan for the expanded settlement?", "answer": "William Richardson and Alcalde Francisco de Haro.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["William Richardson and Alcalde Francisco de Haro."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when was a 43-acre Mission Bay campus opened?", "answer": "2003", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2003"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1977", "extracted_entity": null}
{"question": "when did it sign on the air?", "answer": "In 1941.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1941."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1920", "extracted_entity": null}
{"question": "give an example of the largest two-year community colleges in the country. ?", "answer": "The City College of San Francisco.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The City College of San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "______________________________________________________________", "extracted_entity": null}
{"question": "give an example of the ten Beta World Cities. ?", "answer": "San Trancisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Trancisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "London, New York, Tokyo, Paris, Los", "extracted_entity": null}
{"question": "the only consolidated city-county in California, does it encompass a land area of on the northern end of the San Francisco Peninsula, making it the second most densely populated city in the United States?", "answer": "San Francisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Jose", "extracted_entity": null}
{"question": "is San Francisco also the financial, cultural, and transportation center of the larger San Francisco Bay Area, a region of 7.4 million people?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is also the financial, cultural, and", "extracted_entity": null}
{"question": "in what year did the Spanish establish a fort at the Golden Gate and a mission named for Francis of Assisi on the site?", "answer": "In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": null}
{"question": "does Swahili have dipthongs?", "answer": "Yes, Swahili has diphthongs. (Typo)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili has diphthongs. (Typo)"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Swahili a Bantu language?", "answer": "Yes, Swahili is a Bantu language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is a Bantu language.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili is a Bantu language.", "extracted_entity": null}
{"question": "did Uganda make Swahili a required subject in primary schools?", "answer": "Yes, Uganda made Swahili a required subject in primary schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Uganda made Swahili a required subject in primary schools.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Uganda did not make Swahili a", "extracted_entity": null}
{"question": "what is the Swahili word for the Swahili language?", "answer": "Kiswahili is the Swahili word for the Swahili language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kiswahili is the Swahili word for the Swahili language.", "Kiswahili"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Swahili spoken in Mozambique?", "answer": "Yes, Swahili is spoken in parts of northern Mozambique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is spoken in parts of northern Mozambique.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how does Swahili time run?", "answer": "Swahili time runs from dawn to dusk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili time runs from dawn to dusk.", "from dawn to dusk"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili time is a time standard that is", "extracted_entity": null}
{"question": "how many noun classes does Swahili have?", "answer": "Swahili has sixteen noun classes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili has sixteen noun classes.", "sixteen"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "in what year did Germany seize Tanganyika?", "answer": "Germany seized Tanganyika in 1886.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany seized Tanganyika in 1886.", "1886"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": null}
{"question": "which is the sub-Saharan indigenous language with the greatest number of speakers?", "answer": "Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "Hausa"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Swedish a descendant of Old Norse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Swedish use the perfect participle to form the present perfect tense?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "The present perfect in Swedish is formed by adding the", "extracted_entity": null}
{"question": "how many Swedish speakers were reported in Canada in 2001?", "answer": "16,915.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["16,915.", "There are 16,915 reported Swedish speakers in Canada."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "100000", "extracted_entity": null}
{"question": "how many letters are in the basic Latin alphabet?", "answer": "26.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["26.", "26"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "26", "extracted_entity": null}
{"question": "when did a standard and common language become available to all Swedes?", "answer": "In the 20th Century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the 20th Century.", "It was during the 20th century that a common, standardized national language became available to all Swedes."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "1906", "extracted_entity": null}
{"question": "when did the Soviet military forces occupy Estonia?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "In 1944."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "1944", "extracted_entity": null}
{"question": "what are the differences between English and Swedish pronouns?", "answer": "Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "The Swedish pronouns are: 1.", "extracted_entity": null}
{"question": "how does Swedish language counterbalance the absence of cases?", "answer": "The lack of cases in Swedish is compensated by a wide variety of prepositions.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lack of cases in Swedish is compensated by a wide variety of prepositions."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "the Swedish language counterbalances the absence of cases", "extracted_entity": null}
{"question": "is Taipei in a valley?", "answer": "Taipei is in the valleys of the Keelung and Xindian Rivers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei is in the valleys of the Keelung and Xindian Rivers", "Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei is in a valley.", "extracted_entity": null}
{"question": "was Taipei founded in the 18th century?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei City", "extracted_entity": null}
{"question": "is the National Palace Museum in Taipei?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "National Palace Museum", "extracted_entity": null}
{"question": "what is along the city's western border?", "answer": "The Danshui River borders Taipei to the west.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Danshui River borders Taipei to the west.", "Taipei has the Danshui River along its western border."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The western border of the city is the west branch", "extracted_entity": null}
{"question": "what is Taipei 101?", "answer": "Taipei 101 is a 101-floor landmark skyscraper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei 101 is a 101-floor landmark skyscraper."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei 101 is a sk", "extracted_entity": null}
{"question": "what are some annual festivals in Taipei?", "answer": "The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "Some annual festivals include Tomb-Sweping Day, the Dragon Boat Festival, the Ghost Festival, and the Mid-Autumn Festival."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when is the Pacific typhoon season?", "answer": "Between June and October.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Between June and October.", "It's between June and October."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who sets up shrines?", "answer": "Many homes and business set up shrines.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many homes and business set up shrines.", "Many homes and businesses would set up shrines."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Yasukuni Shrine", "extracted_entity": null}
{"question": "what does Taipei produce?", "answer": "Taipei produces high technology and hi-tech components", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei produces high technology and hi-tech components", "Taipei produces textiles, apparel, electronics, machinery, printed materials, precision equipment, food, and beverages."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the trumpet the musical instrument with the highest register in the brass family?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The trumpet is the musical instrument with the", "extracted_entity": null}
{"question": "is a trumpet played by blowing air through closed lips?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the most common type of trumpet the B trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Bb trumpet", "extracted_entity": null}
{"question": "what is the earliest date of the trumpet?", "answer": "1500 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1500 BC", "around 1500 BC"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is a trumpet constructed of?", "answer": "brass tubing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass tubing", "brass tubing bent twice into an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what are the smallest trumpets referred to as?", "answer": "Piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piccolo trumpets", "piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Piccolo trumpet", "extracted_entity": null}
{"question": "why do trumpets have a bright, loud sound?", "answer": "The trumpet and trombone share a roughly cylindrical bore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The trumpet and trombone share a roughly cylindrical bore.", "Due to the cylindrical bore"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "why does the cornet have a slightly mellower tone than the trumpet?", "answer": "The cornet and flugelhorn have conical bores", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cornet and flugelhorn have conical bores", "because it has conical bores"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what makes a trumpet fully chromatic?", "answer": "able to play all twelve pitches of Western music", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["able to play all twelve pitches of Western music"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were trumpet players heavily guarded?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are pocket trumpets compact B trumpets?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Dizzy Gillespie a famous trumpeter in 1998?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what shape is a trumpet bent into?", "answer": "oblong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["oblong", "an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "trumpet", "extracted_entity": null}
{"question": "what trumpet was the first to be allowed in the Christian Church?", "answer": "slide trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slide trumpets", "Slide trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Corno da Caccia", "extracted_entity": null}
{"question": "the trumpet can be confused with what close relative?", "answer": "the cornet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cornet", "the coronet"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what can be augmented with a fourth valve?", "answer": "the flugelhorn", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the flugelhorn", "flugelhorn"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The 4th valve is the one that", "extracted_entity": null}
{"question": "what was Maynard Ferguson noted for being able to play accurately?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Maynard Ferguson was noted for", "extracted_entity": null}
{"question": "what was built to play in both B and A keys?", "answer": "piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The B-A-flat-G-F", "extracted_entity": null}
{"question": "what happened in 1894?", "answer": "Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\""], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The 1894 World's Fair", "extracted_entity": null}
{"question": "what happened in nineteen fifty seven?", "answer": "Maynard Ferguson formed his own band.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maynard Ferguson formed his own band."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The Party seized power in 1957", "extracted_entity": null}
{"question": "is the Breeze Eazy' ' method sometimes used to teach younger students?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Breeze Eazy' ' method sometimes used", "extracted_entity": null}
{"question": "do many players use a smaller mouthpiece on the piccolo trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the trumpet constructed of brass tubing bent twice into an oblong shape?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "give an example of the most influential musicians of the 20th century. ?", "answer": "Miles Davis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Miles Davis"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "were Slide trumpets the first trumpets allowed in the Christian church?", "answer": "Slide trumpets were the first trumpets allowed in the Christian church. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Slide trumpets were the first trumpets allowed in the Christian church. "], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "are they constructed of brass tubing bent twice into an oblong shape, and are played by blowing air through closed lips, producing a ``buzzing'' sound which starts a standing wave vibration in the air column inside the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are there several types of trumpet?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Trumpet", "extracted_entity": null}
{"question": "was the Ottoman script replaced with a variant of the Latin alphabet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The Ottoman script was replaced with a variant of", "extracted_entity": null}
{"question": "is there a definite article in Turkish language?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes, there is a definite article in Turkish language", "extracted_entity": null}
{"question": "is it possible to alter the word order to stress the importance of a certain phrase?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "\"Hear this, O Israel: The L", "extracted_entity": null}
{"question": "what is the official language of Turkey?", "answer": "Turkish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turkish.", "Turkish is the official language of Turkey."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish", "extracted_entity": null}
{"question": "when was the Turkish Language Association founded?", "answer": "In 1928.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1928.", "The Turkish Language Association was founded in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "1931", "extracted_entity": null}
{"question": "what are the most significant foreign contributors to Turkish vocabulary?", "answer": "Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. "], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Arabic and Persian", "extracted_entity": null}
{"question": "who were the Orkhon inscriptions built for?", "answer": "For the prince Kul Tigin and his brother Emperor Bilge Khan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For the prince Kul Tigin and his brother Emperor Bilge Khan.", "The Orkhon inscriptions were erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The Orkhon inscriptions were built for", "extracted_entity": null}
{"question": "what was one of the results of the introduction of the new Turkish alphabet?", "answer": "A dramatic increase in literacy from Third World levels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A dramatic increase in literacy from Third World levels.", "There was a dramatic increase in literacy from its original Third World levels."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the new alphabet was adopted by the Turkish government on", "extracted_entity": null}
{"question": "why does Lewis use \"absolute case\" instead of \"nominative\"?", "answer": "Because it is also used for the indefinite accusative.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is also used for the indefinite accusative."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "It is not clear why Lewis uses \"absolute case", "extracted_entity": null}
{"question": "how many people speak Turkish worldwide?", "answer": "There are roughly another 10 million native speakers worldwide.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are roughly another 10 million native speakers worldwide."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "75 million", "extracted_entity": null}
{"question": "in Turkish, which syllable usually has the stress?", "answer": "Stress is usually on the last syllable", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stress is usually on the last syllable"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the first.", "extracted_entity": null}
{"question": "is the spelling of Turkish phonetic?", "answer": "yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "\u0130", "extracted_entity": null}
{"question": "what percentage of the words in Turkish vocabulary have Turkish origins?", "answer": "86% of the Turkish vocabulary have Turkish origins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["86% of the Turkish vocabulary have Turkish origins."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "99%", "extracted_entity": null}
{"question": "after what event was the Turkish Language Association established?", "answer": "After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what exception to the rules of vowel harmony do compound words have?", "answer": "In compound words, the vowels need not harmonize between the constituent words of the compound.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In compound words, the vowels need not harmonize between the constituent words of the compound."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "In the word \"cumbersome\", the", "extracted_entity": null}
{"question": "what are the patterns of vowel harmony?", "answer": "Vowels are characterised by two features: front/back and rounded/unrounded.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowels are characterised by two features: front/back and rounded/unrounded."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The patterns of vowel harmony are:", "extracted_entity": null}
{"question": "what are the ways to join groups of nouns?", "answer": "Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Vietnamese have a comparatively large number of vowels?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Vietnamese the mother tongue of the Vietnamese people?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Vietnamese", "extracted_entity": null}
{"question": "is Vietnamese the official administrative language of Vietnam?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Vietnamese influenced by Chinese?", "answer": "Much vocabulary has been borrowed from Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much vocabulary has been borrowed from Chinese.", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "when did written Vietnamese become the official administrative language?", "answer": "the 20th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 20th century", "Vietnamese became the official administrative language by the 20th century."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1910", "extracted_entity": null}
{"question": "have consonant clusters been lost?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Red River Delta traditionally poor?", "answer": "Not compared to Nghe An, Ha Tinh, or Quang Binh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Not compared to Nghe An, Ha Tinh, or Quang Binh", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what kind of words have been borrowed from Chinese?", "answer": "Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "Vietnamese adopts words from Chinese which express abstract ideas."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is Vietnam independent from France?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "was Vietnamese formally written using the Chinese writing system?", "answer": "Yes, Vietnamese was *formerly* written using the Chinese writing system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese was *formerly* written using the Chinese writing system."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Vietnamese have a large number of vowels?", "answer": "Yes, Vietnamese has a comparatively large number of vowels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese has a comparatively large number of vowels."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what are the three dialect regions of Vietnamese?", "answer": "The three regions are North, Central, and South.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three regions are North, Central, and South."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "North, Central, and South.", "extracted_entity": null}
{"question": "when was the reunification of Vietnam?", "answer": "The reunification of Vietnam occurred in 1975-76.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The reunification of Vietnam occurred in 1975-76."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1975", "extracted_entity": null}
{"question": "how many tones do Northern varieties have?", "answer": "Northern Vietnamese has six tones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Vietnamese has six tones."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does Vietnamese borrow from Latin and Greek?", "answer": "No, Vietnamese does not borrow from Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Vietnamese does not borrow from Latin and Greek."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Vietnamese come to predominate politically in 2nd century BC?", "answer": "No, Chinese came to predominate politically in the 2nd century B.C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese came to predominate politically in the 2nd century B.C."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Vietnamese", "extracted_entity": null}
{"question": "do more northern speakers speak the southern regional dialect than southern speakers speak the northern dialect?", "answer": "More southern speakers speak the northern dialect than northerners speak the suthern dialect.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More southern speakers speak the northern dialect than northerners speak the suthern dialect."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened in 1860?", "answer": "Vincent van Gogh attended the Zundert village school from 1860.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vincent van Gogh attended the Zundert village school from 1860."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "The American Civil War", "extracted_entity": null}
{"question": "who frequented the circle of the British-Australian artist John Peter Russell?", "answer": "Van Gogh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Van Gogh"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "The British-Australian artist John Peter Russell", "extracted_entity": null}
{"question": "were the letters first annotated in 1913 by Theo's widow Johanna van Gogh-Bonger?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "was Kee seven years older than Van Gogh as well as had an eight-year-old son?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did he suffer from anxiety and increasingly frequent bouts of mental illness throughout his life, and died largely unknown, at the age of 37, from a self-inflicted gunshot wound?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "appreciated during his lifetime, did his fame grow in the years after his death?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is today, he widely regarded as one of history's greatest painters and an important contributor to the foundations of modern art?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "does a violin have four strings?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes, it has four strings.", "extracted_entity": null}
{"question": "is the violin supported by the left shoulder?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the violin shaped like an hourglass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what were strings first made out of?", "answer": "Sheep Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sheep Gut", "Sheep gut."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "gut", "extracted_entity": null}
{"question": "what is a violin called informally?", "answer": "Fiddle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fiddle", "A fiddle."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "fiddle", "extracted_entity": null}
{"question": "what does vitula mean?", "answer": "Stringed Instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stringed Instrument", "Stringed instrument."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is a person that makes or repairs violins called?", "answer": "Luthier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luthier", "A luthier, or simply a violin maker."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "luthier", "extracted_entity": null}
{"question": "what does the \"voice\" of a violin depend upon?", "answer": "Shape", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shape", "Its shape."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the vibrating length of the string.", "extracted_entity": null}
{"question": "is the violin a percussion instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "what happened in 1980s?", "answer": "The use of traditional (non-synthesized) orchestras declined.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The use of traditional (non-synthesized) orchestras declined."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The 1980s was a de", "extracted_entity": null}
{"question": "who features violinist Boyd Tinsley?", "answer": "Dave Matthews Band features violinist Boyd Tinsley.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dave Matthews Band features violinist Boyd Tinsley."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "regarding this topic, what did Antonio Stradivari do?", "answer": "Antonio Stradivari made violins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antonio Stradivari made violins."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "when did Charles IX order Amati to construct 24 violins for him?", "answer": "1560", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1560"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1572", "extracted_entity": null}
{"question": "is a string first tuned to a standard pitch?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the answer is yes", "extracted_entity": null}
{"question": "is each node at an integer division of the string?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "[1, 2, 3,", "extracted_entity": null}
{"question": "is the tuning G-D-A-E used for most violin music?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "G-D-A-E", "extracted_entity": null}
{"question": "is it the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello?", "answer": "Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Violin", "extracted_entity": null}
{"question": "is a violin sometimes informally called a fiddle, regardless of the type of music played on it?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "does the word ``violin'' come from the Middle Latin word vitula, meaning ``stringed instrument''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is the xylophone a musical instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone", "extracted_entity": null}
{"question": "was the xylophone associated with the folk music of the United States by the 19th century?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone", "extracted_entity": null}
{"question": "was it likely that the xylophone reached Europe during the Crusades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "the xylophone is an instrument from what family?", "answer": "the percussion family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the percussion family", "Percussion"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "percussion", "extracted_entity": null}
{"question": "how do the Java and Bali use xylophones?", "answer": "In gamelan ensembles", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In gamelan ensembles"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Java uses xylophones in their gam", "extracted_entity": null}
{"question": "some versions of xylophones have resonators made of what?", "answer": "gourds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["gourds", "Gourds."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "metal", "extracted_entity": null}
{"question": "which came first, the xylophone or the vibraphone?", "answer": "xylophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["xylophone", "The xylophone."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The vibraphone.", "extracted_entity": null}
{"question": "did the golden age of xylophones come before or after the first usage of the European-derived orchestral?", "answer": "after", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["after", "After."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "xylophone", "extracted_entity": null}
{"question": "does kelon increase or decrease tone quality of the xylophone?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Decrease."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "kelon decreases tone quality of the x", "extracted_entity": null}
{"question": "do the different species of zebras interbreed?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do zebras sleep standing up?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Zebras do not sleep standing up. They", "extracted_entity": null}
{"question": "have plains zebras been crossed with mountain zebras?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "how many species of zebra are there?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three", "Three"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what do zebras eat?", "answer": "mainly grass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly grass", "Grasses, shrubs, herbs, twigs, leaves, and bark"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "grass", "extracted_entity": null}
{"question": "what are zebras hunted for?", "answer": "mainly for their skins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly for their skins", "Skins"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "The zebra is hunted for its meat", "extracted_entity": null}
{"question": "what areas do the Grevy's Zebras inhabit?", "answer": "semi-arid grasslands of Ethiopia and northern Kenya", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["semi-arid grasslands of Ethiopia and northern Kenya", ""], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "[Grevy's Zebras](https", "extracted_entity": null}
{"question": "which species of zebra is known as the common zebra?", "answer": "Plains Zebra (Equus quagga, formerly Equus burchelli)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plains Zebra (Equus quagga, formerly Equus burchelli)", "Plains Zebra"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Equus quagga", "extracted_entity": null}
{"question": "at what age can a zebra breed?", "answer": "five or six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["five or six", "5 or 6"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "5 years old", "extracted_entity": null}
