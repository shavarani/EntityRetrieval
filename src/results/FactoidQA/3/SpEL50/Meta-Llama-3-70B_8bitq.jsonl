{"question": "was Abraham Lincoln the sixteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, Abraham Lincoln was the sixteenth President of", "extracted_entity": "Abraham_Lincoln;President_of_the_United_States"}
{"question": "did Lincoln sign the National Banking Act of 1863?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "National_Bank_Act"}
{"question": "did his mother die of pneumonia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pneumonia"}
{"question": "how many long was Lincoln's formal education?", "answer": "18 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["18 months", "18 months."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln's formal education was Q: What was", "extracted_entity": null}
{"question": "when did Lincoln begin his political career?", "answer": "1832", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1832", "1832."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "He was elected to the Illinois legislature in.", "extracted_entity": null}
{"question": "what did The Legal Tender Act of 1862 establish?", "answer": "the United States Note, the first paper currency in United States history", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United States Note, the first paper currency in United States history", "The United States Note, the first paper currency in United States history."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "It made gold coins the only form of money that", "extracted_entity": null}
{"question": "who suggested Lincoln grow a beard?", "answer": "11-year-old Grace Bedell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11-year-old Grace Bedell", "Grace Bedell."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Grace Bedell", "extracted_entity": "Abraham_Lincoln"}
{"question": "when did the Gettysburg address argue that America was born?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776", "1776."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did Lincoln beat John C. Breckinridge in the 1860 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Abraham_Lincoln;John_C._Breckinridge"}
{"question": "was Abraham Lincoln the first President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was the 16th President of the", "extracted_entity": "Abraham_Lincoln;President_of_the_United_States;United_States"}
{"question": "did Lincoln start his political career in 1832?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln began his political career in,1832,", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Lincoln ever represent Alton & Sangamon Railroad?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes. In,,,,,,", "extracted_entity": null}
{"question": "which county was Lincoln born in?", "answer": "Hardin County", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hardin County", "Southeast Hardin County, Kentucky"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Hardin County,Q: Where did Lincoln go", "extracted_entity": null}
{"question": "when did Lincoln first serve as President?", "answer": "March 4, 1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1861"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1861", "extracted_entity": "Abraham_Lincoln"}
{"question": "who assassinated Lincoln?", "answer": "John Wilkes Booth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Wilkes Booth"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "John Wilkes Booth", "extracted_entity": null}
{"question": "did Lincoln win the election of 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, he won the election of,1860", "extracted_entity": "Abraham_Lincoln"}
{"question": "who was the general in charge at the Battle of Antietam?", "answer": "General McClellan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General McClellan", "John Alexander McClernand"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Confederate Gen. Robert E. Lee's Army of", "extracted_entity": "Battle_of_Antietam"}
{"question": "why did Lincoln issue the Emancipation Proclamation?", "answer": "To free slaves ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To free slaves ", "The goal was to weaken the rebellion, which was led and controlled by slave owners."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "to help the Union win the war", "extracted_entity": "Abraham_Lincoln;Emancipation_Proclamation"}
{"question": "do scholars rank lincoln among the top three presidents?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No. Not even close. He's not even", "extracted_entity": null}
{"question": "did lincoln have 18 months of schooling?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No,A: No,A: No.", "extracted_entity": null}
{"question": "was Lincoln chosen as a presidential candidate in 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Abraham_Lincoln"}
{"question": "how old was Lincoln in 1816?", "answer": "seven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["seven"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was Q: what is the name", "extracted_entity": null}
{"question": "when was the first photgraph of lincoln taken?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Q: when was the first photgraph of", "extracted_entity": null}
{"question": "how long was Lincoln's legal Career?", "answer": "23 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["23 years"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln's legal career lasted for twenty-four years.", "extracted_entity": null}
{"question": "what trail did Lincoln use a Farmers' Almanac in? ?", "answer": "he defended William \"Duff\" Armstrong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he defended William \"Duff\" Armstrong"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln used a Farmers' Almanac to predict the", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Abraham Lincoln live in the Frontier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, he lived in a log cabin.", "extracted_entity": "Abraham_Lincoln"}
{"question": "did Lincoln's Wife's Family support slavery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes. Mary Todd's family owned slaves,Q", "extracted_entity": null}
{"question": "who is most noted for his contributions to the theory of molarity and molecular weight?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Svante Arrhenius", "extracted_entity": null}
{"question": "who graduated in ecclesiastical law at the early age of 20 and began to practice?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro", "blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The first person to graduate in ecclesiastical", "extracted_entity": null}
{"question": "when did he publish another memoria?", "answer": "1821", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1821"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "In QAnswer QAnswer QAnswer", "extracted_entity": null}
{"question": "when did he become a professor?", "answer": "1820", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1820"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is it true that he became a professor in 1820?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Was he a professor in", "extracted_entity": null}
{"question": "was Lorenzo Romano Amedeo Carlo Avogadro an Italian savant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, Lorenzo Romano Amedeo Carlo Av", "extracted_entity": "Italy"}
{"question": "was Amedeo Avogadro born in Turin August 9th 1776 to a noble ancient family of Piedmont, Italy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Turin;Piedmont;Italy"}
{"question": "is he most noted for his contributions to the theory of molarity and molecular weight?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The answer is no. He is most noted for", "extracted_entity": null}
{"question": "was King Victor Emmanuel III there to pay homage to Avogadro ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Victor_Emmanuel_III_of_Italy"}
{"question": "is Avogadro 's number commonly used to compute the results of chemical reactions ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Q) Which of the following is not", "extracted_entity": null}
{"question": "did the scientific community not reserve great attention to his theory ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Did the scientific community not reserve", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", ""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The title of this famous 1811 paper can", "extracted_entity": null}
{"question": "what happened in french?", "answer": "", "dataset": "factoid_qa", "split": "train", "answer_aliases": [""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The French language developed from a Gallo-Roman", "extracted_entity": "French_language"}
{"question": "what happened in 1833?", "answer": "Avogadro had been recalled to Turin university", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro had been recalled to Turin university", "blah blah blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the first election of the first president of the united", "extracted_entity": null}
{"question": "who determined the dependence of the boiling of water with atmospheric pressure?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Antoine Lavoisier", "extracted_entity": null}
{"question": "what is named after him?", "answer": "The Celsius crater on the Moon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Celsius crater on the Moon"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The famous \"I Have a Dream\" speech.", "extracted_entity": null}
{"question": "when did he publish a collection?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Q: where did he publish a collection?", "extracted_entity": null}
{"question": "is it true that he published a collection in 1738?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No,Q: Is it true that he published", "extracted_entity": null}
{"question": "is it true that thermometer had 100 for the freezing point?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why is it true that thermometer", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Uppsala;Sweden"}
{"question": "was Anders Celsius (November 27, 1701 April 25, 1744) a Swedish astronomer?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, he was a Swedish astronomer.", "extracted_entity": "Anders_Celsius;Sweden"}
{"question": "is The Celsius crater on the Moon named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius", "extracted_entity": "Moon"}
{"question": "who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Regnault", "extracted_entity": null}
{"question": "the Celsius crater on the Moon is what?", "answer": "named after him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["named after him"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "a crater", "extracted_entity": "Moon"}
{"question": "is the Celsius crater on the Moon named after him ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": "Moon"}
{"question": "had his thermometer 100 for the freezing point of water and 0 for the boiling point ?", "answer": "Yes it had", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes it had"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The freezing point of water is100 and the boiling", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "es, Celsius was born in Uppsala in", "extracted_entity": "Uppsala;Sweden"}
{"question": "did he not determine the dependence of the boiling of water with atmospheric pressure -LRB- in excellent agreement with modern data -RRB- ?", "answer": "Yes he did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he did"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "what happened from 1730 to 1744?", "answer": "He was professor of astronomy at Uppsala University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was professor of astronomy at Uppsala University"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The War of the Austrian Succession was a conflict", "extracted_entity": null}
{"question": "what happened in 1745?", "answer": "The scale was reversed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The British captured the French fortress of Louisbourg.", "extracted_entity": null}
{"question": "are beetles insects?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: beetles are insects.", "extracted_entity": null}
{"question": "can beetles be found in polar regions?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do beetles prefer warm climates", "extracted_entity": null}
{"question": "do beetles antennae function primarily as organs of smell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "No,Answer these questions", "extracted_entity": null}
{"question": "what are the three sections of a beetle?", "answer": "the head, the thorax, and the abdomen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the head, the thorax, and the abdomen", "The head, the thorax, and the abdomen"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "the three sections of a beetle are the head,", "extracted_entity": "Beetle"}
{"question": "which defense mechanism uses colour or shape to deceive potential enemies?", "answer": "mimicry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mimicry", "Mimicry"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "camouflage", "extracted_entity": null}
{"question": "which type of beetle is a pest of potato plants?", "answer": "Colorado potato beetle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colorado potato beetle"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The Colorado potato beetle is a pest of potato plants", "extracted_entity": "Beetle"}
{"question": "how can beetle larvae be differentiated from other insect larvae?", "answer": "their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "By their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "they can be differentiated by their color,Q:", "extracted_entity": "Beetle"}
{"question": "what do beetles eat?", "answer": "Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "The beetles are herbivores,,", "extracted_entity": null}
{"question": "what are the similarities between beetles and grasshoppers?", "answer": "mouthparts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mouthparts", "Beetles have mouthparts similar to those of grasshoppers"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "both beetles and grasshoppers are insects that", "extracted_entity": "Beetle;Grasshopper"}
{"question": "are certain species of beetles considered pests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes. Some species of beetles,are pests", "extracted_entity": null}
{"question": "is a beetle's general anatomy uniform?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no, the beetle's general anatomy is not uniform", "extracted_entity": "Beetle"}
{"question": "are beetles endopterygotes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do they have a larval", "extracted_entity": null}
{"question": "how many species of beetles are there?", "answer": "350,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["350,000", "about 350,000"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "'t know,Q: What is the most", "extracted_entity": null}
{"question": "when are sperm cells transferred to the female?", "answer": "during pairing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["during pairing", "During pairing"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "during copulation", "extracted_entity": null}
{"question": "what is the study of beetles called?", "answer": "coleopterology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["coleopterology"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Entomology", "extracted_entity": null}
{"question": "is it possible that there are more than 350,000 species of beetles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "No,Q: How many species of beetles", "extracted_entity": null}
{"question": "is the Adephaga suborder larger than the Polyphaga suborder?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, it is larger. It contains all the", "extracted_entity": "Polygonal_chain"}
{"question": "do carrion beetles eat dung?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "No,\u2019t eat dung.", "extracted_entity": null}
{"question": "what are prey of various animals including birds and mammals?", "answer": "Beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Predators of birds include hawks, owls", "extracted_entity": "Bird"}
{"question": "what was given by Aristotle for the hardened shield like forewings?", "answer": "The name \"Coleoptera\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name \"Coleoptera\"."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "the shield was given by Thessalos,", "extracted_entity": "Aristotle"}
{"question": "who or what vary greatly in form within the coleoptera?", "answer": "Antennae.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antennae."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Beetles", "extracted_entity": null}
{"question": "are many beetles territorial?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Beetles are very territorial. They use their mand", "extracted_entity": null}
{"question": "are beetles endopterygotes with complete metamorphosis?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What are the?", "extracted_entity": null}
{"question": "oxygen is what?", "answer": "One kind of gas obtained via a tracheal system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One kind of gas obtained via a tracheal system."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "a gas", "extracted_entity": null}
{"question": "is there a thriving industry in the collection of beetle specimens for amateur and professional collectors ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Beetles are a popular and thriving industry for amateur", "extracted_entity": "Beetle"}
{"question": "have coleopterists formed organisations to facilitate the study of beetles ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, There are several organisations that are involved in", "extracted_entity": "Beetle"}
{"question": "is the study of beetles called coleopterology , and its practitioners are coleopterists ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "o,Q2AnswerAnswer these questions,Q", "extracted_entity": null}
{"question": "may a single female lay from several dozen to several thousand eggs during her lifetime ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many eggs does a single", "extracted_entity": null}
{"question": "can beetles be found in almost all habitats?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the difference between a", "extracted_entity": null}
{"question": "what happened on plants and fungi?", "answer": "They are food to beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are food to beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Plants and fungi were both destroyed in the fire.", "extracted_entity": null}
{"question": "was Calvin Coolidge the twenty-ninth vice president?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Calvin_Coolidge"}
{"question": "was Calvin Coolidge born in Plymouth, Windsor County, Vermont?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, Calvin Coolidge was born in Plymouth,", "extracted_entity": "Calvin_Coolidge;Plymouth,_Vermont;Windsor_County,_Vermont"}
{"question": "did Coolidge graduate from Black River Academy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Calvin_Coolidge"}
{"question": "when did he drop John from his name?", "answer": "upon graduating from college", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon graduating from college", "Upon graduating from college"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "In Q: What was his last name before", "extracted_entity": null}
{"question": "when did Coolidge meet and marry Grace Anna Goodhue?", "answer": "1905", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1905", "In 1905"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge met Grace Anna Goodhue while studying at", "extracted_entity": "Calvin_Coolidge"}
{"question": "what period of rapid economic growth did the United States experience during Coolidge's presidency?", "answer": "the Roaring Twenties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Roaring Twenties", "the \"Roaring Twenties\""], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "United States experienced rapid economic growth during Coolidge", "extracted_entity": "United_States"}
{"question": "what did Coolidge do after graduating from Amherst?", "answer": "Coolidge moved to Northampton, Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge moved to Northampton, Massachusetts", "Coolidge moved to Northampton, Massachusetts to take up the practice of law"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge moved to Northampton, Massachusetts to be", "extracted_entity": "Calvin_Coolidge;Amherst_College"}
{"question": "of what state was Coolidge governor?", "answer": "Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Massachusetts"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Vermont", "extracted_entity": "Calvin_Coolidge"}
{"question": "who appointed Harlan Fiske Stone to the Supreme Court?", "answer": "Coolidge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "President Calvin Coolidge", "extracted_entity": "Harlan_F._Stone"}
{"question": "was Coolidge the thirteenth President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, he was the thirteenth President of", "extracted_entity": "Calvin_Coolidge;President_of_the_United_States"}
{"question": "was Calvin Coolidge Republican?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge was a Republican lawyer from New England", "extracted_entity": "Calvin_Coolidge;Republican_Party_(United_States)"}
{"question": "was Calvin Coolidge a governor of Massachusetts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes, Calvin Coolidge was a governor of Massachusetts", "extracted_entity": "Calvin_Coolidge;Massachusetts"}
{"question": "when was Coolidge born?", "answer": "July 4 1872", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4 1872", "Plymouth, Windsor County, Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1923", "extracted_entity": "Calvin_Coolidge"}
{"question": "where did Coolidge's grandfather had government offices?", "answer": "Plymouth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth", "The town of Plymouth Notch"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He was the president of the United States.", "extracted_entity": null}
{"question": "which state were Coolidge born in?", "answer": "Plymouth, Windsor County, Vermont", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth, Windsor County, Vermont", "Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Massachusetts", "extracted_entity": null}
{"question": "is Calvin Jr. older than John Coolidge?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the name of Calvin", "extracted_entity": null}
{"question": "did Coolidge get in Amherst College?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Calvin_Coolidge;Amherst_College"}
{"question": "what caused Calvin Jr.'s death?", "answer": "heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["heart attack", "Heart attack"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He was hit by a baseball bat and died.", "extracted_entity": null}
{"question": "was John Calvin Coolidge Jr. was born in Las Vegas?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "False", "extracted_entity": "Las_Vegas,_Nevada"}
{"question": "was Coolidge opposed in the Republican nomination for Governor of Massachusetts in 1918?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Calvin_Coolidge;Governor_of_Massachusetts"}
{"question": "did Coolidge meet and marry Grace Anna Goodhue?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Calvin_Coolidge"}
{"question": "what year did Coolidge open his own law office?", "answer": "1898", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1898"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1878", "extracted_entity": "Calvin_Coolidge"}
{"question": "what fraternity was Coolidge a member of?", "answer": "Phi Gamma Delta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Phi Gamma Delta", "phi gamma delta"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Alpha Delta Phi", "extracted_entity": null}
{"question": "in 1905 Coolidge met and married whom?", "answer": "Grace Anna Goodhue", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grace Anna Goodhue"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Anna Goodhue", "extracted_entity": "Calvin_Coolidge"}
{"question": "why did Coolidge not attend law school?", "answer": "It was too expensive", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was too expensive", "cost"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Coolidge did not attend law school because he did", "extracted_entity": "Calvin_Coolidge"}
{"question": "have there been other US presidents that have visited Cuba?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No, President Obama was the first US president to", "extracted_entity": "United_States;Cuba"}
{"question": "what years were Coolidge's two sons born in?", "answer": "1906 and 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1906 and 1908"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Calvin Coolidge,s sons were born in", "extracted_entity": null}
{"question": "did France cede nearly all of its colonies in Europe in 1763?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "France;Europe"}
{"question": "is Canada a member of the OECD?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada;Organisation_for_Economic_Co-operation_and_Development"}
{"question": "is Canada's official language Zulu?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Zulu_language"}
{"question": "what is one significant non-official language?", "answer": "Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese.", "Chinese"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "in addition to Port Royal, where else did Samuel de Champlain establish a settlement?", "answer": "Quebec City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec City"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Quebec and New France", "extracted_entity": "Port_Royal;Samuel_de_Champlain"}
{"question": "what was the Consitution Act formerly called?", "answer": "The British North America Act.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British North America Act.", "British North America Act"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Constitution Act was formerly called the Government of India", "extracted_entity": null}
{"question": "which countries established colonies in Canada?", "answer": "France and Britain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France and Britain.", "Fance and Britain"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Great Britain; France", "extracted_entity": "Canada"}
{"question": "how has Canada helped UN peacekeeping efforts?", "answer": "During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "Canada has played a leading role in UN peacekeeping efforts. During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada has participated in UN peacekeeping missions", "extracted_entity": "Canada"}
{"question": "what percentage of the population don't speak English or French at home?", "answer": "28%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28%", "1.5%"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "0.8%", "extracted_entity": "English_language;French_language"}
{"question": "is Canada bilingual?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, it is."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Canada"}
{"question": "did Canadian soldiers win the Battle of Vimy Ridge in 1917?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No, it did not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": "Canada;Battle_of_Vimy_Ridge"}
{"question": "is an official language of Canada German?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, it is not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Canada"}
{"question": "how many provinces and territories does Canada have?", "answer": "A federation now comprising ten provinces and three territories", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A federation now comprising ten provinces and three territories", "Ten provinces and three territories"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "10 provinces and 3 territories", "extracted_entity": "Canada"}
{"question": "what is Canada's national unemployment rate?", "answer": "While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "In October 2007, Canada's national unemployment rate is 5.9%."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The national unemployment rate is the percentage of the labor", "extracted_entity": null}
{"question": "where is the most densely populated part of Canada?", "answer": "The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Vancouver", "extracted_entity": "Canada"}
{"question": "what is the largest country in the world?", "answer": "Canada is the second largest country in the world, after Russia, and largest on the continent.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada is the second largest country in the world, after Russia, and largest on the continent.", "Russia."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": null}
{"question": "what is the largest religious group in Canada?", "answer": "According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "Christian"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Christians", "extracted_entity": "Canada"}
{"question": "when did Canada have one of the largest armed forces in the world?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "World War II."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1945", "extracted_entity": "Canada"}
{"question": "what have been inhabited for millennia by aboriginal peoples?", "answer": "The lands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lands"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The area now known as Canada", "extracted_entity": "Aboriginal_peoples_in_Canada"}
{"question": "european books and maps began referring to this region as Canada in what year?", "answer": "1545", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1545"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1545", "extracted_entity": "Europe;Canada"}
{"question": "what was Canada's most important industry until the 1800s?", "answer": "The fur trade", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The fur trade"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Fur trapping", "extracted_entity": null}
{"question": "is it true that it angered many residents of the thirteen colonies?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "how do cuba and declining participation relate?", "answer": "Canada maintains full relations with both.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada maintains full relations with both."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Cuba's participation in the international system is declining because", "extracted_entity": null}
{"question": "is it true that provinces have a large degree of autonomy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the difference between a", "extracted_entity": null}
{"question": "what did Canada adopt in 1965?", "answer": "The Maple Leaf Flag", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Maple Leaf Flag"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the maple leaf flag", "extracted_entity": "Canada"}
{"question": "what arrived in Britain?", "answer": "The first Canadian Army units in WW II", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first Canadian Army units in WW II"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Romans", "extracted_entity": "United_Kingdom"}
{"question": "are Canada 's two official languages English and French ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "anada's two official languages are English and French", "extracted_entity": "Canada;English_language;French_language"}
{"question": "are Canada 's official national sports ice hockey ( winter ) and lacrosse ( summer ) ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "rue", "extracted_entity": "Canada;National_sport;Ice_hockey;Lacrosse"}
{"question": "is Canada a geographically vast and ethnically diverse country ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada"}
{"question": "did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ?", "answer": "no!", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no!"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, it did", "extracted_entity": "Nova_Scotia_peninsula;Treaty_of_Utrecht"}
{"question": "did Cartier not use the word ` Canada ' to refer to not only that village , but the entire area subject to Donnacona , Chief at Stadacona ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "tier used the word ` Canada '", "extracted_entity": "Jacques_Cartier;Canada;Stadacona"}
{"question": "did continental European immigrants not settle the prairies ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes,Q: did continental European immigrants not settle", "extracted_entity": null}
{"question": "is it the world 's second largest country by total area?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "List_of_sovereign_states_and_dependencies_by_area"}
{"question": "what happened in 1867?", "answer": "Canada was formed as a federal, semi-autonomous polity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada was formed as a federal, semi-autonomous polity"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "In Q: what happened in Q:", "extracted_entity": null}
{"question": "are ducks in the Arctic Northern Hemisphere migratory?", "answer": "Some are", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes, they are. In fact, they are", "extracted_entity": "Northern_Hemisphere"}
{"question": "is a drake a male?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is a drake a female", "extracted_entity": null}
{"question": "do all ducks \"quack\"?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "No. The ducks that live in the water make", "extracted_entity": null}
{"question": "what is the name of a specialized species of duck adapted to catch large fish?", "answer": "the smew", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the smew"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "the fishing duck", "extracted_entity": null}
{"question": "what animal attracts the most humor and silliness?", "answer": "The duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The duck", "the duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The cat.", "extracted_entity": null}
{"question": "what is an economic use of a duck?", "answer": "Meat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat", "being farmed for their meat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "An economic use of a duck is the most effective", "extracted_entity": null}
{"question": "who is Daffy Duck?", "answer": "A silly cartoon character", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A silly cartoon character", "a silly cartoon character"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Daffy Duck is a fictional character created by", "extracted_entity": "Daffy_Duck"}
{"question": "what makes it more difficult for a diving duck to fly?", "answer": "They are heavier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are heavier", "they are heavier"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The water on their feathers makes it more difficult for", "extracted_entity": null}
{"question": "what are the reasons for hunting wild ducks?", "answer": "Meat, eggs, and feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Hunting wild ducks is a popular sport in the United", "extracted_entity": null}
{"question": "do ducks forage underwater?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Diving ducks and sea ducks forage underwater"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes, the duckbill is designed to be able", "extracted_entity": null}
{"question": "is foie gras often made from ducks?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is foie gras made from", "extracted_entity": null}
{"question": "what economic uses to ducks have?", "answer": "They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "They can be farmed for meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Ducks are raised for eggs,Q: How does", "extracted_entity": null}
{"question": "what kind of ducks feed on land?", "answer": "Dabbling ducks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dabbling ducks", "Dabbling ducks feed on land"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Mallards,Q: what kind of ducks feed", "extracted_entity": null}
{"question": "what unrelated water birds are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Coots,Q: How many legs do", "extracted_entity": null}
{"question": "what does the word duck mean?", "answer": "It is the common name for a number of species in the Anatidae family of birds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the common name for a number of species in the Anatidae family of birds.", "to bend down low as if to get under something"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Duck means a bird that lives in the water.", "extracted_entity": null}
{"question": "what are some common predators of ducks?", "answer": "Pike, crocodilians, herons, hawks and eagles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pike, crocodilians, herons, hawks and eagles.", "pike, crocodilians, and other aquatic hunters"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Common predators of ducks include foxes,Q2", "extracted_entity": null}
{"question": "how does a Mallard's tongue work?", "answer": "It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "It uses short spikes to push struggling prey and other food down its throat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The Mallard's tongue is a highly complex organ", "extracted_entity": null}
{"question": "are diving ducks heavier tha dabbling ducks?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Diving ducks are heavier than dabbling ducks.", "extracted_entity": null}
{"question": "are adult ducks fast fliers?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Can ducks fly at night?", "extracted_entity": null}
{"question": "are ducks an accepted presence in some populated areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do ducks get along with other", "extracted_entity": null}
{"question": "what types of unrelated water birds with similar forms are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Coots,Q: what types of unrelated", "extracted_entity": null}
{"question": "why are ducklings particularly vulnerable?", "answer": "Their inability to fly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their inability to fly.", "their inability to fly makes them easy prey"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Ducklings are particularly vulnerable because they are small,", "extracted_entity": null}
{"question": "what are some economic uses for duck?", "answer": "Meat,eggs,feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat,eggs,feathers", "being farmed for their meat, eggs, feathers, (particularly their down)"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Duck is a waterfowl. Duck is raised", "extracted_entity": null}
{"question": "what allows a duck to filter water out of the side of their beaks and keep food inside?", "answer": "Tiny rows of plates called lamellae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tiny rows of plates called lamellae", "tiny rows of plates called lamellae like a whale's baleen"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The beak has a structure that holds the food", "extracted_entity": null}
{"question": "what expression is part of a conceptual framework for testing computer systems?", "answer": "Quacks like a duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quacks like a duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "test oracle,Q: what is the name of", "extracted_entity": null}
{"question": "where did the expression \"a sitting duck\" come from?", "answer": "In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "In many areas, wild ducks of various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys."], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The expression \u201ca sitting duck\u201d refers to someone", "extracted_entity": null}
{"question": "is Egypt bordered by the Gaza Strip?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Egypt;Gaza_Strip"}
{"question": "is Egypt the most populated country in Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt;Africa"}
{"question": "does Egypt have political influence in the Middle East?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Egypt;Middle_East"}
{"question": "when was the Six Day War?", "answer": "1967", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1967"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "the Six-Day War (Hebrew:,", "extracted_entity": "Six-Day_War"}
{"question": "what religions has Egypt outlawed?", "answer": "All but Christianity, Islam, and Judaism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["All but Christianity, Islam, and Judaism", "all religions and belief except Islam, Christianity and Judaism"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "atheism, because they believe that there is no", "extracted_entity": "Egypt"}
{"question": "what is the poulation of Egypt?", "answer": "more than 78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["more than 78 million", "78 million people"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "Egypt"}
{"question": "why does most of Egypt's population live near the Nile?", "answer": "the only arable agricultural land is found there,", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the only arable agricultural land is found there,", "The only arable agricultural land is found there"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The Nile is the only source of water in the", "extracted_entity": "Egypt;Nile"}
{"question": "are there a large number of Jews living in Egypt today?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "no, the Jewish community in Egypt is small.", "extracted_entity": "Egypt"}
{"question": "during what period was Akhenaten a Pharaoh?", "answer": "The New Kingdom (c.1550\u22121070 BC) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The New Kingdom (c.1550\u22121070 BC) ", "The New Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Akhenaten was an ancient Egyptian pharaoh reigning", "extracted_entity": "Akhenaten"}
{"question": "is Egypt in Asia?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, it is in Africa", "extracted_entity": "Egypt;Asia"}
{"question": "does Egypt receive the least rainfall in the world?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt is the driest country in the world,", "extracted_entity": "Egypt"}
{"question": "does Egypt's foreign policy operates along moderate lines?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Egypt's foreign policy operates along moderate lines.", "extracted_entity": "Egypt"}
{"question": "since when has Egypt been a republic?", "answer": "June 18 1953.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 18 1953.", "June 18 1953"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "2019", "extracted_entity": "Egypt"}
{"question": "when were the Great Sphinx and the Pyramids of Giza built?", "answer": "During the Old Kingdom.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Old Kingdom.", "During the Old Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The Great Sphinx and the Pyramids of Giza", "extracted_entity": "Great_Sphinx_of_Giza;Giza_pyramid_complex"}
{"question": "what countries border Egypt?", "answer": "Libya, Sudan, the Gaza Strip and Israel.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Libya, Sudan, the Gaza Strip and Israel.", "Libya, Sudan, the Gaza Strip and Israel"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Palestine, Israel, Libya, Sudan, Mediterranean Sea", "extracted_entity": "Egypt"}
{"question": "what is the estimated population of Egypt?", "answer": "More than 78 million.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 78 million.", "78 million"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1, 2, 3,", "extracted_entity": "Egypt"}
{"question": "does snow fall in Egypt?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, snow doesn't fall in Egypt.", "extracted_entity": "Egypt"}
{"question": "when did organized agriculture appear in the Nile Valley?", "answer": "6000 BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6000 BC.", "About 6000 BC"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "the  the earliest evidence of organized agriculture in", "extracted_entity": "Nile"}
{"question": "are elephants the largest land animals alive today?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No,Q: How many teeth does an elephant", "extracted_entity": null}
{"question": "can an elephant kill a rhinoceros?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "an elephant can kill a rhinoceros", "extracted_entity": "Rhinoceros"}
{"question": "are elephants good swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Elephants are very good swimmers. They can", "extracted_entity": null}
{"question": "how many living species of African Elephants are there?", "answer": "2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2", "The African Elephant genus contains two (or, arguably, three) living species."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "2", "extracted_entity": "African_elephant"}
{"question": "how long may elephants live?", "answer": "70 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70 years", "As long as 70 years, sometimes longer."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Elephants may live to be more than Q", "extracted_entity": null}
{"question": "why does the phrase 'elephants never forget' have no metaphorical meaning?", "answer": "it refers literally to elephants supposedly having an excellent memory", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it refers literally to elephants supposedly having an excellent memory", "Because it refers literally to elephants supposedly having an excellent memory."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "This phrase is a metaphorical expression for \u2019to", "extracted_entity": null}
{"question": "how many living species of Asian Elephants are there?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1", "one"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The Asian elephant (Elephas maximus), also", "extracted_entity": "Asian_elephant"}
{"question": "how much do elephants weight at birth?", "answer": "120 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["120 kilograms", "At birth it is common for an elephant calf to weigh 120 kilograms (265 lb)."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The average weight of an elephant at birth is", "extracted_entity": null}
{"question": "what did Aristotle say about elephants?", "answer": "the beast which passeth all others in wit and mind", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the beast which passeth all others in wit and mind", "Aristotle once said the elephant was \"the beast which passeth all others in wit and mind.\""], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Aristotle said that elephants were intelligent and had excellent memories", "extracted_entity": "Aristotle"}
{"question": "are elephant populations in West Africa generally small and fragmented?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Elephant populations in West Africa are generally small and fragmented", "extracted_entity": null}
{"question": "is the Asian elephant larger than the African?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Sometimes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Asian_elephant;Africa"}
{"question": "is an elephant's skin tough?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many eyes does an elephant", "extracted_entity": null}
{"question": "how do elephants communicate over long distances?", "answer": "by producing and receiving low frequency sound", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by producing and receiving low frequency sound", "By producing and receiving low-frequency sound (infrasound)"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Elephants use low-frequency sounds to communicate over long", "extracted_entity": null}
{"question": "when do African elephants lie down?", "answer": "when they are sick or wounded", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when they are sick or wounded"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "African elephants lie down when they are sick or injured", "extracted_entity": "African_elephant"}
{"question": "how long is the elephant's gestation period?", "answer": "22 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["22 months"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "edwin m. (1895) wrote", "extracted_entity": null}
{"question": "how many species of African elephants have been proposed?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "text mentions that two species of African elephants have", "extracted_entity": "African_elephant"}
{"question": "what are the elephant's ears important for?", "answer": "temperature regulation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["temperature regulation"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The elephant's ears are important for its hearing.", "extracted_entity": null}
{"question": "what land animal has the largest brain?", "answer": "elephant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["elephant", "The elephant"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "A blue whale. The brain of a blue whale", "extracted_entity": null}
{"question": "are elephants mammals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, elephants are mammals. They are warm-blood", "extracted_entity": null}
{"question": "have elephants been used as working animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes. Elephants are used as working animals in", "extracted_entity": null}
{"question": "do they prefer forested areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do they have a distinctive call", "extracted_entity": null}
{"question": "what is the world population of Asian elephants?", "answer": "60000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60000", "The world population of Asian elephants \u2013 also called Indian Elephants or Elephas maximus \u2013 is estimated to be around 60,000"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "100", "extracted_entity": "Asia"}
{"question": "what part of an elephant is very important for temperature regulation?", "answer": "Ears", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ears", "The large flapping ears of an elephant are also very important for temperature regulation."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The trunk", "extracted_entity": null}
{"question": "where was the largest elephant ever recorded shot?", "answer": "Angola", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Angola", "The largest elephant ever recorded was shot in Angola in 1956."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Angola", "extracted_entity": null}
{"question": "is an Asian elephant smaller than an African elephant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, an Asian elephant is smaller than an African", "extracted_entity": "Asian_elephant;African_elephant"}
{"question": "what land animal is larger than an elephant?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "None, the elephant is the largest land animal."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "A mouse. It\u2019s so large that it can", "extracted_entity": null}
{"question": "has tusklessness become a rare abnormality?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, tusklessness has become a widespread hereditary trait. "], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No,Q: Is tusklessness a rare", "extracted_entity": null}
{"question": "were the treaties signed in 1947 and 1948 with the Ukraine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No, the treaties signed in 1947 and", "extracted_entity": "Ukraine"}
{"question": "is Finnish a member of the Baltic-Finnic subgroup of the Uralic languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": ", it is.", "extracted_entity": "Finnish_language;Uralic_languages"}
{"question": "is the most read newspaper in Finland Taloussanomat?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsingin Sanomat", "extracted_entity": "Finland"}
{"question": "which Russian army general conquered Finland in 1809?", "answer": "Alexander I", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alexander I"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The Russian army general who conquered Finland in 180", "extracted_entity": "Russia;Finland"}
{"question": "what is the life expectancy for men in Finland?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "78.3 years", "extracted_entity": "Finland"}
{"question": "in what year were the \"Games of the XV Olympiad\" held?", "answer": "1952", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1952"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Q: in what year were the \u201cGames", "extracted_entity": null}
{"question": "what body of water lies to the south of Finland?", "answer": "the Gulf of Finland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Gulf of Finland", "Archipelago Sea"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the Gulf of Finland", "extracted_entity": "Finland"}
{"question": "name an animal that is growing in number due to recent conservation efforts?", "answer": "Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "Golden Eagle"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the American bison", "extracted_entity": null}
{"question": "today, are there more cell phone subscriptions than people in Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, there are more cell phone subscriptions than people", "extracted_entity": "Finland"}
{"question": "is Matti Vanhanen the Prime Minister of Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Matti_Vanhanen;Prime_Minister_of_Finland"}
{"question": "is the capital city Oslo?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Oslo"}
{"question": "does it have a border with Norway?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Norway"}
{"question": "what is the life expectancy for men?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Q: what is the life expectancy for women", "extracted_entity": null}
{"question": "what is the biggest city in Finland?", "answer": "Greater Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greater Helsinki", "the cities of the Greater Helsinki metropolitan area"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki", "extracted_entity": "Finland"}
{"question": "who is the most popular rock group in Finland?", "answer": "CMX", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["CMX"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Nightwish", "extracted_entity": "Finland"}
{"question": "what is Finland's economy like?", "answer": "a highly industrialised, free-market economy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a highly industrialised, free-market economy"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland has a highly industrialized, free-market economy", "extracted_entity": "Finland"}
{"question": "where is Finland located?", "answer": "Northern Europe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Europe"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is located in Northern Europe.", "extracted_entity": "Finland"}
{"question": "what is the weather like in summer?", "answer": "relatively warm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["relatively warm"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "In summer,Q: what is the weather like", "extracted_entity": null}
{"question": "does salmon remain the favorite of fly rod enthusiasts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The Atlantic salmon is the favorite of fly rod enthusiasts", "extracted_entity": "Salmon"}
{"question": "are there cathedrals scattered all across Finland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, there are cathedrals scattered all", "extracted_entity": "Finland"}
{"question": "according to Reader's Digest, is Finland best for living?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, Finland is best for living.", "extracted_entity": "Reader's_Digest;Finland"}
{"question": "when did the first verifiable written documents appear?", "answer": "Twelfth Century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twelfth Century", " The first verifiable written documents appeared in the twelfth century. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Q: how did the first verifiable written", "extracted_entity": null}
{"question": "what is the capital city?", "answer": "Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helsinki", "The capital city is Helsinki. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "The capital city is called the capital city.", "extracted_entity": null}
{"question": "what is the most extensively celebrated holiday?", "answer": "Christmas", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas", "Christmas is the most extensively celebrated holiday."], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Christmas.", "extracted_entity": null}
{"question": "does a large proportion of the population speak Swedish as its mother tongue?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, about 9 million people do.", "extracted_entity": "Swedish_language"}
{"question": "do women live longer than men?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, women live longer than men.", "extracted_entity": null}
{"question": "what is a country with which Finland is involved in an international conflict?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "Finland is not involved in international conflicts or border disputes. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": "Finland"}
{"question": "was Ford a member of the House of Representatives?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he was a senator.", "extracted_entity": "United_States_House_of_Representatives"}
{"question": "for how long was Ford a member of the House of Representatives?", "answer": "Over eight years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over eight years.", "Ford served for over eight years as the Republican Minority Leader of the United States House of Representatives"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford served in the House of Representatives for 30", "extracted_entity": null}
{"question": "was Gerald Ford a member of Delta Kappa Epsilon?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Gerald Ford was not a member of Delta Kappa", "extracted_entity": "Gerald_Ford;Delta_Kappa_Epsilon"}
{"question": "did ford get an award called \"Congressman's congressman\"?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Who is the only man to", "extracted_entity": null}
{"question": "did ford attend the University of Michigan?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford attended the University of Michigan.", "extracted_entity": "University_of_Michigan"}
{"question": "what positions Ford played in the school football team?", "answer": "Center and linebacker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Center and linebacker", "Captain"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford was a center for the school team. He", "extracted_entity": null}
{"question": "what did Ford say about his biological father?", "answer": "He was abusive and had a history of hitting his mother.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was abusive and had a history of hitting his mother.", "his biological father was abusive and had a history of hitting his mother"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "He said that he had never met him and that", "extracted_entity": null}
{"question": "who did Ford nominate for Vice President?", "answer": "Bob Dole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bob Dole", "Nelson Rockefeller"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Nelson Rockefeller", "extracted_entity": null}
{"question": "was Ford active about Vietnamese affairs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "had Ford's wife married before?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Had she been married before?", "extracted_entity": null}
{"question": "is Ford related with the assassination of John F. Kennedy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, Ford is not related with the assassination of", "extracted_entity": "Assassination_of_John_F._Kennedy"}
{"question": "was Gerald Ford the 38th President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Gerald Ford", "extracted_entity": "Gerald_Ford;President_of_the_United_States"}
{"question": "did Gerald Ford serve as the Republican Minority Leader of the House of Representatives?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": "Gerald_Ford;Republican_Party_(United_States);United_States_House_of_Representatives"}
{"question": "was Ford an Eagle Scout?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No. Ford was a Boy Scout.", "extracted_entity": null}
{"question": "what district was Ford elected from?", "answer": "Michigan's 5th congressional district", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michigan's 5th congressional district"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford was elected from the Q: what district", "extracted_entity": null}
{"question": "how many Eagle Scouts were involved in Ford's funeral procession?", "answer": "400", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["400", "About 400"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "only  the Eagle Scouts", "extracted_entity": "Eagle_Scout_(Boy_Scouts_of_America)"}
{"question": "which future Heisman Trophy winner did Ford tackle?", "answer": "Jay Berwanger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jay Berwanger"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Jim Brown", "extracted_entity": "Heisman_Trophy"}
{"question": "has another US President been an Eagle Scout?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, only Gerald Ford has been an Eagle Scout", "extracted_entity": "United_States;President_of_the_United_States;Eagle_Scout"}
{"question": "did Ford need to do extra work to pay for college?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes. Ford worked as a waiter and a tutor", "extracted_entity": null}
{"question": "have more than five Presidents lived past the age of 90?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes. John Adams,Q: Who was the", "extracted_entity": null}
{"question": "how do kwajalein and eniwetok relate?", "answer": "The Monterey supported landings at both locations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monterey supported landings at both locations."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Kwajalein and eniwetok are", "extracted_entity": null}
{"question": "what did Ford receive on April 13, 1942?", "answer": "Ford received a commission as ensign in the US Naval Reserve.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ford received a commission as ensign in the US Naval Reserve."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "a patent on a method of making gasoline engines", "extracted_entity": "Henry_Ford"}
{"question": "was Ford released from the hospital?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No,Q: What was Ford's reaction to", "extracted_entity": null}
{"question": "was Ford interred at his Presidential Museum in Grand Rapids, Michigan?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he was interred at his Presidential Museum", "extracted_entity": "Henry_Ford;Gerald_R._Ford_Presidential_Museum;Grand_Rapids,_Michigan;Michigan"}
{"question": "is Ford one of only four former Presidents to live to 90 or more years of age ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes. Ford lived to  than,,", "extracted_entity": null}
{"question": "was Ford the last surviving member of the Warren Commission ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Warren_Commission"}
{"question": "did Grand Rapids supporters not urge him to take on Bartel J. Jonkman , the incumbent Republican congressman ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "did they not claim Ford 's pardon was quid pro quo in exchange for Nixon 's resignation ?", "answer": "They did claim Ford's pardon was quid pro quo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They did claim Ford's pardon was quid pro quo."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "s, they did not claim that", "extracted_entity": "Richard_Nixon"}
{"question": "does Ghana have twice the per capita output of the poorer countries in West Africa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ghana;West_Africa"}
{"question": "is English Ghana's official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do many people speak English in", "extracted_entity": null}
{"question": "did the Dutch build the Elmina Castle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Elmina Castle was erected by the Portuguese in", "extracted_entity": "Elmina_Castle"}
{"question": "when did Ghana achieve independence from the United Kingdom?", "answer": "1957", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1957"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Ghana achieved independence from the United Kingdom on 6", "extracted_entity": "Ghana;United_Kingdom"}
{"question": "how many regions is Ghana divided into?", "answer": "10", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Q: which region is the biggest region in", "extracted_entity": null}
{"question": "who is the head of state of Ghana?", "answer": "President John Agyekum Kofuor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["President John Agyekum Kofuor", "John Agyekum Kufuor"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the president", "extracted_entity": "Ghana"}
{"question": "what is the dominant religion in Ghana?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "christian"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": "Ghana"}
{"question": "what are the two subfamilies of the Native Ghanaian languages?", "answer": "Kwa and Gur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kwa and Gur"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The two subfamilies of the Native Ghanaian", "extracted_entity": null}
{"question": "what is the weather like at Lake Volta?", "answer": "Warm and comparatively dry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Warm and comparatively dry", "warm and comparatively dry"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The weather at Lake Volta is sunny.", "extracted_entity": null}
{"question": "is Ghana in Asia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No, it is not", "extracted_entity": "Asia"}
{"question": "is English the official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes, English is the official language of the United", "extracted_entity": "English_language"}
{"question": "is Ghana's head of state John Agyekum Kufuor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ghana;John_Kufuor"}
{"question": "what does the word Ghana mean?", "answer": "warrior king", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["warrior king", "Warrior King"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Ghana means \"Warrior King\" in the Son", "extracted_entity": "Ghana"}
{"question": "who was Kwame Nkrumah?", "answer": "founder and first president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["founder and first president", "founder and first president of the modern Ghanaian state"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Kwame Nkrumah was the first prime", "extracted_entity": "Kwame_Nkrumah"}
{"question": "where is old Ghana in relation to present Ghana?", "answer": "500 miles north", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500 miles north", "500 miles north of the present Ghana, and occupied the area between Rivers Senegal and Niger"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "old Ghana was in the sahel region", "extracted_entity": "Ghana"}
{"question": "who makes up Ghana's Parliament?", "answer": "the New Patriotic Party and National Democratic Congress", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the New Patriotic Party and National Democratic Congress"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "the Parliament of Ghana consists of the President of Ghana", "extracted_entity": "Ghana"}
{"question": "what European countries established states in Ghana?", "answer": "The UK", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The UK", "Portugual, France, Britain, Netherlands"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Britain", "extracted_entity": "Europe;Ghana"}
{"question": "does the Gray Wolf share an ancestry with the domestic dog?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes. The Gray Wolf is a direct ancestor of", "extracted_entity": null}
{"question": "do wolf pups tend to have darker fur than adults?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, They do tend to have darker furs"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do wolf pups tend to have", "extracted_entity": null}
{"question": "do wolves leave their pack?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "They leave their birth pack when they reach sexual maturity"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolves do not leave their pack. They are born", "extracted_entity": null}
{"question": "what kinds of coats do wolves have?", "answer": "bulky coats with two layers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bulky coats with two layers", "Tough guard hairs that repel water and dirt, and a dense, water-resistant layer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolves have two different kinds of coats. They have", "extracted_entity": null}
{"question": "where do wolves have scent glands?", "answer": "Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "between a wolf's toes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Wolves have scent glands on their tails,Q:", "extracted_entity": null}
{"question": "how much do wolves weigh?", "answer": "typically varies between 32 and 62 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["typically varies between 32 and 62 kilograms", "between 32 and 68 kilograms "], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "The average weight of a wolf is Q:", "extracted_entity": null}
{"question": "why do wolves howl?", "answer": "Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "To pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves howl to communicate with other wolves and to", "extracted_entity": null}
{"question": "what is surplus killing?", "answer": "Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "the killing of several prey animals too numerous to eat at one sitting"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "surplus killing is the killing of a prey that is", "extracted_entity": null}
{"question": "are Gray Wolves native to North America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No. Current theory suggests that it's from Eurasia"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Gray Wolves are native to North America", "extracted_entity": "North_America"}
{"question": "is the Gray Wolf a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Does the Gray Wolf have hair", "extracted_entity": null}
{"question": "are a wolf's teeth its main weapons?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes. The wolf has four canine teeth that are", "extracted_entity": null}
{"question": "are wolves built for stamina?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes. Wolves are built for stamina. They can", "extracted_entity": null}
{"question": "where is the largest gray wolf population thought to be found?", "answer": "Kazakhstan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kazakhstan"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Alaska", "extracted_entity": null}
{"question": "to what genus does the gray wolf belong?", "answer": "Canis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canis"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Canis", "extracted_entity": null}
{"question": "when do wolves molt?", "answer": "Late Spring or Early Summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Late Spring or Early Summer", "late spring or early summer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves molt in", "extracted_entity": "Gray_wolf"}
{"question": "why is it beneficial for alpha males and females to forcefully prevent other wolves from mating?", "answer": "A pack can only suppport one littler per year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A pack can only suppport one littler per year", "The pack tension rises as each mature wolf feels urged to mate"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "It is beneficial for alpha males and females to prevent", "extracted_entity": null}
{"question": "what type of tools do biologists use to capture wolves for tagging?", "answer": "Darting and Foot hold traps.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Darting and Foot hold traps.", "Darting and foot hold traps"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Biologists use leg hold traps and foot snares", "extracted_entity": null}
{"question": "forward, erect ears and slightly bristle hackles are a sign of what in wolves?", "answer": "Dominance", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dominance"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "submission", "extracted_entity": "Gray_wolf"}
{"question": "was Grover Cleveland born in New York?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Grover_Cleveland"}
{"question": "did Grover Cleveland win the 1884 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Grover_Cleveland"}
{"question": "did Grover Cleveland support women's suffrage?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No, he was against it", "extracted_entity": "Grover_Cleveland"}
{"question": "where was Grover Cleveland married?", "answer": "In the White House", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the White House", "in the Blue Room in the White House"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he was married in the White House", "extracted_entity": "Grover_Cleveland"}
{"question": "what did Cleveland die from?", "answer": "A heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A heart attack", "a heart attack"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Cleveland died from a heart attack.", "extracted_entity": "Cleveland"}
{"question": "how many years after Cleveland left office did the U.S. win the Spanish-American War?", "answer": "One Year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One Year", "one"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "14 years", "extracted_entity": "Grover_Cleveland;United_States;Spanish\u2013American_War"}
{"question": "what did Cleveland's opponents say in 1884 to counter his innocent image?", "answer": "That he had fathered an illegitimate child", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["That he had fathered an illegitimate child", "that he had fathered an illegitimate child while he was a lawyer in Buffalo"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "They said he was a drunkard,Q:", "extracted_entity": null}
{"question": "who did Cleveland run against in 1884?", "answer": "former Senator James G. Blaine of Maine", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["former Senator James G. Blaine of Maine", "James G. Blaine"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Cleveland was defeated by the Chicago White Stockings in", "extracted_entity": null}
{"question": "why did Cleveland want to hide his cancer surgery from the public?", "answer": "To avoid further market panic", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To avoid further market panic", "because of the financial depression of the country"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "He did not want to be seen as weak.", "extracted_entity": null}
{"question": "was Grover Cleveland the twenty-seventh president of the United States?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he was the twenty-seventh president of", "extracted_entity": "Grover_Cleveland;President_of_the_United_States;United_States"}
{"question": "is Grover Cleveland honest?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Grover_Cleveland"}
{"question": "was Grover Cleveland married in the whitehouse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he married Frances Folsom in the", "extracted_entity": "Grover_Cleveland"}
{"question": "when was he elected sheriff of Erire County, New York?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "1870"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "In 1921, Grover Cleveland, the", "extracted_entity": "Erie_County,_New_York"}
{"question": "when did he die?", "answer": "June 24 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 24 1908", "1908 ", "1908"], "entity_annotations": ["Henri_Becquerel", "Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "A.D. A: A.D", "extracted_entity": null}
{"question": "which election did Grover Cleveland win?", "answer": "1884 and 1892 presidential elections", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1884 and 1892 presidential elections"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1884", "extracted_entity": "Grover_Cleveland"}
{"question": "who were Grover Cleveland's parents?", "answer": "Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "Reverend Richard Cleveland and Anne Neal."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Grover Cleveland was born on March 18,", "extracted_entity": "Grover_Cleveland"}
{"question": "how many children did Grover Cleveland have?", "answer": "5", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5", "Six."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he had 5 children", "extracted_entity": "Grover_Cleveland"}
{"question": "who lost control of his party to the agrarians and silverites in 1896?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "William Jennings Bryan", "extracted_entity": null}
{"question": "is it true that he sent in federal troops to chicago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Chicago"}
{"question": "was Cleveland born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Caldwell,_New_Jersey"}
{"question": "was Grover Cleveland elected Sheriff of Erie County, New York?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "No, he was elected sheriff of Eire County", "extracted_entity": "Grover_Cleveland;Erie_County,_New_York;New_York"}
{"question": "was Cleveland 's portrait on the U.S. $ 1000 bill from 1928 to 1946 ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "es, it was.", "extracted_entity": "Grover_Cleveland;United_States"}
{"question": "is he buried in the Princeton Cemetery of the Nassau Presbyterian Church ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Princeton_Cemetery"}
{"question": "did the Department of the Interior not charge that the rights of way for this land must be returned to the public because the railroads failed to extend their lines according to agreements ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes, the Department of the Interior did charge that", "extracted_entity": "United_States_Department_of_the_Interior"}
{"question": "did the team , sedating Cleveland with nitrous oxide -LRB- laughing gas -RRB- , not remove his upper left jaw and portions of his hard palate ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "not remove his upper left jaw", "extracted_entity": "Cleveland;Nitrous_oxide"}
{"question": "he became chief engineer in the Department of Bridges and Highways in what year?", "answer": "in 1894", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 1894"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": ":  ::  ::  :: :: ::", "extracted_entity": null}
{"question": "what led Becquerel to investigate the spontaneous emission of nuclear radiation?", "answer": "phographic plates being fully exposed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["phographic plates being fully exposed"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "The Curies' experiments with bombarding uranium with", "extracted_entity": null}
{"question": "is it true that he married louise d\u00e9sir\u00e9e lorieux in 1890?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No,Q: What was the name of his", "extracted_entity": null}
{"question": "is it true that he shared the nobel prize in physics?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What was his contribution to science", "extracted_entity": null}
{"question": "when did he marry louise d\u00e9sir\u00e9e lorieux?", "answer": "1890", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1890"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Q: how did he marry louise d\u00e9s", "extracted_entity": null}
{"question": "was Becquerel elected Permanent Secretary of the Acad\u00e9mie des Sciences?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, in 1822", "extracted_entity": "Becquerel;French_Academy_of_Sciences"}
{"question": "was Becquerel born in Paris into a family which, including he and his son Jean, produced four generations of scientists?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he was born in a family which,", "extracted_entity": "Paris"}
{"question": "who won the 1903 Nobel Prize in Physics for discovering radioactivity ?", "answer": "Henri Becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel", "extracted_entity": "Nobel_Prize_in_Physics"}
{"question": "did he not die at the age of 55 in Le Croisic ?", "answer": "Yes he died at the age of 55", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he died at the age of 55"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he died at the age of 55", "extracted_entity": "Le_Croisic"}
{"question": "did one places between the phosphorescent substance and the paper a piece of money or a metal screen not pierce with a cut-out design ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "It is a metal screen with a cut-out design", "extracted_entity": null}
{"question": "what happened in 1896?", "answer": " Becquerel accidentally discovered radioactivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Becquerel accidentally discovered radioactivity."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "The,,,,,", "extracted_entity": null}
{"question": "what happened with a bromide emulsion in two sheets of very thick black paper?", "answer": "the plate does not become clouded upon being exposed to the sun for a day", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the plate does not become clouded upon being exposed to the sun for a day"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "It was a bromide emulsion in two sheets", "extracted_entity": null}
{"question": "is there a becquerel crater on the moon and a becquerel crater on mars?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the difference between a", "extracted_entity": null}
{"question": "are the Javanese the largest and politically dominant ethnic group in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "es, they are the largest and politically dominant ethnic", "extracted_entity": "Javanese_people;Indonesia"}
{"question": "is Indonesia a monarchy with a presidential system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Indonesia"}
{"question": "did the Japanese invasion and subsequent occupation during WWII end Dutch rule?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": ", the Japanese occupation of the Dutch East Indies", "extracted_entity": "Japan"}
{"question": "in what year did East Timor secede from Indonesia?", "answer": "1999", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1999"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "1975", "extracted_entity": "East_Timor;Indonesia"}
{"question": "when did Islam become the dominant religion in Java and Sumatra?", "answer": "the end of the 16th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the end of the 16th century", "by the end of the 16th century"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the 13th century.", "extracted_entity": "Islam;Java;Sumatra"}
{"question": "does Indonesia have the world's hightest level of biodiversity?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "was Indonesia named after it became an independent country?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "why did Indonesia break off relations with the People's Republic of China?", "answer": "because of anti-communist purges early in the Suharto era", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["because of anti-communist purges early in the Suharto era", "anti-communist purges early in the Suharto era"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "The People's Republic of China and Indonesia had a", "extracted_entity": "Indonesia;People's_Republic_of_China"}
{"question": "which spice originally attracted Europeans to Indonesia?", "answer": "nutmeg, cloves, or cubeb pepper", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nutmeg, cloves, or cubeb pepper", "nutmeg, cloves, and cubeb pepper"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "nutmeg", "extracted_entity": "Europe;Indonesia"}
{"question": "what is Jakarta?", "answer": "Indonesia's special capital region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia's special capital region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "akarta is the capital and largest metropolis of Indonesia", "extracted_entity": "Jakarta"}
{"question": "what shares land borders with Papua New Guinea, East Timor and Malaysia?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": "Papua_New_Guinea;East_Timor;Malaysia"}
{"question": "what are the largest and politically dominant ethnic group?", "answer": "the Javanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Javanese"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the han Chinese", "extracted_entity": null}
{"question": "what happens when the srivijaya kingdom formed trade links?", "answer": "The indonesian archipelago became an important trade region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The indonesian archipelago became an important trade region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "It became very rich.", "extracted_entity": "Srivijaya"}
{"question": "is it true that indonesia has vast areas of wilderness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "is Indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "are Sports in Indonesia generally male-orientated?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, the most popular sports in Indonesia are football", "extracted_entity": "Indonesia"}
{"question": "is Indonesia the world's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Indonesia"}
{"question": "are sports in Indonesia generally male-orientated and spectator sports are often associated with illegal gambling ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, sports in Indonesia are generally male-orient", "extracted_entity": "Indonesia"}
{"question": "is Pencak Silat an Indonesian martial art ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, Pencak silat is an Indonesian", "extracted_entity": "Pencak_Silat;Indonesia"}
{"question": "has Indonesia the world 's largest Muslim population ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": ", it has the world's largest Muslim", "extracted_entity": "Indonesia;Islam"}
{"question": "who has its own political legislature and governor ?", "answer": "each province", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["each province"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "California", "extracted_entity": null}
{"question": "is it the world 's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No,Q: Which is the world \u2018s", "extracted_entity": null}
{"question": "is it the world 's fourth most populous country and the most populous muslim-majority nation?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "List_of_countries_and_dependencies_by_population"}
{"question": "is indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Indonesia"}
{"question": "is James Monrow the fifth president of US?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "what is the first word on the page?", "answer": "James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James", "James_Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The first word is \u201cA\u201d.", "extracted_entity": null}
{"question": "what is the first number on the page?", "answer": "28", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Q: what is the second number on the", "extracted_entity": null}
{"question": "when did Monroe's presidency expired?", "answer": "March 4, 1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1825", "March 4, 1825."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The answer is. In. The", "extracted_entity": null}
{"question": "what is Monroe's father's name?", "answer": "Spence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spence", "Spence Monroe."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Monroe's father's name is George.", "extracted_entity": null}
{"question": "what is Monroe's daughter's name?", "answer": "Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Mary Monroe", "extracted_entity": null}
{"question": "what does \"Era of Good Feelings\" refers to?", "answer": "Reduced tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Reduced tension", "Monroe allowed his political base to decay, which reduced tensions and led to the naming of his era as the \"Era of Good Feelings\"."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the  of the  of the", "extracted_entity": null}
{"question": "did Monroe' wedding happen at the Trinity Church in New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": "Marilyn_Monroe;Trinity_Church_(Manhattan);New_York_City"}
{"question": "what is Monroe Doctrine?", "answer": "A doctrine declaring U.S. opposition to European interference in the Americas.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A doctrine declaring U.S. opposition to European interference in the Americas.", "In it, Monroe proclaimed the Americas should be free from future European colonization and free from European interference in sovereign countries' affairs. It further stated the United States' intention to stay neutral in European wars and wars between European powers and their colonies, but to consider any new colonies or interference with independent countries in the Americas as hostile acts toward the United States. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Monroe Doctrine is a United States foreign policy position", "extracted_entity": "Monroe_Doctrine"}
{"question": "was James Monroe President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "James_Monroe;President_of_the_United_States;United_States"}
{"question": "did James Monroe attend the College of William and Mary?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe was a student at William and Mary.", "extracted_entity": "James_Monroe;College_of_William_&_Mary"}
{"question": "did James Monroe fight in the Continental Army?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe served in the Continental Army", "extracted_entity": "James_Monroe;Continental_Army"}
{"question": "where was James Monroe born?", "answer": "Westmoreland County, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westmoreland County, Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe was born in Westmoreland County,", "extracted_entity": "James_Monroe"}
{"question": "where was James Monroe shot?", "answer": "at the Battle of Trenton, in his left shoulder", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the Battle of Trenton, in his left shoulder", "The Battle of Trenton"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "in the chest", "extracted_entity": "James_Monroe"}
{"question": "who did James Monroe marry?", "answer": "Elizabeth Kortright", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Elizabeth Kortright"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe married Elizabeth Kortright in 178", "extracted_entity": "James_Monroe"}
{"question": "what did James Monroe do before being elected governor?", "answer": "He practiced law in Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He practiced law in Virginia.", "He practiced law"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "he served as a senator", "extracted_entity": "James_Monroe"}
{"question": "in which years were John Monroe elected as President?", "answer": "1817-1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1817-1825"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "QAnswerA: QAnswer Q", "extracted_entity": null}
{"question": "who was John Monroe standing behind in the painting of  Washington Crossing the Delaware?", "answer": "George Washington", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["George Washington"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "John Monroe was standing behind George Washington in the painting", "extracted_entity": "Monroe,_New_York"}
{"question": "when was James Monroe appointed to Secretary of War?", "answer": "1814", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1814"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe was appointed to Secretary of War in", "extracted_entity": "James_Monroe;United_States_Secretary_of_War"}
{"question": "when did James Monroe die?", "answer": "July 4, 1831", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4, 1831"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "July 4, 1831", "extracted_entity": "James_Monroe"}
{"question": "when did James Monroe graduate from William and Mary?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "in 1776", "extracted_entity": "James_Monroe"}
{"question": "when was James Monroe elected president?", "answer": "Monroe was elected president in the election of 1816, and re-elected in 1820.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president in the election of 1816, and re-elected in 1820."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1820", "extracted_entity": "James_Monroe"}
{"question": "which property did James Monroe sell in 1817?", "answer": "Monroe Hill on the grounds of the University of Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe Hill on the grounds of the University of Virginia."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "his farm", "extracted_entity": "James_Monroe"}
{"question": "when did James Monroe introduce the Monroe Doctrine?", "answer": "December 2, 1823", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 2, 1823"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe introduced the Monroe Doctrine on December 2", "extracted_entity": "James_Monroe;Monroe_Doctrine"}
{"question": "who did James Monroe live with in New York City?", "answer": "His daughter Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His daughter Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe lived with his parents in New York City", "extracted_entity": "James_Monroe;New_York_City"}
{"question": "what did James Monroe's letters not contain?", "answer": "No letters survive in which he might have discussed his religious beliefs.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No letters survive in which he might have discussed his religious beliefs."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "his signature", "extracted_entity": "James_Monroe"}
{"question": "what was the result of the rejection of the Jay Treaty?", "answer": "As a result, the two nations moved closer toward the War of 1812. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As a result, the two nations moved closer toward the War of 1812. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "It led to the War of 1812.", "extracted_entity": null}
{"question": "who is depicted holding the flag in the famous painting of Washington Crossing the Delaware?", "answer": "Monroe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Lieutenant James Monroe", "extracted_entity": "Washington_Crossing_the_Delaware_(1851_painting)"}
{"question": "what dwindled and eventually died out, starting with the Hartford Convention?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The Federalist Party", "extracted_entity": null}
{"question": "is it true that he practiced law in fredericksburg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it true that he was", "extracted_entity": null}
{"question": "what did James Monroe make in 1817?", "answer": "two long tours", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two long tours"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "In 1817, James Monroe became the fifth", "extracted_entity": "James_Monroe"}
{"question": "was Monroe anticlerical?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "James_Monroe"}
{"question": "was Monroe appointed Minister to France from 1794 to 1796?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Ambassador_to_France;France"}
{"question": "what expired on March?", "answer": "Monroe's presidency", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe's presidency"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "The answer is D. It is the only one", "extracted_entity": null}
{"question": "had Monroe racked up many debts during his years of public life ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What did Monroe do to reduce", "extracted_entity": null}
{"question": "who dismantled partisan and sectional coalitions ?", "answer": "many congressmen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["many congressmen"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Buchanan", "extracted_entity": null}
{"question": "what happened in 1811?", "answer": "Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": ",Q: what happened in Q:", "extracted_entity": null}
{"question": "what happened in the election of 1816?", "answer": "Monroe was elected president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "In the election of 1816 Monroe was re", "extracted_entity": null}
{"question": "who died in childbirth in 1772?", "answer": "Margaret Miller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Jane Austen,Q: What was the name", "extracted_entity": null}
{"question": "who died in 1832?", "answer": "Margaret Miller.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "John Adams and John Quincy Adams", "extracted_entity": null}
{"question": "is watt commemorated by statuary in George Square, Glasgow and Princes Street, Edinburgh?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "George_Square;Glasgow;Princes_Street;Edinburgh"}
{"question": "was watt a gentleman, greatly respected by other prominent men of the Industrial Revolution?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt", "extracted_entity": "Industrial_Revolution"}
{"question": "was watt a fellow of the Royal Society of Edinburgh and the Royal Society of London?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Watt was a Fellow of the Royal Society of Edinburgh", "extracted_entity": "Royal_Society_of_Edinburgh;Royal_Society"}
{"question": "four years after opening his shop , Watt began what?", "answer": "Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Watt began to build his first steam engine", "extracted_entity": null}
{"question": "was Watt ranked 22nd in Michael H. Hart 's list of the most influential figures in history ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "ames Watt was ranked In39th in Michael H", "extracted_entity": "James_Watt"}
{"question": "did John Adams represent the Continental Congress in Europe?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes, John Adams did represent the Continental Congress in", "extracted_entity": "John_Adams;Continental_Congress;Europe"}
{"question": "was Adams raised Congregationalist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes, Adams was raised Congregationalist,", "extracted_entity": "Congregational_church"}
{"question": "was Adams an opponent of the Stamp Act?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams was an opponent of the Stamp Act", "extracted_entity": "Stamp_Act_1765"}
{"question": "when did Adams graduate from college?", "answer": "1755.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1755.", "1755"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who was on the committee with Adams to draft  a Declaration of Independence?", "answer": "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson", "extracted_entity": "United_States_Declaration_of_Independence"}
{"question": "what did Jefferson call John Adams?", "answer": "The \"Colossus of Independence\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The \"Colossus of Independence\".", "Colossus of Independence"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams was a \u201ccolossus of independence", "extracted_entity": "Thomas_Jefferson;John_Adams"}
{"question": "what was Adams' political party?", "answer": "The Federalist Party.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party.", "Federalist"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The Federalist Party.", "extracted_entity": null}
{"question": "was Adams the first to introduce a bicameral legislature?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did John Adams get along with Alexander Hamilton?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Adams;Alexander_Hamilton"}
{"question": "did John Adams go to Harvard? ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": "John_Adams;Harvard_University"}
{"question": "did John Adams support the Stamp Act of 1765?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, John Adams supported the Stamp Act of", "extracted_entity": "John_Adams;Stamp_Act_1765"}
{"question": "is Adams' birthplace part of a national park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is Adams' birthplace part", "extracted_entity": null}
{"question": "when did John Adams serve as Vice President?", "answer": "1789-1797", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1789-1797"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1789-1797", "extracted_entity": "John_Adams;Vice_President_of_the_United_States"}
{"question": "with what party did Adams run for presidency?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Federalist", "extracted_entity": null}
{"question": "where is Adams buried?", "answer": "United First Parish Church", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["United First Parish Church"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Quincy,Q: where is Adams buried?", "extracted_entity": null}
{"question": "who were the midnight judges?", "answer": "They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "They were judges formally appointed days before Adams term expired"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The midnight judges were the judges who were appointed by", "extracted_entity": null}
{"question": "in what ways was Adams opposed by Anderw Hamilton?", "answer": "Hamilton wanted to control the army differently than Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hamilton wanted to control the army differently than Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Hamilton and Adams were both Federalists, but Hamilton", "extracted_entity": null}
{"question": "what information did he record in his diary?", "answer": "Descriptions of events and ompressions of men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Descriptions of events and ompressions of men", "He wrote descriptions of events and impressions of men"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "he recorded the information about the places he visited,", "extracted_entity": null}
{"question": "who was defeated for re-election in the`` Revolution of 1800'' by Thomas Jefferson?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": "Thomas_Jefferson"}
{"question": "who represented the Continental Congress in Europe?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The Continental Congress was represented by the European Union", "extracted_entity": "Continental_Congress;Europe"}
{"question": "what is now part of Adams National Historical Park?", "answer": "John Adams' birthplace", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams' birthplace"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The Adams National Historical Park consists of the birthplace", "extracted_entity": null}
{"question": "is it true that adams had spent some time as the ambassador?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No,Q: Is it true that adams", "extracted_entity": null}
{"question": "is it true that massachusetts sent him in 1774?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Massachusetts"}
{"question": "who did Massachusetts send in 1774?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams, John Hancock, Elbridge Gerry,", "extracted_entity": "Massachusetts"}
{"question": "are his last words often quoted as \" Thomas Jefferson survives \" . ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Thomas_Jefferson"}
{"question": "the John Adams Library , housed at the Boston Public Library , contains what?", "answer": "Adams's personal collection of more than 3,500 volumes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams's personal collection of more than 3,500 volumes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "t contains more than  14,000 books", "extracted_entity": "Boston_Public_Library"}
{"question": "adams ' opponents were what?", "answer": "Democratic Republicans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Democratic Republicans"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "dams\u2019 opponents were the Federalists.", "extracted_entity": null}
{"question": "did the election of 1800 not become a bitter and volatile battle , with each side expressing extraordinary fear of the other party and its policies ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "o, the election of 1800 was a", "extracted_entity": null}
{"question": "what happened in 1764?", "answer": "Adams married Abigail Smith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams married Abigail Smith"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "The British won the war.", "extracted_entity": null}
{"question": "is a kangaroo a reptile?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Kangaroo;Reptile"}
{"question": "is a kangaroo on the Australian coat of arms?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes. The kangaroo is the animal on the", "extracted_entity": null}
{"question": "was James Cook the first to record the name \"Kangooroo?\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, he was not the first to record the", "extracted_entity": "James_Cook"}
{"question": "what is a collective noun for kangaroos?", "answer": "mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mob, troop, or court", "Mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A collective noun for kangaroos is a troop", "extracted_entity": "Kangaroo"}
{"question": "where do joeys complete postnatal development?", "answer": "marsupium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["marsupium", "A pouch called a marsupium"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "The nest", "extracted_entity": null}
{"question": "what do kangaroos use for \"crawl-walking?\"?", "answer": "its hind feet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["its hind feet", "Their hind feet and their tails"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroos use their tails for crawl-walking", "extracted_entity": "Kangaroo"}
{"question": "why do kangaroos have a wide bite?", "answer": "The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "Because of grazing"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "they need to chew through tough vegetation", "extracted_entity": null}
{"question": "what is responsible for converting the hydrogen byproduct of fermentation into acetate?", "answer": "bacteria", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bacteria", "The digestive system of a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "acetyl-CoA synthetase", "extracted_entity": null}
{"question": "are wild kangaroos shot for meat?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Wild kangaroos are not shot for meat.", "extracted_entity": null}
{"question": "have kangaroos fared well since European settlement?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Europe"}
{"question": "do kangaroos have many natural predators?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No,Q: Do kangaroos have many", "extracted_entity": null}
{"question": "what is the average life expectancy of a kangaroo?", "answer": "4.6 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["4.6 years", "about 4.6 years", "4 to 6 years"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos have short life expectancy of", "extracted_entity": "Kangaroo"}
{"question": "when did the first official report of kangaroo blindness take place?", "answer": "1994", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1994"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A. In B. InC.", "extracted_entity": null}
{"question": "what are vehicles that frequent isolated roads often fitted with?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Traction Aids", "extracted_entity": null}
{"question": "do kangaroos eat plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, kangaroos eat plants.", "extracted_entity": "Kangaroo"}
{"question": "what are some extinct predators of the kangaroo?", "answer": "Thylacine, Marsupial Lion, Maagalania, and Wonambi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thylacine, Marsupial Lion, Maagalania, and Wonambi", "The Thylacine"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos are four marsupials from the", "extracted_entity": "Kangaroo"}
{"question": "is a Kangaroo a Marsupial?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, a kangaroo is a marsupial", "extracted_entity": "Marsupial"}
{"question": "is the kangaroo an herbivour?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the kangaroo's", "extracted_entity": null}
{"question": "are Kangaroos Shy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes. Kangaroos are shy. They avoid", "extracted_entity": null}
{"question": "what method of locomotion do Kangaroos Use?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Kangaroos use a method of locomotion known", "extracted_entity": null}
{"question": "what is a collective noun for a kangaroo?", "answer": "a mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a collective noun for a kangaroo is a mob", "extracted_entity": "Kangaroo"}
{"question": "what is a roo?", "answer": "a kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a roo is a root of a number.", "extracted_entity": null}
{"question": "what is used to protect a vehicale from a Kangaroo?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a bullbar", "extracted_entity": "Kangaroo"}
{"question": "what method is used by Kangaroos to travel?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hoppingB: jumpingC: walkingD: crawling", "extracted_entity": null}
{"question": "who asked a nearby local what the creatures were called?", "answer": "Captain James Cook and naturalist Sir Joseph Banks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Captain James Cook and naturalist Sir Joseph Banks"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "The local replied that they were called \"Hoppers", "extracted_entity": null}
{"question": "what are four species that are commonly referred to as kangaroos?", "answer": "The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "red kangaroo,Q: what are four species", "extracted_entity": null}
{"question": "are kangaroos and wallabies adept swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, wallabies and kangaroos are not", "extracted_entity": "Kangaroo;Wallaby"}
{"question": "are kangaroos farmed to any extent?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it illegal to kill kang", "extracted_entity": null}
{"question": "who also discovered that less than three percent of kangaroos exposed to the virus developed blindness ?", "answer": "Veterinarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Veterinarians"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Dr. Frank Fenner", "extracted_entity": null}
{"question": "different species of kangaroos eat what?", "answer": "different diets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different diets"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "different species of kangaroos eat different types of", "extracted_entity": null}
{"question": "have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes. In fact, many of the collisions that", "extracted_entity": null}
{"question": "is a collision with a vehicle capable of killing a kangaroo ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, it is", "extracted_entity": "Kangaroo"}
{"question": "was the game of Marn grook played using a ball made from kangaroo by the Kurnai people ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, the game was played using a ball made", "extracted_entity": "Kangaroo;Gunai"}
{"question": "have kangaroos large , powerful hind legs , large feet adapted for leaping , a long muscular tail for balance , and a small head ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "kangaroos are adapted for leaping", "extracted_entity": null}
{"question": "is a kangaroo a marsupial from the family macropodidae -LRB- macropods , meaning (`` ` large foot ('' ' -RRB-?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Kangaroo;Marsupial;Macropodidae"}
{"question": "what kinds of changes have larger kangaroos adapted much better to?", "answer": "Changes wrought to the Australian landscape by humans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Changes wrought to the Australian landscape by humans."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "They have adapted to the hot and dry climate of", "extracted_entity": null}
{"question": "have larger kangaroos adapted much better to changes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How?", "extracted_entity": null}
{"question": "is the leopard smaller than the other members of Panthera?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No,Q: Is the leopard the largest member", "extracted_entity": null}
{"question": "is a leopard larger and less lanky than a cheetah?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, a leopard is larger and less lanky", "extracted_entity": "Leopard"}
{"question": "are large pythons potential prey for leopards?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do leopards eat large", "extracted_entity": null}
{"question": "what may a leopard be mistaken for?", "answer": "A cheetah or a jaguar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cheetah or a jaguar"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "A leopard may be mistaken for a jaguar,", "extracted_entity": null}
{"question": "what is a hybrid animal resulting from a union between a leopard and a puma?", "answer": "a pumapard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pumapard", "A pumapard"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "A cougar", "extracted_entity": "Leopard;Cougar"}
{"question": "where do leopards often hide their kills?", "answer": "in dense vegetation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in dense vegetation", "in dense vegetation or in trees"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopards often hide their kills in caves or", "extracted_entity": null}
{"question": "how did early naturalists distinguish between leopards and panthers?", "answer": "by the length of their tails", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by the length of their tails", "By the length of the tail, panthers having longer tails than leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "they distinguished them by the color of their fur.", "extracted_entity": null}
{"question": "what are dark leopards known as colloquially?", "answer": "Black Panthers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Black Panthers", "black panthers"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Dark leopards are the most common form of", "extracted_entity": null}
{"question": "what shape are a leopard's black rosettes in East Africa?", "answer": "circular", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["circular"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a leopard's black rosettes in East Africa", "extracted_entity": "East_Africa"}
{"question": "is the leopard an Old World mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the leopard is an Old World mammal", "extracted_entity": "Leopard;Old_World"}
{"question": "is the leopard of the Felidae family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Leopard;Felidae"}
{"question": "is the leopard one of the four 'big cats'?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No,Q: Do you have any idea what", "extracted_entity": null}
{"question": "where is the leopard distributed?", "answer": "southern Eurasia and Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["southern Eurasia and Africa", "southern Eurasia and Africa, from Korea to South Africa and Spain"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The leopard is found in the rainforests of", "extracted_entity": null}
{"question": "how long is a leopard's tail?", "answer": "60 to 110cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 to 110cm", "60-110cm"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The tail of a leopard is about Q:", "extracted_entity": null}
{"question": "how much does a leopard weigh?", "answer": "Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "(because of ambiguity) - 28-90kg "], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Leopard"}
{"question": "does a male leopard weigh more than a female leopard?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the male leopard is heavier than the female", "extracted_entity": "Leopard"}
{"question": "what is the difference between leopards and cheetahs?", "answer": "The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "The leopard has rosettes rather than cheetah's simple spots, but they lack internal spots, unlike the jaguar. The leopard is larger and less lanky than the cheetah but smaller than the jaguar."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "leopards and cheetahs are both", "extracted_entity": null}
{"question": "how does the distribution size of the leopard compare to the distribution of other wild cats?", "answer": "As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "the leopard had the largest distribution of any wild cat."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The distribution size of the leopard is larger than the", "extracted_entity": null}
{"question": "what resembles that of the similarly-sized cougar in the Americas?", "answer": "The leopard's ecological role", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard's ecological role"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The cougar is a large cat native to the Americas", "extracted_entity": "Cougar;Americas"}
{"question": "what was one of the many species described in Linnaeus's 18th-century work, Systema Naturae?", "answer": "Felis pardus", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felis pardus"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The scientific name of the blue whale is Bala", "extracted_entity": "Carl_Linnaeus;Systema_Naturae"}
{"question": "what sort of cats are solitary?", "answer": "Leopards", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "All cats are solitary,Q: what sort of", "extracted_entity": null}
{"question": "are leopards circular?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the difference between a", "extracted_entity": null}
{"question": "is the leopard solitary?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What does the leopard eat?", "extracted_entity": null}
{"question": "what centred in Sierra?", "answer": "The leopard men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard men"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the Sierra Club is an American environmental organization with chapters", "extracted_entity": "Sierra_Club"}
{"question": "felis pardus was what?", "answer": "One of the many species described in Linnaeus's 18th-century work, Systema Naturae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of the many species described in Linnaeus's 18th-century work, Systema Naturae"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "He was a Roman politician and general who played a", "extracted_entity": null}
{"question": "has the New Zealand Rugby League featured the Otahuhu Leopards and then the Tamaki Leopards ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "New_Zealand_Rugby_League;Otahuhu_Leopards"}
{"question": "were the Leopard men a West African secret society who practised cannibalism ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the Leopard men were a West African secret", "extracted_entity": "West_Africa;Cannibalism"}
{"question": "was the leopard tank a German designed tank which entered service in 1965 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Germany"}
{"question": "is the black color heritable and caused by only one recessive gene locus ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No, because it is caused by multiple genes and", "extracted_entity": "Black"}
{"question": "is the leopard -LRB- panthera pardus -RRB- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "ard -LRB- panthera pardus", "extracted_entity": "Leopard;Felidae;Panthera;Tiger"}
{"question": "can a panther be any of several species of large felid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No. There are only two species of panther", "extracted_entity": null}
{"question": "is the leopard an agile and graceful predator?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why does the leopard have a", "extracted_entity": null}
{"question": "is Liechtenstein bordered by Switzerland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Switzerland", "extracted_entity": "Liechtenstein;Switzerland"}
{"question": "is Liechtenstein the smallest German-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is the smallest German-speaking country in", "extracted_entity": "Liechtenstein;German_language"}
{"question": "was Liechtenstein part of the ancient Roman province of Raetia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein;Ancient_Rome;Raetia"}
{"question": "when was the first factory opened?", "answer": "1836", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1836"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The first factory was opened in Q: Where", "extracted_entity": null}
{"question": "how many municipalities is Liechtenstein divided into?", "answer": "11", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is divided into Q2:", "extracted_entity": null}
{"question": "what is the national currency of Liechtenstein?", "answer": "Swiss franc", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swiss franc"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the Swiss franc (CHF) is the national", "extracted_entity": "Liechtenstein"}
{"question": "has Leichtenstein worked to promote the county's image by prosecuting international money-laundering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "\"I have been working on the international money-l", "extracted_entity": null}
{"question": "what company administers Leichtenstein's railways?", "answer": "Austrian Federal Railways", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austrian Federal Railways"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The company administers Leichtenstein's railways are", "extracted_entity": null}
{"question": "what percentage of Liechtenstein's population is foreign-born?", "answer": "two-thirds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two-thirds", "67%"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "34%", "extracted_entity": "Liechtenstein"}
{"question": "is there an airport in Liechtenstein?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, Liechtenstein has an airport.", "extracted_entity": "Liechtenstein"}
{"question": "is Liechtenstein heavily urbanized?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein"}
{"question": "is Liechtenstein doubly landlocked?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Liechtenstein"}
{"question": "does Liechtenstein have an army?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Liechtenstein"}
{"question": "when was Liechtenstein's current constitution adopted?", "answer": "October 1921.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["October 1921.", "October 1921"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1862", "extracted_entity": "Liechtenstein"}
{"question": "what is the official language of Liechtenstein?", "answer": "German.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German.", "German"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "german", "extracted_entity": "Liechtenstein"}
{"question": "what countries border Liechtenstein?", "answer": "Switzerland and Austria.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Switzerland and Austria.", "Switzerland and Austria"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is bordered by Switzerland to the west", "extracted_entity": "Liechtenstein"}
{"question": "what Roman province was Liechtenstein part of?", "answer": "Raetia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Raetia.", "Raetia"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The Roman province of Raetia", "extracted_entity": "Roman_province;Liechtenstein"}
{"question": "how many municipalities are within Oberland?", "answer": "6.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.", "6"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Oberbergischer_Kreis"}
{"question": "what is the smallest German-speaking country in the world?", "answer": "Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein", "extracted_entity": "German_language"}
{"question": "the Savings and Loans Bank was founded, as was the first cotton-weaving mill in what year?", "answer": "1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1861"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The Savings and Loans Bank was founded in", "extracted_entity": null}
{"question": "what enjoys one of the world's highest standards of living?", "answer": "Liechtenstein's population", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein's population"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Switzerland", "extracted_entity": null}
{"question": "does the state court rule on the conformity of laws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No. The state court rules on the constitutionality", "extracted_entity": null}
{"question": "what do most recognizable international company and largest employer have in common?", "answer": "They are Hilti.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are Hilti."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "They are both headquartered in Bentonville, Arkansas.", "extracted_entity": null}
{"question": "was Liechtenstein completed in November 2000?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "November Q: Was Liechtenstein completed in", "extracted_entity": null}
{"question": "what completed in November?", "answer": "the Kunstmuseum Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Kunstmuseum Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The  of the first World War.", "extracted_entity": null}
{"question": "the State Court rules what?", "answer": "on the conformity of laws with the constitution", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["on the conformity of laws with the constitution"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "The State Court rules are the rules that govern the", "extracted_entity": null}
{"question": "the Historical Society of the Principality of Liechtenstein plays what?", "answer": "a role in preserving the culture and history of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a role in preserving the culture and history of the country."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "a major role in the preservation of the Principality", "extracted_entity": null}
{"question": "is the museum collection also the national art collection of Liechtenstein ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Liechtenstein"}
{"question": "is there a small heliport at Balzers in Liechtenstein available for charter helicopter flights ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, there is a small heliport at", "extracted_entity": "Balzers;Liechtenstein"}
{"question": "are nationals referred to by the plural : Liechtensteiners ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the capital of Lie", "extracted_entity": null}
{"question": "is it a winter sports resort , although it is perhaps best known as a tax haven ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "es, it is a winter sports resort, although", "extracted_entity": "Tax_haven"}
{"question": "is it a winter sports resort?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it in the Alps?", "extracted_entity": null}
{"question": "is it the smallest german-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it the richest german-speaking", "extracted_entity": null}
{"question": "what happened in moravia , lower austria , silesia , and styria , though in all cases , these territories were held in fief under other more senior feudal lords?", "answer": "The Liechtenstein Dynasty acquired vast swaths of land", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Liechtenstein Dynasty acquired vast swaths of land"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "aves of moravia, lower a", "extracted_entity": "Moravia;Silesia;Styria"}
{"question": "did Webster write, \"I can now sleep of nights\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Where did Webster write \u201cI", "extracted_entity": null}
{"question": "was the arrival of Louis Kossuth (exiled leader of a failed Hungarian revolution) another issue that presented itself during Fillmore's presidency?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "illmore had to deal with the arrival of Louis", "extracted_entity": "Lajos_Kossuth;Hungary"}
{"question": "did he die at 11:10 p.m. on March 8, 1874, of the after-effects of a stroke?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What was the cause of his", "extracted_entity": null}
{"question": "who got Seward elected to the senate?", "answer": "Weed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Weed", "Weed ultimately got Seward elected to the senate"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The people of Alaska", "extracted_entity": "Seward,_Alaska"}
{"question": "where was Commodore Matthew C. Perry sent to open Japan to Western trade?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "Commodore Matthew C. Perry send to Japan"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "mmodore Matthew C. Perry was sent to open", "extracted_entity": "Matthew_C._Perry;Japan;Western_world"}
{"question": "what is The Remarkable Millard Fillmore?", "answer": "A fake biography based on real events that happened in Fillmore's life.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A fake biography based on real events that happened in Fillmore's life.", "In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events that happened in Fillmore's life"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The Remarkable Millard Fillmore was the last", "extracted_entity": "Millard_Fillmore"}
{"question": "did Fillmore form a law partnership before or after he founded the private University of Buffalo?", "answer": "Before.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Before.", "Law partnership founded before University of Buffalo"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "The law partnership was formed in 1825,", "extracted_entity": "Millard_Fillmore;University_at_Buffalo"}
{"question": "did Fillmore run for President a second time?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "I don't know (semantic ambiguity)"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore ran for president again in 1856", "extracted_entity": "Millard_Fillmore"}
{"question": "how long after the death of his first wife did Fillmore marry Caroline McIntosh?", "answer": "where is the death date of his first wife?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where is the death date of his first wife?"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "as soon as he could.", "extracted_entity": "Millard_Fillmore"}
{"question": "was Millard Fillmore the thirteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore was the thirteenth President", "extracted_entity": "Millard_Fillmore;President_of_the_United_States"}
{"question": "was Millard Fillmore born on January 7, 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Millard_Fillmore"}
{"question": "did Millard Fillmore die on March 8, 1974?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Millard_Fillmore"}
{"question": "where was Millard Fillmore born?", "answer": "in a log cabin in Summerhill, New York", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in a log cabin in Summerhill, New York"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore was born in a log cabin on a", "extracted_entity": "Millard_Fillmore"}
{"question": "to whom was Millard Fillmore born to?", "answer": "to Nathaniel and Phoebe Millard Fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to Nathaniel and Phoebe Millard Fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore was born to Nathaniel Fill", "extracted_entity": "Millard_Fillmore"}
{"question": "who did Millard Fillmore fall in love with?", "answer": "He fell in love with Abigail Powers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He fell in love with Abigail Powers", "Abigail Powers"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore fell in love with his wife", "extracted_entity": "Millard_Fillmore"}
{"question": "what did Taylor and Fillmore disagree upon?", "answer": "slavory issues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slavory issues", "Taylor and Fillmore disagreed on the slavery issue in the new western territories taken from Mexico in the Mexican-American War"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Slavery", "extracted_entity": null}
{"question": "how did Fillmore ascend to the presidency?", "answer": "upon the death of the sitting president, Taylor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon the death of the sitting president, Taylor", "Official White House portrait of Millard FillmoreFillmore ascended to the presidency upon the sudden and unexpected death of President Taylor in July 1850."], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "he was the vice president under Zachary Taylor and", "extracted_entity": "Millard_Fillmore"}
{"question": "how did the supporters of Henry Clay feel about Fillmore in 1848?", "answer": "they were ok with him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they were ok with him", "angry"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "they felt that Fillmore was a weak candidate and", "extracted_entity": "Henry_Clay;Millard_Fillmore"}
{"question": "who was first apprenticed to a fuller to learn the cloth-making trade?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "William Bradford", "extracted_entity": null}
{"question": "he founded the private university of buffalo on what date?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "who or what fell in love with abigail powers?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "A man named Nabal fell in love with ab", "extracted_entity": null}
{"question": "was Fillmore one of the founders of the University of Buffalo?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_at_Buffalo"}
{"question": "was Another primary objective of Fillmore to preserve the Union from the intensifying slavery debate?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes, the primary objective of Fillmore was to", "extracted_entity": "Millard_Fillmore;United_States"}
{"question": "was Fillmore the second Chancellor, a position he maintained while both Vice President and President?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No. Fillmore was the second Chancellor from", "extracted_entity": "Millard_Fillmore"}
{"question": "is Fillmore the first of two presidents to have been an indentured servant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Millard_Fillmore"}
{"question": "is the comic strip Mallard Fillmore named after the president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Millard_Fillmore"}
{"question": "was Fillmore the first U.S. President born after the death of a former president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore was the first U.S.", "extracted_entity": "Millard_Fillmore;United_States;President_of_the_United_States"}
{"question": "did Fillmore not turn down the honor , explaining that he had neither the `` literary nor scientific attainment '' to justify the degree ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Millard_Fillmore"}
{"question": "what happened in a log cabin in summerhill , new york?", "answer": "Fillmore was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fillmore was born"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "a baby was born.", "extracted_entity": null}
{"question": "who was born precisely at midnight during an electrical storm , to a Serbian family in the village of Smiljan near Gospi\u0107 , in the Lika region of the Croatian Krajina in Military Frontier ( part of the Austrian Empire ) , in the present-day Croatia . ?", "answer": "Nikola Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Broz Tito (1892-", "extracted_entity": "Serbia;Gospi\u0107;Lika;Croatia_proper"}
{"question": "are there at least two films describing Tesla 's life ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "s, there are at least two films describing Tesla", "extracted_entity": "Nikola_Tesla"}
{"question": "have a number of live theatrical plays based on Tesla 's life been produced and staged worldwide ?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "2. 3.", "extracted_entity": "Nikola_Tesla"}
{"question": "do sea otters have long muscular tails?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sea_otter"}
{"question": "can otters survive in cold water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters can survive in cold water. They have", "extracted_entity": null}
{"question": "does the giant otter inhabit South Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "South_Africa"}
{"question": "how many species of otter are there?", "answer": "13", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "The 13 extant otter species are all", "extracted_entity": "Otter"}
{"question": "what do river otters eat?", "answer": "a variety of fish and shellfish, as well as small land mammals and birds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a variety of fish and shellfish, as well as small land mammals and birds", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds. ", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "North American river otters eat a variety of fish", "extracted_entity": "North_American_river_otter"}
{"question": "how much do sea otters weigh?", "answer": "30 kg (about 65 pounds)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["30 kg (about 65 pounds)"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they typically weigh between  and, making them", "extracted_entity": "Sea_otter"}
{"question": "how do sea otters insulate themselves?", "answer": "a layer of air trapped in their fur(!)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a layer of air trapped in their fur(!)", "As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Sea otters insulate themselves with their thick fur", "extracted_entity": "Sea_otter"}
{"question": "why are sea otters nearly extinct?", "answer": "They've been hunted for their fur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They've been hunted for their fur", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Sea otters are nearly extinct because they are hunted", "extracted_entity": null}
{"question": "what is one of the challenges of re-establishing a population of Eurasian otters in the UK?", "answer": "roadkill deaths", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roadkill deaths", "Roadkill deaths have become one of the significant threats to the success of their re-establishment. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they are difficult to reintroduce otters into the", "extracted_entity": "Eurasian_otter;United_Kingdom"}
{"question": "is otter a kind of mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes, otters are mammals.", "extracted_entity": "Otter"}
{"question": "how many species and genera does otter have?", "answer": "13 species and 7 genera", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13 species and 7 genera", "13 species in 7 genera."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "there are 13 species of otter in seven", "extracted_entity": "Otter"}
{"question": "do otters live in water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No. Otters live in land.", "extracted_entity": null}
{"question": "where do sea otters live?", "answer": "Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "The sea otter lives actually in the sea. ", "Beside water", "Pacific coast of North America"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "Sea otters are found in the North Atlantic and", "extracted_entity": null}
{"question": "where does the word \"otter\" derive from?", "answer": "The word \"otter\" derives from the Old English word otr, otor or oter. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The word \"otter\" derives from the Old English word otr, otor or oter. ", "The word \"otter\" derives from the Old English word otr, otor or oter. This and cognate words in other Indo-European languages ultimately stem from a root which also gave rise to the English words \"water\", \"wet\" and \"winter\". "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "The word \u201cotter\" derives from the Dutch", "extracted_entity": null}
{"question": "does otter give birth or lay egg?", "answer": "give birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["give birth"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are mammals,,,,,,", "extracted_entity": null}
{"question": "what drives sea otter almost to extinction?", "answer": "Humans hunted them almost to extinction. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Humans hunted them almost to extinction. ", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them almost to extinction. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Overfishing and hunting for fur trade", "extracted_entity": null}
{"question": "why otters are considered as totem animals?", "answer": "The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are considered as totem animals because they", "extracted_entity": null}
{"question": "do sea otters live along the Pacific coast?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sea_otter"}
{"question": "are otters totem animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otters are a type of weasel. They", "extracted_entity": null}
{"question": "are otters herbivores?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Herbivores are animals that eat plants.", "extracted_entity": null}
{"question": "what is the primary item in an otter's diet?", "answer": "Fish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fish.", "fish"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish and invertebrates", "extracted_entity": "Otter"}
{"question": "what is an otter's den called?", "answer": "Holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holt", "holt", "A holt"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "An otter's den is called a holt", "extracted_entity": null}
{"question": "why is the giant otter becoming increasingly rare?", "answer": "Poaching, habitat loss, and toxins in gold mining.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Poaching, habitat loss, and toxins in gold mining.  ", "poaching, habitat loss, and the use of mercury in illegal alluvial gold mining"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "The giant otter is a large,,,", "extracted_entity": null}
{"question": "how do otters keep themselves warm without blubber?", "answer": "A layer of air trapped in their fur.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A layer of air trapped in their fur.  ", "a layer of air trapped in their fur"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "They have special blood vessels that allow them to lose", "extracted_entity": null}
{"question": "how are otters playful animals?", "answer": "The slide down snowy slopes, apparently for sheer enjoyment.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The slide down snowy slopes, apparently for sheer enjoyment.  ", "They slide repeatedly down snowy slopes for sheer enjoyment."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "They are very playful animals. They love to play", "extracted_entity": null}
{"question": "what animals are related to otters?", "answer": "weasels, polecats, and badgers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["weasels, polecats, and badgers"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "beavers,Q: what animals are related to", "extracted_entity": null}
{"question": "what traps a layer of air, and keeps them dry and warm under water?", "answer": "Long guard hair", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Long guard hair"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "A thermos bottle. A thermos bottle is", "extracted_entity": null}
{"question": "the collective noun romp is sometimes used for a group of what?", "answer": "Otters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Otters"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "children B: animals C: people D: adults", "extracted_entity": null}
{"question": "is it true that otters eat a variety of fish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes. Otters are opportunistic feeders that", "extracted_entity": null}
{"question": "are otters playful animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the lifespan of an", "extracted_entity": null}
{"question": "are otters very active?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do they like to be pet", "extracted_entity": null}
{"question": "are male otters dog-otters?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No, male otters are dog-otters", "extracted_entity": null}
{"question": "an otter 's den is what?", "answer": "A holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A holt"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "An otter \u2018s den is a room in", "extracted_entity": null}
{"question": "sea otters eat what?", "answer": "Shellfish and other invertebrates", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shellfish and other invertebrates"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Sea otters eat sea sea sea", "extracted_entity": "Sea_otter"}
{"question": "is the myth of Otter 's Ransom the starting point of the Volsunga saga ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "V\u00f6lsunga_saga"}
{"question": "are Male otters dog-otters , females are bitches and babies are cubs or pups ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "es,Q: What are the major differences between", "extracted_entity": null}
{"question": "is an otter 's den called a holt ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "es, an otter's den is called", "extracted_entity": "Otter"}
{"question": "have most otters fish as the primary item in their diet , supplemented by frogs , crayfish and crabs ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Otter;Frog;Crayfish;Crab"}
{"question": "do penguins feed on krill?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Penguin;Krill"}
{"question": "what is the largest living species of penguin?", "answer": "Emperor Penguin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Emperor Penguin", "the Emperor Penguin (Aptenodytes forsteri)", "Emperor"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The emperor penguin is the largest living species of", "extracted_entity": null}
{"question": "do penguins live almost exclusively in the Southern Hemisphere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, penguins live almost exclusively in the Southern", "extracted_entity": "Southern_Hemisphere"}
{"question": "what is \"tobogganing\"?", "answer": "when penguins slide on their bellies across the snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when penguins slide on their bellies across the snow", "Tobogganing is when penguins slide on their bellies across the snow."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "it is the practice of eating a very low-cal", "extracted_entity": null}
{"question": "why are penguins countershaded?", "answer": "for camouflage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["for camouflage"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are countershaded for a number of reasons", "extracted_entity": null}
{"question": "are penguins afraid of humans?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes,Penguins are the same as any other", "extracted_entity": null}
{"question": "how much time to penguins spend on land?", "answer": "half of their life", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["half of their life", "They spend half of their life on land."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The amount of time penguins spend on land depends", "extracted_entity": null}
{"question": "how many species of penguins are there?", "answer": "between 17 and 20 living species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["between 17 and 20 living species", "Penguin biodiversity varies between 17 and 20 living species, all in the subfamily Spheniscinae."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "There are,", "extracted_entity": null}
{"question": "how are Isabelline penguins different from most penguins?", "answer": "they have brown rather than black plumage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they have brown rather than black plumage", "Because they are born with brown rather than black plumage."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Isabelline penguins are different from most", "extracted_entity": null}
{"question": "are penguins birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Penguin"}
{"question": "do penguins have a better than average sense of hearing for birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, they have a below average sense of hearing", "extracted_entity": "Penguin"}
{"question": "are penguins considered \"higher waterbirds\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, because they live on land and in water", "extracted_entity": "Penguin"}
{"question": "what order do penguins belong to?", "answer": "Sphenisciformes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sphenisciformes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins belong to the order Sphenisciformes", "extracted_entity": null}
{"question": "how tall were the tallest prehistoric penguins?", "answer": "1.80 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.80 meters", "as tall as an adult human"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The tallest prehistoric penguins were about Q", "extracted_entity": null}
{"question": "what makes penguins so agile in the water?", "answer": "Their wings have become flippers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their wings have become flippers", "Smooth plumage preserves a layer of air, ensuring buoyancy, wings are flippers"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The penguin's feathers.", "extracted_entity": null}
{"question": "why do penguins \"tobaggan\"?", "answer": "It conserves energy while moving quickly", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It conserves energy while moving quickly", "Conserves energy while moving quickly"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are flightless birds. They live in the", "extracted_entity": null}
{"question": "who spend half of their life on land and half in the oceans?", "answer": "Penguins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Amphibians", "extracted_entity": null}
{"question": "what is also the distance that Antarctic tourists are told to keep from penguins?", "answer": "3 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 meters"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "10 metres (32 ft)", "extracted_entity": "Antarctic"}
{"question": "what have become flippers, useless for flight in the air?", "answer": "Penguins' wings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins' wings"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "They are used for the balance and the control of", "extracted_entity": null}
{"question": "what happens when mothers lose a chick?", "answer": "They sometimes attempt to \"steal\" another chick.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes attempt to \"steal\" another chick."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "The mothers die of grief.", "extracted_entity": null}
{"question": "is it true that each penguin gets a turn in the center?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes. The penguins are very orderly and they", "extracted_entity": null}
{"question": "are all penguins countershaded?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, all penguins are countershaded.", "extracted_entity": "Countershading"}
{"question": "are penguins astonishingly agile?", "answer": "In the water they are.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the water they are."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins are not agile. They are not able to", "extracted_entity": null}
{"question": "are penguins at risk?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes. The main threat is from climate change,", "extracted_entity": null}
{"question": "is it also possible that penguin comes from the Latin pinguis , fat ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "es,Q: Is it also possible that p", "extracted_entity": null}
{"question": "have penguins an average sense of hearing for birds ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Penguins have an average sense of hearing for birds", "extracted_entity": "Penguin"}
{"question": "can larger penguins dive deep in case of need ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No,", "extracted_entity": null}
{"question": "is the smallest penguin species the Little Blue Penguin -LRB- also known as the Fairy Penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2.2 lb -RRB- ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Little_penguin"}
{"question": "is it not even known whether the gigantic palaeeudyptines constitute a monophyletic lineage , or whether gigantism was evolved independently in a much restricted Palaeeudyptinae and the Anthropornithinae ?", "answer": "It is not even known.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not even known."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "his is a matter of debate. See for instance", "extracted_entity": "Monophyly"}
{"question": "is the largest living species the emperor penguin -LRB- aptenodytes forsteri -RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Emperor_penguin"}
{"question": "is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Little_penguin"}
{"question": "what happened in a region not quite 2000 km south of the equator 35 mya?", "answer": "At least one giant penguin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At least one giant penguin."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "it was a desert", "extracted_entity": "Year"}
{"question": "what is the last word on the page?", "answer": "Connecticut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Connecticut", "(SSN-22)"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The last word is \"happiness.\"", "extracted_entity": null}
{"question": "is polar bear a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "polar bear is a large bear native to the Arctic", "extracted_entity": "Polar_bear"}
{"question": "is polar bear a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "es, it is a carnivore.", "extracted_entity": "Polar_bear;Carnivore"}
{"question": "what is largest polar bear on record?", "answer": "2200 lb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2200 lb", "A huge male, allegedly weighing 1002 kg (2200 lb) shot at Kotzebue Sound in northwestern Alaska in 1960."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The largest polar bear ever recorded was shot in Kot", "extracted_entity": null}
{"question": "what is polar bear's skin color?", "answer": "white or cream", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["white or cream", "Black"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Polar bear's skin color is black.", "extracted_entity": null}
{"question": "how long is polar bear's guard hair?", "answer": "5-15 cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5-15 cm"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "4-5 inches", "extracted_entity": "Polar_bear"}
{"question": "what is cause of polar bear's skin diseases?", "answer": "mites or other parasites", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mites or other parasites", "Mites or other parasites"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "polar bear's skin diseases are caused by parasites and", "extracted_entity": "Polar_bear"}
{"question": "why polar bear is a special species?", "answer": "The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The polar bear is a large bear native to the", "extracted_entity": "Polar_bear"}
{"question": "what do fossil and DNA evidence tell us?", "answer": "The polar bear diverged from the brown bear about 200 thousand years ago.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The polar bear diverged from the brown bear about 200 thousand years ago.", "The polar bear diverged from the brown bear roughly 200 thousand years ago."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they tell us that evolution has occurred,Q:", "extracted_entity": null}
{"question": "does a polar bear live in the Arctic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Polar_bear;Arctic"}
{"question": "is a polar bear white in color?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, polar bears are white in color.", "extracted_entity": "Polar_bear"}
{"question": "is a polar bear at high risk of extinction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes. Polar bears are at high risk of extinction", "extracted_entity": null}
{"question": "how heavy is a male polar bear?", "answer": "300-600 kg (660-1320 lb)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300-600 kg (660-1320 lb)", "Most adult males weigh 350-650 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The polar bear is the largest extant species of", "extracted_entity": "Polar_bear"}
{"question": "how heavy was the largest polar bear on record?", "answer": "1002 kg (2200 lb) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1002 kg (2200 lb) ", "He allegedly weighed 1002 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The largest polar bear on record was a whopping", "extracted_entity": null}
{"question": "what does a polar bear's fur provide?", "answer": "It provides the animal with effective camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It provides the animal with effective camouflage.", "A polar bear's fur provides camouflage and insulation"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Insulation", "extracted_entity": null}
{"question": "do female polar bears weight more than the male?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes,Q: do female polar bears have thicker", "extracted_entity": null}
{"question": "how much weight do female polar bears gain during pregnancy?", "answer": "They gain double their weight.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They gain double their weight.", "They double their weight during pregnancy"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "about 200 kg", "extracted_entity": "Polar_bear"}
{"question": "can polar bears be seen under infrared photography?", "answer": "Polar bears are nearly invisible under infrared photography.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Polar bears are nearly invisible under infrared photography.", "Only their breath and muzzles can be easily seen"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Polar_bear"}
{"question": "what is actually black in color?", "answer": "A polar bear's skin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A polar bear's skin."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "A black hole.", "extracted_entity": null}
{"question": "what insulate it against the cold?", "answer": "Its think blubber and fur.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its think blubber and fur."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The roof.", "extracted_entity": null}
{"question": "what includes a lengthy justification of why this species is listed as vulnerable?", "answer": "Database entry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Database entry."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "A full species account.", "extracted_entity": null}
{"question": "what do with greenpeace and the natural resources defense council have in common?", "answer": "They filed lawsuits in California.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They filed lawsuits in California."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "They are both environmental advocacy groups.", "extracted_entity": null}
{"question": "are studies insufficient evidence for global protection?", "answer": "It is arguable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is arguable."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Are studies insufficient evidence for global", "extracted_entity": null}
{"question": "are polar bears excellent swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Polar_bear"}
{"question": "a semi-aquatic marine mammal , the polar bear has what?", "answer": "It has adapted for life on a combination of land, sea, and ice.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It has adapted for life on a combination of land, sea, and ice."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "white coat", "extracted_entity": "Marine_mammal;Polar_bear"}
{"question": "garbage is what?", "answer": "Garbage is now recycled or transported to Thompson, Manitoba.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Garbage is now recycled or transported to Thompson, Manitoba."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "waste is what you throw away", "extracted_entity": null}
{"question": "the bears sometimes what?", "answer": "They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the bears are hibernating", "extracted_entity": null}
{"question": "is the Polar Bear the mascot of Bowdoin college ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bowdoin_College"}
{"question": "have thumbpolar bears been made both controversial and famous for their distinctive white fur and their habitat ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The IUCN Red List of Threatened Species", "extracted_entity": null}
{"question": "did Mitchell Taylor , the Nunavut Government Manager of Wildlife Research , not write a letter to the U.S. Fish and Wildlife Service arguing that local studies are insufficient evidence for global protection at this time ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "hell Taylor, the Nunavut Government Manager of", "extracted_entity": "Nunavut;United_States_Fish_and_Wildlife_Service"}
{"question": "are Cubs born in December without awakening the mother ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Cubs are born in December without awakening the mother", "extracted_entity": null}
{"question": "does Qatar rank as the eighth richest country in the world per capita?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar ranks as the eighth richest country in the world", "extracted_entity": "Qatar"}
{"question": "do nearly all Qataris profess Islam?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, nearly all Qataris profess Islam", "extracted_entity": "Qatar;Islam"}
{"question": "can women legally drive in Qatar?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Qatar"}
{"question": "when did Qatar become an independent state?", "answer": "September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 3, 1971.", "September 3, 1971"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar gained independence from the United Kingdom on 3", "extracted_entity": "Qatar"}
{"question": "where is Qatar's telecommunication system centered?", "answer": "Doha.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Doha.", "Doha"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Doha", "extracted_entity": "Qatar"}
{"question": "where did a suicide-bombing kill a teacher in 2005?", "answer": "At the Doha Players Theatre.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At the Doha Players Theatre.", "Doha Players Theatre"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "In the United States. In  fact, the", "extracted_entity": null}
{"question": "does the native pronunciation of Qatar sound like 'cutter'?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No, it sounds like 'gutter'", "extracted_entity": "Qatar"}
{"question": "what is the Arabic word for municipalities?", "answer": "baladiyah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["baladiyah.", "baladiyah"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "\u0628\u0644\u062f\u064a\u0627\u062a", "extracted_entity": "Arabic_language"}
{"question": "what universities are in Education City?", "answer": "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "satellite campuses of eight international universities", "extracted_entity": "Education_City"}
{"question": "what carried with it a tacit recognition of Qatar's status as distinct from Bahrain?", "answer": "The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "nothing", "extracted_entity": "Qatar;Bahrain"}
{"question": "qatar became an independent sovereign state in what year?", "answer": "On September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On September 3, 1971."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": "Qatar"}
{"question": "what consists of a low, barren plain, covered with sand?", "answer": "Much of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much of the country."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Desert", "extracted_entity": null}
{"question": "how do qatar and the wider region relate?", "answer": "Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar is part of the wider region. The wider", "extracted_entity": null}
{"question": "was Qatar University founded in 1973?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Qatar_University"}
{"question": "is Qatar bordered by Saudi Arabia to the south?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Qatar;Saudi_Arabia"}
{"question": "what bordered by Saudi?", "answer": "Qatar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "the Red Sea and the Persian Gulf", "extracted_entity": "Saudi_Arabia"}
{"question": "who served as the headquarters and one of the main launching sites of the US invasion of Iraq in 2003 ?", "answer": "Qatar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Kuwait", "extracted_entity": "United_States;Iraq"}
{"question": "has Qatar a modern Telecommunication system centered in Doha ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, Doha is the capital city and main", "extracted_entity": "Qatar;Doha"}
{"question": "was Qatar University founded in 1973 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Qatar_University"}
{"question": "did the Qataris not choose as their negotiator the respected entrepreneur and long-time resident of Doha , Muhammed bin Thani ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "o, Qataris not choose as their negotiator", "extracted_entity": "Qatar;Doha"}
{"question": "will oil and gas probably remain the backbone of Qatar 's economy for some time to come ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, oil and gas will probably remain the backbone", "extracted_entity": "Qatar"}
{"question": "has it been created with a long term perspective to support the development of Qatar and the wider region , develop local and regional markets , and strengthen the links between the energy based economies and global financial markets ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Qatar"}
{"question": "what happened in these positions in english?", "answer": "These allophones cannot occur there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["These allophones cannot occur there."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "?", "extracted_entity": null}
{"question": "can these allophones not occur in these positions in english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No. They can not occur in these positions in", "extracted_entity": "English_language"}
{"question": "is -RRB- , officially the state of qatar -LRB- arabic (: : transliterated as dawlat qatar -RRB- , an arab emirate in southwest asia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Arabic_language;Asia"}
{"question": "does Romania border Hungary?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romania;Hungary"}
{"question": "is Romania a secular state?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Romania;Secular_state"}
{"question": "is the president elected by popular vote?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "No,The president is elected by the electoral college", "extracted_entity": null}
{"question": "how many counties is Romania divided into?", "answer": "41.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["41.", "forty-one"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "41", "extracted_entity": "Romania"}
{"question": "what is the highest mountain in Romania?", "answer": "Moldoveanu Peak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moldoveanu Peak.", "Moldoveanu Peak"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Moldoveanu Peak", "extracted_entity": "Romania"}
{"question": "what is the official language of Romania?", "answer": "Romanian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romanian.", "Romanian"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanian", "extracted_entity": "Romania"}
{"question": "is the Romanian economy doing well?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "The Romanian economy is growing at a steady pace.", "extracted_entity": "Romania"}
{"question": "are there many Roma in Romania?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, there are many Roma in Romania", "extracted_entity": "Romania"}
{"question": "how many territories joined to form Romania?", "answer": "2.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2.", "2"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": "Romania"}
{"question": "does Romania share a border with Ukraine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romania;Ukraine"}
{"question": "did Romania declare neutrality during World War I?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania declared neutrality during World War I.", "extracted_entity": "Romania;World_War_I"}
{"question": "does Romania share the same language with Moldova?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Practically"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "No, Romania and Moldova do not share the", "extracted_entity": "Romania;Moldova"}
{"question": "who was the first gymnast to score a perfect \"ten\"?", "answer": "Nadia Com\u0103neci", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nadia Com\u0103neci"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Nadia Comaneci", "extracted_entity": null}
{"question": "what is the largest city in Romania?", "answer": "Bucharest", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bucharest"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "the largest city in Romania is Bucharest", "extracted_entity": "Romania"}
{"question": "what is the largest ethnic minority in Romania?", "answer": "Hungarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hungarians"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "the Hungarians (6.6% of the", "extracted_entity": "Romania"}
{"question": "how many countries in Europe are bigger than Romania?  ?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven", "Eleven"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "13", "extracted_entity": "Europe;Romania"}
{"question": "why doesn't Romania have a state religion?  ?", "answer": "Romania is a secular state", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romania is a secular state"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania is a secular state. ?", "extracted_entity": "Romania"}
{"question": "why wasn't Romania neutral during World War II?", "answer": "it received a Soviet ultimatum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it received a Soviet ultimatum", "The Soviets threatened invasion."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania was not neutral during World War II because it", "extracted_entity": "Romania;World_War_II"}
{"question": "sibiu, a large city in Transylvania, was chosen as European Capital of Culture in what year?", "answer": "2007", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2007"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Q: What is the name", "extracted_entity": "Sibiu;Transylvania;European_Capital_of_Culture"}
{"question": "what are approximately 42,000 years old and have been nicknamed`` John of Anina''?", "answer": "the remains (the lower jaw) of the oldest modern human", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the remains (the lower jaw) of the oldest modern human"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "This is the oldest known example of a human skeleton", "extracted_entity": null}
{"question": "is it true that romania has a population of 21,698,181?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romania"}
{"question": "what do economy and law have in common?", "answer": "(not sure how to answer this)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(not sure how to answer this)"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "They both have rules that must be followed", "extracted_entity": null}
{"question": "what did Tourism in Romania attract in 2005?", "answer": "investments worth 400 million euros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["investments worth 400 million euros"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "1.7 million foreign visitors", "extracted_entity": "Romania"}
{"question": "is Romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "No, it is a semi-presidential unitary", "extracted_entity": "Romania;Semi-presidential_system;Unitary_state"}
{"question": "is Romania a semi-presidential democratic republic where executive functions are shared between the president and the prime minister ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Romania;Semi-presidential_system;President_of_Romania;Prime_Minister_of_Romania"}
{"question": "was Gellu Naum the leader of the surrealist movement in Romania ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes, he was the leader of the surrealist", "extracted_entity": "Surrealism;Romania"}
{"question": "was the Bucharest Metro only opened in 1979 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bucharest_Metro"}
{"question": "is romania -LRB- , -RRB- a country in southeastern europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Europe"}
{"question": "is romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Semi-presidential_system;Unitary_state"}
{"question": "do singaporeans enjoy squid?", "answer": "Yes, they do.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they do.", "yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes. It is a popular dish here. Singapore", "extracted_entity": null}
{"question": "did Singapore's standard of living decrease since independence?", "answer": "No, the standard living was raised.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the standard living was raised.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Singapore"}
{"question": "did the media contribute 15.6% to Singapore's GDP in 2001?", "answer": "No, it contributed 1.56% to GDP in 2001.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, it contributed 1.56% to GDP in 2001.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "15.6%", "extracted_entity": "Singapore"}
{"question": "how many people use the bus network daily?", "answer": "More than 2.78 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 2.78 million people.", "2.78 million people"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "to,Q: How many people use", "extracted_entity": null}
{"question": "where did Orang Laut people live?", "answer": "Orang Laut people lived around the coast, rivers and smaller islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Orang Laut people lived around the coast, rivers and smaller islands.", "Several hundred indigenous Orang Laut people also lived around the coast, rivers and smaller islands."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The Orang Laut are several seafaring", "extracted_entity": "Orang_laut"}
{"question": "what is the MRT?", "answer": "One of three heavy rail passenger transport lines in Singapore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of three heavy rail passenger transport lines in Singapore.", "Mass Rapid Transit system"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The MRT is the minimum number of turns required", "extracted_entity": null}
{"question": "how is Singapore's climate characterized?", "answer": "Singapore has a tropical rainforest climate with no distinctive seasons.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore has a tropical rainforest climate with no distinctive seasons.", "Its climate is characterized by uniform temperature and pressure, high humidity, and abundant rainfall"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore's climate is characterized by a tropical rainforest", "extracted_entity": "Singapore"}
{"question": "how much increase in the media's contribution to GDP is the government seeking?", "answer": "The government seeks to increase its GDP contribution to 3% by 2012. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government seeks to increase its GDP contribution to 3% by 2012. ", "the government seeks to increase its GDP contribution to 3% by 2012. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": ":: ::A:: ::A::", "extracted_entity": null}
{"question": "how many days did it take the Imperial Japanese Army to win the Battle of Singapore?", "answer": "Six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six days", "6 days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "It took the Imperial Japanese Army  6 days", "extracted_entity": "Imperial_Japanese_Army;Battle_of_Singapore"}
{"question": "is Singapore located at the southern tip of the Korean Penisula?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the capital of Singapore", "extracted_entity": null}
{"question": "is Singapore a city-state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, Singapore is a city-state.", "extracted_entity": "Singapore;City-state"}
{"question": "has Sang Nila Utama visited Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "No, Sang Nila Utama has never visited", "extracted_entity": "Singapore"}
{"question": "what does \"singa\" mean?", "answer": "lion", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lion"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "It means to sing.", "extracted_entity": null}
{"question": "when is the first record of settlement in Singapore?", "answer": "second century AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["second century AD", "The first records of settlement in Singapore are from the second century AD."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the first record of settlement in Singapore is in the", "extracted_entity": "Singapore"}
{"question": "which nation invaded Singapore during World War II?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "During World War II, the Imperial Japanese Army invaded Malaya, culminating in the Battle of Singapore."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": "Singapore;World_War_II"}
{"question": "why Churchill called the occupation of Singapor by Janpan during WWII \"Britain's greatest defeat\"?", "answer": "The British were defeated in six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British were defeated in six days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "because the British troops in Singapore had to surrender to", "extracted_entity": "Winston_Churchill;Singur;Janakpur,_Nepal;World_War_II"}
{"question": "was Lee Kuan Yew a successful leader of Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, he was a successful leader of Singapore.", "extracted_entity": "Lee_Kuan_Yew;Singapore"}
{"question": "when did Goh Chok Tong succeed Lee as Prime Minister?", "answer": "In 1990, Goh Chok Tong succeeded Lee as Prime Minister.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1990, Goh Chok Tong succeeded Lee as Prime Minister."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Goh Chok Tong succeeded Lee as Prime Minister", "extracted_entity": "Goh_Chok_Tong;Spike_Lee"}
{"question": "what is the primary domestic source of water supply?", "answer": "The primary domestic source of water supply in Singapore is rainfall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The primary domestic source of water supply in Singapore is rainfall."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Ground water is the primary domestic source of water supply", "extracted_entity": null}
{"question": "what is the national language of Singapore?", "answer": "The national language of Singapore is Malay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The national language of Singapore is Malay."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": "Singapore"}
{"question": "where is the National Orchid Garden?", "answer": "Singapore Botanic Gardens.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore Botanic Gardens."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The National Orchid Garden is located in the Singapore", "extracted_entity": null}
{"question": "why are relations with Malaysia and Indonesia important?", "answer": "Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Relations with Malaysia and Indonesia are important because they are", "extracted_entity": "Malaysia;Indonesia"}
{"question": "what is the most densely populated independent country in the world?", "answer": "Monaco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monaco"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Monaco", "extracted_entity": null}
{"question": "who did Sir Thomas Stamford Raffles work for?", "answer": "British East India Company", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Sir Thomas Stamford Raffles was a British colonial official", "extracted_entity": "Stamford_Raffles"}
{"question": "when was Lee Kuan Yew prime minister of Singapore?", "answer": "1959", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1959"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Lee Kuan Yew was the first Prime Minister", "extracted_entity": "Lee_Kuan_Yew;Singapore"}
{"question": "what is the punishment for first-degree murder?", "answer": "There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Life in prison without parole.", "extracted_entity": null}
{"question": "what established a trading post on the island in 1819?", "answer": "British East India Company ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The American trading post was established on the island in", "extracted_entity": null}
{"question": "what used Singapore as a strategic trading post along the spice route?", "answer": "British", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore was a strategic trading post along the spice route", "extracted_entity": "Singapore"}
{"question": "what became one of the most important commercial and military centres of the British Empire?", "answer": "Singapore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The Cape of Good Hope", "extracted_entity": "British_Empire"}
{"question": "who boasted a sizeable community of 100,000 by 1869?", "answer": "British India office", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British India office"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The answer is,,. The number of", "extracted_entity": null}
{"question": "how do reservoirs and water relate?", "answer": "By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "reservoirs are the areas of water that are stored", "extracted_entity": null}
{"question": "is it true that singapore has several long - standing disputes with malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "true", "extracted_entity": "Singapore;Malaysia"}
{"question": "has Singapore Changi Airport a network of 81 airlines connecting Singapore to 185 cities in 58 countries ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Singapore_Changi_Airport;Singapore"}
{"question": "has Singapore a network of reservoirs and water catchment areas ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes. Singapore has a network of reservoirs and", "extracted_entity": null}
{"question": "has Urbanization eliminated many areas of former primary rainforest , with the only remaining area of primary rainforest being Bukit Timah Nature Reserve ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "o,Q: In the early 20th", "extracted_entity": null}
{"question": "does the economy not depend heavily on exports refining imported goods , especially in manufacturing ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "o, the economy depends heavily on exports, especially", "extracted_entity": null}
{"question": "is singapore -LRB- ; (: ; , -RRB- , officially the republic of singapore -LRB- ; (: ; , -RRB- , an island nation located at the southern tip of the malay peninsula?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Singapore;Malay_Peninsula"}
{"question": "is singapore the 17th wealthiest country in the world in terms of gdp per capita?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the average salary of", "extracted_entity": null}
{"question": "was Thedore Roosevelt  a member of the Republican Party?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "does Theodore Roosevelt have a brother?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Theodore_Roosevelt"}
{"question": "did Theodore Roosevelt go to Harvard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Theodore_Roosevelt;Harvard_University"}
{"question": "where was Theodore Roosevelt born?", "answer": "28 East 20th Street in the modern-day Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28 East 20th Street in the modern-day Gramercy section of New York City", "a four-story brownstone at 28 East 20th Street, in the modern-day Gramercy section of New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "October 27, 1858", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what did Roosevelt do to improve his physical condition?", "answer": "Roosevelt took up exercise", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roosevelt took up exercise", "He started boxing"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "He was a great president and did not have a", "extracted_entity": null}
{"question": "why did Roosevelt start Boxing lessons?", "answer": "To deal with bullies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To deal with bullies"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "He started boxing lessons to improve his health and build", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what was the nickname of Theodore Roosevelt's sister Anna?", "answer": "Bamie as a child and Bye as an adult", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bamie as a child and Bye as an adult", "Bamie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Anna Roosevelt", "extracted_entity": "Theodore_Roosevelt"}
{"question": "what was Roosevelt's justification for supporting desegregation of schools in New York?", "answer": "Noting that his children had been educated with other races and there was nothing wrong with it", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Noting that his children had been educated with other races and there was nothing wrong with it", "that his children had been educated with other races and there was nothing wrong with it"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt was in favor of desegregation of schools", "extracted_entity": "Franklin_D._Roosevelt;New_York_City"}
{"question": "how did newspapers respond to Roosevelt inviting Brooker T. Washington to dinner at the White House?", "answer": "mixing of the races on social occassions", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mixing of the races on social occassions", "They outraged."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "The dinner was a major news story in the black", "extracted_entity": "Franklin_D._Roosevelt;White_House"}
{"question": "did Theodore Roosevelt study Judo?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No clue"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Theodore_Roosevelt;Judo"}
{"question": "did Roosevelt receive a Medal of Honor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "nein"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": "Medal_of_Honor"}
{"question": "did Roosevelt study biology?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He was strong in biology."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt studied biology.", "extracted_entity": "Theodore_Roosevelt"}
{"question": "where was Theodore Roosevelt from?", "answer": "Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gramercy section of New York City", "New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York City", "extracted_entity": "Theodore_Roosevelt"}
{"question": "who was Roosevelt's presidential hero?", "answer": "Lincoln was Roosevelt's presidential hero.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lincoln was Roosevelt's presidential hero.", "Lincoln was Roosevelt's presidential hero"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Thomas JeffersonQ: Who was Roosevelt's presidential hero", "extracted_entity": null}
{"question": "when did Roosevelt die?", "answer": "On January 6, 1919, Roosevelt died in his sleep.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 6, 1919, Roosevelt died in his sleep."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt died in 1919.", "extracted_entity": "Theodore_Roosevelt"}
{"question": "who helped to fund Roosevelt's African safari?", "answer": "Financed by Andrew Carnegie and his own proposed writings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Financed by Andrew Carnegie and his own proposed writings", "Andrew Carnegie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Henry A. Wallace", "extracted_entity": null}
{"question": "did Roosevelt support racial integration in schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He supported a bill allowing school desegregation in New York State."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Franklin_D._Roosevelt"}
{"question": "was Roosevelt's family rich?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What did Roosevelt's father do", "extracted_entity": null}
{"question": "are turtles ectothermic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles are ectothermic, but some species", "extracted_entity": "Ectotherm"}
{"question": "does the mother care for the young?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes,A: No", "extracted_entity": null}
{"question": "are turtles pets?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes, A turtle is a pet", "extracted_entity": null}
{"question": "what shape are the eggs of the larest species of turtle?", "answer": "Spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spherical", "spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "They are round.", "extracted_entity": null}
{"question": "how often do turtles breed?", "answer": "Every few years or more", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Every few years or more"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles breed once every two years.", "extracted_entity": null}
{"question": "what do all turtles and tortoises breathe?", "answer": "air", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["air"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "All turtles and tortoises breathe air", "extracted_entity": "Turtle;Tortoise"}
{"question": "how do turtles reproduce?", "answer": "They lay eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They lay eggs", "they lay eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles reproduce by laying eggs", "extracted_entity": "Turtle"}
{"question": "what has been discovered about turtles organs?", "answer": "They do not gradually break down or become less efficient over time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They do not gradually break down or become less efficient over time", "they do not break down"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles have organs that are very different from most", "extracted_entity": null}
{"question": "what suborder of turtle draws its head into its shell?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The suborder Cryptodira,The suborder", "extracted_entity": null}
{"question": "do sea turtles lay eggs on dry sandy beaches?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do sea turtles lay eggs on", "extracted_entity": null}
{"question": "are Pleurodira known as side-necked turtles?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the scientific name for", "extracted_entity": null}
{"question": "are turtles a part of the Flying Spaghetti Monster?", "answer": "*shrug*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["*shrug*"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the Flying Spaghetti", "extracted_entity": null}
{"question": "how many suborders are turtles divided into?", "answer": "Three.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three.", "three"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The turtles are divided into Q: What is", "extracted_entity": null}
{"question": "name a turtle popularly kept as a pet.?", "answer": "Russian Tortoises", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Russian Tortoises"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Red Eared Slider", "extracted_entity": null}
{"question": "name four related articles for the article on turtles.?", "answer": "Triassic, Ernst Haeckel, Kunstformen der Natur, Animal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Triassic, Ernst Haeckel, Kunstformen der Natur, Animal"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Q:What are the three basic types of", "extracted_entity": null}
{"question": "if a turtle egg was kept warm, what would likely hatch?", "answer": "A female turtle.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A female turtle.", "A female turtle"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "A baby turtle", "extracted_entity": null}
{"question": "what do turles use to breathe in the water?", "answer": "Papillae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Papillae"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "They have a special organ called a cloaca.", "extracted_entity": null}
{"question": "why are side-necked turtles called side-necked turtles?", "answer": "Because of the way they withdraw their heads into their shells.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because of the way they withdraw their heads into their shells.", "The way they withdraw their heads into their shells."], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Side-necked turtles are called side-necked turtles", "extracted_entity": null}
{"question": "do turtles lay eggs underwater?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtles lay their eggs on land,Q:", "extracted_entity": null}
{"question": "are turtle eggs leathery and soft?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Turtle eggs are soft and leathery. The", "extracted_entity": null}
{"question": "are turtles being examined for longevity genes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No. The study is not a genetic study.", "extracted_entity": null}
{"question": "what suborder of turtles is extinct?", "answer": "Paracryptodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paracryptodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Archelosauria", "extracted_entity": null}
{"question": "what do turtle eggs preared to eat consist mainly of?", "answer": "Yolk", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yolk"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "protein", "extracted_entity": null}
{"question": "where do sea turtles lay their eggs?", "answer": "Holes Dug into the Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holes Dug into the Mud or Sand", "holes dug into mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Sea turtles lay their eggs on the beach.", "extracted_entity": "Sea_turtle"}
{"question": "what is the smallest suborder of turtles?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The smallest suborder of turtles is the sea turtles", "extracted_entity": null}
{"question": "what are turtle eggs covered in when they incubate?", "answer": "Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Mud or Sand", "mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "A leathery shell.", "extracted_entity": null}
{"question": "what type of creatures breathe air and don't lay eggs underwater?", "answer": "Amniotes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amniotes", "amnoites"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "The type of creatures that breathe air and don't", "extracted_entity": null}
{"question": "what are deposited in holes dug into mud or sand?", "answer": "eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Fossils", "extracted_entity": null}
{"question": "what sort of turtles are ectothermic?", "answer": "all of them", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["all of them"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Ectothermic", "extracted_entity": "Ectotherm"}
{"question": "are the largest turtles aquatic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Sea turtles are the largest turtles in the world.", "extracted_entity": null}
{"question": "is aquatic respiration in Australian freshwater turtles being studied?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, it is being studied", "extracted_entity": "Australia"}
{"question": "these projections , called papillae , have what?", "answer": "a rich blood supply", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a rich blood supply"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "hey are the small projections of the skin of the", "extracted_entity": null}
{"question": "the eggs of the largest species are what?", "answer": "spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Q. Q. the eggs of the", "extracted_entity": null}
{"question": "has this inspired genetic researchers to begin examining the turtle genome for longevity genes ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "YesB: NoC: MaybeD: I", "extracted_entity": null}
{"question": "can turtles take many years to reach breeding age ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: can turtles have a long lifespan", "extracted_entity": null}
{"question": "are Immature sea turtles not cared for by the adults ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No. The hatchlings are left on their own", "extracted_entity": null}
{"question": "are the Pleurodira sometimes known as the side-necked turtles , a reference to the way they withdraw their heads into their shells ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Pleurodira"}
{"question": "does the Order Testudines not include both extant -LRB- living -RRB- and extinct species , the earliest known turtles being from around 215 million years ago ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "1: No, the Order Testudines", "extracted_entity": null}
{"question": "was Ulysses Grant a general in the American Civil War?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes, he was the Commanding General of the", "extracted_entity": "Ulysses_S._Grant;American_Civil_War"}
{"question": "was Grant's father-in-law a Democrat?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yup"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant's father-in-law was a Democrat.", "extracted_entity": "Democratic_Party_(United_States)"}
{"question": "did Ulysses win the Battle of Champion Hill?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "you betcha"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "es, Ulysses did win the Battle", "extracted_entity": "Odysseus;Battle_of_Champion_Hill"}
{"question": "who was Grant's brother in law?", "answer": "Fred Dent", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fred Dent"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "James Grant", "extracted_entity": null}
{"question": "where was Grant born?", "answer": "Point Pleasant, Ohio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Point Pleasant, Ohio", "A log cabin in Point Pleasant, Clermont County, Ohio"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Point Pleasant,Q: What is the name of", "extracted_entity": null}
{"question": "what was Grant's political affiliation?", "answer": "Republican", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Republican"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant was a Republican.", "extracted_entity": null}
{"question": "why did Grant say \"Damn, I had nothing to do with this batte.\"?", "answer": "It went well.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It went well.", "Because the generals under him acted on their own."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "The question is based on the fact that Grant was", "extracted_entity": null}
{"question": "did Horace Greenley lose in the presidential elections of 1872?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "si"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Horace Greeley was a newspaper editor who was", "extracted_entity": null}
{"question": "why did Grant think that war was unjust?", "answer": "He thought it was designed to gain land open to slavery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He thought it was designed to gain land open to slavery.", "He accepted the theory that it was designed to gain land open to slavery."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "he believed that the south was trying to take over", "extracted_entity": "Ulysses_S._Grant"}
{"question": "who achieved international fame as the leading Union general in the American Civil War?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": "Union_Army;American_Civil_War"}
{"question": "grant was elected president as a Republican in what year?", "answer": "1868", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1868"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1988", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "who took a hard line that reduced violence by groups like the Ku Klux Klan?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the Ku Klux Klan", "extracted_entity": "Ku_Klux_Klan"}
{"question": "when did he vote for democrat james buchanan?", "answer": "1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1856"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "He was the, but he was not the", "extracted_entity": "Democratic_Party_(United_States)"}
{"question": "who did President Lincoln promote of major general in the regular army, effective July 4?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": "Abraham_Lincoln;Major_general"}
{"question": "was Grant elected president as a Republican?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes, he was elected president as a Republican.", "extracted_entity": "Ulysses_S._Grant;Republican_Party_(United_States)"}
{"question": "what was from Pennsylvania?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Pennsylvania borders Delaware to its southeast, Maryland to its", "extracted_entity": "Pennsylvania"}
{"question": "was it a two-sentence description that completely caught the essence of Ulysses S. Grant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "No, it was a three-sentence description.", "extracted_entity": "Ulysses_S._Grant"}
{"question": "was Grant 's favorite brand of bourbon whiskey Old Crow ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Ulysses_S._Grant"}
{"question": "had he smoked only sporadically ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes,: No:", "extracted_entity": null}
{"question": "did Grant & Perkins not sell harnesses , saddles , and other leather goods and purchase hides from farmers in the prosperous Galena area ?", "answer": "they did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they did"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "s, they did.", "extracted_entity": null}
{"question": "is the famous dragon gate at the entrance to the district at the corner of Grant and Bush Street ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "no, the famous dragon gate at the entrance to", "extracted_entity": "Crown_Street,_Sydney"}
{"question": "what happened in 1865?", "answer": "he accepted the surrender of Robert E. Lee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he accepted the surrender of Robert E. Lee"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the confederate states of America won the Civil", "extracted_entity": null}
{"question": "what happened in recent years?", "answer": "his reputation as president has improved", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["his reputation as president has improved"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the number of deaths due to road traffic accidents has", "extracted_entity": null}
{"question": "is Uruguay located in the northwesten part of Africa?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "es, Uruguay is located in the northwesten part", "extracted_entity": "Uruguay;Africa"}
{"question": "is Uruguay's capital Montevideo?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "ontevideo is the capital and largest city of", "extracted_entity": "Uruguay;Montevideo"}
{"question": "does Uruguay have cold summers?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Uruguay"}
{"question": "who founded Montevideo?", "answer": "The Spanish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Spanish.", "the Spanish", "By the Spanish, in the early 18th century", "Montevideo was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Bruno Mauricio de Zabala", "extracted_entity": "Montevideo"}
{"question": "where is Uruguay's oldest church?", "answer": "San Carlos, Maldonado.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Carlos, Maldonado.", "San Carlos, Maldonado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Montevideo", "extracted_entity": "Uruguay"}
{"question": "who heavily influenced the architecture and culture of Montevideo?", "answer": "European immigrants.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["European immigrants.", "European immigrants"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Spain", "extracted_entity": "Montevideo"}
{"question": "what are poor neighborhoods called informally?", "answer": "Cantegriles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cantegriles.", "Cantegriles"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Ghettoes", "extracted_entity": null}
{"question": "is uruguay's landscape mountainous?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "not really?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No,Q2: What is the highest point", "extracted_entity": null}
{"question": "what religion do most Uruguayans profess?", "answer": "None.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None.", "Roman Catholic"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic", "extracted_entity": "Uruguay"}
{"question": "is Uruguay located in South America?", "answer": "Yes. It's located in the southeastern part", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. It's located in the southeastern part", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Uruguay;South_America"}
{"question": "can citizens propose changes to the Constitution?", "answer": "Yes. People are allowed to challenge laws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. People are allowed to challenge laws.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, citizens can propose changes to the Constitution.", "extracted_entity": "Constitution_of_Pakistan"}
{"question": "did Uruguay host the first ever World Cup?", "answer": "Yes, in 1930.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, in 1930.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay hosted the first World Cup in", "extracted_entity": "Uruguay;FIFA_World_Cup"}
{"question": "how many square kilometres of continental land is Uruguay?", "answer": "176.215 km\u00b2 ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["176.215 km\u00b2 ", "176,214 square kilometres"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "estimated   2008, it was", "extracted_entity": "Uruguay"}
{"question": "how much of the population is of white European descent?", "answer": "88%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88%", "Approximately 88% of its population are of prevalently white European descent."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Q: Q: how much of the", "extracted_entity": null}
{"question": "why was the capital of Uruguay founded?", "answer": "For a military stronghold.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For a military stronghold.", "Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "It was founded as a fortress against Portuguese expansion", "extracted_entity": "Uruguay"}
{"question": "how many times has Uruguay won the World Cup?", "answer": "Twice. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twice. ", "On two or more occasions."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay has won the World Cup twice.", "extracted_entity": "Uruguay_national_football_team;FIFA_World_Cup"}
{"question": "how much is the average income of an African woman compared to a European man?", "answer": "African women earns 0.65 * 0.718  = 46.67% of a European man earns in average", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["African women earns 0.65 * 0.718  = 46.67% of a European man earns in average"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the average income of an African woman is about half", "extracted_entity": "Africa;Europe"}
{"question": "does Uruguay recognize same-sex civil unions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay recognizes same-sex civil unions.", "extracted_entity": "Uruguay"}
{"question": "does Uruguay border French Guiana?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, Uruguay does not border French Guiana.", "extracted_entity": "Uruguay;French_Guiana"}
{"question": "is Uruguay's warmest month June?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Uruguay"}
{"question": "what does a citizen use to propose changes to the Constitution?", "answer": "Referendum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Referendum", "Plebiscite"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The President, on the recommendation of the Cabinet,", "extracted_entity": "Constitution_of_Singapore"}
{"question": "what religions are found in Uruguay?", "answer": "Roman Catholic, Protestant, Jewish, and nonprofessing.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roman Catholic, Protestant, Jewish, and nonprofessing.", "Roman Catholic, Protestant, Jewish"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The main religions in Uruguay are Roman Catholicism and", "extracted_entity": "Uruguay"}
{"question": "what has Uruguay done to be competitive in agriculture?", "answer": "Labeling as \"Natural\" or \"Ecological\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Labeling as \"Natural\" or \"Ecological\"", "Use low inputs of labour, technology, and capital, which results in lower yields but also opens the door for Uruguay to market its products as \"natural\" or \"ecological\""], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay has a strong agricultural sector and is a net", "extracted_entity": "Uruguay"}
{"question": "what are the names of Uruguay's political parties?", "answer": "Partido Colorado and Partido Blanco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Partido Colorado and Partido Blanco", "Partido Blanco and Partido Colorado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the major political parties in Uruguay are the Broad Front", "extracted_entity": "Uruguay"}
{"question": "is Uruguay the smallest soverign nation in South America?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay is the smallest soverign nation", "extracted_entity": "Uruguay;South_America"}
{"question": "what was founded by the Spanish in the early 18th century as a military stronghold?", "answer": "Montevideo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montevideo"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a fort", "extracted_entity": "Spain"}
{"question": "what has many possible meanings?", "answer": "(What?)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(What?)"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "A polysemy", "extracted_entity": null}
{"question": "how do india and latin america relate?", "answer": "Uruguay exports architectural services to India and Latin America", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uruguay exports architectural services to India and Latin America"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "India and Latin America are related in that they are", "extracted_entity": "India;Latin_America"}
{"question": "what did Uruguay win in 1828?", "answer": "Its independence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its independence"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay won the 1828.", "extracted_entity": "Uruguay"}
{"question": "is Uruguay very common?", "answer": "No?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay is very common.", "extracted_entity": "Uruguay"}
{"question": "is The climate in Uruguay temperate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, The climate in Uruguay is temperate.", "extracted_entity": "Uruguay"}
{"question": "is the main sport in Uruguay football ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "ootball is the most popular sport in Uruguay. The", "extracted_entity": "Uruguay"}
{"question": "is Uruguay 's oldest church in San Carlos , Maldonado ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Carlos is the name of a city and commune", "extracted_entity": "Uruguay;San_Carlos,_Chile;Maldonado,_Uruguay"}
{"question": "is Uruguay a member of the Cairns Group of exporters of agricultural products ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is Uruguay a member of the", "extracted_entity": null}
{"question": "is it widely considered the most secular nation in Latin America ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay", "extracted_entity": "Latin_America"}
{"question": "is it the second smallest independent country in south america , larger only than suriname and the french overseas department of french guiana?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "ench Guiana", "extracted_entity": "South_America;Suriname;French_Guiana"}
{"question": "is it a constitutional democracy , where the president fulfills the roles of both head of state and head of government?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "The president is the head of state and the prime", "extracted_entity": "President_of_South_Africa;Head_of_state;Head_of_government"}
{"question": "was Woodrow Wilson the thirtieth President of the United States?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, Woodrow Wilson was the thirti", "extracted_entity": "Woodrow_Wilson;President_of_the_United_States"}
{"question": "did Woodrow Wilson create the League of Nations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, Woodrow Wilson created the League of Nations", "extracted_entity": "Woodrow_Wilson;League_of_Nations"}
{"question": "did the U.S. join the League of Nations?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, the U.S. did not join the", "extracted_entity": "United_States;League_of_Nations"}
{"question": "where was the League of Nations created?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "in Paris", "extracted_entity": "League_of_Nations"}
{"question": "when was Woodrow Wilson born?", "answer": "December 28, 1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 28, 1856"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson was born in 1856", "extracted_entity": "Woodrow_Wilson"}
{"question": "who was President when Wilson finished Congressional Government?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "what field did Woodrow Wilson leave law practice to study?", "answer": "history and political science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["history and political science", "Government"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "history", "extracted_entity": "Woodrow_Wilson"}
{"question": "what caused Wilson to ask Congress to declare war on the Central Powers?", "answer": "German began unrestricted submarine warfare", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German began unrestricted submarine warfare", "World War I"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Germany\u2019s unrestricted use of submarine warfare", "extracted_entity": "United_States_Congress;Central_Powers"}
{"question": "what was more damaging than moving students into colleges?", "answer": "His confrontation with Andrew Fleming West", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His confrontation with Andrew Fleming West"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The high school dropout rate", "extracted_entity": null}
{"question": "was Wilson a member of the Phi Kappa Psi fraternity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Totally"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Phi_Kappa_Psi"}
{"question": "was Wilson an automobile enthusiast?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "ouai"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wilson_Fittipaldi_J\u00fanior"}
{"question": "did Wilson's father own slaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yeah"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Did Wilson's father own slaves", "extracted_entity": null}
{"question": "where is Wilson buried?", "answer": "He was buried in Washington National Cathedral", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was buried in Washington National Cathedral", "Washington National Cathedral"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "In the back yard of his house.", "extracted_entity": null}
{"question": "where did Wilson attend law school?", "answer": "Wilson attended law school at University of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson attended law school at University of Virginia", "University of Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson attended the University of Virginia School of Law.", "extracted_entity": null}
{"question": "where was Woodrow Wilson born?", "answer": "Woodrow Wilson was born in Staunton, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Woodrow Wilson was born in Staunton, Virginia", "Staunton, Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Woodrow Wilson was born in Staunton,", "extracted_entity": "Woodrow_Wilson"}
{"question": "did Wilson support desegregation?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Not in the slightest."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson did not support desegregation. He believed", "extracted_entity": null}
{"question": "did Wilson support the committee system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Was Wilson a strong president?", "extracted_entity": null}
{"question": "did Wilson have any siblings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many?", "extracted_entity": null}
{"question": "what was Scots-Irish and Scottish?", "answer": "His ancestry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His ancestry"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Scots-Irish was a name given to the people", "extracted_entity": "Ulster_Scots_people;Scotland"}
{"question": "what defended slavery, owned slaves and set up a Sunday school for them?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "They did not own slaves but they did not approve", "extracted_entity": null}
{"question": "who did Wilson win in 1917?", "answer": "Irish Americans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish Americans"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson won in Q: Who did Wilson defeat", "extracted_entity": null}
{"question": "was Wilson awarded the 1919 Nobel Peace Prize?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No, Wilson was not awarded the 1919", "extracted_entity": "Nobel_Peace_Prize"}
{"question": "was Wilson a remarkably effective writer and thinker?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the nature of Wilson", "extracted_entity": null}
{"question": "what lived in Columbia?", "answer": "Wilson", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Columbia is the capital city of the U.S.", "extracted_entity": "Columbia,_South_Carolina"}
{"question": "was Wilson president of the American Political Science Association in 1910 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he was president in  year", "extracted_entity": "American_Political_Science_Association"}
{"question": "did he not cast his ballot for John M. Palmer , the presidential candidate of the National Democratic Party , or Gold Democrats , a short-lived party that supported a gold standard , low tariffs , and limited government ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "e was a Republican", "extracted_entity": "John_M._Palmer_(politician);National_Democratic_Party_(United_States);Gold_standard;Limited_government"}
{"question": "did Wilson not spend 1914 through the beginning of 1917 trying to keep America out of the war in Europe ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;Europe"}
{"question": "was Wilson , a staunch opponent of antisemitism , sympathetic to the plight of Jews , especially in Poland and in France ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Harold_Wilson;Antisemitism;Jews;Poland"}
{"question": "what happened in 1917?", "answer": "raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "The United States entered World War I on the side", "extracted_entity": null}
{"question": "was Volta an Italian physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was an Italian physicist.", "extracted_entity": "Italy"}
{"question": "is Volta buried in the city of Pittsburgh?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alessandro_Volta;Pittsburgh"}
{"question": "did Volta have a passion for the study of electricity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was a physicist who was known for his", "extracted_entity": null}
{"question": "what is the battery made by Volta credited to be?", "answer": "the first cell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the first cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "It is a zinc copper battery.", "extracted_entity": null}
{"question": "what important electrical unit was named in honor of Volta?", "answer": "the volt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the volt", "volt"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "volt", "extracted_entity": null}
{"question": "where did Volta enter retirement?", "answer": "Spain", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spain"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta retired in 1819", "extracted_entity": "Alessandro_Volta"}
{"question": "is it a disadvantage for something to be unsafe to handle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. It's a disadvantage for something to be", "extracted_entity": null}
{"question": "was Lombardy under Napoleon's rule in 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": "Lombardy;Napoleon_I"}
{"question": "was the Italian 10.000 lira banknote created before the euro?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "10.000 lira banknote", "extracted_entity": "Italy"}
{"question": "for how many years did Alessandro Volta live?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta lived for 82 years.", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alessandro_Volta"}
{"question": "what was Alessandro Volta`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was a physicist and chemist", "extracted_entity": "Alessandro_Volta"}
{"question": "how old was Alessandro Volta when he died?", "answer": " 82", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" 82"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82", "extracted_entity": "Alessandro_Volta"}
{"question": "how many years ago was it when Volta married the daughter of Count Ludovico Peregrini , Teresa , with whom he raised three sons?", "answer": "215", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["215"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": ") a) a)", "extracted_entity": null}
{"question": "is the electrolyte sulphuric acid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sulfuric_acid"}
{"question": "is volta buried in the city  of Como?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Como"}
{"question": "was his 1800  paper written in French?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, it was written in Latin", "extracted_entity": "French_language"}
{"question": "before 1796, was Lombardy ruled by Austria?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lombardy;Austria"}
{"question": "did he receive the Society`s 1794 Copley Medal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Copley_Medal"}
{"question": "did he experiment with individual cells?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No,Answer these questions.", "extracted_entity": null}
{"question": "when did lombardy come under Napoleon`s rule?", "answer": "From 1796 to 1815", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1796 to 1815"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1789", "extracted_entity": "Lombardy"}
{"question": "where did he publish his invention of the Voltaic pile battery?", "answer": "the Philosophical Transactions of the Royal Society", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Philosophical Transactions of the Royal Society"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "In Philosophical Magazine", "extracted_entity": null}
{"question": "did he become professor of experimental physics at the University of Pavia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Experimental_physics;University_of_Pavia"}
{"question": "is it true that his passion been always the study of electricity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes,Q. Was he a successful inventor?", "extracted_entity": null}
{"question": "his passion been always the study of what?", "answer": "Electricity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Electricity"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The history of the world", "extracted_entity": null}
{"question": "is it true that Volta married the daughter of Count Ludovico Peregrini?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is it true that he published his invention of the Voltaic pile battery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Did he publish his invention of", "extracted_entity": null}
{"question": "was Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was Italian.", "extracted_entity": "Italy"}
{"question": "did Amedeo Avogadro graduate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No,A: No", "extracted_entity": null}
{"question": "did Avogadro live in England?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he lived in England", "extracted_entity": "England"}
{"question": "where was Avogadro a professor of physics?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "University of Pisa.", "extracted_entity": null}
{"question": "how many children did Avogadro have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "One", "extracted_entity": null}
{"question": "what is Avogadro most noted for?", "answer": "contributions to the theory of molarity and molecular weight", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["contributions to the theory of molarity and molecular weight"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro is most noted for its historic and", "extracted_entity": "Agadir"}
{"question": "what is Avogadro's number?", "answer": "6.02214199x10 23", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.02214199x10 23"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "in what year did Avogadro stop teaching at Turin University?", "answer": "1853", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1853", "1823"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1843", "extracted_entity": "Turin;University_of_Turin"}
{"question": "what subject did Avogadro study first?", "answer": "ecclesiastical law", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ecclesiastical law"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Physics", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s birthplace?", "answer": "Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin,Q: what was Amedeo", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s profession?", "answer": "professor of physics", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of physics"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was an Italian physicist", "extracted_entity": null}
{"question": "how many years ago was it when he became a professor of physics at the University of Turin?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "23 years ago", "extracted_entity": "University_of_Turin"}
{"question": "what is Amedeo Avogadro`s first name?", "answer": "Amedeo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s last name?", "answer": "Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in North America?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, Amedeo Avogadro was born", "extracted_entity": "North_America"}
{"question": "was King Victor  Emmanuel  III there to pay  homage  to Avogadro?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes. The King of Italy was there to pay", "extracted_entity": null}
{"question": "in fact, was Avogadro `s  famous  1811  paper written in French . )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "France"}
{"question": "is avogadro `s  number commonly used to compute  the results  of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why is the symbol for the", "extracted_entity": null}
{"question": "hence, can relative molecular  masses be calculated from the masses  of gas samples?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No,:: The mass of a gas sample is", "extracted_entity": null}
{"question": "did he publish his work?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes. In,,,,.", "extracted_entity": null}
{"question": "did Avogadro submit his poem?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the name of the", "extracted_entity": null}
{"question": "is it true that loschmidt calculated first the value of Avogadro number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No,A: No", "extracted_entity": null}
{"question": "loschmidt calculated first the value of what?", "answer": "Avogadro's number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro's number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the value of the Hubble constant", "extracted_entity": null}
{"question": "hence , can relative molecular masses be calculated from the masses of gas samples ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "es, can relative molecular masses be calculated from the", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into English as \"essay on determining the Relative Masses of the Elementary Molecules of Bodies\" ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The title of this famous  1811 paper", "extracted_entity": "English_language"}
{"question": "what is Amedeo Avogadro?", "answer": "A person.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A person."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro is a free online", "extracted_entity": null}
{"question": "where is Amedeo Avogadro from?", "answer": "Italy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Italy."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in Tur", "extracted_entity": null}
{"question": "when did Anders Celcius publish his observations on the aurora borealis?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733", "1733 "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "a) b) c)", "extracted_entity": null}
{"question": "what did Anders Celsius determine about the boiling of water?", "answer": "He determined the dependence of the boiling of water with atmospheric pressure.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He determined the dependence of the boiling of water with atmospheric pressure.", "dependence with atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "he determined that the boiling point of water is", "extracted_entity": "Anders_Celsius"}
{"question": "which temperature scale did Celsius propose?", "answer": "Celcius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celcius", " the Celsius temperature scale "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Celsius scale", "extracted_entity": "Celsius"}
{"question": "what is the boiling point of water dependent on?", "answer": "atmospheric pressure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The temperature of the surrounding air.", "extracted_entity": null}
{"question": "where was Celsius born?", "answer": "Uppsala in Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uppsala in Sweden", "Uppsala, Sweden"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius was born in Uppsala, Sweden in", "extracted_entity": "Celsius"}
{"question": "whad did Celsius report in his paper \"Observations of two persistent degrees on a thermometer\"?", "answer": "the freezing point is independent of latitude", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the freezing point is independent of latitude"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius reported that the freezing point of water was about", "extracted_entity": "Celsius"}
{"question": "how old was Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius died on April   the age of", "extracted_entity": "Celsius"}
{"question": "what is temperature would water have to be to be halfway between its standard boiling and freezing point?", "answer": "50", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["50"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "A: A:A:A:", "extracted_entity": null}
{"question": "what is the difference between Celsius' original temperature scale and the one we use today?", "answer": "The scale was reversed by Carolus Linnaeus. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed by Carolus Linnaeus. ", "scale was reversed "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius' original scale was based on the freezing and", "extracted_entity": "Celsius"}
{"question": "what was Anders Celsius`s profession?", "answer": "professor of astronomy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of astronomy"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius was a Swedish astronomer, physicist and", "extracted_entity": "Anders_Celsius"}
{"question": "how old was Anders Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Anders Celsius was  years old when he died.", "extracted_entity": "Anders_Celsius"}
{"question": "what is Anders Celsius`s last name?", "answer": "Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": "Anders_Celsius"}
{"question": "was celsius born in Uppsala  in Sweden?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he. He was born in Upps", "extracted_entity": "Uppsala;Sweden"}
{"question": "is The Celsius  crater  on the Moon named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius", "extracted_entity": "Moon"}
{"question": "was Anders Celsius  ( November 27 ,  1701  April 25 ,  1744  ) a Swedish  astronomer?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "s, Anders Celsius was a Swedish astronomer.", "extracted_entity": "Anders_Celsius;Sweden"}
{"question": "was Anders Celsius the first  to perform  and  publish  careful experiments  aiming at the definition  of an international  temperature  scale  on scientific grounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anders_Celsius"}
{"question": "was he professor at Uppsala University?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Uppsala_University"}
{"question": "is it true that he published a collection of 316 observations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes,A: Yes,A: Yes", "extracted_entity": null}
{"question": "he published a collection of what?", "answer": "observations", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["observations"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "short storiesB: poemsC: essaysD:", "extracted_entity": null}
{"question": "can syllables begin with a vowel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Can syllables end with a", "extracted_entity": null}
{"question": "are calligraphers held in great esteem?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Calligraphers are not held in great esteem because", "extracted_entity": null}
{"question": "have many European languages borrowed numerous words from it?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": "Europe"}
{"question": "are the most active in Damascus and Cairo?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "mascus is more active than Cairo.", "extracted_entity": "Damascus;Cairo"}
{"question": "do arabs consider literary Arabic?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arab_people;Arabic_language"}
{"question": "the sociolinguistic situation of Arabic provides a prime example of what?", "answer": "the linguistic phenomenon of diglossia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the linguistic phenomenon of diglossia"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Diglossia", "extracted_entity": "Arabic_language"}
{"question": "are pronouns in Literary Arabic marked for person , number and gender ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Modern_Standard_Arabic"}
{"question": "however , are non-human plural nouns grammatically considered to be feminine singular ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "es.", "extracted_entity": null}
{"question": "is a bee an insect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bee;Insect"}
{"question": "are bees related to ants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "No,Q: How do bees mate?", "extracted_entity": null}
{"question": "have managed populations of European honey bees experienced substantial declines?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Yes, in recent decades", "extracted_entity": "Europe"}
{"question": "where do bees live?", "answer": "in colonies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in colonies"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "In a beehive.", "extracted_entity": null}
{"question": "when do bumblebee colonies reach peak population?", "answer": "mid to late summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mid to late summer"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "In mid-July", "extracted_entity": null}
{"question": "what do people commonly call cleptoparasitic bees?", "answer": "cuckoo bees", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cuckoo bees"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "parasitic bees,Q: what is the scientific", "extracted_entity": null}
{"question": "how did vespoid wasps evolve?", "answer": "from predatory ancestors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from predatory ancestors"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "vespid wasps evolved from the same common ancestor", "extracted_entity": null}
{"question": "what do humans use bees for?", "answer": "honey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["honey"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Bees are very important to humans because they are", "extracted_entity": null}
{"question": "why do honey bee queens continue to lay eggs during the foraging season?", "answer": "to replace daily casualties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to replace daily casualties"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "The queen is the mother of the colony and continues", "extracted_entity": null}
{"question": "does Beijing mean \"northern capital\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Beijing"}
{"question": "are the caves of Dragon Bone Hill near the village of Zhoukoudian in Fangshan District?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": "Zhoukoudian;Fongshan_District"}
{"question": "are hutongs disappearing?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "No. The number of hutongs has increased by", "extracted_entity": null}
{"question": "when did Beijing host the Olympic Games?", "answer": "2008", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2008"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing hosted the Olympic Games in 2008.", "extracted_entity": "Beijing;Olympic_Games"}
{"question": "when was the An Shi Rebellion launched?", "answer": "in 755 AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 755 AD"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "A: A: A:", "extracted_entity": null}
{"question": "in 1949, where did Communist forces enter without a fight?", "answer": "Beiping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beiping"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "France", "extracted_entity": "Communism"}
{"question": "when did Yuan Shikai die?", "answer": "1916", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1916"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yuan Shikai died on 6 June191", "extracted_entity": "Yuan_Shikai"}
{"question": "where does air pollution in Beijing come from?", "answer": "surrounding cities and provinces", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["surrounding cities and provinces"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Air pollution in Beijing comes from the burning of coal", "extracted_entity": "Beijing"}
{"question": "as of August 1st, 2006, how many trains stop daily at the Beijing Railway Station or the Beijing West Railway Station?", "answer": "167", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["167"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "2,000 trains", "extracted_entity": "Beijing_Railway_Station;Beijing_West_Railway_Station"}
{"question": "what is the name of a university (or similar institution for imparting higher education) in Beijing?", "answer": "Tsinghua University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tsinghua University"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing University", "extracted_entity": "Beijing"}
{"question": "are famous middle  schools  in Beijing :?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Beijing Haidian Middle School, Beijing No.", "extracted_entity": "Beijing"}
{"question": "was Blaise Pascal a mathematician of the first order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es. Pascal's mathematical work was of the first", "extracted_entity": "Blaise_Pascal"}
{"question": "could Blaise Pascal move without crutches?", "answer": "He could move without crutches until a paralytic attack in 1647.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He could move without crutches until a paralytic attack in 1647.", "no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "o, he could not move without crutches.", "extracted_entity": "Blaise_Pascal"}
{"question": "has the name Pascal been given to the SI unit of pressure?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pascal_(unit)"}
{"question": "from what did Pascal suffer throughout his life?", "answer": "ill health", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ill health", "poor health"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "rom what did Pascal suffer throughout his life?", "extracted_entity": "Blaise_Pascal"}
{"question": "what did Pascal argue was as perfect as possible?", "answer": "the procedure used in geometry ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the procedure used in geometry ", "the procedure used in geometry"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "ascal argued that the best thing to do was to", "extracted_entity": "Blaise_Pascal"}
{"question": "what is the best physician?", "answer": "Time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Time"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "A physician that can answer all of the above questions", "extracted_entity": null}
{"question": "who was the eldest sibling?", "answer": "Gilberte", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gilberte"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The eldest sibling was named Jemima.", "extracted_entity": null}
{"question": "aside from the Provincial Letters' religious influence, were they popular as a literary work?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "It was very popular, the Provincial Letters' was", "extracted_entity": null}
{"question": "of whose continual poor health was the cause never precisely determined?", "answer": "Blaise Pascal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Blaise Pascal"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "A. S. C. D. E.", "extracted_entity": null}
{"question": "is the violin cello a bowed string instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the violin cello a", "extracted_entity": null}
{"question": "is the purfling just for decoration?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "No. It\u2019s a crucial part of the tea", "extracted_entity": null}
{"question": "are the z-holes used as access points to the interior of the cello? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Cello"}
{"question": "what does violincello mean?", "answer": "Violincello means \"little violene\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Violincello means \"little violene\"."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The violincello is a stringed instrument that", "extracted_entity": null}
{"question": "what are the two materials that bows are made of traditionally? ?", "answer": "Traditionally, bows are made from pernambuco or brazilwood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Traditionally, bows are made from pernambuco or brazilwood."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Wood and horn", "extracted_entity": null}
{"question": "what is used to tune a cello?", "answer": "Pegs are used to tune a cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pegs are used to tune a cello.", "the pegs"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "A tuning fork.", "extracted_entity": null}
{"question": "which harmonics can produce any notes above middle C? ?", "answer": "Artificial harmonics can produce any notes above middle C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Artificial harmonics can produce any notes above middle C.", "Artificial harmonics"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The harmonics can produce any notes above middle C", "extracted_entity": "C_(musical_note)"}
{"question": "who referred to the cello as \"basso de viola da braccio\"?", "answer": "Monteverdi referred to the cello as \"basso de viola da braccio\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monteverdi referred to the cello as \"basso de viola da braccio\".", "Monteverdi"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "the cello was referred to as \"basso", "extracted_entity": "Cello"}
{"question": "where is the violincello held?", "answer": "The violincello is held on the shoulder.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The violincello is held on the shoulder.", "on the shoulder"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "The violincello is held between the legs.", "extracted_entity": null}
{"question": "is there cello in performances by Rihanna?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes, there is cello in performances by Rihanna", "extracted_entity": "Rihanna"}
{"question": "are cellos constructed with glue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "No, the cellulose is constructed with a mixture", "extracted_entity": null}
{"question": "is the cello a stringed instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes, the cello is a stringed instrument", "extracted_entity": "Cello"}
{"question": "what is cello an abbreviation of?", "answer": "violoncello", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["violoncello"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "cello is an abbreviation of violoncello", "extracted_entity": "Cello"}
{"question": "what position is used to play the cello?", "answer": "a gamba", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a gamba", "seated"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "between the legs", "extracted_entity": "Cello"}
{"question": "what is a person who plays the cello called?", "answer": "cellist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cellist"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "A cellist", "extracted_entity": null}
{"question": "why is there purling on a cello?", "answer": "to stop cracks from forming ****misspelled****", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to stop cracks from forming ****misspelled****"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "because the string is attached to the top of the", "extracted_entity": null}
{"question": "when did the first educational works appear for the cello?", "answer": "the 18th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 18th century", "18th century"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "the first educational works for the cello appeared in", "extracted_entity": null}
{"question": "what cello manufacturer should I buy from if I want to play outside?", "answer": "Luis & Clark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luis & Clark"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "There are many cello manufacturers who make excellent cel", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a member of the National Institute?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb find any relationship between electric charges and magnetic poles? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was Charles-Augustin de Coulomb's father's family in Montpellier? ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Montpellier"}
{"question": "where did Charles-Augustin de Coulomb die?", "answer": "Charles-Augustin de Coulomb died in Paris.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb died in Paris.", "Paris, France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb died in Paris, France.", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "when was Charles-Augustin de Coulomb permanently stationed in Paris?", "answer": "Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He was permanently stationed in Paris in   and", "extracted_entity": "Paris"}
{"question": "what contribution did Charles-Augustin de Coulomb make to the field of geotechnical engineering?", "answer": "Retaining wall design", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Retaining wall design"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He developed the torsion balance to measure the resistance", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "where did the construction of Fort Bourbon take place?", "answer": "The construction of Fort Bourbon took place in Martinique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The construction of Fort Bourbon took place in Martinique.", "Martinique "], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "On the site of the Battle of the Little B", "extracted_entity": null}
{"question": "what is the definition of the electrostatic force of attraction and repulsion? ?", "answer": "Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "Coulomb's Law"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The electrostatic force of attraction is the attraction between", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb retire to a small estate he possessed at Blois?", "answer": "Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "1789"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1806", "extracted_entity": "Charles-Augustin_de_Coulomb;Blois"}
{"question": "if Charles-Augustin de Coulomb was alive today, how old would he have been?", "answer": "273", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["273"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb would have been", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "for how many years did Charles-Augustin de Coulomb live?", "answer": "70", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "ize=14px", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he did not live to be 80", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "how old was Charles-Augustin de Coulomb when he died?", "answer": "1806", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1806"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": ",A: A: A:", "extracted_entity": null}
{"question": "how many years ago did he resign his appointment as intendant de eaux et fontaine ?", "answer": "220", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["220"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "It was in Q: what is the name", "extracted_entity": null}
{"question": "how many years ago did Charles-Augustin de Coulomb die?", "answer": "203", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["203"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "23 August 1806", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was The SI  unit  of charge , the  coulomb , named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "es, it was named for him", "extracted_entity": "International_System_of_Units"}
{"question": "was coulomb born in Angoul\u00eame,  France ,  to a well  to do  family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He was born in Angoul\u00eame, to a", "extracted_entity": "Angoul\u00eame"}
{"question": "was his father ,  Henri  Coulomb , inspector of the Royal  Fields  in Montpellier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Montpellier"}
{"question": "is coulomb distinguished in the history  of mechanics  and  of electricity and  magnetism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "coulomb is distinguished in the history of mechanics", "extracted_entity": "Mechanics;Electricity;Magnetism"}
{"question": "was Charles Augustin  de Coulomb  ( born  June  14 ,  1736 ,  Angoul\u00eame ,  France  -  died August 23 ,  1806 ,  Paris ,  France  ) a French  physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "he was.", "extracted_entity": "France"}
{"question": "does Coulomb leave a legacy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the legacy?", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb born?", "answer": "June 14, 1736", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 14, 1736"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did he publish an important investigation of the laws of friction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it possible to find the", "extracted_entity": null}
{"question": "was Coulomb born in Angoul\u00eame, France, to a well to do family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb was born in Angoul\u00eame, France", "extracted_entity": "Angoul\u00eame;France"}
{"question": "was his father inspector of the Royal Fields?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes,Q. Was his father's name also", "extracted_entity": null}
{"question": "he discovered an inverse relationship of what?", "answer": "distance and electric force", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["distance and electric force"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the velocity of the object and the force", "extracted_entity": null}
{"question": "is it true that coulomb leaves a legacy as a hero?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes. He was a hero.", "extracted_entity": null}
{"question": "where is Charles-Augustin de Coulomb from?", "answer": "France", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb is from France", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "was Old Chinese wholly uninflected?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No, it had a number of inflectional", "extracted_entity": "Old_Chinese"}
{"question": "are The modern Chinese dialects more like a family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No,Q: Do the Chinese dialects have", "extracted_entity": null}
{"question": "is it true that one-fifth of world population speak some form of Chinese?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, about 16% of the world's", "extracted_entity": "Chinese_language"}
{"question": "one-fifth of world population speak some form of what?", "answer": "Chinese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "is mandarin spoken Chinese distinguished by its high level?", "answer": "no, it's distinguished by diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no, it's distinguished by diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, it is distinguished by its high level", "extracted_entity": "Chinese_language"}
{"question": "copenhagen is the capital of what country?", "answer": "Denmark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Denmark"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Denmark.", "extracted_entity": null}
{"question": "what is the population of Copenhagen?", "answer": "1,161,063", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,161,063", "1,153,615"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "1.4 million", "extracted_entity": "Copenhagen"}
{"question": "what transnational bridge was completed in 2000?", "answer": "Oresund Bridge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oresund Bridge"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "The Akashi Kaikyo Bridge", "extracted_entity": null}
{"question": "why is Copenhagen a regional hub?", "answer": "Its strategic location and excellent infrastructure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its strategic location and excellent infrastructure", "Its strategic location and excellent infrastructure with the largest airport in Scandinavia[6] located 14 minutes by train from the city centre"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Copenhagen is a regional hub because it is a very", "extracted_entity": "Copenhagen"}
{"question": "what happened to Copenhagen between 1251 and 1255?", "answer": "a bunch of things", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a bunch of things"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "it became a city of the law of the Danish", "extracted_entity": "Copenhagen"}
{"question": "is Copenhagen completely surrounded with water? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Copenhagen is on the islands of Zealand and Amager", "extracted_entity": "Copenhagen"}
{"question": "copenhagen is ranked number one worldwide for which things?", "answer": "Most Livable City in the World, ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most Livable City in the World, ", "Most Livable City, Location Ranking Survey"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "happiness,B: quality of lifeC: cleanliness", "extracted_entity": null}
{"question": "why did German troops occupy Copenhagen?", "answer": "Because it was WW2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it was WW2"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "they occupied Copenhagen because the Nazis wanted to occupy the", "extracted_entity": "Germany;Copenhagen"}
{"question": "have cymbals been used historically to suggest bacchanal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Cymbals have been used historically to suggest bac", "extracted_entity": null}
{"question": "can the origins of cymbals be traced to prehistoric times?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The origin of cymbals can be traced to", "extracted_entity": null}
{"question": "are cymbals used in moden orchestras?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is a cymbal", "extracted_entity": null}
{"question": "what are the most common Cymbals?", "answer": " The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "Hi-Hats, Crash, Splash, Ride, and China"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The most common Cymbals are the hi-h", "extracted_entity": null}
{"question": "what is the second main orchestral use of cymbals?", "answer": "The suspended cymbal is the second main orchestral use of symbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The suspended cymbal is the second main orchestral use of symbals.", "The suspended cymbal", "the suspended cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The second main orchestral use of cymbals", "extracted_entity": null}
{"question": "what have clash cymbals traditionally been accompanied by?", "answer": "Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "The bass drum playing an identical part"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "a gong", "extracted_entity": null}
{"question": "what happens when the center of a Cymbal is hit?", "answer": "It causes a sound which is in a higher register than the rest of the symbal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It causes a sound which is in a higher register than the rest of the symbal.", "It causes a sound which is in a higher register than the rest of the cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the cymbal moves up and down.", "extracted_entity": "Cymbal"}
{"question": "what is an effective way to accentuate a note?", "answer": "Playing a bass drum playing an identical part is an effective way to accentuate a note.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Playing a bass drum playing an identical part is an effective way to accentuate a note.", "The combination of a clash cymbol being accompanied by a bass drum playing an identical part, played loudly"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Use a different note length", "extracted_entity": null}
{"question": "from what type of Cymbals can a expert player obtain an enormous dynamic range?", "answer": "An expert player can obtain an enormous dynamic range from crash cymbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An expert player can obtain an enormous dynamic range from crash cymbals.", "Crash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "A) Cymbals with a big bell B", "extracted_entity": null}
{"question": "can the origins of cymbals be traced back to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes,The origins of cymbals can be", "extracted_entity": null}
{"question": "did the modern hi-hat evolve from clash cymbals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "hi-hat yes, clash cymbals evolved", "extracted_entity": "Hi-hat"}
{"question": "is the word cymbal derived from a Germanic word?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cymbal;Germanic_languages"}
{"question": "what is the center of a cymbal called?", "answer": "Bell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bell", "bell"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The center of a cymbal is called the", "extracted_entity": null}
{"question": "what part of the cymbal gets the best \"crash\"?", "answer": "Edge or rim", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Edge or rim", "edge or rim"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the bell", "extracted_entity": null}
{"question": "what two instruments form the modern drum kit?", "answer": "the cymbals and the drum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cymbals and the drum"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The snare drum and the bass drum", "extracted_entity": null}
{"question": "what Greek mythical creatures were depicted holding cymbals?", "answer": "fauns and satyrs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["fauns and satyrs"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Sirens", "extracted_entity": null}
{"question": "what are orchestral crash cymbals technically called?", "answer": "a pair of clash cymbals", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pair of clash cymbals", "clash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The cymbals used in an orchestra are called", "extracted_entity": null}
{"question": "is Dhaka the capital of Bangladesh?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "s, Dhaka is the capital of Bangladesh.", "extracted_entity": "Dhaka;Bangladesh"}
{"question": "is Dhaka located on the eastern banks of the Buriganga River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes, Dhaka is located on the eastern banks", "extracted_entity": "Buriganga_River"}
{"question": "is University of Dhaka the largest public university in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "University of Dhaka is the largest public university in", "extracted_entity": "Dhaka"}
{"question": "how much area does Dhaka cover?", "answer": "1462.60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1462.60 square kilometers"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "1,528 square kilometers", "extracted_entity": "Dhaka"}
{"question": "how many people did the 1970 Bhola cyclone kill?", "answer": "500,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500,000"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": ",A: The A:The", "extracted_entity": null}
{"question": "when is the end of the independence war of Bangladesh?", "answer": "December 16, 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 16, 1971"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "971", "extracted_entity": "Bangladesh"}
{"question": "did the Mughals arrive in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes, the Mughals arrived in Dhaka", "extracted_entity": "Mughal_Empire;Dhaka"}
{"question": "is University of Dhaka older than the Dhaka College?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dhaka_College"}
{"question": "what is given for the number of native speakers?", "answer": "No figure is given for the number of native speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No figure is given for the number of native speakers."], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "The number of native speakers is given as Q", "extracted_entity": null}
{"question": "is english an intonation  language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is english a tone language?", "extracted_entity": null}
{"question": "is english a de  jure  official  language  of Israel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "English is not an official language of Israel.", "extracted_entity": "English_language;Israel"}
{"question": "is the stressed  syllable called the nuclear  syllable?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "The nuclear syllable is the stressed syllable in", "extracted_entity": null}
{"question": "does english grammar have minimal inflection?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Does English grammar have inflection", "extracted_entity": null}
{"question": "did old English develop into Middle English?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Middle_English"}
{"question": "are books, magazines, and newspapers written in English available?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": "English_language"}
{"question": "is the syntax of German different with different rules?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "German grammar is different from English grammar in several ways", "extracted_entity": "German_language"}
{"question": "is it true that semantics causes a number of false friends?", "answer": "true", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["true"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it true that semantics causes", "extracted_entity": null}
{"question": "here , are all syllables unstressed , except the syllables/words best and done , which are stressed ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": ", they are all unstressed, except the", "extracted_entity": null}
{"question": "are many words describing the navy , types of ships , and other objects or activities on the water of dutch origin ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "s, for example, the words \" fluit", "extracted_entity": null}
{"question": "are local variations in the formal written version of the language quite limited , being restricted largely to the spelling differences between British and American English ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "would say so, yes.", "extracted_entity": "British_English;American_English"}
{"question": "is the flute a musical instrument?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it possible to open flutes at one or both ends?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes. The flutes are open at both ends", "extracted_entity": null}
{"question": "are Indian concert flutes available in standard pitches?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, they are available in standard pitches", "extracted_entity": "India"}
{"question": "what do we refer musicians who play flute?", "answer": "A flute player, a flautist or a flutist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute player, a flautist or a flutist. ", "A flute player, a flautist, or a flutist"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Flutists", "extracted_entity": null}
{"question": "when was a three-holed flute made from a mammoth tusk discovered?", "answer": "2004.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2004.", "In 2004"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "A: A: The flute was discovered", "extracted_entity": null}
{"question": "when did the tin whistle first appear?", "answer": "12th century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["12th century.", "In the 12th century"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The tin whistle was first used in the  of", "extracted_entity": null}
{"question": "what are the oldest known musical instruments?", "answer": "A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "A three-holed flute made from a mammoth tusk and two flutes made from swans' bones are among the oldest known musical instruments"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The oldest known musical instruments are flutes and drums", "extracted_entity": null}
{"question": "when was the pan flute spread to other parts of Europe?", "answer": "7th century BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7th century BC.", "After the 7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The pan flute spread to other parts of Europe in", "extracted_entity": "Europe"}
{"question": "what does the air stream across this hole create?", "answer": "A Bernoulli, or siphon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A Bernoulli, or siphon."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "A vortex street", "extracted_entity": null}
{"question": "are foxes wary of humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Foxes are generally wary of humans,Q:", "extracted_entity": null}
{"question": "are fennec foxes endangered?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Fennec foxes are not endangered. They", "extracted_entity": null}
{"question": "does the diet of foxes include reptiles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do foxes eat lizards", "extracted_entity": null}
{"question": "how long do most foxes live?", "answer": "Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "2 to 3 years"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Foxes can live up to Q: What", "extracted_entity": null}
{"question": "in what country did fox hunting originate?", "answer": "the United Kingdom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United Kingdom", "in the United Kingdom"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "England", "extracted_entity": "Fox_hunting"}
{"question": "what is the smallest species of fox?", "answer": "The Fennec Fox is the smallest species of fox. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Fennec Fox is the smallest species of fox. ", "the Fennec Fox"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Fennec Fox (Fennecus zer", "extracted_entity": null}
{"question": "what are female foxes called?", "answer": "vixens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["vixens"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Vixens.", "extracted_entity": null}
{"question": "what is caching food?", "answer": "Caching is burying it for later consumption, usually under leaves, snow, or soil.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Caching is burying it for later consumption, usually under leaves, snow, or soil.", "burying it for later consumption"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Caching food is a method of storing food in", "extracted_entity": null}
{"question": "do foxes damage fruit on farms?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Foxes can cause damage to fruit on farms.", "extracted_entity": null}
{"question": "how many years ago did African people spread across 31 francophone African countries can speak French either as a first or second language?", "answer": "2 years ago", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2 years ago"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "1 year ago, 2 years ago,", "extracted_entity": "France"}
{"question": "in Old French, was the plural  for animal animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "the plural for animal was animals", "extracted_entity": "Old_French"}
{"question": "is catalan the only  official  language  of Andorra?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, catalan is the only, official,", "extracted_entity": "Andorra"}
{"question": "in Belgium, however, is quatre-vingts universally used?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": "Belgium"}
{"question": "is french taught in many schools as a primary language along with Arabic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "French is taught in many schools as a primary language", "extracted_entity": "Arabic_language"}
{"question": "what is widely used by the Lebanese, especially for administrative purposes?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Arabic is widely used by the Lebanese, especially for", "extracted_entity": "Lebanon"}
{"question": "is french descendant of the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "France;Latin"}
{"question": "are there also a variety of regional languages In addition?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are also a variety of regional languages", "extracted_entity": "Regional_language"}
{"question": "is it true that france signed the European Charter for Regional Languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "France;European_Charter_for_Regional_or_Minority_Languages"}
{"question": "are the prefixes en- and em- always nasalized ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What are the prefixes en-", "extracted_entity": null}
{"question": "how many years ago was the Luther Bible by Martin Luther printed?", "answer": "475", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["475"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "The Luther Bible () is a German language Bible translation", "extracted_entity": "Luther_Bible;Martin_Luther"}
{"question": "is german an inflected  language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "German_language"}
{"question": "in Switzerland, is \u00df used at all?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Switzerland"}
{"question": "are there two common  word  orders?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are two common word orders.", "extracted_entity": null}
{"question": "are feminines declined in the singular?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the plural of \u201c", "extracted_entity": null}
{"question": "is 9 %  of the Internet  population German?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "1, 1", "extracted_entity": "Germany"}
{"question": "is german written using the Latin  alphabet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "German_language;Latin"}
{"question": "was the Luther Bible by Martin Luther printed?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes, in 1534", "extracted_entity": "Luther_Bible;Martin_Luther"}
{"question": "does a  tz indicate that the preceding vowel is short?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the vowel of a tz", "extracted_entity": null}
{"question": "are there 622,650 speakers of German In Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are 622,650 speakers of", "extracted_entity": "German_language;Canada"}
{"question": "are there also large populations of German ancestry In Mexico?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "I am not sure, but I think so.", "extracted_entity": "Germany;Mexico"}
{"question": "would Bundesl\u00e4nder not accept North Rhine Westphalia and Bavaria ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "States_of_Germany;North_Rhine-Westphalia;Bavaria"}
{"question": "have adverbs of time to appear in the third place in the sentence , just after the predicate ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "es,Q: What is the function of the", "extracted_entity": null}
{"question": "do both sexes of giraffe have horns?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, both male and female giraffes have", "extracted_entity": "Giraffe"}
{"question": "do giraffes give birth standing up?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "No,Q: How many babies do giraff", "extracted_entity": null}
{"question": "are giraffes hunted for their hides?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes,Giraffes are hunted for their", "extracted_entity": null}
{"question": "what areas can giraffes inhabit?", "answer": "savannas, grasslands, or open woodlands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["savannas, grasslands, or open woodlands", "inhabit savannas, grasslands, or open woodlands"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Giraffes are often found in areas with a", "extracted_entity": null}
{"question": "what will a giraffe use to clean off any bugs that appear on its face?", "answer": "extremely long tongue ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["extremely long tongue ", "It's extremely long tongue"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "its tongue", "extracted_entity": null}
{"question": "how do giraffes defend themselves?", "answer": "with a powerful kick", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["with a powerful kick", "With powerful kicks"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Giraffes have large, powerful hooves that", "extracted_entity": "Giraffe"}
{"question": "do male giraffes weigh more than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do male giraffes live", "extracted_entity": null}
{"question": "do male giraffes have larger horns than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do giraffes have a", "extracted_entity": null}
{"question": "are male females generally taller than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the giraffe's neck", "extracted_entity": null}
{"question": "was Henri Becquerel one of the discoverers of radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was one of the discoverers of", "extracted_entity": "Henri_Becquerel"}
{"question": "is the SI unit for radioactivity named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel", "extracted_entity": "International_System_of_Units"}
{"question": "was Henri Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Henri_Becquerel;France"}
{"question": "in what year did Henri Becquerel die?", "answer": "1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1908", "August 24, 1908"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": "Henri_Becquerel"}
{"question": "where was Henri Becquerel born?", "answer": " Paris ", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Paris ", "Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": "Henri_Becquerel"}
{"question": "in what year did Henri Becquerel win the Nobel Prize in Physics?", "answer": "1903", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1903"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": "Henri_Becquerel;Nobel_Prize_in_Physics"}
{"question": "was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, he was the first in his family to", "extracted_entity": "Henri_Becquerel;National_Museum_of_Natural_History_(France)"}
{"question": "was Henri Becquerel the sole winner of the 1903 Nobel Prize in Physics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henri_Becquerel;Nobel_Prize_in_Physics"}
{"question": "did Henri Becquerel intentionally discover radioactivity?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "He discovered it accidentally"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel discovered radioactivity by accident.", "extracted_entity": "Henri_Becquerel"}
{"question": "if Henri Becquerel was alive today, how old would he have been?", "answer": "157", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["157"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He would have been  years old.", "extracted_entity": "Henri_Becquerel"}
{"question": "for how many years did Henri Becquerel live?", "answer": "56", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["56"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "for  years.", "extracted_entity": "Henri_Becquerel"}
{"question": "did Henri Becquerel live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he died when he was 56.", "extracted_entity": "Henri_Becquerel"}
{"question": "what was Henri Becquerel`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was a French engineer, physicist", "extracted_entity": "Henri_Becquerel"}
{"question": "how old was Henri Becquerel when he died?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "as we can see in the text, Henri", "extracted_entity": "Henri_Becquerel"}
{"question": "how many years ago was it when he became the third in his family to occupy the physics chair at the Musum National d`Histoire Naturelle?", "answer": "117", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["117"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "er,er", "extracted_entity": null}
{"question": "in 1908, the year  of his death, was Becquerel elected Permanent Secretary  of the Acad\u00e9mie  des  Sciences?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Becquerel"}
{"question": "when was Henri Becquerel born?", "answer": "December 15, 1852", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 15, 1852"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": ", | Q:", "extracted_entity": "Henri_Becquerel"}
{"question": "did he become chief engineer in the Department of Bridges and Highways?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes. He became chief engineer in the Department of", "extracted_entity": null}
{"question": "did he share the Nobel Prize in Physics?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nobel_Prize_in_Physics"}
{"question": "is it true that Becquerel wrapped a fluorescent substance in photographic plates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Henri_Becquerel"}
{"question": "becquerel wrapped a fluorescent substance in what?", "answer": "photographic plates and black material", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["photographic plates and black material"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "a membrane", "extracted_entity": null}
{"question": "is there a Becquerel crater on the Moon for radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, the Becquerel crater is on the", "extracted_entity": "Moon"}
{"question": "must \u2026 One conclude from these experiments that the phosphorescent substance in question emits rays which pass through the opaque paper and reduces silver salts ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Phosphorescent substances emit light and this light", "extracted_entity": null}
{"question": "what is Henri Becquerel?", "answer": "Henri Becquerel was a famous physicist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel was a famous physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He is a French physicist", "extracted_entity": "Henri_Becquerel"}
{"question": "where is Henri Becquerel from?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "France", "extracted_entity": "Henri_Becquerel"}
{"question": "what does Henri Becquerel do?", "answer": "was a physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["was a physisist", "Henri Becquerel was a physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel discovered evidence of radioactivity.", "extracted_entity": "Henri_Becquerel"}
{"question": "was Newton a English physicist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "England"}
{"question": "did Newton reject the church's doctrine of the Trinity?", "answer": "Newton may have rejected the church's doctrine of the Trinity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newton may have rejected the church's doctrine of the Trinity.", "Maybe"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Trinity"}
{"question": "was Newton admitted into Havard College?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "Maybe, but it doesn't say so in the article"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "when was Newton's date of birth recorded?", "answer": "Christmas Day, Decembeer 25, 1642.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas Day, Decembeer 25, 1642.", "his date of birth was recorded as Christmas Day, December 25, 1642"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many laws of motion did Netwon have?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Q: What is the first law of motion", "extracted_entity": null}
{"question": "who did Newton see as the master creator?", "answer": "God", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["God", "Newton saw God as the master creator whose existence could not be denied in the face of the grandeur of all creation"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "God", "extracted_entity": null}
{"question": "when did Netwon investigate the refraction of light?", "answer": "From 1670 to 1672", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1670 to 1672", "1670-1672"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "in Q: when did Netwon investigate the", "extracted_entity": null}
{"question": "what principles did Newton explain for mechanics?", "answer": "The principles of conservation of momentum and angular momentum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The principles of conservation of momentum and angular momentum", "In mechanics, Newton enunciated the principles of conservation of momentum and angular momentum"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton's three laws of motion are:", "extracted_entity": "Isaac_Newton;Mechanics"}
{"question": "interaction with what man stirred up Newton's interest in alchemy?", "answer": "Henry More", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henry More", "The contact with the theosophist Henry More, revived his interest in alchemy"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Robert Boyle, a chemist, and the al", "extracted_entity": "Isaac_Newton"}
{"question": "james Watt's improvements of what were fundamental to the changes wrought by the Industrial Revolution?", "answer": "The steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The steam engine.", "steam engine"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Steam engine", "extracted_entity": "Industrial_Revolution"}
{"question": "james Watt was born where?", "answer": "Greenock, a seaport on the Firth of Clyde", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greenock, a seaport on the Firth of Clyde"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Edinburgh", "extracted_entity": null}
{"question": "what was the name of James Watt's mother?", "answer": "Agnus Muirhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Agnus Muirhead", "Agnes Muirhead"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Agnes Muirhead", "extracted_entity": "James_Watt"}
{"question": "how many of James Watt's children did not live to adulthood?", "answer": "3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "destiny", "extracted_entity": "James_Watt"}
{"question": "james Watt ranked first among how many people in Charles Murray's survey of historiometry?", "answer": "229", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["229"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "Charles_Murray_(political_scientist);Historiography"}
{"question": "in what year did James travel to Lodon to study instrument-making?", "answer": "1753", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1753"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1822", "extracted_entity": "Loddon,_Norfolk"}
{"question": "for how many years of James Watt's life was his mother alive?", "answer": "17 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["17 years"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "ly, 't forget to give the", "extracted_entity": "James_Watt"}
{"question": "what is James Watt most famous for?", "answer": "Steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "He developed what would now be known as Watt steam", "extracted_entity": "James_Watt"}
{"question": "do most Japanese people employ politeness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Japanese_people"}
{"question": "is it true that Japanese borrowed a considerable number of words?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Japanese_language"}
{"question": "are the several dialects of Kagoshima famous to speakers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "es, the dialects of Kagoshima are", "extracted_entity": "Kagoshima"}
{"question": "is Lima the capital of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lima;Peru"}
{"question": "is the Museum of Gold in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Museum of Gold is in Lima", "extracted_entity": "Lima"}
{"question": "does Lima have volleyball venues?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, it has a volleyball venue", "extracted_entity": "Lima"}
{"question": "when was Lima founded?", "answer": "January 18,1535.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["January 18,1535."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima was founded on January    in", "extracted_entity": "Lima"}
{"question": "where is the Park of the Reserve located?", "answer": "Near the downtown area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Near the downtown area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The Park of the Reserve is located in the heart", "extracted_entity": null}
{"question": "what is a colectivo?", "answer": "Automobiles that renders express service on some major roads of the Lima Metropolitan Area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Automobiles that renders express service on some major roads of the Lima Metropolitan Area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "A group of people who come together to work for", "extracted_entity": null}
{"question": "is the National University of San Marcos in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "National_University_of_San_Marcos;Lima"}
{"question": "has the San Lorenzo Megaport Project been completed?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the San Lorenzo Megap", "extracted_entity": null}
{"question": "how many years ago was the Lima Stock Exchange the most profitable in the world?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "as of, the Lima Stock Exchange is", "extracted_entity": "Lima"}
{"question": "is rainfall very low?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No,Q2. Is the soil very fertile", "extracted_entity": null}
{"question": "is lima the capital  and  largest  city  of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Lima is the capital and largest city of Peru.", "extracted_entity": "Peru"}
{"question": "is The GDP  per capita in Lima $ 7,600  /  ref )?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, it is $7,600", "extracted_entity": "Lima"}
{"question": "the Spanish Crown named Pizarro governor of what?", "answer": "Hemming.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hemming."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "the Spanish Crown named Pizarro governor of what", "extracted_entity": "Monarchy_of_Spain"}
{"question": "is it true that the ideas of the Enlightenment shaped the development of the city?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "no, the ideas of the Enlightenment did not shape", "extracted_entity": "Age_of_Enlightenment"}
{"question": "the ideas of the Enlightenment shaped the development of what?", "answer": "the city, Lima", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the city, Lima"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "the ideas of the Enlightenment shaped the development of modern", "extracted_entity": "Age_of_Enlightenment"}
{"question": "is London the capital of the United Kingdom?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes, London is the capital of the United Kingdom", "extracted_entity": "London;United_Kingdom"}
{"question": "does London's population draw from a wide range of religions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes, London's population draws from a wide range", "extracted_entity": "London"}
{"question": "did Caunte take control of the English throne in 101?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "England"}
{"question": "over how many languages are spoken in London?", "answer": "Over 300 languages are spoken in london", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over 300 languages are spoken in london", "Over 300"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "over  100", "extracted_entity": "London"}
{"question": "what had the Anglo-Saxons created by the 600s?", "answer": "By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "a new settlement called Lundenwic"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "they had created a culture that drew from both roman", "extracted_entity": "Anglo-Saxons"}
{"question": "what city in the UK has been subjected to bouts of terrorism?", "answer": "London has been subjected to bouts of terrorism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London has been subjected to bouts of terrorism.", "London"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": "United_Kingdom"}
{"question": "what countries did James VI of Scotland unite?", "answer": "James VI of Scotland united Scotland and England.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James VI of Scotland united Scotland and England.", "England and Scotland"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Scotland and England", "extracted_entity": "James_VI_and_I"}
{"question": "how did civil wars affect England during the Middle Ages?", "answer": "London remained relatively untouched", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London remained relatively untouched"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "England was affected by civil wars because the country was", "extracted_entity": "England_in_the_Middle_Ages"}
{"question": "are the strings of a classical lyre made of gut?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes. The strings of a classical lyre were", "extracted_entity": null}
{"question": "does a classical lyre have a sound-chest?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why are the strings of a", "extracted_entity": null}
{"question": "is lyre played in mordern Greece?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Greece"}
{"question": "which part of the strings does the left hand touch?", "answer": "The lower strings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lower strings"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The left hand touches the neck.", "extracted_entity": null}
{"question": "in which place is lyre still played?", "answer": "North-eastern parts of Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North-eastern parts of Africa"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Lyre is still played in the theatre. The", "extracted_entity": null}
{"question": "does a classical lyre have a fingerboard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No,Q: What is a classical lyre", "extracted_entity": null}
{"question": "is a guitar played with a plectrum like a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No, a lyre is not played with a", "extracted_entity": "Guitar;Plectrum;Lyre"}
{"question": "was Michael Faraday an English chemist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Michael_Faraday;England"}
{"question": "was Faraday a devout Christian?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was a devout Christian.", "extracted_entity": "Christianity"}
{"question": "was Faraday considered a gentleman?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Faraday"}
{"question": "who was a member of the Sandemanian sect of Christianity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Jesus", "extracted_entity": "Christianity"}
{"question": "what did Joseph Henry likely discover?", "answer": "self-induction", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["self-induction"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "that electricity can produce magnetism", "extracted_entity": "Joseph_Henry"}
{"question": "what did the University of Oxford grant Faraday?", "answer": "a Doctor of Civil Law degree (honorary)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Doctor of Civil Law degree (honorary)", "Doctor of Civil Law degree (honorary)"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The University of Oxford granted Faraday the honor of", "extracted_entity": "University_of_Oxford;Michael_Faraday"}
{"question": "who was made to travel outside the coach?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Negro passengers.", "extracted_entity": null}
{"question": "did Faraday construct the ancestor of modern power generators?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Faraday lecture on education in 1854?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday receive an extensive formal education?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday discover the chemical substance benzene?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No. Benzene was discovered by the German chem", "extracted_entity": "Michael_Faraday;Benzene"}
{"question": "was Faraday born in England?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "England"}
{"question": "what do some historians of science refer to Faraday as?", "answer": "the best experimentalist in the history of science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the best experimentalist in the history of science", "best experimentalist in the history of science"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "A natural philosopher", "extracted_entity": null}
{"question": "who was the sponsor and mentor of Faraday?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday", "extracted_entity": null}
{"question": "what did Faraday invent that is used almost universally in science laboratories?", "answer": "the Bunsen burner", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Bunsen burner", "Bunsen burner"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "an electric motor", "extracted_entity": null}
{"question": "does the production of chemical weapons raise an ethical issue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes. There are several ethical issues raised by the", "extracted_entity": null}
{"question": "what was the Faraday effect first called?", "answer": "diamagnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["diamagnetism"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday effect", "extracted_entity": null}
{"question": "can you meet someone through attending a church?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "I have met a lot of great people through church", "extracted_entity": null}
{"question": "what was Michael Faraday`s birthplace?", "answer": "Newington Butts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was born in Newington Butts", "extracted_entity": "Michael_Faraday"}
{"question": "what was Michael Faraday`s profession?", "answer": "chemist and physicist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["chemist and physicist"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was a scientist", "extracted_entity": "Michael_Faraday"}
{"question": "how many years ago did Faraday report on the first synthesis of compound make from carbon and chlorine , c 2 h 6 and c 2 h 4 ?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "araday reported the first synthesis of compound make from", "extracted_entity": "Michael_Faraday;Carbon;Chlorine"}
{"question": "how many years ago was it when he discovered the phenomenon that he named diamagnetism , and what is now called the Faraday effect?", "answer": "164", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["164"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "160 years ago", "extracted_entity": "Diamagnetism;Faraday_effect"}
{"question": "how many years ago was it when Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames , which resulted in an oft-reprinted cartoon in Punch?", "answer": "154", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["154"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "855", "extracted_entity": "The_Times;River_Thames;Punch_(magazine)"}
{"question": "how many years ago was it when he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal?", "answer": "162", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["162"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "of the optical properties of gold colloids differed", "extracted_entity": null}
{"question": "was education another area  of service  for Faraday?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What did Faraday discover?", "extracted_entity": null}
{"question": "was faraday `s  earliest  chemical  work as an assistant  to Davy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was employed as a bookbinder after", "extracted_entity": null}
{"question": "who died at his house at Hampton Court on August 25, 1867?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "who was the architect of the palace?", "extracted_entity": "Hampton_Court_Palace"}
{"question": "when was Michael Faraday born?", "answer": "September 22, 1791", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 22, 1791"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "22 September 1791", "extracted_entity": "Michael_Faraday"}
{"question": "where did he serve two terms?", "answer": "the Church of Scotland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Church of Scotland"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "He served two terms as a senator from Q", "extracted_entity": null}
{"question": "who was born in Newington Butts, near present-day South London, England?", "answer": "Michael Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "William Blake", "extracted_entity": "Newington_Butts;South_London;England"}
{"question": "is it true that institution formed the foundation of electric motor technology?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No,Q: Why do you think that?", "extracted_entity": null}
{"question": "institution formed the foundation of what?", "answer": "modern electromagnetic technology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern electromagnetic technology"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Church", "extracted_entity": null}
{"question": "was his father member of the Sandemanian sect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Did he ever claim to be", "extracted_entity": null}
{"question": "was faraday Christian of the small Sandemanian denomination?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was a member of the Sandemanian", "extracted_entity": null}
{"question": "is Nassau Sound a body of water in Maine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No, Nassau Sound is a body of water", "extracted_entity": "Maine"}
{"question": "was Nassau placed in operation by Robert Fulton?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau was placed in operation by Robert Fulton", "extracted_entity": "Robert_Fulton"}
{"question": "who was Fort Nassau built by?", "answer": "the Dutch", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Dutch"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The Dutch", "extracted_entity": null}
{"question": "what is Nassau Coliseum?", "answer": "an arena in Uniondale, New York, USA", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["an arena in Uniondale, New York, USA"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Nassau Veterans Memorial Coliseum (or simply the", "extracted_entity": "Nassau_Veterans_Memorial_Coliseum"}
{"question": "what was the Dead or Alive 4 fighting arena modeled after?", "answer": "a Magnetic Accelerator Cannon station from Halo 2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Magnetic Accelerator Cannon station from Halo 2"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "The Dead or Alive 4 fighting arena is modeled", "extracted_entity": "Dead_or_Alive_4"}
{"question": "is Nassau Range the highest mountain range in the world?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the highest mountain range", "extracted_entity": null}
{"question": "is Nassau County named after a German town?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nassau_County,_New_York;Germany"}
{"question": "does the United States have a base near Glasgow?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "Yes, it is called RAF Machrihanish", "extracted_entity": "United_States;Glasgow"}
{"question": "did Tesla study electrical engineering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tesla_Motors;Electrical_engineering"}
{"question": "was Tesla born in the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, he was born in what is now Croatia", "extracted_entity": "United_States"}
{"question": "was Tesla hired by Edison?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Was Tesla hired by Edison?", "extracted_entity": null}
{"question": "when did Tesla demonstrate wireless communication (radio)?", "answer": "1893", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1893"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "In 1893.", "extracted_entity": "Nikola_Tesla"}
{"question": "what is the SI unit measuring magnetic flux density or magnetic induction?", "answer": "the tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla (T)", "extracted_entity": "International_System_of_Units"}
{"question": "was Tesla regarded as a mad scientist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "He was called a mad scientist because he was known", "extracted_entity": "Nikola_Tesla"}
{"question": "what type of current did Tesla invent?", "answer": "AC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["AC"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "alternating current", "extracted_entity": null}
{"question": "who was the victor of the \"War of Currents\"?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla", "Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison.", "extracted_entity": null}
{"question": "where did Tesla study electrical engineering?", "answer": "the Austrian Polytechnic in Graz ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Austrian Polytechnic in Graz ", "Austrian Polytechnic in Graz", "The Austrian Polytechnic in Graz"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "In Graz, Austria.", "extracted_entity": "Nikola_Tesla"}
{"question": "is Ottawa the capital of Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ottawa;Canada"}
{"question": "does Ottawa sit on the Ottowa River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": "Ottawa;Ottawa_River"}
{"question": "do small tremors occur in Ottawa?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, they do occur in Ottawa.", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's population?", "answer": "1,190,982", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,190,982"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "1,168,700", "extracted_entity": "Ottawa"}
{"question": "what are Ottawa's primary industries?", "answer": "Logging and lumber", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Logging and lumber"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa's primary industries are technology, health care,", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's junior ice hockey team?", "answer": "the Ottawa 67's", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa 67's"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa 67's", "extracted_entity": "Ottawa"}
{"question": "what is Ottawa's major league hockey team?", "answer": "the Ottawa Senators", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa Senators"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa Senators", "extracted_entity": "Ottawa"}
{"question": "what are Carleton University's athletic teams called?", "answer": "Carleton Ravens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carleton Ravens"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The Carleton Ravens", "extracted_entity": "Carleton_University"}
{"question": "is Ottawa colder than Moscow in January?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, it is", "extracted_entity": "Ottawa;Moscow"}
{"question": "is Ottawa on a river?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is Ottawa in Canada?", "extracted_entity": null}
{"question": "does it snow in Ottawa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, it snows in Ottawa.", "extracted_entity": "Ottawa"}
{"question": "was there a cholera outbreak in 1832?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the name of the", "extracted_entity": null}
{"question": "who is the mayor of Ottawa?", "answer": "Larry O'Brien", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Larry O'Brien"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Jim Watson", "extracted_entity": "Ottawa"}
{"question": "how many townships are in Ottawa?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa has 23 townships.", "extracted_entity": "Ottawa"}
{"question": "what's the main highway in Ottawa?", "answer": "Highway 417, The Queensway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Highway 417, The Queensway", "provinical Highway 417"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The main highway in Ottawa is highway 417.", "extracted_entity": "Ottawa"}
{"question": "what was Ottawa's name in 1850?", "answer": "Bytown", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bytown"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Ottawa was named Bytown in 1850.", "extracted_entity": "Ottawa"}
{"question": "based on the average, in what year will the next small tremor occur in Ottawa?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "in 3.5 years", "extracted_entity": "Ottawa"}
{"question": "where were immigrants from in the 1800s?", "answer": "Irish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish", "Ireland"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Most of the immigrants in the 1800s", "extracted_entity": null}
{"question": "are there also discussions of having OC Transpo Light Rail and STO Transitway networks linking on the Gatineau side of the Ottawa River ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "es, there are.", "extracted_entity": "OC_Transpo;Gatineau;Ottawa_River"}
{"question": "does the giant otter live in South America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes, South America is a continent entirely in the", "extracted_entity": "South_America"}
{"question": "do otters have claws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Otters are carnivorous mammals in the subfamily", "extracted_entity": "Otter"}
{"question": "do otters eat fish?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes, otters are carnivorous mammals that eat", "extracted_entity": "Otter"}
{"question": "why are otters vulnerable to prey depletion?", "answer": "Prey-dependence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Prey-dependence", "prey-dependency"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Because they are large and slow growing,Q:", "extracted_entity": null}
{"question": "do otters enjoy playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes, they do. They enjoy playing with other", "extracted_entity": null}
{"question": "do sea otters have a layer of fat like whales?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "No, sea otters rely on their fur to", "extracted_entity": "Sea_otter"}
{"question": "is the otter in the same family as the badger?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Otter;Badger"}
{"question": "are there any cities named Santiago in the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "No. There are no cities named Santiago in the", "extracted_entity": "Santiago,_Chile;United_States"}
{"question": "was Santiago the name of an indie/punk band?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, Santiago was the name of an indie/p", "extracted_entity": "Santiago,_Chile"}
{"question": "is Santiago a name in Spanish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Santiago,_Chile;Spanish_language"}
{"question": "what is the origin of the name Santiago?", "answer": "Spanish for St. James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spanish for St. James", "sanit + iago, James"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The name Santiago comes from the Spanish version of the", "extracted_entity": "Santiago,_Chile"}
{"question": "who has released an album or song named Santiago?", "answer": "The Chieftains; Loreena McKennitt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Chieftains; Loreena McKennitt", "The Chieftains and Loreena McKennitt"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Chilean singer-songwriter Jorge Gonz\u00e1lez (", "extracted_entity": null}
{"question": "what is the Order of Santiago?", "answer": "a Spanish knightly order", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Spanish knightly order", "a Spanish Knightly order"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Order of Santiago is a religious and military order", "extracted_entity": "Order_of_Santiago"}
{"question": "what notable people have been named Santiago?", "answer": "Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "Benito, Hugo, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago Calatrava,Q: what notable people", "extracted_entity": null}
{"question": "what fictional stories include a main character named Santiago?", "answer": "Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "Interview with the Vampire, The Alchemist, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Old Man and the Sea", "extracted_entity": null}
{"question": "is Santiago the national capital of a country?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Santiago,_Chile"}
{"question": "is santiago Spanish for St. James  ( from santo ,  saint  +  iago ,  James  )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "s Santiago Spanish for St. James ( from s", "extracted_entity": "Spanish_language"}
{"question": "is santiago Spanish for St. James?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why is it called St.", "extracted_entity": null}
{"question": "what is Santiago?", "answer": "Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry."], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the capital and largest city of Chile and", "extracted_entity": "Santiago,_Chile"}
{"question": "has swahili no diphthongs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "There is no diphthong in swah", "extracted_entity": null}
{"question": "is a third  prefix the object  prefix?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the difference between the", "extracted_entity": null}
{"question": "is      / ref Swahili unusual among sub-Saharan languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes, it is a Bantu language that has", "extracted_entity": "Swahili_language"}
{"question": "are vowels never reduced , regardless of stress ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es. Q: What is the difference between a", "extracted_entity": null}
{"question": "does the Australian Black Swan have white feathers on its wings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the difference between a", "extracted_entity": null}
{"question": "do swans belong to the family Anatidae?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes, the Anatidae family includes swans.", "extracted_entity": "Anatidae"}
{"question": "can black swans swim with only one leg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do black swans have a", "extracted_entity": null}
{"question": "what are young swans known as?", "answer": "cygnets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cygnets", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "what is the Irish legend of the Children of Lir about?", "answer": "a stepmother transforming her children into swans for 900 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a stepmother transforming her children into swans for 900 years", "A stepmother transforming her children into swans for 900 years"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Children of Lir is a legend from Irish", "extracted_entity": "Republic_of_Ireland;Children_of_Lir"}
{"question": "what is the Sanskrit word for swan?", "answer": "hamsa or hansa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hamsa or hansa", "Hamsa or hansa"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "an is called \u0939\u0902\u0938 in Sanskrit.", "extracted_entity": "Sanskrit"}
{"question": "which album was the song \"The Bonny Swans\" from?", "answer": "The Mask and Mirror", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Mask and Mirror"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Bonny Swans", "extracted_entity": null}
{"question": "what is the Polish word for swan?", "answer": "\u0141ab\u0119d\u017a", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["\u0141ab\u0119d\u017a"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "\u0142ab\u0119d\u017a", "extracted_entity": "Polish_language"}
{"question": "which ballet by Pyotr Tchaikovsky is partially based on an Ancient German legend about a princess who was turned into a swan?", "answer": "Swan Lake", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swan Lake"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swan Lake", "extracted_entity": "Pyotr_Ilyich_Tchaikovsky;Germany"}
{"question": "are swan birds of the family Anatidae?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Anatidae"}
{"question": "do swans feature strongly in mythology?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What are the most common characteristics", "extracted_entity": null}
{"question": "is the Trumpeter Swan the largest bird?", "answer": "It is the largest bird of North America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the largest bird of North America.", "The Trumpeter Swan is the largest bird of North America"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the smallest bird?", "extracted_entity": null}
{"question": "what is the name of young swans?", "answer": "Cygnets.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cygnets.", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "where is the word \"swan\" derived from?", "answer": "Old English swan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old English swan.", "The Old English word \"swan,\" derived from the Indo-European root \"*swen\", meaning to sound or to sing"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The word \u201cswan\u201d is derived from the", "extracted_entity": null}
{"question": "what is the best known story about a swan?", "answer": "The Ugly Duckling fable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Ugly Duckling fable.", "The Ugly Duckling"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "The Ugly Duckling by Hans Christian Andersen", "extracted_entity": null}
{"question": "are swans and ducks birds of the same family?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do swans and ducks live", "extracted_entity": null}
{"question": "what is the name of an adult female?", "answer": "Pen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pen.", "An adult female swan is called a pen."], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Doe", "extracted_entity": null}
{"question": "what is the color of the Australian Black Swan?", "answer": "Completely black except for the white flight feathers on its wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Completely black except for the white flight feathers on its wings.", "Completely black except for the white feathers on its wings"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Black and White", "extracted_entity": null}
{"question": "is the Tiger a member of the Felidar family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many subspecies of Tiger", "extracted_entity": null}
{"question": "are tigers solitary animals?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do tigers live in the", "extracted_entity": null}
{"question": "are panthers a type of tiger?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Are panthers nocturnal?", "extracted_entity": null}
{"question": "what do tigers' powerfully built legs and shoulders allow tigers to do?", "answer": "They have the ability to pull down prey substantially heavier than themselves.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They have the ability to pull down prey substantially heavier than themselves.", "have the ability to pull down prey substantially heavier than themselves"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Tigers' powerfully built legs and shoulders allow them", "extracted_entity": null}
{"question": "what would a tiger do when seized by a crocodile?", "answer": "A tiger will strike at the reptile's eyes with its paws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A tiger will strike at the reptile's eyes with its paws.", "strike at the reptile's eyes with its paws"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "He would say,Q: what would a tiger", "extracted_entity": null}
{"question": "what are the typical features of a tiger country?", "answer": "It will always have good cover, it will always be close to water and plenty of prey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It will always have good cover, it will always be close to water and plenty of prey.", "Good cover, close to water, and plenty of prey "], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "The typical features of a tiger country are as follows", "extracted_entity": null}
{"question": "would it be common for tigers to mate in January?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes. Tigers usually mate between November and April.", "extracted_entity": null}
{"question": "about how many square kilometers would be needed to house 3 female tigers?", "answer": "60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 square kilometers", "60"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": ",,,. So,. So what", "extracted_entity": null}
{"question": "would a tiger be likely to feedon smaller animals, such as mice?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tiger"}
{"question": "did the predecessors to trumpets have valves?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes,The predecessor to the trumpet was the corn", "extracted_entity": null}
{"question": "are trumpets constructed of brass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, but not all", "extracted_entity": "Brass"}
{"question": "is the bass trumpet the smallest trumpet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No,Q: Is the bass trumpet the smallest", "extracted_entity": null}
{"question": "what musical styles is the trumpet used in?", "answer": "Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The trumpet is a brass instrument commonly used in classical", "extracted_entity": "Trumpet"}
{"question": "what is the most common type of trumpet?", "answer": "The B trumpet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The B trumpet."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The most common type of trumpet is the B\ufffd", "extracted_entity": null}
{"question": "how does a trumpet produce sound?", "answer": "By blowing air through closed lips.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By blowing air through closed lips."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The sound is produced by buzzing the lips.", "extracted_entity": "Trumpet"}
{"question": "how do trumpets compensate for wolf tones?", "answer": "Somce trumpetes have a slide mechanism built in to compensate.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Somce trumpetes have a slide mechanism built in to compensate."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "trumpets compensate for wolf tones by using a mute", "extracted_entity": null}
{"question": "did Miles Davis play the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Miles_Davis"}
{"question": "does a trumpet have a mellower tone than a cornet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Trumpet;Cornet"}
{"question": "is the name of the upper shell of a turtle called the plastron?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Turtle_shell"}
{"question": "do scutes fall away from the turtle's shell?", "answer": "Some of the scutes eventually fall away from the shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some of the scutes eventually fall away from the shell.", "yes, eventually"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No,Q: Do scutes grow back?", "extracted_entity": null}
{"question": "do Terrestrial tortoises have short feet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "how many groups are turtles broken down into?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two", "two"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "?", "extracted_entity": null}
{"question": "where are the only surviving giant tortoises?", "answer": "They are on the Seychelles and Gal\u00e1pagos Islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are on the Seychelles and Gal\u00e1pagos Islands.", "Seychelles and Gal\u00e1pagos Islands"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "The Galapagos Islands,Q: Why are", "extracted_entity": null}
{"question": "how do turtles chew food?", "answer": "Turtles use their jaws to cut and chew food.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles use their jaws to cut and chew food.", "Turtles use their jaws to cut and chew food"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtles chew food using their beak,Q", "extracted_entity": null}
{"question": "are tortoises land based?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No. A tort is a civil wrong. It", "extracted_entity": null}
{"question": "where are turtle eggs layed?", "answer": "Turtles lay eggs on land.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles lay eggs on land.", "on land"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtle eggs are laid on the beach. Turtles", "extracted_entity": null}
{"question": "is turtle soup considered a delicacy?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Why do turtles have such a", "extracted_entity": null}
{"question": "are Testudines the crown group of the superorder Chelonia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Crown_group"}
{"question": "do turtles breathe air?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtles breathe air through their lungs. They have", "extracted_entity": null}
{"question": "are harvesting wild turtles legal anywhere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No. In fact,Q: What is the", "extracted_entity": null}
{"question": "approximately how many species of Testudines are alive today?", "answer": "300", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "where is harvesting wild turtles legal?", "answer": "Florida", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Florida"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Harvesting wild turtles is legal in many states,", "extracted_entity": null}
{"question": "what was the largest ever chelonian?", "answer": "Archelon ischyros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Archelon ischyros", "The great letherback sea tutrtle"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Archelon.", "extracted_entity": null}
{"question": "is there a way to approximate the age of a turtle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Turtles have rings on their scales just like trees", "extracted_entity": "Turtle"}
{"question": "can turtles spend all their time underwater?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No,A: No,Q: can turtles", "extracted_entity": null}
{"question": "are violinists and fiddlers the same thing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Fiddle"}
{"question": "are violins a single size?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes. Violins are a single size and can", "extracted_entity": null}
{"question": "are violas and cellos in the same family of instruments as violins?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Viola;Cello"}
{"question": "where does the word \"violin\" come from?", "answer": "the Middle Latin word vitula, meaning \"stringed instrument\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Middle Latin word vitula, meaning \"stringed instrument\"", "the Middle Latin word vitula"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "It is derived from the Italian word violino,", "extracted_entity": null}
{"question": "what is someone who makes violins called?", "answer": "a luthier, or simply a violin maker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a luthier, or simply a violin maker", "a luthier"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "A violinmaker", "extracted_entity": null}
{"question": "how many strings does a violin usually have?", "answer": "four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["four", "4"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "A violin usually has Q: how many", "extracted_entity": null}
{"question": "what are violins made of?", "answer": "different types of wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different types of wood", "maple, ebony, sheep gut"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Most violins have a hollow wooden body.", "extracted_entity": "Violin"}
{"question": "how long have people been making instruments like violins?", "answer": "since ancient times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["since ancient times", "since 1555"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The first bowed instruments were developed in the", "extracted_entity": null}
{"question": "whate is the usual pitch range of a violin?", "answer": "from G3 to C8", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from G3 to C8"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The usual pitch range of a violin is G3", "extracted_entity": null}
{"question": "are xylophone bars made of rosewood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, the xylophone bars are made of", "extracted_entity": "Xylophone;Rosewood"}
{"question": "is the xylophone a precursor to the vibraphone?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xylophone;Vibraphone"}
{"question": "were ancient mallets made of copper?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "where did the xylophone originate?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The xylophone was invented in. It", "extracted_entity": null}
{"question": "what is the earliest historical reference in Europe?", "answer": "Arnold Schlick's Spiegel der Orgelmacher und Organisten", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arnold Schlick's Spiegel der Orgelmacher und Organisten"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "5000 BC", "extracted_entity": "Europe"}
{"question": "how are western-style xylophones characterised?", "answer": "by a bright, sharp tone and high register", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by a bright, sharp tone and high register"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "They are characterised by a single reed and", "extracted_entity": null}
{"question": "can a xylophone be 3 octaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes, a xylophone can be 3", "extracted_entity": "Xylophone"}
{"question": "can a short bar follow a long bar?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes,Q: can a long bar follow a", "extracted_entity": null}
{"question": "did vibraphones exist in 1930?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "No,Q: What is a vibraphone", "extracted_entity": null}
{"question": "was Alessandro Volta a professor of chemistry?", "answer": "Alessandro Volta was not a professor of chemistry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was not a professor of chemistry.", "No"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was a professor of chemistry", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta invent the remotely operated pistol?", "answer": "Alessandro Volta did invent the remotely operated pistol.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta did invent the remotely operated pistol.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alessandro_Volta"}
{"question": "was Alessandro Volta taught in public schools?", "answer": "Volta was taught in public schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was taught in public schools.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta was taught in public schools.", "extracted_entity": "Alessandro_Volta"}
{"question": "who did Alessandro Volta marry?", "answer": "Alessandro Volta married Teresa Peregrini.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta married Teresa Peregrini.", "Teresa Peregrini"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Teresa Peregrini", "extracted_entity": "Alessandro_Volta"}
{"question": "what did Alessandro Volta invent in 1800?", "answer": "In 1800, Alessandro Volta invented the voltaic pile.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800, Alessandro Volta invented the voltaic pile.", "voltaic pile"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He invented the voltaic pile", "extracted_entity": "Alessandro_Volta"}
{"question": "what is the battery made by Alessandro Volta credited as?", "answer": "The battery made by Volta is credited as the first electrochemical cell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The battery made by Volta is credited as the first electrochemical cell.", "the first electrochemical cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the electric battery", "extracted_entity": "Alessandro_Volta"}
{"question": "did Alessandro Volta die and retire in the same place?", "answer": "Alessandro Volta retired and died in the same place.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta retired and died in the same place.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alessandro_Volta"}
{"question": "when did Alessandro Volta improve  and popularize the electrophorus?", "answer": "Alessandro Volta improved and popularized the electrophorus in 1775.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta improved and popularized the electrophorus in 1775.", "1775"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "in 1799", "extracted_entity": "Alessandro_Volta"}
{"question": "how long was Alessandro Volta a professor at the University of Pavia?", "answer": "Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "almost 25 years"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "40 years", "extracted_entity": "Alessandro_Volta;University_of_Pavia"}
{"question": "was Alessandro Volta an Egyptian?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, Alessandro Volta was not an Egyptian", "extracted_entity": "Alessandro_Volta;Egypt"}
{"question": "was Volta taught in public schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. Volta was not taught in public schools", "extracted_entity": null}
{"question": "was Volta made a count in 1810?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was made a count in Q:", "extracted_entity": null}
{"question": "who made Volta a count?", "answer": "Napoleon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Napoleon"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The French physicist and chemist,Q: What", "extracted_entity": null}
{"question": "where was Volta born?", "answer": "Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Como", "Como, Italy"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Como", "extracted_entity": "Alessandro_Volta"}
{"question": "when did Volta retire?", "answer": "1819", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1819", "In 1819."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "a year before improving and popularizing the electrophorus, what did Volta become?", "answer": "A professor of physics at the Royal School in Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A professor of physics at the Royal School in Como"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta became a battery.", "extracted_entity": null}
{"question": "was Volta buried where he died or was he buried someplace else?", "answer": "where he died", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where he died", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He was buried where he died.", "extracted_entity": null}
{"question": "did Volta marry before he became professor of experimental physics at the University of Pavia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Experimental_physics;University_of_Pavia"}
{"question": "what happened in 1810?", "answer": "Volta was made a count by Napoleon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The French Revolution was a period of far-reaching social", "extracted_entity": null}
{"question": "wasn't Alessandro Volta born in Como?", "answer": "Yes, Volta was born in Como, Italy and was taught in the public schools there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was born in Como, Italy and was taught in the public schools there."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, he was born in Como.", "extracted_entity": "Alessandro_Volta;Como"}
{"question": "was Alessandro Volta born in Como?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, Alessandro Volta was born in Como", "extracted_entity": "Alessandro_Volta;Como"}
{"question": "who became a professor of physics at the Royal School in Como?", "answer": "Volta.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta", "extracted_entity": "Como"}
{"question": "when was Volta made a count by Napoleon?", "answer": "Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1801", "extracted_entity": "Alessandro_Volta;Napoleon_I"}
{"question": "is Volta's legacy celebrated by a Temple on the shore of Lake Como in the center of the town?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Lake_Como"}
{"question": "is the battery made by Volta credited as the first electrochemical cell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No,Q: Why did the battery made by", "extracted_entity": null}
{"question": "was Volta made a count by Napoleon in 1810?", "answer": "Yes, Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Volta was made a count by Napoleon in", "extracted_entity": "Napoleon_I"}
{"question": "in what year did he become a professor of physics at the Royal School in Como?", "answer": "1774", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1774"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Como", "extracted_entity": "Como"}
{"question": "was Avogadro a  professor at the University of Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avogadro was a professor at the University of Turin."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "University_of_Turin"}
{"question": "was he a member of the Royal Superior Council on Public Instruction?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes,  Avogadro was a member of the Royal Superior Council on Public Instruction."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is Avogadro's number used to compute the results of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avagadro's number is used to compute the results of chemical reactions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the chemical formula for", "extracted_entity": null}
{"question": "who first calculated the value of Avogadro's number?", "answer": "Johann Josef Loschmidt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Johann Josef Loschmidt", "Johann Josef Loschmidt first calculated the value of Avogadro's number."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "what does Avogadro's Law state?", "answer": "The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "Avogadro's Law states that the relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's law states that the volume of", "extracted_entity": null}
{"question": "who showed that Avogadro's theory held in dilute solutions?", "answer": "Jacobus Henricus van Hoff", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacobus Henricus van Hoff", "Jacobus Henricus van 't Hoff showed that Avogadro's theory holds in dilute solutions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Jean Baptiste Perrin", "extracted_entity": null}
{"question": "in what language was his 1811 paper published?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French", "Avogadro's 1811 paper was published in French."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "who was Avogadro's wife?", "answer": "Felicita Mazz", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felicita Mazz", "Felicita Mazz\u00e9 was Avogadro's wife."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's wife was Maria Stefana Inc", "extracted_entity": null}
{"question": "why did Avogadro lose his chair at the University of Turin?", "answer": "He was active in the revolutionary movements of 1821 against the king of Sardinia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was active in the revolutionary movements of 1821 against the king of Sardinia", "Avogadro lost his chair at the University of Turin because he was active in the revolutionary movements of 1821 against the king of Sardinia."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro lost his chair at the University of", "extracted_entity": "University_of_Turin"}
{"question": "was Amedeo Avogadro was born in Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Turin", "extracted_entity": "Turin"}
{"question": "is Avogadro hailed as a founder of the atomic-molecular theory?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No,Q: Is Avogadro hailed as", "extracted_entity": null}
{"question": "did Johann Josef Loschmidt first calculate the value of Avogadro's number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Did Johann Josef Loschmidt", "extracted_entity": null}
{"question": "in 1820, Avogadro became a professor of physics where?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin", "extracted_entity": null}
{"question": "avogadro did not actually use what word?", "answer": "atom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atom"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "atom", "extracted_entity": null}
{"question": "the number of elementary entities in 1 mole of a substance is known as what?", "answer": "Avogadro constant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro constant", "Avogadro's constant"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro number", "extracted_entity": null}
{"question": "what would a German call Avogadro's number?", "answer": "Loschmidt number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loschmidt number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The number is  of course, 1-", "extracted_entity": "Germany"}
{"question": "is Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, Amedeo Avogadro is Italian", "extracted_entity": "Italy"}
{"question": "did he become a professor before the revolutionary movements against the king of Sardinia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "he became a professor after the revolutionary movements against the", "extracted_entity": "Sardinia"}
{"question": "do ants belong to the Hymenoptera order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": "Hymenoptera"}
{"question": "are ants used in cuisine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Ants are used in cuisine.", "extracted_entity": null}
{"question": "does an ant's head contain sensory organs?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes. The head contains many sensory organs to receive", "extracted_entity": "Ant"}
{"question": "how do most ants travel?", "answer": "most ants travel by walking", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["most ants travel by walking", "by walking"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Most ants travel in columns.", "extracted_entity": null}
{"question": "in ant colonies, what are the fertile female ants called?", "answer": "queens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["queens"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The fertile female ants are called the queens.", "extracted_entity": null}
{"question": "who wrote about ants in A Tramp Abroad?", "answer": "mark twain wrote about ants", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mark twain wrote about ants", "Mark Twain"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Ants in A Tramp Abroad was written", "extracted_entity": null}
{"question": "do the ants eat plants, meats, or both?", "answer": "both", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["both"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "both", "extracted_entity": null}
{"question": "what organs gives a bull ant its good sight?", "answer": "compound eyes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["compound eyes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The bull ant has compound eyes,. These eyes", "extracted_entity": null}
{"question": "what may happen to red fire ants if we use boiling water on the queen?", "answer": "nests of red fire ants may be destroyed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nests of red fire ants may be destroyed", "die"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The red fire ant queen may die if we use", "extracted_entity": null}
{"question": "are ants found in Antartica?", "answer": "No, there are no ants in antarctica.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, there are no ants in antarctica."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No, ants are not found in Antartica", "extracted_entity": "North_Antrim_(Assembly_constituency)"}
{"question": "do male ants take flight before females?", "answer": "Yes, male ants take flight before females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, male ants take flight before females."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No. Male ants take flight after females.", "extracted_entity": null}
{"question": "do worker ants have wings?", "answer": "No, worker ants do not have wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, worker ants do not have wings."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No,Q: How do worker ants communicate with", "extracted_entity": null}
{"question": "what are the three segments of an ant?", "answer": "The head, mesosoma and metasoma are the three distinct body segments.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The head, mesosoma and metasoma are the three distinct body segments."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "the head, thorax, and gaster.", "extracted_entity": "Ant"}
{"question": "what are the ant colonies that lack queens called?", "answer": "Colonies that lack queens are called gamergate colonies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colonies that lack queens are called gamergate colonies."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Workerless colonies", "extracted_entity": null}
{"question": "where are bullet ants located?", "answer": "Bullet ants are located in Central and South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bullet ants are located in Central and South America."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Bullet ants are found in the southern United States,", "extracted_entity": null}
{"question": "do ants belong to the same order as bees?", "answer": "Yes, ants belong to the same order as bees.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, ants belong to the same order as bees."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, both belong to the order Hymen", "extracted_entity": "Bee"}
{"question": "what is the most common color of ants?", "answer": "Most ants are red or black.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most ants are red or black."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Red", "extracted_entity": null}
{"question": "do all ants build nests?", "answer": "No, not all ants build nests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, not all ants build nests."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, all ants build nests. Some ants build", "extracted_entity": "Ant"}
{"question": "ultraviolet vision was first discovered in ants by Sir who in 1881?", "answer": "John Lubbok", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Lubbok"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Sir John Lubbock", "extracted_entity": null}
{"question": "do ants thrive in most ecosystems?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Ants thrive in most ecosystems because they play an", "extracted_entity": null}
{"question": "are several species of ants able to use the Earth's magnetic field?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How do they do this?", "extracted_entity": null}
{"question": "are termites actually more closely related to cockroaches as well as mantids?", "answer": "Yes, termites are actually more closely related to cockroaches as well as mantids.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, termites are actually more closely related to cockroaches as well as mantids."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Termites are actually more closely related to cockro", "extracted_entity": "Cockroach;Mantidae"}
{"question": "do some caterpillars produce vibrations as well as sounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes. The sounds are produced by the caterpill", "extracted_entity": null}
{"question": "did ants evolve from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, ants evolved from wasp-like ancestors in", "extracted_entity": "Cretaceous"}
{"question": "is the ant a marsupial?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Marsupial"}
{"question": "are they easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist?", "answer": "Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Antwerp a city?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the population of Ant", "extracted_entity": null}
{"question": "is Antwerp a municipality?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is Antwerp a city?", "extracted_entity": null}
{"question": "is Antwerp in Belgium?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Royal_Antwerp_FC;Belgium"}
{"question": "what is Antwerp?", "answer": "Antwerp is a city and municipality in Belgium.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is a city and municipality in Belgium."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is a city in Belgium", "extracted_entity": "Antwerp"}
{"question": "what is the population of the city of Antwerp?", "answer": "Antwerp's population is 472,071.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp's population is 472,071."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "The population of the city of Antwerp is", "extracted_entity": "Antwerp"}
{"question": "where is the city of Antwerp?", "answer": "Antwerp is in Belgium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is in Belgium"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is in Belgium", "extracted_entity": "Antwerp"}
{"question": "why is Antwerp important to fashion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is an important city for fashion because it", "extracted_entity": "Antwerp"}
{"question": "antwerp is to the east of what river?", "answer": "Antwerp is to the east of the Scheldt river", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is to the east of the Scheldt river"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Scheldt", "extracted_entity": "Antwerp"}
{"question": "how many municipalities does Antwerp have?", "answer": "None, Antwerp is a municipality.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None, Antwerp is a municipality."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antwerp"}
{"question": "is Arabic a Central Semitic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arabic_language"}
{"question": "was Arabic a major vehicle of culture in Europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arabic_language;Europe"}
{"question": "is Arabic classified alongside Semitic languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Semitic_languages"}
{"question": "how many people speak the Arabic language?", "answer": "280 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["280 million people."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "The Arabic language is spoken by over   the", "extracted_entity": "Arabic_language"}
{"question": "when was Arabic calligraphy invented?", "answer": "Many styles were developed after 786.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many styles were developed after 786."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic calligraphy was invented in Q: when", "extracted_entity": null}
{"question": "where is Arabic spoken?", "answer": "The Middle East and North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Middle East and North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "The Arabic language is spoken in North Africa, the", "extracted_entity": "Arabic_language"}
{"question": "why is Arabic related to Islam?", "answer": "Arabic is the liturgical language of Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic is the liturgical language of Islam"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "ecause the Quran is written in", "extracted_entity": "Arabic_language;Islam"}
{"question": "why does Arabic heavily influence European languages?", "answer": "Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "it is because the Arabs were the ones who invented", "extracted_entity": "Arabic_language;Europe"}
{"question": "is Arabic the largest member of the Semitic language family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arabic_language"}
{"question": "does Modern Standard Arabic continue to evolve like other languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No. It is a fixed,,: How", "extracted_entity": null}
{"question": "is Hassaniya Arabic spoken in Mauritania?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": ", Hassaniya Arabic is spoken in Maurit", "extracted_entity": "Hassaniya_Arabic;Mauritania"}
{"question": "what is the only variety of modern Arabic that has acquired official language status?", "answer": "maltese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["maltese", "Maltese"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Classical Arabic", "extracted_entity": "Arabic_language"}
{"question": "egyptian Arabic is spoken by how many in Egypt?", "answer": "76 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["76 million"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "76,300,000", "extracted_entity": "Egypt"}
{"question": "where are the Western Arabic numerals used?", "answer": "North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North Africa", "present-day North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Middle East,Q: where are the", "extracted_entity": "Arabic_numerals"}
{"question": "the most active Academies of Arabic Language are found where?", "answer": "damascus and cairo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["damascus and cairo"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "MoroccoB: Saudi ArabiaC: EgyptD:", "extracted_entity": null}
{"question": "hassan Massoudy is a master of what genre?", "answer": "Hassan Massoudy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hassan Massoudy", "Arabic calligraphy"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Rock", "extracted_entity": null}
{"question": "is Berlin the capital city of Germany?", "answer": "Berlin is the capital city of Germany.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is the capital city of Germany.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Berlin;Germany"}
{"question": "is Berlin the largest city in Germany?", "answer": "Berlin is Germany's largest city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is Germany's largest city.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin is the capital and largest city of Germany by", "extracted_entity": "Berlin;Germany"}
{"question": "is Schloss Charlottenburg the largest existing palace in Berlin?", "answer": "Schloss Charlottenburg is the largest existing palace in Berlin. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Schloss Charlottenburg is the largest existing palace in Berlin. ", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no, it's not the largest existing palace in", "extracted_entity": "Charlottenburg_Palace;Berlin"}
{"question": "which building is the site of the German parliament?", "answer": "The Reichstag building is the site of the German parliament. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building is the site of the German parliament. ", "The Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Reichstag building.", "extracted_entity": null}
{"question": "which two sports events did the Olympiastadion host?", "answer": "The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "the 1936 Summer Olympics and the 2006 FIFA World Cup final"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "1936 Summer Olympics, 1994 Summer", "extracted_entity": "Olympiastadion_(Berlin)"}
{"question": "where is the Berliner Dom located?", "answer": "The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "on the Spree Island"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berliner Dom is located on the Museum Island in", "extracted_entity": "Berlin_Cathedral"}
{"question": "which Berlin building did architect Norman Foster remodel in the 1990s?", "answer": "The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Reichstag building", "extracted_entity": "Berlin;Norman_Foster,_Baron_Foster_of_Thames_Bank"}
{"question": "why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?", "answer": "Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "Because burning lignite produces harmful emissions"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Vattenfall is shifting towards reliance on cleaner,", "extracted_entity": "Vattenfall"}
{"question": "what are the names of the two zoos in Berlin?", "answer": "The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "Zoologischer Garten Berlin and Tierpark Friedrichsfelde"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin zoo and Tierpark Berlin", "extracted_entity": "Berlin"}
{"question": "is Berlin the headquarters of Springer?", "answer": "Yes, Berlin is the headquarters of Springer.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Berlin is the headquarters of Springer."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin is the headquarters of Springer", "extracted_entity": "Berlin;Axel_Springer_SE"}
{"question": "does the Gendarmenmarkt border the French Cathedral?", "answer": "Yes, the Gendarmenmarkt borders the French Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the Gendarmenmarkt borders the French Cathedral."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gendarmenmarkt;France"}
{"question": "is the Nauen plain north of Berlin?", "answer": "No, the Nauen plain stretches to the west of Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the Nauen plain stretches to the west of Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nauen;Berlin"}
{"question": "where is the bust of Queen Nefertiti?", "answer": "The bust of Queen Nefertiti is in the Altes Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The bust of Queen Nefertiti is in the Altes Museum."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "in the Cairo Museum of Egyptian Antiquities.", "extracted_entity": null}
{"question": "where does the German President live?", "answer": "The German President lives west of the center, Schloss Bellevue.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The German President lives west of the center, Schloss Bellevue."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the president lives in Berlin", "extracted_entity": "President_of_Germany"}
{"question": "when did Berlin give up its status as a free Hanseatic city?", "answer": "In 1451 Berlin gave up its status as a free Hanseatic city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1451 Berlin gave up its status as a free Hanseatic city."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin gave up its status as a free Hanse", "extracted_entity": "Berlin;Hanseatic_League"}
{"question": "which is the busiest airport in Berlin?", "answer": "Tegel International Airport is the busiest airport in Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tegel International Airport is the busiest airport in Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Berlin Tegel Airport", "extracted_entity": "Berlin"}
{"question": "how many occupation zones was Germany divided into after the war?", "answer": "Germany was divided into four occupation zones after the war.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany was divided into four occupation zones after the war."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Germany was divided into four occupation zones.", "extracted_entity": null}
{"question": "where was Popkomm held before moving to Berlin?", "answer": "Popkomm was held in Cologne before moving to Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Popkomm was held in Cologne before moving to Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "it was held in Cologne", "extracted_entity": "Popkomm;Berlin"}
{"question": "who was Blaise Pascal's father?", "answer": "His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "\u00c3\u2030tienne Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "tienne Pascal", "extracted_entity": "Blaise_Pascal"}
{"question": "was Pascal a French mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he was a French mathematician.", "extracted_entity": "Blaise_Pascal;France"}
{"question": "did Pascal have poor health throughout his life?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "ascal had poor health throughout his life.", "extracted_entity": "Blaise_Pascal"}
{"question": "how old was Pascal when he lost his mother?", "answer": "at the age of three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the age of three", "3."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "s far as I can tell, he was", "extracted_entity": "Blaise_Pascal"}
{"question": "who was Pascal's younger sister?", "answer": "Jacqueline", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacqueline", "Jacqueline."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "is younger sister was Jacqueline Pascal", "extracted_entity": "Blaise_Pascal"}
{"question": "what led Pascal to his religious conversion?", "answer": "Two basic influences led him to his conversion: sickness and Jansenism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two basic influences led him to his conversion: sickness and Jansenism", "sickness and Jansenism"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "is father\u2019s death in  the mid-164", "extracted_entity": "Blaise_Pascal"}
{"question": "how old was Pascal when he died?", "answer": "39", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["39", "1662-08-19."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "y  answer these questions:", "extracted_entity": "Blaise_Pascal"}
{"question": "did Pascal write about cycloid before 1658?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cycloid"}
{"question": "is there a programming language called Pascal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes, there is a programming language called Pascal.", "extracted_entity": "Pascal_(programming_language)"}
{"question": "crowds of believers came to see and kiss the thorn; all of Catholic where acclaimed a miracle?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "crowds of believers came to see and kiss the th", "extracted_entity": "Catholic_Church"}
{"question": "wasn't Blaise Pascal a work of Desargues on conic sections?", "answer": "Yes, it was", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it was"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "o, Blaise Pascal was a work of Des", "extracted_entity": "Blaise_Pascal;Girard_Desargues"}
{"question": "who died the next morning?", "answer": "Pascal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The woman who was killed the night before.", "extracted_entity": null}
{"question": "when did his father die?", "answer": "1651", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1651", "His father died in 1651."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "At the age of Q: What was his", "extracted_entity": null}
{"question": "was he a child prodigy who was educated by his father, a civil servant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "He was a child prodigy who was educated by", "extracted_entity": null}
{"question": "was Pascal's earliest work in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "ascal was a child prodigy who was educated by", "extracted_entity": "Blaise_Pascal;Mechanical_calculator;Fluid;Pressure"}
{"question": "did Pascal also write in defense of the scientific method?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "es, he wrote in defense of the scientific method", "extracted_entity": "Blaise_Pascal"}
{"question": "do butterflies make sounds?", "answer": "Some butterflies make sounds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some butterflies make sounds."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes, they make sounds", "extracted_entity": "Butterfly"}
{"question": "do butterflies have two eyes?", "answer": "Yes, butterflies have two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, butterflies have two eyes."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": "Butterfly"}
{"question": "does a black moth mean that someone has died?", "answer": "In the Philippines a black butterfly or moth mean that someone has died.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Philippines a black butterfly or moth mean that someone has died."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "No,Q: What does a black moth mean", "extracted_entity": null}
{"question": "what is the outer layer of the cuticle made of?", "answer": "The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "cutin", "extracted_entity": null}
{"question": "where was there a vast swarm of butterflies?", "answer": "In Kyoto there was a vast swarm of butterflies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Kyoto there was a vast swarm of butterflies."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "It was in the sky.", "extracted_entity": null}
{"question": "what butterfly is migratory?", "answer": "The Monarch butterfly is migratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monarch butterfly is migratory."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Monarch butterfly", "extracted_entity": null}
{"question": "do butterflies carry more pollen than Hymenoptera?", "answer": "No, butterflies carry less pollen than Hymenoptera.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, butterflies carry less pollen than Hymenoptera."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "No, butterflies carry less pollen than Hymen", "extracted_entity": "Butterfly;Hymenoptera"}
{"question": "what allows the butterfly to evade predators?", "answer": "A butterfly's hind wings allow the butterfly to evade predators.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A butterfly's hind wings allow the butterfly to evade predators."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "the butterfly has a unique wing pattern that is different", "extracted_entity": null}
{"question": "what happens if a wing is removed?", "answer": "If a ring is removed the other three will grow to a larger size.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["If a ring is removed the other three will grow to a larger size."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "The bird will fall to the ground.", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a Spanish Biologist?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, he was a French physicist."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb was a French Bi", "extracted_entity": "Spain"}
{"question": "was the SI unit of charge named after Charles-Augustin de Coulomb?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, the SI unit of charge, the coulomb, was named after him."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes,Q: was the SI unit of charge", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb ever employed at La Rochelle?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, upon his return to France, with the rank of Captain, he was employed at La Rochelle."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Charles-Augustin_de_Coulomb;La_Rochelle"}
{"question": "what is Charles-Augustin de Coulomb best known for?", "answer": "He is best known for developing Coulomb's law.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He is best known for developing Coulomb's law."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He is best known for his work in electricity and", "extracted_entity": null}
{"question": "whose ideas inspired Charles-Augustin de Coulomb's experiments on the resistance of masonries?", "answer": "Pieter van Musschenbroek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pieter van Musschenbroek.", "He carried out several experiments on the resistance of masonries which were inspired by the ideas of Pieter van Musschenbroek on friction."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The ideas of Charles-Augustin de Coulomb's", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "when did Charles-Augustin de Coulomb join his father's family in Montpeillier?", "answer": "From 1757 to 1759 he joined his father's family in Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1757 to 1759 he joined his father's family in Montpellier."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "did Charles-Augustin de Coulomb come from a poor family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, his mother came from a wealthy family in the wool trade."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb came from a rich", "extracted_entity": "Charles-Augustin_de_Coulomb"}
{"question": "what is Coulomb's law?", "answer": "The definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The definition of the electrostatic force of attraction and repulsion."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb's law is a mathematical formula that describes", "extracted_entity": null}
{"question": "where is Coll\u00e8ge des Quatre-Nations located?", "answer": "In Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Montpellier.", "In Paris."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coll\u00e8ge des Quatre-Nations is located in", "extracted_entity": "Coll\u00e8ge_des_Quatre-Nations"}
{"question": "what happened in 1785?", "answer": "Coulomb presented his three reports on Electricity and Magnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb presented his three reports on Electricity and Magnetism"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the US declared war on Great Britain.", "extracted_entity": null}
{"question": "what happened in 1757 to 1759?", "answer": "Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Q: the answer is Q:", "extracted_entity": null}
{"question": "regarding this topic, what did the Revolution do?", "answer": "The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The Revolution changed the lives of the people who lived", "extracted_entity": null}
{"question": "who determined to pursue mathematics?", "answer": "Charles-Augustin de Coulomb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The people who were good at math", "extracted_entity": null}
{"question": "when did Coulomb return to Paris?", "answer": "In 1759.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1759."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "In June 1827, Coulomb returned to", "extracted_entity": "Daniel_Ducruet;Paris"}
{"question": "does Coulomb leave a legacy as a pioneer in the field of geotechnical engineering for his contribution to retaining wall design?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No,Coulomb was a French physicist known", "extracted_entity": null}
{"question": "did the family move to Paris as well as there Coulomb studied at the prestigious Collge des Quatre-Nations?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, they moved to Paris and he studied there", "extracted_entity": "Paris;Coll\u00e8ge_des_Quatre-Nations"}
{"question": "was Coulomb born in Angoulme, France to a well-to-do family?", "answer": "Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, Coulomb was born in Angoulme", "extracted_entity": "Angoul\u00eame;France"}
{"question": "is he best known for developing Coulomb's law, the definition of the electrostatic force of attraction and repulsion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb", "extracted_entity": "Coulomb's_law;Electrostatics"}
{"question": "the SI unit of charge, the coulomb, was named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles-Augustin de Coulomb (1736-", "extracted_entity": "International_System_of_Units;Coulomb"}
{"question": "did Coulomb die in Angoul\u00c3\u00aame, France, to a well-to-do family?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Angoul\u00eame;France"}
{"question": "are all spoken varieties of Chinese tonal and analytical?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, all spoken varieties of Chinese are tonal", "extracted_entity": "Chinese_language"}
{"question": "is fifty percent or more of Korean vocabulary of Chinese origin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "does \"ketchup\" come from the Minnan pronunciation of \u9bad\u6c41 (koe-tsiap)?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Southern_Min"}
{"question": "what is the most common romanization standard for Standard Mandarin today?", "answer": "Hanyu Pinyin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hanyu Pinyin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The most common romanization standard for Standard Mandarin today", "extracted_entity": "Standard_Chinese"}
{"question": "what is the official standard language used by the People's Republic of China, the Republic of China, and Singapore (where it is called \"Huayu\")?", "answer": "Putonghua / Guoyu, often called \"Mandarin\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Putonghua / Guoyu, often called \"Mandarin\"", "Standard Mandarin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Mandarin Chinese", "extracted_entity": "People's_Republic_of_China;Taiwan;Singapore"}
{"question": "how many head entries for character definitions does the Zhonghua Zihai \u4e2d\u534e\u5b57\u6d77 (1994) contain?", "answer": "85,568 head entries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["85,568 head entries", "54,678"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": ":: ::Q: how many head entries", "extracted_entity": null}
{"question": "what does the PRC government classify as literacy amongst workers?", "answer": "knowledge of 2,000 characters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["knowledge of 2,000 characters", "a knowledge of 2000 characters"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "the ability to read and write", "extracted_entity": "People's_Republic_of_China"}
{"question": "how is South China linguistically different from North China?", "answer": "South China displays more linguistic diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["South China displays more linguistic diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "South China is linguistically different from North China because", "extracted_entity": "South_China;North_China"}
{"question": "\u5348\u9910\u76d2 \u201clunchbox or boxed lunch\u201d (from bento) and \u6599\u7406 \u201cprepared cuisine\u201d are loan words from which language?", "answer": "Japanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japanese", "Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Japanese", "extracted_entity": "Bento"}
{"question": "what is the standardized form of spoken chinese?", "answer": "The standardized form of spoken Chinese is Standard Mandarin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The standardized form of spoken Chinese is Standard Mandarin.", "The standardized from of spoken Chinese is Standard Mandarin."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "mandarin", "extracted_entity": "Chinese_language"}
{"question": "is diglossia a common feature in mainland China and Taiwan?", "answer": "Yes, diglossia is a common feature in mainland China and Taiwan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, diglossia is a common feature in mainland China and Taiwan."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "People's_Republic_of_China;Taiwan"}
{"question": "do linguists often view Chinese as a language family?", "answer": "Yes, linguists often view Chinese as a language family.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, linguists often view Chinese as a language family."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinese_language"}
{"question": "when did classical Chinese literature begin?", "answer": "Classical Chinese literature began in the Spring and Autumn period.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Classical Chinese literature began in the Spring and Autumn period.", "Classical Chinese literature began in the 14th to 11th centuires BCE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Q. What is the earliest known piece of", "extracted_entity": null}
{"question": "how does the government of the People's Republic of China often refer to all divisions of the Sinitic language(s) besides Standard Mandarin?", "answer": "The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "The PRC commonly refers to all divisons of the Sinitic language(s) beside Standard Mandarin as fangyan, or \"regional tongues.\""], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "by the term \"\u6c49\u8bed/\u6f22\u8a9e\"", "extracted_entity": "People's_Republic_of_China;Sinitic_languages;Standard_Chinese"}
{"question": "when was Middle Chinese used?", "answer": "Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "Middle Chinese was the language used in the 6th through 1-th centuries CE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Middle Chinese was used from the  6th", "extracted_entity": "Middle_Chinese"}
{"question": "how is the pronunciation of foreign words determined?", "answer": "The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "Foreign words are transliterated into close-match existing Chinese morphemes."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The pronunciation of foreign words is determined by the rules", "extracted_entity": null}
{"question": "what is the total number of Chinese words?", "answer": "The Hanyu Da Zidian includes 54,678 head entries for characters.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Hanyu Da Zidian includes 54,678 head entries for characters."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "1.3 billion", "extracted_entity": "Chinese_language"}
{"question": "what language was used in the Zhou and Southern Dynasties?", "answer": "Old Chinese was the language common during the early and middle Zhou Dynasty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old Chinese was the language common during the early and middle Zhou Dynasty."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The Zhou dynasty was the longest dynasty in Chinese history", "extracted_entity": "Zhou_dynasty;Northern_and_Southern_dynasties"}
{"question": "is a cougar also known as a mountain lion?", "answer": "A cougar is also known as a mountain lion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cougar is also known as a mountain lion.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes, a cougar is also known as a mountain", "extracted_entity": "Cougar"}
{"question": "are cougars larger than jaguars?", "answer": "Cougars are not larger than jaguars.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cougars are not larger than jaguars.", "no"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cougar;Jaguar"}
{"question": "does the World Conservation Union consider the cougar a \"least concern\" species?", "answer": "The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "International_Union_for_Conservation_of_Nature;Cougar"}
{"question": "what are some of the cougar's primary food sources?", "answer": "The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses, and sheep"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "The cougar's primary food sources include deer,Q", "extracted_entity": null}
{"question": "how long are cougar adult males (from nose to tail)?", "answer": "The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "2.4 meters"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "The adult male cougar measures   from nose to", "extracted_entity": "Cougar"}
{"question": "how long is an adult cougar's paw print?", "answer": "An adult cougar's paw print is approximately 10 cm (4 inches) long.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An adult cougar's paw print is approximately 10 cm (4 inches) long.", "4 inches"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cougar"}
{"question": "what are the three heaviest cats in the world?", "answer": "The three heaviest cats in the world are the tiger, lion and jaguar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three heaviest cats in the world are the tiger, lion and jaguar.", "tiger, lion, and jaguar"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "The cat of communications,Q: Why do cats", "extracted_entity": null}
{"question": "on average, are cougar males heavier than females?", "answer": "On average, cougar males are heavier than females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On average, cougar males are heavier than females.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cougar"}
{"question": "what are the three cat species that are native to Canada?", "answer": "The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "cougar, bobcat, Canadian lynx"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Canada lynx, bobcat, cougar", "extracted_entity": "Canada"}
{"question": "is one of the parts of a cymbal called a disc?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cymbal"}
{"question": "are ride cymbals very common?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Are ride cymbals very", "extracted_entity": null}
{"question": "does Fender make cymbals?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, Fender does not make cymbals", "extracted_entity": "Fender_Musical_Instruments_Corporation"}
{"question": "what part of the cymbal is the bell?", "answer": "The center of a Cymbal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The center of a Cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The part of the cymbal that produces the", "extracted_entity": null}
{"question": "what's the timber of ancient cimbals like? disfluent?", "answer": "like that of small hand-bells or of the notes of the keyed harmonica", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["like that of small hand-bells or of the notes of the keyed harmonica"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "The timber of ancient cimbals is the same", "extracted_entity": null}
{"question": "what does it sound like to play cymbals with a cello bow?", "answer": "squealing car brakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["squealing car brakes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "I have no idea. I've never done it", "extracted_entity": null}
{"question": "what are cymbals made of?", "answer": "brass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbals are made of thin normally round plates", "extracted_entity": "Cymbal"}
{"question": "how many cymbals typically go in a drum kit?", "answer": "at least 3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at least 3"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "drum kits are not standardized, they can have", "extracted_entity": null}
{"question": "what kinds of musical groups use cymbals?", "answer": "Cymbals are used in modern orchestras and many military, marching, concert and other bands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cymbals are used in modern orchestras and many military, marching, concert and other bands"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbals are used in marching bands,Q", "extracted_entity": null}
{"question": "is the dragonfly associated with late summer as well as early autumn?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the dragonfly associated with", "extracted_entity": null}
{"question": "does Exposure to air cause the larva to begin breathing?", "answer": "Yes (capitalization).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (capitalization)."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "No,Q: Do larvae breathe?A:", "extracted_entity": null}
{"question": "is it characterized by large multifaceted eyes, two pairs of strong transparent wings, and an elongated body?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "Yes, it is a dragonfly.", "extracted_entity": null}
{"question": "is the drum a member of the percussion group?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes. The drum is a member of the percussion", "extracted_entity": null}
{"question": "does each type of drum head serve its own musical purpose?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drum_kit"}
{"question": "is the Aburukuwa a type of drum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes. The Aburukuwa is a type", "extracted_entity": null}
{"question": "how long has the basic design of the drum remained unchanged?", "answer": "remained virtually unchanged for thousands of years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["remained virtually unchanged for thousands of years", "thousands of years"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "The drum is believed to have been invented by the", "extracted_entity": null}
{"question": "what is the second biggest factor affecting the sound produced by a drum?", "answer": "tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["tension", "the tension at which the drum head is held against the shell of the drum."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the size of the drum", "extracted_entity": "Drum_kit"}
{"question": "by what name are drums classified as under the percussion group?", "answer": "membranophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["membranophone", "membranous"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Membranophones", "extracted_entity": null}
{"question": "how do rock and roll drummers like their drumheads?", "answer": "rock and roll drummer may prefer drums that sound loud and deep", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["rock and roll drummer may prefer drums that sound loud and deep", "drums that sound loud and deep"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Rock and roll drummers like their drumheads to", "extracted_entity": "Rock_and_roll"}
{"question": "whare is the name for drums that have a set of wires held across some of all of the drum heads?", "answer": "snares", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["snares"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "The name for drums that have a set of wires", "extracted_entity": null}
{"question": "had the drums been used in Sri Lanka since Before Christ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, the drums had been used in Sri Lanka", "extracted_entity": "Sri_Lanka"}
{"question": "are drums often used in music therapy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, drums are often used in music therapy to", "extracted_entity": "Music_therapy"}
{"question": "do drums consist of at least eight membranes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do drums consist of at least", "extracted_entity": null}
{"question": "what has the most effect on how a drum sounds?", "answer": "the drumhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the drumhead"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the type of drum head", "extracted_entity": null}
{"question": "who constructed a kit using diatonic and chromatically tuned drums?", "answer": "Terry Bozzio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Terry Bozzio"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "A) John Bonham B) Keith Moon C", "extracted_entity": null}
{"question": "drums are usually played by what?", "answer": "the hands, or by one or two sticks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the hands, or by one or two sticks", "the hands"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "drummers", "extracted_entity": null}
{"question": "has the design of drums changed recently?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes. The design of drums has changed recently.", "extracted_entity": null}
{"question": "can some cylindrical shell drums have no drum heads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes,Q: can some cylindrical shell drums have", "extracted_entity": null}
{"question": "does every drumhead make the same sound?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No,Q: How does a drumhead make", "extracted_entity": null}
{"question": "aren't drums often used in music therapy?", "answer": "Yes, drums are often used in music therapy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, drums are often used in music therapy."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Music_therapy"}
{"question": "aren't drums usually played by the hands?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "They are,A: They are played by the", "extracted_entity": null}
{"question": "are drums usually played by the hands?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the difference between a", "extracted_entity": null}
{"question": "do many such drums have six to ten tension rods?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No. Most of them have eight.", "extracted_entity": null}
{"question": "do drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with parts of a player's body, or with some sort of implement such as a drumstick, to produce sound?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, drums consist of at least one membrane,", "extracted_entity": "Drumhead;Drum_kit"}
{"question": "have other techniques been used to cause drums to make sound, such as the ``Thumb roll''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "There are many techniques that can be used to make", "extracted_entity": null}
{"question": "is the electric eel a true eel?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, the electric eel is not a true eel."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "no, it is not a true eel,", "extracted_entity": "Electric_eel"}
{"question": "are most eels predators?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, most eels are predators."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Eels are not predators. They are secondary consumers", "extracted_entity": null}
{"question": "is eel blood toxic to humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, eel blood is toxic to humans."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No,Q: Is eel blood toxic to", "extracted_entity": null}
{"question": "how many species of true eels are there?", "answer": "approximately 800 species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["approximately 800 species", "There are approximately 800 species of true eels."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "There are about Q: how many species of", "extracted_entity": null}
{"question": "where is smoked eel considered a delicacy?", "answer": "Northern Germany, The Netherlands, Denmark, Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Germany, The Netherlands, Denmark, Sweden", "Smoked eel is considered a delicacy in northern Germany, The Netherlands, Denmark and Sweden."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Smoked eel is considered a delicacy in", "extracted_entity": "Eel"}
{"question": "how do eels begin life?", "answer": "As flat and transparent larvae, called leptocephali", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As flat and transparent larvae, called leptocephali", "Eels begin life as flat and transparent larvae, called leptocephali."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Most fish lay eggs. Some of these eggs are", "extracted_entity": null}
{"question": "what types of fins do eels have?", "answer": "Dorsal, anal, caudal, tail fins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dorsal, anal, caudal, tail fins", "Eels have dorsal and anal fins fused with a tail fin. Some also have  pectoral fins."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Eels have pectoral,Q: what", "extracted_entity": null}
{"question": "what do leptocephali eat?", "answer": "Marine snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Marine snow", "Leptocephali eat small particles called marine snow."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "leptocephali eat algae,Q: what", "extracted_entity": null}
{"question": "do all eels live in fresh water?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, not all eels live in fresh water."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No,Q: Do all fish have scales?", "extracted_entity": null}
{"question": "is Finnish an official language of the European Union?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Finnish is an official language of the European Union."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Finnish_language;European_Union"}
{"question": "are the dialects of Finnish divided into two groups?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, the dialects of Finnish are divided into two groups."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Finnish_language"}
{"question": "is Finnish threatened by English?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Finnish is not threatened by English."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "yes, Finnish is threatened by English, it is", "extracted_entity": "Finland"}
{"question": "when did Proto-Uralic language arrive in Finland?", "answer": "Around 1900 BCE", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Around 1900 BCE", "Proto-Uralic language arrived in Finland around 1900 BCE."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "about 7000\u20132000 BCE", "extracted_entity": "Proto-Uralic_language;Finland"}
{"question": "what are characteristic features of Finnish?", "answer": "Vowel harmony and an agglutinative morphology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowel harmony and an agglutinative morphology", "Characteristic features of Finnish are vowel harmony and an agglutinative morphology."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "The Finnish language, the sauna, and the thousands", "extracted_entity": "Finland"}
{"question": "where is the Kven language spoken?", "answer": "Norway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norway", "The Kven language is spoken in Northern Norway."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "in the northernmost parts of Norway by the K", "extracted_entity": "Kven_language"}
{"question": "for how long has the classification of dialects spoken outside of Finland been a controversial issue?", "answer": "Since Finland's independence in 1917", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Since Finland's independence in 1917", "The classification of dialects spoken outside of Finland has been a controversial issue since Finland's independence in 1917."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "The classification of dialects spoken outside of Finland has", "extracted_entity": "Finland"}
{"question": "who wrote the first novel in Finnish?", "answer": "Aleksis Kivi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Aleksis Kivi", "The first novel in Finnish was published by Aleksis Kivi."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "ksis Kivi", "extracted_entity": "Finnish_language"}
{"question": "what book defined the official Finnish language?", "answer": "The Dictionary of Contemporary Finnish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Dictionary of Contemporary Finnish", "The Dictionary of Contemporary Finnish defined the official Finnish language."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "nish is one of the two official languages of Finland", "extracted_entity": "Finnish_language"}
{"question": "what is the earliest extant transverse flute?", "answer": "it dates from 433 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it dates from 433 BC", "Chi"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The earliest extant transverse flute is a bone", "extracted_entity": null}
{"question": "when does a flute produce sound?", "answer": "when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "When a stream of air directed across a hole in the instrument creats a vibration of air at the hole."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "When air vibrates inside the tube of a flute", "extracted_entity": null}
{"question": "how can a flute's volume be increased?", "answer": "a flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute's volume can generally be increased by making its resonator and tone holes larger", "A flute's volume can be increased by making its resonator and tone holes larger."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The volume of a flute can be increased by using", "extracted_entity": null}
{"question": "do open-ended flutes have brighter timbres?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "No. Open-holed flutes tend to have", "extracted_entity": null}
{"question": "where was the oldest flute ever discovered found?", "answer": "Germany", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany", "Various parts of Germany."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "In a cave in southern Germany.", "extracted_entity": null}
{"question": "are Western flutes simpler than Indian flutes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Western_world;India"}
{"question": "why is a police whistle very wide for its pitch?", "answer": "A flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute's volume can generally be increased by making its resonator and tone holes larger", "It's volume is high either because of its larger resonator or larger tone holes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "A police whistle is very wide for its pitch because", "extracted_entity": null}
{"question": "what are the two main varieties of Indian flutes?", "answer": "The first, the Bansuri. The second, the Venu or Pullanguzhal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first, the Bansuri. The second, the Venu or Pullanguzhal", "Bansuri and the Venu or Pullanguzhal."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The North Indian flutes are called bansuri.", "extracted_entity": null}
{"question": "has the flute been dated to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the earliest evidence of", "extracted_entity": null}
{"question": "was the pan flute used in Greece?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Greece"}
{"question": "can a flute be played with several different air sources?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many different air sources can", "extracted_entity": null}
{"question": "when was the pan flute used in Greece?", "answer": "the 7th century BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 7th century BC", "7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The pan flute was used in Greece in the", "extracted_entity": "Greece"}
{"question": "when did concert flutes begin appearing in concert ensembles?", "answer": "the 16th-century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 16th-century", ""], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The use of concert flutes began during the Bar", "extracted_entity": null}
{"question": "how many main varieties of Indian flutes are currently used?", "answer": "two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two", "Two"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "(1) (2)Bans", "extracted_entity": null}
{"question": "what material is a chi flute fashioned from?", "answer": "lacquered bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lacquered bamboo", "Lacquered bamboo"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Bamboo", "extracted_entity": null}
{"question": "how does a flute player change the pitch of the sound?", "answer": "by opening and closing holes in the body of the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by opening and closing holes in the body of the instrument", "Opening and closing holes in the body of the instrument"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "A flute player changes the pitch of the sound by", "extracted_entity": null}
{"question": "what is the most basic form of the flute?", "answer": "a flute can be an open tube which is blown like a bottle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute can be an open tube which is blown like a bottle", "Open tube which is blown like a bottle"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The oldest flute is a bone flute from Q", "extracted_entity": null}
{"question": "has the dragon historically served as China's national emblem?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes. The dragon has served as China's national", "extracted_entity": null}
{"question": "do pandas hibernate?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No,\u2019t be silly. They\u2019re just a", "extracted_entity": null}
{"question": "do giant pandas attack humans?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do giant pandas attack humans?", "extracted_entity": null}
{"question": "what family is the panda a part of?", "answer": "Ursidae (bear)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ursidae (bear)", "Ursidae"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The panda is a part of the bear family.", "extracted_entity": null}
{"question": "how long does it take for the panda cubs skin to turn gray?", "answer": "one to two weeks after birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["one to two weeks after birth", "One to two weeks"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "A: It takes A: It takes", "extracted_entity": null}
{"question": "how many different names did Chinese writings give to the panda?", "answer": "20", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["20"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "\u2019t have", "extracted_entity": "People's_Republic_of_China"}
{"question": "what foods do pandas eat?", "answer": "bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bamboo", "bamboo, honeys, eggs, fish, yams, shrub leaves, oranges, and bananas"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Pandas are herbivores and eat a variety", "extracted_entity": null}
{"question": "what are the two subspecies of giant pandas?", "answer": "Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "Ailuropoda melanoleuca melanoleuca and The Qinling Panda, Ailuropoda melanoleuca qinlingensis"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Ailuropoda melanoleuca melanole", "extracted_entity": null}
{"question": "what is panda diplomacy?", "answer": "Loans of Giant Pandas to American and Japanese zoos", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loans of Giant Pandas to American and Japanese zoos"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "panda diplomacy is the practice of sending pandas from China", "extracted_entity": null}
{"question": "is the Giant Panda a mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Giant_panda"}
{"question": "is the Giant Panda a terrestrial animal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Terrestrial_animal"}
{"question": "is the Giant Panda an endangered species?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Endangered_species"}
{"question": "where do Giant Pandas live?", "answer": "a few mountain ranges in central china", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a few mountain ranges in central china", "Sichuan province, Shaanxi and Gansu provinces"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Giant Pandas live in the forests of southwest China", "extracted_entity": null}
{"question": "when was the Giant Panda's genome sequenced?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "how many subspecies of the Giant Panda are there?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "There are two subspecies of the Giant Panda:", "extracted_entity": "Giant_panda"}
{"question": "is the Giant Panda a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Giant_panda;Carnivore"}
{"question": "why is the Giant Panda an endangered species?", "answer": "threatened by habitat loss", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["threatened by habitat loss", "It is threatened by continued habitat loss and by a very low birthrate"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "The giant panda is an endangered species because its population", "extracted_entity": null}
{"question": "is the population of wild Giant  Pandas growing?", "answer": "according to some reports, yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["according to some reports, yes", "Yes but uncertain"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No,Q: What are the threats to wild", "extracted_entity": null}
{"question": "is an acoustic guitar dependent on an external device?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "No,Q: How do I get a good", "extracted_entity": null}
{"question": "is the battente smaller than a classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes. The battente is smaller than a classical", "extracted_entity": null}
{"question": "is the flamenco guitar similar to the classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Does the flamenco guitar have", "extracted_entity": null}
{"question": "where is the headstock located?", "answer": "end of the guitar neck furthest from the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["end of the guitar neck furthest from the body", "At the end of the guitar."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The headstock is located in the front of the", "extracted_entity": null}
{"question": "whom are guitars made and repaired by?", "answer": "luthiers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["luthiers", "Luthiers."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Guitars are made and repaired by luth", "extracted_entity": null}
{"question": "what are modern guitar strings constructed of?", "answer": "metal, polymers, animal or plant product materials", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["metal, polymers, animal or plant product materials", "Metal, polymers, or animal or plant product materials."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Nylon,Q: What is the best way to", "extracted_entity": null}
{"question": "why are Harp Guitars difficult to classify?", "answer": "as there are many variations within this type of guitar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["as there are many variations within this type of guitar", "There are many variations within this type."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Harp Guitars are difficult to classify because", "extracted_entity": null}
{"question": "what is the bridge used for?", "answer": "transfer the vibration from the strings to the soundboard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["transfer the vibration from the strings to the soundboard", "The transfer of string vibrations."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "To connect the LAN to the MAN.", "extracted_entity": null}
{"question": "which guitars use three single-coil pickups?", "answer": "Fender Statocaster type guitars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fender Statocaster type guitars", "Fender Stratocaster type guitars."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Gibson Les Paul,Q: Which guitar is the", "extracted_entity": null}
{"question": "is a guitar an instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is a guitar?", "extracted_entity": null}
{"question": "can guitars be divided into two broad categories?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Acoustic and electric", "extracted_entity": null}
{"question": "do picks come in many shapes and sizes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes. They can be round,A: Yes", "extracted_entity": null}
{"question": "how many strings does a guitar typically have?", "answer": "Six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six", "six"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "A guitar typically has six strings.", "extracted_entity": null}
{"question": "what is located at the end of the guitar neck furthest from the body?", "answer": "Headstock", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Headstock", "headstock"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the headstock", "extracted_entity": null}
{"question": "what are most electric guitar bodies made of?", "answer": "Wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wood", "wood"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Wood. Most electric guitar bodies are made of wood", "extracted_entity": null}
{"question": "why do some people believe that left-handed people should learn to play guitars as right-handed people do?", "answer": "to standardise the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to standardise the instrument"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Because they think it's best to be like everyone", "extracted_entity": null}
{"question": "how old is the oldest known representation of a guitar-like intrument being played?", "answer": "3,300 years old", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3,300 years old"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The oldest known representation of a guitar-like instrument being", "extracted_entity": null}
{"question": "what is the point called that is bolted or glued to the body of the guitar?", "answer": "Neck Joint", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Neck Joint", "Neck joint or 'Heel'"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "The bridge.", "extracted_entity": null}
{"question": "did Becquerel study science at the \u00c9cole des Ponts et Chauss\u00e9es?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Frederick_William,_Duke_of_Brunswick-Wolfenb\u00fcttel"}
{"question": "did he become chief engineer in the Department of Bridges and Highways in 1892?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes,1892", "extracted_entity": null}
{"question": "was Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, Becquerel was a French physicist.", "extracted_entity": "Becquerel;France"}
{"question": "whom did he share the Nobel Prize with?", "answer": "Pierre and Marie Curie", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pierre and Marie Curie"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "a scientist", "extracted_entity": "Nobel_Prize"}
{"question": "what is the SI unit for radioactivity called?", "answer": "becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["becquerel", "the becquerel (Bq)"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel (Bq)", "extracted_entity": null}
{"question": "when Becquerel discovered radioactivity, whose work did he investigate?", "answer": "Wilhelm Conrad R\u00f6ntgen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilhelm Conrad R\u00f6ntgen"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel", "extracted_entity": "Becquerel"}
{"question": "where are there craters named Becquerel?", "answer": "Moon and Mars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moon and Mars", "on the moon and on mars"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "There are craters named Becquerel on the", "extracted_entity": null}
{"question": "why did Bequerel win the Nobel Prize in Physics?", "answer": "discovery of spontaneous radioactivity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["discovery of spontaneous radioactivity", "was the discoverer of radioactivity"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Becquerel discovered radioactivity", "extracted_entity": "Becquerel;Nobel_Prize_in_Physics"}
{"question": "was Isaac Newton British?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, he was English."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, Isaac Newton was British.", "extracted_entity": "Isaac_Newton;United_Kingdom"}
{"question": "was Isaac Newton religious?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear.", "Yes, he was highly religious, though an unorthodox Christian."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton was a deeply religious man. He believed", "extracted_entity": "Isaac_Newton"}
{"question": "did Isaac Newton die in 1898?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, Newton died in his sleep in London on 31 March 1727."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Isaac_Newton"}
{"question": "where was Isaac Newton born?", "answer": "At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "He was born at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire.", "Woolsthorpe Manor in Woolsthorpe-by-Colsterworth"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton was born in the manor house of", "extracted_entity": "Isaac_Newton"}
{"question": "who shares credit with Isaac Newton for developing calculus?", "answer": "Gottfried Leibniz.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gottfried Leibniz.", "Newton shares the credit with Gottfried Leibniz for the development of the differential and integral calculus."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Gottfried Leibniz", "extracted_entity": "Isaac_Newton;Calculus"}
{"question": "when did Isaac Newton discover the generalized binomial theorem?", "answer": "In 1665.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1665."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton discovered the generalized binomial theorem in 167", "extracted_entity": "Isaac_Newton"}
{"question": "did the Occult influence Newton's theory of gravitation?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "If Newton had not relied on the occult idea of action at a distance, he might not have developed his theory of gravity."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Isaac_Newton"}
{"question": "were Newton's religious views consistent with Anglicanism?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, a conflict between Newton's religious views and Anglican orthodoxy was averted."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "No. His views were closer to Quakerism", "extracted_entity": "Anglicanism"}
{"question": "which fruit inspired Isaac Newton's theory of gravitation?", "answer": "The apple.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The apple.", "Isaac Newton's theory of gravitation was inspired by watching the fall of an apple from a tree."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "an apple", "extracted_entity": "Isaac_Newton"}
{"question": "was Sir Isaac Newton an English physicist and mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, he was. He was a key figure", "extracted_entity": "Isaac_Newton;England"}
{"question": "did he build the first practical reflecting telescope?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Reflecting_telescope"}
{"question": "was Isaac Newton educated at The King's Schol, Grantham?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": "Isaac_Newton;Grantham"}
{"question": "when was the Principia published?", "answer": "1687", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1687"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "?Q: when was the Principia published", "extracted_entity": null}
{"question": "where is there a Newton statue on display?", "answer": "Oxford University Museum of Natural History ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oxford University Museum of Natural History ", "Oxford University Museum of Natural History"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "The Newton statue is on display at the British Library", "extracted_entity": null}
{"question": "where was Isaac Newton buried?", "answer": "Westminster Abbey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westminster Abbey"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Isaac Newton was buried in Westminster Abbey.", "extracted_entity": "Isaac_Newton"}
{"question": "what religion did Isaac Newton follow?", "answer": "he never made a public declaration of his private faith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he never made a public declaration of his private faith", "unorthodox Christian"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was born on Christmas Day  25 December", "extracted_entity": "Isaac_Newton"}
{"question": "what food gave Isaac Newton clues to his theory of gravity?", "answer": "apple", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apple"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "He had a dream in which he saw an apple", "extracted_entity": "Isaac_Newton"}
{"question": "who was a farmer and later a land surveyor for the government?", "answer": " LeRoy Pollock was a farmer and later a land surveyor for the government.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" LeRoy Pollock was a farmer and later a land surveyor for the government."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "George Washington", "extracted_entity": null}
{"question": "regarding this topic, what did David Alfaro Siqueiros do?", "answer": "Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "He was a Mexican social realist painter best known", "extracted_entity": "David_Alfaro_Siqueiros"}
{"question": "who used hardened brushes, sticks,?", "answer": "Jackson Pollock used hardened brushes, sticks.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jackson Pollock used hardened brushes, sticks."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "a) a) a) a) a)", "extracted_entity": null}
{"question": "when were his papers donated by Lee Krasner?", "answer": "In 1983.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1983."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "his papers were donated by Lee Krasner", "extracted_entity": "Lee_Krasner"}
{"question": "did Free Jazz feature a Pollock painting as its cover artwork?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes, in 1960", "extracted_entity": "Free_jazz;Jackson_Pollock"}
{"question": "was Paul Jackson Pollock an influential American painter as well as a major figure in the abstract expressionist movement?", "answer": "Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Yes, Paul Jackson Pollock was an influential American", "extracted_entity": "Jackson_Pollock;United_States;Abstract_expressionism"}
{"question": "give an example of the origins of the term action painting. ?", "answer": "Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "The term was coined by the American art critic Harold", "extracted_entity": "Action_painting"}
{"question": "during his lifetime, did Pollock enjoy considerable fame and notoriety?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Yes, he was a major figure in the abstract", "extracted_entity": "Jackson_Pollock"}
{"question": "was he regarded as a mostly reclusive artist?", "answer": "Yes, he was regarded as a mostly reclusive artist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, he was regarded as a mostly reclusive artist. "], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "He was regarded as a mostly reclusive artist.", "extracted_entity": null}
{"question": "is the Wisma building the tallest building in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes,"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Indonesia"}
{"question": "is there a chinese community in jakarta?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es, there is a chinese community in jakarta", "extracted_entity": "Jakarta"}
{"question": "is Jakarta the 12th largest city in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "o. Jakarta is the 12th largest city", "extracted_entity": "Jakarta"}
{"question": "what is the capital of Indonesia?", "answer": "Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta", "The capital of Indonesia is Jakarta."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": "Indonesia"}
{"question": "what is the official name of Jakarta?", "answer": "Daerah Khusus Ibukota Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Daerah Khusus Ibukota Jakarta", "Daerah Khusus."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta", "extracted_entity": "Jakarta"}
{"question": "how is the climate in the city?", "answer": "Jakarta has a hot and humid equatorial/tropical climate ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta has a hot and humid equatorial/tropical climate ", "The city is hot and humid."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The climate is very pleasant in the city.", "extracted_entity": null}
{"question": "where does the name of the city Jakarta come from?", "answer": "The name Jakarta is derived from the Sanskrit word \"Jayakarta.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name Jakarta is derived from the Sanskrit word \"Jayakarta.\""], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is the capital and largest metropolis of Indonesia", "extracted_entity": "Jakarta"}
{"question": "how is the city's governor chosen?", "answer": "election", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["election", "Through election."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The city's governor is chosen by the people who", "extracted_entity": null}
{"question": "why does Jakarta suffer frequent flooding?", "answer": "Because it is located approximately eight meters above the sea level", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is located approximately eight meters above the sea level", "The fact that the northern part of Jakarta lies on a plain, approximately eight meters above the sea level."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "ecause Jakarta is located on the coast of Java,", "extracted_entity": "Jakarta"}
{"question": "is the the biggest university in Jakarta the University of Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jakarta;University_of_Indonesia"}
{"question": "is Jakarta a city?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jakarta"}
{"question": "is the most important river the Ciliwung River?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the most important lake", "extracted_entity": null}
{"question": "where is Jakarta located?", "answer": "northwest coast of Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["northwest coast of Java", "On the northwest coast of Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is located on the northwest coast of Java", "extracted_entity": "Jakarta"}
{"question": "who created Monas Park?", "answer": "General Herman Willem Deandels", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General Herman Willem Deandels", "Dutch Governor General Herman Willem Deandels"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Monas Park is a park located in the city", "extracted_entity": null}
{"question": "do mikrolets travel on the main roads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do mikrolets travel on", "extracted_entity": null}
{"question": "what contributes to frequent flooding in Jakarta?", "answer": "Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "It lies on a plain, approximately eight meters above the sea level"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "akarta is located in the northwest of the Java Island", "extracted_entity": "Jakarta"}
{"question": "did the economy improve in Jakarta in 2007?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Jakarta"}
{"question": "in AD 39, King Purnawarman established Sunda Pura as a new capital city for the kingdom, located at the northern coast of where?", "answer": "Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sunda Pura was established as a new capital", "extracted_entity": "Pest,_Hungary"}
{"question": "in 1602, the British East India Company's first voyage, commanded by Sir who, arrived in Aceh and sailed on to Banten where they were allowed to build a trading post?", "answer": "James Lancaster", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James Lancaster"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "James Lancaster", "extracted_entity": "East_India_Company;Aceh;Banten"}
{"question": "who began control of migration to the city in order to stem the overcrowding and poverty?", "answer": "Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The British Government", "extracted_entity": null}
{"question": "did Jayawikarta's soldiers attack the Dutch fortress?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Netherlands"}
{"question": "did Suharto resign as president?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Suharto"}
{"question": "was the Jakarta area part of the fourth century Indianized kingdom of Tarumanagara?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jakarta;Tarumanagara"}
{"question": "do other landmarks include the Istiqlal Mosque as well as Jakarta Cathedral?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes, both of these landmarks are located in Jakarta", "extracted_entity": "Istiqlal_Mosque,_Jakarta;North_Jakarta"}
{"question": "give an example of the many Sukarno era monuments in the city. ?", "answer": "The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. "], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The many Sukarno era monuments in the city", "extracted_entity": null}
{"question": "is Jakarta the country's economic, cultural and political center?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "es, it is", "extracted_entity": "Jakarta"}
{"question": "is James Watt French?", "answer": "No, he was Scottish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, he was Scottish.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "No, he is Scottish", "extracted_entity": "James_Watt"}
{"question": "did James Watt improve an engine?", "answer": "Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "Yes"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "James_Watt"}
{"question": "is there a statue of Watt in the White House?", "answer": "No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes, there is a statue of Watt in the", "extracted_entity": "James_Watt;White_House"}
{"question": "what was James Watt's father's job?", "answer": "His father was a shipwright, ship owner and contractor, and served as the town's chief baillie.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father was a shipwright, ship owner and contractor, and served as the town's chief baillie."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt's father was a shipwright.", "extracted_entity": "James_Watt"}
{"question": "when did Watt retire?", "answer": "In 1800.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800.", "Watt retired in 1800."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Q. when did he retire?", "extracted_entity": null}
{"question": "where is the James Watt Memorial College?", "answer": "In Greenock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Greenock.", "The James Watt Memorial  is in Greenock."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt Memorial College is in Greenock, Scotland", "extracted_entity": "James_Watt"}
{"question": "why is the watt named after Watt?", "answer": "The watt is named after him for his contributions to the development of the steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The watt is named after him for his contributions to the development of the steam engine.", "The watt is named after James Watt for his contributions to the development of the steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "The watt is named after Watt because he invented it", "extracted_entity": "James_Watt"}
{"question": "where did Watt attend university?", "answer": "Watt did not attend school regularly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt did not attend school regularly.", "He did not attend a university."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Glasgow University", "extracted_entity": null}
{"question": "what did he invent?", "answer": "He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "The cotton gin", "extracted_entity": null}
{"question": "what happened in the early 20th century?", "answer": "The koalas of South Australia were largely exterminated.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "The Great Depression happened.", "extracted_entity": null}
{"question": "do Koalas have a slow metabolism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Koalas have a slow metabolism.", "extracted_entity": null}
{"question": "did eucalypt forests grow in the place of rainforests?", "answer": "Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eucalypt;Rainforest"}
{"question": "does the koala fill the same ecological role as the sloth of South America?", "answer": "The koala fills the same ecological role as the sloth of South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koala fills the same ecological role as the sloth of South America."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Koala;South_America"}
{"question": "is the long term viability of the koala therefore threatened by genetic weakness?", "answer": "The long term viability of the koala is therefore threatened by genetic weakness.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The long term viability of the koala is therefore threatened by genetic weakness."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the long term viability of", "extracted_entity": null}
{"question": "is the koala found in coastal regions of eastern and southern Australia, from near Adelaide to the southern part of Cape York Peninsula?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Koala;Australia;Adelaide;Cape_York_Peninsula"}
{"question": "do populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands?", "answer": "Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What are the most important factors", "extracted_entity": null}
{"question": "were the koalas of South Australia largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock?", "answer": "The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "The koalas of South Australia were largely exter", "extracted_entity": "South_Australia;Victoria_(Australia)"}
{"question": "is Korean the official language of Korea?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Korean_language;South_Korea"}
{"question": "is the word \"Korean\" derived from Goryeo?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Goryeo"}
{"question": "are all dialects of Korean similar to each other?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "o, the Jeju dialect is very different from", "extracted_entity": "Korean_language"}
{"question": "about how many Koreans speakers are there?", "answer": "78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["78 million", "78 million Korean speakers"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "According to the,,,(1)", "extracted_entity": null}
{"question": "older English sources used the name \"Korean\" to refer to what?", "answer": "language, country, and people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["language, country, and people"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The Jurchen people,Q: What does", "extracted_entity": null}
{"question": "what is the official language of North Korea?", "answer": "Korean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The official language of North Korea is Korean.", "extracted_entity": "North_Korea"}
{"question": "why may English translation of Koreans adjectives misleadingly suggest that they are verbs?", "answer": "English does not have an identical grammatical category", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["English does not have an identical grammatical category"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Because Korean adjectives are verbs.", "extracted_entity": null}
{"question": "where do the majority of loanwords other than Sino-Korean come from?", "answer": "modern times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern times", "modern times, 90% of which are from English"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Japanese", "extracted_entity": "Sino-Korean_vocabulary"}
{"question": "how many Korean speakers are there?", "answer": "There are about 78 million Korean speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are about 78 million Korean speakers."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "bout 80 million", "extracted_entity": "Korean_language"}
{"question": "how many parts of speech does the Korean language contain?", "answer": "The Korean Language contains nine parts of speech. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Korean Language contains nine parts of speech. "], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Korean_language"}
{"question": "how many verb paradigms are there in Korean?", "answer": "There are seven verb paradigms or speech levels in Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are seven verb paradigms or speech levels in Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Korean_language"}
{"question": "what is the Sino-Korean system of numbers based on?", "answer": "The Sino-Korean system is nearly entirely based on the Chinese numerals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sino-Korean system is nearly entirely based on the Chinese numerals."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "The Sino-Korean system of numbers is based", "extracted_entity": null}
{"question": "what is another term for Korean adjectives?", "answer": "Adjectives are also known as \"descriptive verbs\" or \"stative verbs\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adjectives are also known as \"descriptive verbs\" or \"stative verbs\"."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "descriptive verbs", "extracted_entity": null}
{"question": "is modern Korean written in columns or rows?", "answer": "It is written in rows.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is written in rows."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Columns", "extracted_entity": null}
{"question": "what percentage of the Korean language does Jeong Jae-do estimate to be Sino-Korean?", "answer": "Sino-Korean makes up 30% of the Korean language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sino-Korean makes up 30% of the Korean language."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "0%", "extracted_entity": "Korean_language;Sino-Korean_vocabulary"}
{"question": "do Chinese and Japanese have spaces between words?", "answer": "No, Chinese and Japanese do not have spaces between words.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese and Japanese do not have spaces between words."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Chinese has spaces between words, but Japanese doesn\u2019t", "extracted_entity": "Chinese_language;Japanese_language"}
{"question": "from which languages is Korean descended?", "answer": "Korean is descended from Old Korean, Middle Korean and Modern Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean is descended from Old Korean, Middle Korean and Modern Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "JapaneseB: ChineseC: EnglishD: French", "extracted_entity": null}
{"question": "are there seven verb paradigms or speech levels in Korean?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Q: what is the lowest speech level?", "extracted_entity": "Korean_language"}
{"question": "is Modern Korean written with spaces between words?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es, it is.", "extracted_entity": "Korean_language"}
{"question": "are the Korean names for the language based on the names for Korea used in North as well as South Korea?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "s, they are.", "extracted_entity": "Korean_language;Korea;North_Korea;South_Korea"}
{"question": "is it not also one of the two official languages of the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "Yanbian_Korean_Autonomous_Prefecture;People's_Republic_of_China"}
{"question": "is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists?", "answer": "The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "es, it is classified as a different language by", "extracted_entity": "Jeju_language;South_Korea"}
{"question": "is it also one of the two official languages in the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is one of the two official languages", "extracted_entity": "Yanbian_Korean_Autonomous_Prefecture;People's_Republic_of_China"}
{"question": "is Kuala Lumpur the capitol of Malaysia? disfluent?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes, Kuala Lumpur is the capitol of Malaysia", "extracted_entity": "Kuala_Lumpur;Malaysia"}
{"question": "is Kuala Lumpur in the Selangor state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes, it is in the Selangor state", "extracted_entity": "Kuala_Lumpur;Selangor"}
{"question": "what's the population of Kuala lumpur?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Q: How many countries are in the European", "extracted_entity": null}
{"question": "what language do they speak in Kuala Lumpur?", "answer": "Bahasa Melayu", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bahasa Melayu"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "they speak Malay in Kuala Lumpur", "extracted_entity": "Kuala_Lumpur"}
{"question": "what does the name \"Kuala Lumpur\" mean?", "answer": "muddy confluence, \"muddy estuary,\" and \"muddy city\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["muddy confluence, \"muddy estuary,\" and \"muddy city\""], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The name \"Kuala Lumpur\" means \"m", "extracted_entity": "Kuala_Lumpur"}
{"question": "what airports does Kuala Lumpur have?", "answer": "Kuala Lumpur International Airport and Subang International Airport", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kuala Lumpur International Airport and Subang International Airport"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur International Airport", "extracted_entity": "Kuala_Lumpur"}
{"question": "how long was Kuala Lumpur occupied by the Japanese?", "answer": "42 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42 months"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "ala Lumpur was occupied by the Japanese for three years", "extracted_entity": "Kuala_Lumpur;Japan"}
{"question": "how many seasons does Kuala Lumpur experience?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "two", "extracted_entity": "Kuala_Lumpur"}
{"question": "when were the colonial buildings in Kuala Lumpur constructed?", "answer": "toward the end of the 19th and early 20th centuries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["toward the end of the 19th and early 20th centuries"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The colonial buildings in Kuala Lumpur were constructed in the", "extracted_entity": "Kuala_Lumpur"}
{"question": "is it a host city for the Formula One World Championship?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Formula_One"}
{"question": "is the central market located in the proximity of the pertama complex?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the central market located in", "extracted_entity": null}
{"question": "is it home to the parliament of malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes, the parliament of malaysia is home to the", "extracted_entity": "Malaysia"}
{"question": "when did kuala lumpur become a federal territory of malaysia?", "answer": "February 1, 1974", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["February 1, 1974"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur, the capital city of Malaysia, became", "extracted_entity": "Malaysia"}
{"question": "what is the city's population?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "what is the official religion in the country?", "answer": "Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Islam"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Islam is the official religion in the country.", "extracted_entity": null}
{"question": "is there a more industrialized region in malaysia?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Peninsular Malaysia.", "extracted_entity": null}
{"question": "after the 2008 elections, what party had the most parliament seats?", "answer": "DAP", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["DAP"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the most parliament seats were won by the democrats", "extracted_entity": null}
{"question": "what is the name of the event that draws the worlds top riders to malaysia?", "answer": "KL Grand Prix CSI 5*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["KL Grand Prix CSI 5*"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "The event that draws the world's top riders to", "extracted_entity": "Malaysia"}
{"question": "wasn't Leonardo da Vinci valued as an engineer?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Yes, he was also known for his achievements as", "extracted_entity": "Leonardo_da_Vinci"}
{"question": "wasn't Leonardo da Vinci born on April 15?", "answer": "Yes, Leonardo da Vinci was born on April 15.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Leonardo da Vinci was born on April 15."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Leonardo_da_Vinci"}
{"question": "who left Verroccio's studio?", "answer": "Leonardo da Vinci left Verroccio's studio.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo da Vinci left Verroccio's studio."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Verrocchio's son,Q: What was", "extracted_entity": null}
{"question": "when did Baldassare Castiglione, author of Il Cortegiano ( \" The Courtier \" ), write?", "answer": "1528", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1528"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "All in all, the answers are", "extracted_entity": "Baldassare_Castiglione"}
{"question": "when did he flee to Venice?", "answer": "In 1499.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1499."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1586", "extracted_entity": "Venice"}
{"question": "was Leonardo a contemporary of Botticelli, Domenico Ghirlandaio as well as Perugino?", "answer": "Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Leonardo_da_Vinci;Sandro_Botticelli;Domenico_Ghirlandaio;Pietro_Perugino"}
{"question": "is the larger work now almost universally attributed to Leonardo?", "answer": "Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Was the painting of the Last", "extracted_entity": null}
{"question": "has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes, Leonardo has often been described as the archetype", "extracted_entity": "Polymath"}
{"question": "is he widely considered to be one of the greatest painters of all time and perhaps the most diversely talented person ever to have lived?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Pablo PicassoQ. What was the name of the", "extracted_entity": null}
{"question": "according to art historian Helen Gardner, were the scope and depth of his interests without precedent and ``his mind and personality seem to us superhuman, the man himself mysterious and remote''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "According to art historian Helen Gardner, the scope and", "extracted_entity": null}
{"question": "are lobsters invertebrates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many legs does a lobster", "extracted_entity": null}
{"question": "are lobsters kosher?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes, they are kosher. Lobsters are a", "extracted_entity": "Judaism"}
{"question": "do lobsters have blue blood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": "Lobster"}
{"question": "how many legs do lobsters have?", "answer": "10.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10.", "10"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "5", "extracted_entity": "Lobster"}
{"question": "where are lobsters found?", "answer": "In all oceans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In all oceans.", "In all oceans"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Lobsters are found in the sea.", "extracted_entity": null}
{"question": "what is the mean level of mercury in American lobsters?", "answer": "0.31 ppm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["0.31 ppm"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "the mean level of mercury in American lobsters is", "extracted_entity": "United_States"}
{"question": "do lobsters feel pain?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Lobsters have a tough exoskeleton that protects", "extracted_entity": null}
{"question": "did the Moche people worship lobsters?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes, they did worship lobsters. The lob", "extracted_entity": "Moche_culture"}
{"question": "what is the caridoid escape reaction?", "answer": "Swimming backwards quickly by curling and uncurling their abdomen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swimming backwards quickly by curling and uncurling their abdomen.", "When lobsters swim backwards quickly by curling and uncurling their abdomen"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "it is a chemical reaction that occurs in the car", "extracted_entity": null}
{"question": "were the recitations of the Ancient Greeks accompanied by lyre playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ancient_Greece"}
{"question": "does a classical lyre have a hollow body?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lyre"}
{"question": "is the lyre a stringed musical instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes, the lyre is a stringed musical", "extracted_entity": "Lyre"}
{"question": "which constellation is said to resemble a lyre shape?", "answer": "Lyra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lyra"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Orion", "extracted_entity": null}
{"question": "how many raised arms are extending from the sound-chest of a classical lyre?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": ",, a, a)  a", "extracted_entity": null}
{"question": "what were the strings of a classical lyre made of?", "answer": "Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gut"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Tortoise shell,?", "extracted_entity": null}
{"question": "is the foot pedal framework for a piano called a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No, the foot pedal framework for a piano is", "extracted_entity": "Lyre"}
{"question": "where was the deepest note of the classical lyre in relation to the player's body?", "answer": "It was farthest from the player's body.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was farthest from the player's body."], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the lowest string of the lyre", "extracted_entity": null}
{"question": "does a standard piano have fewer strings than a harp?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "The piano has more strings than a harp.", "extracted_entity": "Piano"}
{"question": "is Malay the official language of Iraq?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malay_language;Iraq"}
{"question": "could Malay have originated from Sumatra island?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Malay_language;Sumatra"}
{"question": "is Malay an agglutinative language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malay_language"}
{"question": "what family is Malay a member of?", "answer": "Austronesian family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austronesian family", "Austronesian."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Austronesian", "extracted_entity": "Malays_(ethnic_group)"}
{"question": "what is the basic word order in Malay?", "answer": "Subject Verb Object", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Subject Verb Object", "Subject Object Verb."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "SVO", "extracted_entity": "Malay_language"}
{"question": "what languages has Malay borrowed words from?", "answer": "Sanskrit, Arabic and English", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sanskrit, Arabic and English", "Arabic, Sanskrit, Tamil, Persian, Portuguese, Dutch, certain Chinese dialects, and English."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Arabic, Persian, Sanskrit, Tamil, Portuguese", "extracted_entity": "Malay_language"}
{"question": "are there many words in Malay that use natural gender?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are many words in Malay that use", "extracted_entity": "Malay_language"}
{"question": "how are new words formed in Malay?", "answer": "three methods.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three methods.", "Affixation, composition, and reduplication."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "New words are formed in Malay by:", "extracted_entity": "Malay_language"}
{"question": "is Malay in the Austronesian family of languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Malay_language;Austronesian_languages"}
{"question": "is there one negation word in Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malay_language"}
{"question": "how many parts of speech are there in Malay?", "answer": "Four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Four"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "8", "extracted_entity": "Malay_language"}
{"question": "what is the national language in Malaysia?", "answer": "Malay ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malay "], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "The national language in Malaysia is Malay", "extracted_entity": "Malaysia"}
{"question": "do linguists consider Malay to be a single language?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, linguists consider Malay to be a single", "extracted_entity": "Malay_language"}
{"question": "can speakers of modern Malay understand Old Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No, they can't.", "extracted_entity": "Malay_language;History_of_the_Malay_language"}
{"question": "what language besides Malay is in the Austronesia language family?", "answer": "Malagasy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malagasy"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Austronesia", "extracted_entity": "Malay_language;Austronesia"}
{"question": "wasn't Malay language found in Sumatra?", "answer": "The Malay language was found in Sumatra.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Malay language was found in Sumatra."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, it was found in Sumatra.", "extracted_entity": "Malay_language;Sumatra"}
{"question": "wasn't Malay language written using Pallava?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malay_language;Pallava_dynasty"}
{"question": "was Malay language written using Pallava?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, Malay language was written using Pallava", "extracted_entity": "Malay_language;Pallava_alphabet"}
{"question": "had the language already been used as the lingua franca throughout the archipelago since the 15th century?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. The language was a lingua franca only", "extracted_entity": null}
{"question": "do another distinguishing feature of Malay ( include Indonesian Malay )?", "answer": "Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan)."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay ( include Indonesian Malay ) have another distinguishing feature", "extracted_entity": "Malay_Indonesian"}
{"question": "is Malay spoken as local dialect of ethnic Malays?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, the ethnic Malays speak Malay as their local", "extracted_entity": "Malay_language;Malays_(ethnic_group)"}
{"question": "give an example of the tributary states of the Majapahit kingdom in eastern Java. ?", "answer": "Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "he tributary states of the Majapah", "extracted_entity": "Majapahit;Java"}
{"question": "is Malay the one of the most widely spoken languages in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Malay is the one of the most widely spoken languages", "extracted_entity": "Malay_language"}
{"question": "was it derived from Latin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No, it was originally spoken in Latium.", "extracted_entity": "Latin"}
{"question": "is Melbourne the capital and most populous city in the state of Victoria?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Melbourne;Victoria_(Australia)"}
{"question": "is Melbourne home to Australia's busiest seaport?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Melbourne"}
{"question": "does Melbourne have a large international student community?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Yes, there are many international students in Melbourne", "extracted_entity": "Melbourne"}
{"question": "what is a resident of Melbourne known as?", "answer": "Melburnian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Melburnian", "A \"Melburnian\"."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "A Melburnian", "extracted_entity": "Melbourne"}
{"question": "when was Melbourne officially declared as a city?", "answer": "1847", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1847", "In 1847."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "In  it was officially declared a city.", "extracted_entity": "Melbourne"}
{"question": "how many sister cities does the City of Melbourne have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six", "Six."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "0", "extracted_entity": "City_of_Melbourne"}
{"question": "where is Melbourne situated?", "answer": "boundary of the very hot inland areas and the cold southern ocean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["boundary of the very hot inland areas and the cold southern ocean"], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne is situated on the banks of the Yarra", "extracted_entity": "Melbourne"}
{"question": "how many domestic tourists visit Melbourne?", "answer": "7.6 million domestic visitors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7.6 million domestic visitors", "7.6 million."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourn", "extracted_entity": "Melbourne"}
{"question": "what religion holds majority in Melbourne?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "Christianity."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": "Melbourne"}
{"question": "did Albert Einstein keep a photograph of Faraday on his study wall?", "answer": "Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Albert_Einstein;Michael_Faraday"}
{"question": "did Faraday marry Sarah Barnard?", "answer": "Yes, Faraday married Sarah Barnard.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday married Sarah Barnard.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Faraday"}
{"question": "did Faraday also discover the laws of electrolysis?", "answer": "Yes, Faraday also discovered the laws of  electrolysis.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday also discovered the laws of  electrolysis.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes,A: Yes,A: Yes,", "extracted_entity": null}
{"question": "who discovered benzene?", "answer": "Michael Faraday discovered benzene.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday discovered benzene.", "Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Benzene was first discovered by the English polymath", "extracted_entity": "Benzene"}
{"question": "where was Michael Faraday born?", "answer": "Michael Faraday was born in Newington Butts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday was born in Newington Butts.", "Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was born in Newington Butts", "extracted_entity": "Michael_Faraday"}
{"question": "what is the Faraday effect?", "answer": "The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "The plane of polarisation of linearly polarised light can be rotated by the application of an exernal magnetic field aligned in the direction the light is moving."], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Faraday effect is a physical magneto-opt", "extracted_entity": "Faraday_effect"}
{"question": "did Faraday discover diamagnetism?", "answer": "Yes, Faraday discovered diamagnetism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday discovered diamagnetism.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he discovered electromagnetic induction and electrolysis", "extracted_entity": "Michael_Faraday"}
{"question": "how many siblings did Faraday have?", "answer": "Faraday had three siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday had three siblings.", "two"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday had Answer these questions.", "extracted_entity": null}
{"question": "where is Michael Faraday buried?", "answer": "Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "in the dissenters' section of Highgate Cemetery"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday is buried in Highgate Cemetery in", "extracted_entity": "Michael_Faraday"}
{"question": "is faraday best known for his work with kinematics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was he active in the area now called environmental science?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he was not active in the area now", "extracted_entity": "Environmental_science"}
{"question": "did he discover benzene?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What did he call it?", "extracted_entity": null}
{"question": "who was his mentor?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "His mentor was his father. He was a painter", "extracted_entity": null}
{"question": "who was Faraday's wife?", "answer": "Sarah Barnard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sarah Barnard"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did he advise the british government on the production of chemical weapons for use in the crimean war?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_Kingdom"}
{"question": "did faraday have strong maritime interests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Faraday was the son of a blacksmith and", "extracted_entity": null}
{"question": "does the charge reside on the interior of a charged conductor?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The charge resides on the exterior of a charged conductor", "extracted_entity": null}
{"question": "wasn't Michelangelo abstemious?", "answer": "Michelangelo was abstemious.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michelangelo was abstemious."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "No, he was not abstemious.", "extracted_entity": "Michelangelo"}
{"question": "wasn't Michelangelo a homosexual?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Yes. Michelangelo was a homosexual.", "extracted_entity": "Michelangelo"}
{"question": "when did Filippo Strozzi sell it to Francis I?", "answer": "In 1529.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1529."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "in 1529", "extracted_entity": "Francis_I_of_France"}
{"question": "when was his house demolished?", "answer": "1874", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1874"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "The house was demolished in. Q.", "extracted_entity": null}
{"question": "was Michelangelo's mother Francesca di Neri del Miniato di Siena?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Michelangelo;Siena"}
{"question": "give an example of the most renowned works of the Renaissance. ?", "answer": "The Statue of David, completed by Michelangelo in 1504", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Statue of David, completed by Michelangelo in 1504"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "eonardo da Vinci's Mona Lisa, Michelangelo's", "extracted_entity": "Renaissance"}
{"question": "was Michelangelo's output in every field during his long life prodigious?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "yes, his output was prodigious in every field", "extracted_entity": "Michelangelo"}
{"question": "is English the language most often spoken in Montreal?", "answer": "The language most spoken at home in the city is French.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The language most spoken at home in the city is French.", "No, the language most spoken in Montreal is French."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "English is the language most often spoken in Montreal.", "extracted_entity": "English_language;Montreal"}
{"question": "is Montreal's economy the third largest of all cities in Canada?", "answer": "No, Montreal's economy is the second largest of all cities in Canada.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Montreal's economy is the second largest of all cities in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Montreal;Canada"}
{"question": "is the most popular sport in Montreal ice hockey?", "answer": "yes, The most popular sport in Montreal is ice hockey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, The most popular sport in Montreal is ice hockey.", "Yes, the most popular sport in Montreal is ice hockey."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Montreal"}
{"question": "how many international airports does Montreal have?", "answer": "Montreal has two international airports.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montreal has two international airports."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal has  least one international airport.", "extracted_entity": "Montreal"}
{"question": "who is the Mayor of Montreal?", "answer": "The mayor is G\u00c3\u00a9rald Tremblay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mayor is G\u00c3\u00a9rald Tremblay."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "The Mayor of Montreal is head of the executive branch", "extracted_entity": "Mayor_of_Montreal"}
{"question": "what is the name of the largest church in Montreal?", "answer": "The largest church in Montreal is named Saint Joseph's Oratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The largest church in Montreal is named Saint Joseph's Oratory.", "Saint Joseph's Oratory is the largest church in Montreal."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "The largest church in Montreal is the Basilique Notre", "extracted_entity": "Montreal"}
{"question": "where was much of Montreal's industry during the late 19th and early-to-mid 20th century?", "answer": "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "much of Montreal's industry during the late 19", "extracted_entity": "Montreal"}
{"question": "does Montreal contain the largest church in Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Montreal contains the largest church in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Montreal;Canada"}
{"question": "what is the largest primarily French-speaking city in the western world?", "answer": "Paris is the largest primarily French-speaking city in the western world.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris is the largest primarily French-speaking city in the western world.", "The largest primarily French-speaking city in the western world is Paris."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montreal", "extracted_entity": "Western_world"}
{"question": "is Nairobi the capital of Kenya?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi;Kenya"}
{"question": "was Nairobi founded in 1899?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi"}
{"question": "is the current estimated population of Nairobi about 6 milion?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Nairobi"}
{"question": "what is the current estimated population of Nairobi?", "answer": "About 3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["About 3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4,397,073", "extracted_entity": "Nairobi"}
{"question": "in what year was Nairobi founded?", "answer": "1899", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1899"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "2019", "extracted_entity": "Nairobi"}
{"question": "which embassy in Nairobi was bombed in August 1998?", "answer": "The United States Embassy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The United States Embassy"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "US embassy", "extracted_entity": "Nairobi"}
{"question": "how many civilians died in the 1998 U.S. embassy bombing?", "answer": "Over two hundred", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over two hundred"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "19", "extracted_entity": "United_States"}
{"question": "how many trades can the NSE make per day?", "answer": "10 million trades", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10 million trades"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1.5 million trades", "extracted_entity": "National_Stock_Exchange_of_India"}
{"question": "about how many people are estimated to live in Nairobi?", "answer": "3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "4,397,073", "extracted_entity": "Nairobi"}
{"question": "the district is bordered to the southwest by Uhuru Park and where?", "answer": "Central Park", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "The district is bordered to the southwest by Lake Victoria", "extracted_entity": null}
{"question": "when did the Catholic University of Eastern Africa which obtain its Letter of Interim Authority?", "answer": "In 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1989."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "The Catholic University of Eastern Africa obtained its Letter of", "extracted_entity": null}
{"question": "is Central Park adjacent to Uhuru Park?", "answer": "Central Park is adjacent to Uhuru Park.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park is adjacent to Uhuru Park."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Central_Park"}
{"question": "give an example of the highest growth rates of any city in Africa. ?", "answer": " The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "agos, Nigeria is the largest city in Africa with", "extracted_entity": "Africa"}
{"question": "give an example of the most prominent cities in Africa politically and financially. ?", "answer": "Nairobi is now one of the most prominent cities in Africa politically and financially", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is now one of the most prominent cities in Africa politically and financially"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "outh Africa, Egypt, Nigeria, Ghana, Kenya", "extracted_entity": "Africa"}
{"question": "is Nairobi not the capital as well as largest city of Kenya?", "answer": "Nairobi is the capital and largest city of Kenya.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is the capital and largest city of Kenya."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi;Kenya"}
{"question": "does the city and its surrounding area also form the Nairobi Province?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes. The Nairobi Province is a geographical area made", "extracted_entity": null}
{"question": "does the name ``Nairobi'' come from the Maasai phrase Enkare Nyirobi, which translates to ``the place of cool waters''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nairobi;Maasai_language"}
{"question": "did Nikola Tesla die alone?", "answer": "Nikola Tesla died alone.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla died alone.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla died alone", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla close friends with Mark Twain?", "answer": "Nikola Tesla was close friends with Mark Twain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla was close friends with Mark Twain.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla and Mark Twain were close friends.", "extracted_entity": "Nikola_Tesla;Mark_Twain"}
{"question": "did Nikola Tesla use a technique called picture thinking?", "answer": "Nikola Tesla used a technique called picture thinking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla used a technique called picture thinking.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he did. He used a technique called", "extracted_entity": "Nikola_Tesla"}
{"question": "when did Nikola Tesla appear on the cover of Time Magazine?", "answer": "On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "1931"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Nikola Tesla appeared on the cover of Time Magazine in", "extracted_entity": "Nikola_Tesla;Time_(magazine)"}
{"question": "who became adversaries with Nikola Tesla?", "answer": "Nikola Tesla became adversaries with Edison.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla became adversaries with Edison.", "Edison"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Edison and Marconi", "extracted_entity": "Nikola_Tesla"}
{"question": "where did Nikola Tesla move to in 1880?", "answer": "In 1880, Nikola Tesla moved to Budapest.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1880, Nikola Tesla moved to Budapest.", "Budapest"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "in 1880, Nikola Tesla moved to the", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla's mother illiterate?", "answer": "Nikola Tesla's mother never learned to read.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla's mother never learned to read.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nikola_Tesla"}
{"question": "how many siblings did Nikola Tesla have?", "answer": "Nikola Tesla had four siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla had four siblings.", "4"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "3 brothers and 1 sister.", "extracted_entity": "Nikola_Tesla"}
{"question": "was Nikola Tesla a vegetarian?", "answer": " In his later years Tesla became a vegetarian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" In his later years Tesla became a vegetarian.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No. Tesla was not a vegetarian.", "extracted_entity": "Nikola_Tesla"}
{"question": "is there a monument to Tesla at Niagara Falls, New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nikola_Tesla;Niagara_Falls,_New_York;New_York"}
{"question": "was Tesla an American citizen?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, he was not an American citizen.", "extracted_entity": "United_States"}
{"question": "was Tesla on the cover of Time magazine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tesla_Motors;Time_(magazine)"}
{"question": "when did Tesla receive his last patent?", "answer": "1928", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1928"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "In 1941 Tesla received his last patent.", "extracted_entity": "Nikola_Tesla"}
{"question": "what languages did Tesla speak?", "answer": "Serbian, Czech, English, French, German, Hungarian, Italian, and Latin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Serbian, Czech, English, French, German, Hungarian, Italian, and Latin"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "he spoke eight languages", "extracted_entity": "Nikola_Tesla"}
{"question": "did Tesla win the Nobel Prize?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, he didn't", "extracted_entity": "Tesla_Motors;Nobel_Prize"}
{"question": "was Tesla rich at the time of his death?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No. He died with a net worth of less", "extracted_entity": null}
{"question": "what music group was named after Tesla?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": "Nikola_Tesla"}
{"question": "consequently, what is not considered a \"serious painter\" by some contemporary artists, who often regard his work as bourgeois and kitsch?", "answer": "Rockwell ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Gustave Courbet.", "extracted_entity": "Bourgeoisie;Kitsch"}
{"question": "who spent the next 10 years painting for Look magazine?", "answer": "Norman Rockwell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norman Rockwell."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Ben Shahn", "extracted_entity": "Look_(American_magazine)"}
{"question": "when was his last painting for the Post published?", "answer": "1963", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1963"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Q: How did he die?", "extracted_entity": null}
{"question": "when did Rockwell marry his first wife, Irene O'Connor,?", "answer": "In 1916.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1916."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Q: what year did Rockwell marry his", "extracted_entity": null}
{"question": "did Norman Rockwell spend the winter months as artist-in-residence at Otis College of Art as well as Design?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, he did. In fact, he did", "extracted_entity": "Norman_Rockwell;Otis_College_of_Art_and_Design"}
{"question": "did Norman Rockwell publish a total of 321 original covers for The Saturday Evening Post over 47 years?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Norman Rockwell published a total of 321 original", "extracted_entity": "Norman_Rockwell;The_Saturday_Evening_Post"}
{"question": "was Rockwell also commissioned to illustrate over 40 books including Tom Sawyer as well as Huckleberry Finn?", "answer": "Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Sawyer;Huckleberry_Finn"}
{"question": "do his works enjoy a broad popular appeal in the United States, where Rockwell is most famous for the cover illustrations of everyday life scenarios he created for The Saturday Evening Post magazine over more than four decades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, he is most famous for the cover illustrations", "extracted_entity": "United_States;The_Saturday_Evening_Post"}
{"question": "are among the best-known of Rockwell's works the Willie Gillis series, Rosie the Riveter (although his Rosie was reproduced less than others of the day), Saying Grace (1951), and the Four Freedoms series?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rosie_the_Riveter;Four_Freedoms"}
{"question": "is he also noted for his work for the Boy Scouts of America (BSA)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes, he is also noted for his work for", "extracted_entity": "Boy_Scouts_of_America"}
{"question": "is the octopus a cephalopod?", "answer": "Yes, the octopus is a cephalopod.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus is a cephalopod."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the octopus'", "extracted_entity": null}
{"question": "does the octopus have two eyes?", "answer": "Yes, the octopus has two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has two eyes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the scientific name of", "extracted_entity": null}
{"question": "does the octopus have a hard beak?", "answer": "Yes, the octopus has a hard beak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has a hard beak."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "No,Q: What does it mean when a", "extracted_entity": null}
{"question": "where is the mouth of an octopus?", "answer": "The mouth of an octopus is at the center point of the arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mouth of an octopus is at the center point of the arms.", "The octopus' mouth is at the center point of the arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "at the center of the arms", "extracted_entity": "Octopus"}
{"question": "around how many recognized octopus species are there?", "answer": "There are around 300 recognized octopus species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are around 300 recognized octopus species."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "There are about 300 species of octopus.", "extracted_entity": "Octopus"}
{"question": "how many arms does an octopus have?", "answer": "An octopus has four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An octopus has four pairs of arms.", "The octopus has four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "An octopus has eight arms.", "extracted_entity": "Octopus"}
{"question": "what is the range of lifespans of the octopus?", "answer": "Octopuses can live from six months to five years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Octopuses can live from six months to five years.", "The octopus has a short lifespan."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "2 to 3 years", "extracted_entity": "Octopus"}
{"question": "what is an example of something an octopus learned in a research experiment?", "answer": "In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "An octopus learned to solve mazes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "A learned behavior that an octopus learned in a", "extracted_entity": null}
{"question": "how can an octopus defend against predators?", "answer": "To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "They can hide, flee, expel ink, or use camouflage."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "An octopus has a number of defenses against predators", "extracted_entity": "Octopus"}
{"question": "are Octopus highly intelligent?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many hearts do octop", "extracted_entity": null}
{"question": "do males use a specialized arm called a hectocotylus to insert spermatophores?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "No, they use it to insert sperm directly into", "extracted_entity": "Spermatophore"}
{"question": "do the octopuses in the less familiar Cirrina suborder have two fins and an internal shell?", "answer": "Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "The octopuses in the Cirrina suborder", "extracted_entity": null}
{"question": "do octopuses have two eyes and four pairs of arms?", "answer": "Yes, octopuses have two eyes and four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, octopuses have two eyes and four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many tentacles do they", "extracted_entity": null}
{"question": "is octopus a common food in Mediterranean cuisine as well as Portuguese cuisine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes, it is a common food in Mediterranean cuisine", "extracted_entity": "Mediterranean_cuisine;Portuguese_cuisine"}
{"question": "is the collective form octopus usually reserved for animals consumed for food?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "No. Octopi is the correct pluralization of", "extracted_entity": "Octopus"}
{"question": "has an octopus a hard beak, with its mouth at the center point of the arms?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the scientific name for", "extracted_entity": null}
{"question": "what happened in 1968 through 1971?", "answer": "Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "In,The questions that you have asked are", "extracted_entity": null}
{"question": "what happened in 1838?", "answer": "Don Jos\u00e9 Ruiz y Blasco was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Don Jos\u00e9 Ruiz y Blasco was born"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The US Government signed the Treaty of New Ech", "extracted_entity": null}
{"question": "who was not long in finding another lover, Jacqueline Roque?", "answer": "Picasso.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Picasso."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Josephine de Beauharnais", "extracted_entity": null}
{"question": "when did Picasso make his first trip to Paris?", "answer": "1900", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1900"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Picasso made his first trip to Paris in 190", "extracted_entity": "Pablo_Picasso;Paris"}
{"question": "give an example of the most comprehensive records extant of any major artists beginnings. ?", "answer": "Museu Picasso", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Museu Picasso"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The Beatles,,.", "extracted_entity": null}
{"question": "is in the 1996 movie Surviving Picasso Picasso played by actor Anthony Hopkins?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Yes, Anthony Hopkins played the role of Picasso in", "extracted_entity": "Anthony_Hopkins"}
{"question": "is the U.S. copyright representative for the Picasso Administration the Artists Rights Society?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "no, the Picasso Administration is not the U.S", "extracted_entity": "United_States;Artists_Rights_Society"}
{"question": "is he one of the most recognized figures in 20th-century art?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Picasso", "extracted_entity": "20th-century_art"}
{"question": "is he best known for co-founding the Cubist movement and for the wide variety of styles embodied in his work?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cubism"}
{"question": "are among his most famous works the proto-Cubist Les Demoiselles d'Avignon (1907) and Guernica (1937), his portrayal of the German bombing of Guernica during the Spanish Civil War?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Picasso's Guernica (1937) is", "extracted_entity": "Proto-Cubism;Guernica_(Picasso);Bombing_of_Guernica;Spanish_Civil_War"}
{"question": "are pianos used in Western music?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes,Q: How many strings does a piano", "extracted_entity": null}
{"question": "are \"upright pianos\" called \"vertical pianos\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "are \u201cupright pianos\u201d called \u201cvertical", "extracted_entity": null}
{"question": "did Bartolomeo Cristofori invent the modern piano?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bartolomeo_Cristofori;Piano"}
{"question": "what is the middle pedal called on grand pianos?", "answer": "the sostenuto pedal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the sostenuto pedal.", "sostenuto pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "The Sostenuto pedal.", "extracted_entity": null}
{"question": "how many black keys do modern pianos have?", "answer": "36.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["36.", "36"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "A modern piano has A: A modern piano", "extracted_entity": null}
{"question": "what is the sustain pedal called?", "answer": "The pedal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pedal", "damper pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "The sustain pedal is called the damper pedal,", "extracted_entity": null}
{"question": "where is Irving Berlin's piano located?", "answer": "In the Smithsonian Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Smithsonian Museum.", "in the Smithsonian Museum"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "in the Smithsonian Museum", "extracted_entity": "Irving_Berlin"}
{"question": "what kind of piano did Irving Berlin play?", "answer": "The transposing piano.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The transposing piano.", "transposing piano"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "a grand piano", "extracted_entity": "Irving_Berlin"}
{"question": "is the left-most pedal on a grand piano called the una corda?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes, the left-most pedal on a grand piano is called the una corda."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Piano"}
{"question": "is it advantageous for a grand piano's metal plate to be quite massive?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "It is advantageous for the plate to be quite massive."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes. The plate is there to hold the strings", "extracted_entity": null}
{"question": "is the mechanism in an upright piano perpendicular to its keys?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "The mechanism in upright pianos is perpendicular to the keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "about how tall is a typical studio piano?", "answer": "Studio pianos are around 42 to 45 inches tall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Studio pianos are around 42 to 45 inches tall."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": ",,,...but,\u2026.", "extracted_entity": null}
{"question": "what are a piano's keys generally made of?", "answer": "Piano keys are generally made of spruce or basswood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano keys are generally made of spruce or basswood."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "ivory", "extracted_entity": null}
{"question": "how many total keys does a typical modern piano have?", "answer": "88 keys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88 keys.", "A modern piano has 88 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "AnswerAnswer A: AnswerAnswer Answer", "extracted_entity": null}
{"question": "why are upright pianos more compact?", "answer": "Upright pianos are more compact because the frame and strings are vertical.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Upright pianos are more compact because the frame and strings are vertical."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "They are designed for smaller spaces.", "extracted_entity": null}
{"question": "do older pianos have more keys than modern pianos?", "answer": "no.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no.", "Many older pianos only have 85 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "No. The number of keys has remained constant for", "extracted_entity": null}
{"question": "what are the names of a piano's pedals?", "answer": "Piano pedals from left to right: una corda, sostenuto, and damper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano pedals from left to right: una corda, sostenuto, and damper.", "The names of a piano's pedals are una corda, sostenuto, and damper."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "The three pedals on a piano are called the dam", "extracted_entity": null}
{"question": "what happened in the January 15, 1882?", "answer": "On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "A major earthquake occurred in the January15,188", "extracted_entity": null}
{"question": "what happened in 1887?", "answer": "In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "The French and British were at war. The French", "extracted_entity": null}
{"question": "is one of the best known Impressionist works Renoir's 1876 Dance at Le Moulin de la Galette?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Impressionism;Pierre-Auguste_Renoir"}
{"question": "was Pierre-Auguste Renoir born in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "erre-Auguste Renoir was born in Limog", "extracted_entity": "Pierre-Auguste_Renoir;Limoges;Haute-Vienne;France"}
{"question": "have two of Renoir's paintings sold for more than US0 million?", "answer": "Yes (any painting sells for more than $0 million though...)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (any painting sells for more than $0 million though...)"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pierre-Auguste_Renoir"}
{"question": "as a celebrator of beauty, and especially feminine sensuality, has it been said that ``Renoir is the final representative of a tradition which runs directly from Rubens to Watteau''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": ": Rubens' highly charged compositions referenced", "extracted_entity": "Pierre-Auguste_Renoir;Peter_Paul_Rubens;Jean-Antoine_Watteau"}
{"question": "did Pierre-Auguste Renoir die in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "s, he died in Limoges, Ha", "extracted_entity": "Limoges;Haute-Vienne;France"}
{"question": "as a boy, did he work in a porcelain factory where his drawing talents led to him being chosen to paint designs on fine china?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "He worked in a china factory, but it is", "extracted_entity": null}
{"question": "is Portuguese an official language of Andorra?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Portuguese is not an official language of Andorra."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Portuguese_language;Andorra"}
{"question": "does the Portuguese language have its roots in the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese is derived from Latin."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Portuguese is a Romance language and derives from", "extracted_entity": "Portuguese_language;Latin"}
{"question": "does Portuguese contain words from the Arabic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese contains words from the Arabic language."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Portuguese does contain words from the Arabic language", "extracted_entity": "Portuguese_language;Arabic_language"}
{"question": "where are there small Portuguese-speaking communities?", "answer": "Former overseas colonies of Portugal such as Macau and East Timor ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Former overseas colonies of Portugal such as Macau and East Timor ", "There are small Portuguese-speaking communities in Macau and East Timor."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "In the state of Goa,,,: What", "extracted_entity": null}
{"question": "what are the two main groups of Portuguese dialects?", "answer": "Those of Brazil and those of the Old World", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Those of Brazil and those of the Old World", "The two main groups of dialects are Brazil and the Old World."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese is divided into two main groups of dialects", "extracted_entity": "Portugal"}
{"question": "what event marked the end of the Old Portuguese period?", "answer": "The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "The end of the Old Portuguese period was marked by the publication of the Cancioneiro Geral by Garcia de Resende."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "the Carnation Revolution", "extracted_entity": null}
{"question": "which government established Portuguese as it's third official language in order to meet the requirements to apply for full membership in the CPLP?", "answer": "Equatorial Guinea", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Equatorial Guinea", "The government of Equatorial Guinea established Portuguese as its third official language in order to meet the requirements to apply for full membership in the CPLP."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "The East Timor government", "extracted_entity": "Portuguese_language;Community_of_Portuguese_Language_Countries"}
{"question": "by 2050, what will the total population of Portuguese speakers in the world be?", "answer": "335 million people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["335 million people", "By 2050, Portuguese-speaking countries will have a total population of 335 million people."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "300 million", "extracted_entity": "Portuguese_language"}
{"question": "in which country is Portuguese the co-official language with Cantonese Chinese?", "answer": "Macau", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Macau", "Portuguese is the co-official language with Cantonese Chinese in Macau."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Brazil", "extracted_entity": "Portuguese_language;Cantonese"}
{"question": "wasn't Portuguese language spread by arriving Roman soldiers?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "No, Portuguese language was spread by the Romans.", "extracted_entity": "Portuguese_language"}
{"question": "(where French has a similar phenomenon, with alveolar affricates instead of postalveolars?", "answer": "Quebec.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "(postalveolar consonants are not common in", "extracted_entity": "France;Postalveolar_consonant"}
{"question": "give an example of the official languages of the special administrative region of Macau and East Timor, . ?", "answer": "Portuguese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Portuguese"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "rtuguese and Chinese", "extracted_entity": "Macau;East_Timor"}
{"question": "is Portuguese an official language of several international organizations?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is Portuguese an official language of", "extracted_entity": null}
{"question": "is it derived from the Latin spoken by the romanized pre-Roman peoples of the Iberian Peninsula (namely the Gallaeci, the Lusitanians, the Celtici and the Conii) around 2000 years ago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is derived from the Latin spoken by", "extracted_entity": "Latin;Iberian_Peninsula;Gallaeci;Lusitanians"}
{"question": "was it used as the exclusive lingua franca on the island of Sri Lanka for almost 350 years?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Portuguese", "extracted_entity": "Sri_Lanka"}
{"question": "is the city named after the apostle Saint Peter?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Saint_Peter"}
{"question": "has the terrain in the city been artificially raised?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the city subject to flooding", "extracted_entity": null}
{"question": "what were some other names of the city?", "answer": "Petrograd, Leningrad, and Piter", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Petrograd, Leningrad, and Piter", "Petrograd and Leningrad."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The city has been known as the following names:", "extracted_entity": null}
{"question": "what festival features fireworks  celebrating the end of the school year?", "answer": "The White Nights Festival in Saint Petersburg", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The White Nights Festival in Saint Petersburg", "The White Nights Festival."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The Festival of the Arts, which is a traditional", "extracted_entity": null}
{"question": "where do most people in urban Saint Petersburg live?", "answer": "apartments", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apartments", "Apartments."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Most people in Saint Petersburg live in the city centre", "extracted_entity": "Saint_Petersburg"}
{"question": "what was made in Russia's largest foundry?", "answer": "thousands of sculptures and statues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["thousands of sculptures and statues", "Thousands of sculptures and statues."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The largest foundry in Russia is located in the", "extracted_entity": "Russia"}
{"question": "when did the crime level become higher?", "answer": "After the October revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the October revolution."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "In,,,,The crime level became", "extracted_entity": null}
{"question": "how to travel to work from the city to Moscow?", "answer": "railway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["railway", "One way is through the Moscow-Saint Petersburg Railway."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "I live in the city and I have a job", "extracted_entity": "Moscow"}
{"question": "who laid down the Peter and Paul Fortress?", "answer": "Peter the Great ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Peter the Great "], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Peter the Great", "extracted_entity": null}
{"question": "when was the 7th symphony premiered?", "answer": "1942", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1942"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "In A: The A: In", "extracted_entity": null}
{"question": "has Equestrianism been a long tradition, popular among the Tsars and aristocracy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "e Tsars and aristocracy have always been great", "extracted_entity": "Equestrianism;Tsar"}
{"question": "give an example of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995. ?", "answer": "Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "KiroV Stadium", "extracted_entity": "FC_Zenit_Saint_Petersburg"}
{"question": "give an example of the best known symphony orchestras in the world under the leadership of conductors Yevgeny Mravinsky and Yuri Temirkanov. ?", "answer": "The Leningrad Philharmonic Orchestra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Leningrad Philharmonic Orchestra"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "One of the most famous symphony orchestras in", "extracted_entity": "Yevgeny_Mravinsky;Yuri_Temirkanov"}
{"question": "give an example of the longest, most destructive, and most lethal sieges of major cities in modern history. ?", "answer": "The Siege of Leningrad", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Siege of Leningrad"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The longest most destructive and most lethal sieges of", "extracted_entity": null}
{"question": "were the city's other names Petrograd (, 1914 -- 1924) and Leningrad (, 1924 -- 1991)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The city's other names were Petrograd (191", "extracted_entity": null}
{"question": "is it often called just Petersburg (-RRB- and is informally known as Piter (-RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is it near Moscow?A", "extracted_entity": null}
{"question": "founded by Tsar Peter I of Russia on 27 May 1703, was it the capital of the Russian Empire for more than two hundred years (1713 -- 1728, 1732 -- 1918)?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Moscow was the capital of the Russian Empire for more", "extracted_entity": "Peter_the_Great;Russian_Empire"}
{"question": "is the standard of living in San Franciscio high?", "answer": "Yes, San Francisco is characterized by a high standard of living.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, San Francisco is characterized by a high standard of living.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "San_Francisco;San_Francisco_Giants"}
{"question": "does San Francisco have a high percentage of gay and lesbian individuals?", "answer": "Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "San_Francisco"}
{"question": "is San Franscico a popular tourist destination?", "answer": "Yes. San Francisco is a popular international tourist destination.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. San Francisco is a popular international tourist destination.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is a popular tourist destination. It is", "extracted_entity": null}
{"question": "how large is the population of San Francisco?", "answer": "The estimated population of San Francisco in the year 2008 was 808,976.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The estimated population of San Francisco in the year 2008 was 808,976.", "San Francisco has an estimated population of 808,976."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The city proper is the fourth most populous city in", "extracted_entity": "San_Francisco"}
{"question": "how many state parks are in San Francisco?", "answer": "There is only one park managed by the California State Park system: Candlestick Point.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There is only one park managed by the California State Park system: Candlestick Point.", "There is one state park in San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": "San_Francisco"}
{"question": "where is San Francisco?", "answer": "San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "San Francisco is in California."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is in the United States.", "extracted_entity": "San_Francisco"}
{"question": "what is the cause of local earthquakes?", "answer": "It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "The nearby San Andreas and Hayward Faults are responsible for much earthquake activity."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The cause of local earthquakes is the movement of rocks", "extracted_entity": null}
{"question": "are more residents employed by small businesses than in 1977?", "answer": "The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "how does poverty in San Francisco compare to the nation-wide average?", "answer": "The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "San Francisco's poverty rate is lower than the national average."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco has a lower poverty rate than the nation", "extracted_entity": "San_Francisco"}
{"question": "was Mission Bay campus opened in 2003?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Mission Bay campus was opened in 2003.", "extracted_entity": null}
{"question": "was the University of San Fransisco founded in 1855?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "University_of_San_Francisco"}
{"question": "is Golden Gate Park the largest city park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, Golden Gate Park is the largest city park", "extracted_entity": "Golden_Gate_Park"}
{"question": "what is Northern California's most widely circulated newspaper?", "answer": "San Francisco Chronicle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco Chronicle"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The Sacramento Bee", "extracted_entity": "Northern_California"}
{"question": "what makes San Francisco among the top-ten North American destinations for conventions and conferences?", "answer": "San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "a large hotel infrastructure and a world-class convention facility in the Moscone Center"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The city\u2019s charm, mild climate, and status", "extracted_entity": "San_Francisco;North_America"}
{"question": "who manages Candlestick point?", "answer": "California State Park system", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["California State Park system", "the California State Park system"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The candlestick point is managed by the candlestick", "extracted_entity": null}
{"question": "is Crissy field an airfield?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No,Q: Is Crissy field an air", "extracted_entity": null}
{"question": "does the de Young museum house the Asian Art Museum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No. The de Young Museum is a separate museum", "extracted_entity": null}
{"question": "what prompted the city to upgrade its building codes?", "answer": "earthquakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["earthquakes", "The threat of major earthquakes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "A recent earthquake", "extracted_entity": null}
{"question": "like many larger where cities, San Francisco is a minority-majority city, as non-Hispanic whites comprise less than half of the population?", "answer": "U.S.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["U.S."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes. Non-Hispanic whites make up", "extracted_entity": "San_Francisco;Non-Hispanic_whites"}
{"question": "who laid out a street plan for the expanded settlement?", "answer": "William Richardson and Alcalde Francisco de Haro.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["William Richardson and Alcalde Francisco de Haro."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the street plan was laid out by the British.", "extracted_entity": null}
{"question": "when was a 43-acre Mission Bay campus opened?", "answer": "2003", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2003"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": ",,,,,,,,,", "extracted_entity": null}
{"question": "when did it sign on the air?", "answer": "In 1941.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1941."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "I believe it was in Q: Who starred", "extracted_entity": null}
{"question": "give an example of the largest two-year community colleges in the country. ?", "answer": "The City College of San Francisco.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The City College of San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The largest two-year community colleges in the country are", "extracted_entity": null}
{"question": "give an example of the ten Beta World Cities. ?", "answer": "San Trancisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Trancisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Tokyo,Q: What is the population of the", "extracted_entity": null}
{"question": "the only consolidated city-county in California, does it encompass a land area of on the northern end of the San Francisco Peninsula, making it the second most densely populated city in the United States?", "answer": "San Francisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "California;San_Francisco_Peninsula;United_States"}
{"question": "is San Francisco also the financial, cultural, and transportation center of the larger San Francisco Bay Area, a region of 7.4 million people?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "San_Francisco;San_Francisco_Bay_Area"}
{"question": "in what year did the Spanish establish a fort at the Golden Gate and a mission named for Francis of Assisi on the site?", "answer": "In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": "Spain;Golden_Gate;Francis_of_Assisi"}
{"question": "does Swahili have dipthongs?", "answer": "Yes, Swahili has diphthongs. (Typo)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili has diphthongs. (Typo)"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "No, Swahili does not have dipth", "extracted_entity": "Swahili_language"}
{"question": "is Swahili a Bantu language?", "answer": "Yes, Swahili is a Bantu language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is a Bantu language.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es, it is a Bantu language.", "extracted_entity": "Swahili_language;Bantu_languages"}
{"question": "did Uganda make Swahili a required subject in primary schools?", "answer": "Yes, Uganda made Swahili a required subject in primary schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Uganda made Swahili a required subject in primary schools.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes, Uganda made Swahili a required subject", "extracted_entity": "Uganda;Swahili_language"}
{"question": "what is the Swahili word for the Swahili language?", "answer": "Kiswahili is the Swahili word for the Swahili language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kiswahili is the Swahili word for the Swahili language.", "Kiswahili"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Kiswahili", "extracted_entity": "Swahili_language"}
{"question": "is Swahili spoken in Mozambique?", "answer": "Yes, Swahili is spoken in parts of northern Mozambique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is spoken in parts of northern Mozambique.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es, Swahili is spoken in Mozambique", "extracted_entity": "Swahili_language;Mozambique"}
{"question": "how does Swahili time run?", "answer": "Swahili time runs from dawn to dusk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili time runs from dawn to dusk.", "from dawn to dusk"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili time runs on a tz of", "extracted_entity": "Swahili_language"}
{"question": "how many noun classes does Swahili have?", "answer": "Swahili has sixteen noun classes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili has sixteen noun classes.", "sixteen"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili has 16, noun classes,", "extracted_entity": "Swahili_language"}
{"question": "in what year did Germany seize Tanganyika?", "answer": "Germany seized Tanganyika in 1886.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany seized Tanganyika in 1886.", "1886"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "1916", "extracted_entity": "Germany;Tanganyika_(territory)"}
{"question": "which is the sub-Saharan indigenous language with the greatest number of speakers?", "answer": "Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "Hausa"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili", "extracted_entity": null}
{"question": "is Swedish a descendant of Old Norse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "yes, Swedish is a descendant of Old Norse.", "extracted_entity": "Sweden;Old_Norse"}
{"question": "does Swedish use the perfect participle to form the present perfect tense?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "no, the Swedish language does not use the perfect", "extracted_entity": "Sweden"}
{"question": "how many Swedish speakers were reported in Canada in 2001?", "answer": "16,915.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["16,915.", "There are 16,915 reported Swedish speakers in Canada."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "29015", "extracted_entity": "Swedish_language;Canada"}
{"question": "how many letters are in the basic Latin alphabet?", "answer": "26.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["26.", "26"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "A:Q:how many letters are in", "extracted_entity": null}
{"question": "when did a standard and common language become available to all Swedes?", "answer": "In the 20th Century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the 20th Century.", "It was during the 20th century that a common, standardized national language became available to all Swedes."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "In the  it was the  the first standard", "extracted_entity": "Sweden"}
{"question": "when did the Soviet military forces occupy Estonia?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "In 1944."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "17 June  1940", "extracted_entity": "Soviet_Union;Estonia"}
{"question": "what are the differences between English and Swedish pronouns?", "answer": "Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "In English, there are 3 pronouns:", "extracted_entity": "English_language;Sweden"}
{"question": "how does Swedish language counterbalance the absence of cases?", "answer": "The lack of cases in Swedish is compensated by a wide variety of prepositions.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lack of cases in Swedish is compensated by a wide variety of prepositions."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "Swedish language counterbalances the absence of cases with definite", "extracted_entity": "Swedish_language"}
{"question": "is Taipei in a valley?", "answer": "Taipei is in the valleys of the Keelung and Xindian Rivers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei is in the valleys of the Keelung and Xindian Rivers", "Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "No,Q: Is Taipei a part of a", "extracted_entity": null}
{"question": "was Taipei founded in the 18th century?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Taipei"}
{"question": "is the National Palace Museum in Taipei?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": "National_Palace_Museum;Taipei"}
{"question": "what is along the city's western border?", "answer": "The Danshui River borders Taipei to the west.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Danshui River borders Taipei to the west.", "Taipei has the Danshui River along its western border."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The city is bordered on the west by a river", "extracted_entity": null}
{"question": "what is Taipei 101?", "answer": "Taipei 101 is a 101-floor landmark skyscraper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei 101 is a 101-floor landmark skyscraper."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei|101 is a skyscraper in Taipei,", "extracted_entity": "Taipei_101"}
{"question": "what are some annual festivals in Taipei?", "answer": "The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "Some annual festivals include Tomb-Sweping Day, the Dragon Boat Festival, the Ghost Festival, and the Mid-Autumn Festival."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "the Double Tenth Festival, the Taipei Lantern Festival", "extracted_entity": "Taipei"}
{"question": "when is the Pacific typhoon season?", "answer": "Between June and October.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Between June and October.", "It's between June and October."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The Pacific typhoon season is from May to November", "extracted_entity": null}
{"question": "who sets up shrines?", "answer": "Many homes and business set up shrines.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many homes and business set up shrines.", "Many homes and businesses would set up shrines."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "The shrine is set up by the person who is", "extracted_entity": null}
{"question": "what does Taipei produce?", "answer": "Taipei produces high technology and hi-tech components", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei produces high technology and hi-tech components", "Taipei produces textiles, apparel, electronics, machinery, printed materials, precision equipment, food, and beverages."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei produces electronics, textiles, and metals", "extracted_entity": "Taipei"}
{"question": "is the trumpet the musical instrument with the highest register in the brass family?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes, the trumpet is the musical instrument with the", "extracted_entity": "Trumpet"}
{"question": "is a trumpet played by blowing air through closed lips?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Trumpet"}
{"question": "is the most common type of trumpet the B trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No,Q: What is the most common type", "extracted_entity": null}
{"question": "what is the earliest date of the trumpet?", "answer": "1500 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1500 BC", "around 1500 BC"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The earliest date of the trumpet is the  of", "extracted_entity": null}
{"question": "what is a trumpet constructed of?", "answer": "brass tubing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass tubing", "brass tubing bent twice into an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "A trumpet is constructed of brass. The brass is", "extracted_entity": null}
{"question": "what are the smallest trumpets referred to as?", "answer": "Piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piccolo trumpets", "piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": null}
{"question": "why do trumpets have a bright, loud sound?", "answer": "The trumpet and trombone share a roughly cylindrical bore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The trumpet and trombone share a roughly cylindrical bore.", "Due to the cylindrical bore"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The sound is produced by the vibration of the lips", "extracted_entity": null}
{"question": "why does the cornet have a slightly mellower tone than the trumpet?", "answer": "The cornet and flugelhorn have conical bores", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cornet and flugelhorn have conical bores", "because it has conical bores"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "because of the cornet has a conical bore", "extracted_entity": "Cornet;Trumpet"}
{"question": "what makes a trumpet fully chromatic?", "answer": "able to play all twelve pitches of Western music", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["able to play all twelve pitches of Western music"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The valves.", "extracted_entity": null}
{"question": "were trumpet players heavily guarded?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What was the first thing that", "extracted_entity": null}
{"question": "are pocket trumpets compact B trumpets?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No. Pocket trumpets are smaller than B trump", "extracted_entity": null}
{"question": "was Dizzy Gillespie a famous trumpeter in 1998?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, he was not a famous trumpeter in", "extracted_entity": "Dizzy_Gillespie"}
{"question": "what shape is a trumpet bent into?", "answer": "oblong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["oblong", "an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a circle", "extracted_entity": null}
{"question": "what trumpet was the first to be allowed in the Christian Church?", "answer": "slide trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slide trumpets", "Slide trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The first trumpet allowed in the Christian Church was the", "extracted_entity": "Christian_Church"}
{"question": "the trumpet can be confused with what close relative?", "answer": "the cornet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cornet", "the coronet"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the cornet, the cornetto, and the", "extracted_entity": "Trumpet"}
{"question": "what can be augmented with a fourth valve?", "answer": "the flugelhorn", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the flugelhorn", "flugelhorn"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "An engine.", "extracted_entity": null}
{"question": "what was Maynard Ferguson noted for being able to play accurately?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Maynard Ferguson was noted for his bands, which", "extracted_entity": "Maynard_Ferguson"}
{"question": "what was built to play in both B and A keys?", "answer": "piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "A and B are both built to play in both", "extracted_entity": null}
{"question": "what happened in 1894?", "answer": "Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\""], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The Battle of Manila Bay", "extracted_entity": null}
{"question": "what happened in nineteen fifty seven?", "answer": "Maynard Ferguson formed his own band.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maynard Ferguson formed his own band."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "A nuclear bomb exploded in the United States.", "extracted_entity": null}
{"question": "is the Breeze Eazy' ' method sometimes used to teach younger students?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "es,Q: What is the Breeze E", "extracted_entity": null}
{"question": "do many players use a smaller mouthpiece on the piccolo trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The piccolo trumpet is the same as the B", "extracted_entity": null}
{"question": "is the trumpet constructed of brass tubing bent twice into an oblong shape?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes. The tubing is bent twice into an ob", "extracted_entity": null}
{"question": "give an example of the most influential musicians of the 20th century. ?", "answer": "Miles Davis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Miles Davis"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "The most influential musicians of the 20th century", "extracted_entity": null}
{"question": "were Slide trumpets the first trumpets allowed in the Christian church?", "answer": "Slide trumpets were the first trumpets allowed in the Christian church. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Slide trumpets were the first trumpets allowed in the Christian church. "], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Christianity"}
{"question": "are they constructed of brass tubing bent twice into an oblong shape, and are played by blowing air through closed lips, producing a ``buzzing'' sound which starts a standing wave vibration in the air column inside the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Standing_wave;Trumpet"}
{"question": "are there several types of trumpet?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Yes,,Q: What are the different types", "extracted_entity": null}
{"question": "was the Ottoman script replaced with a variant of the Latin alphabet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the Ottoman script was replaced with a variant of the", "extracted_entity": "Latin_alphabet"}
{"question": "is there a definite article in Turkish language?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Turkish_language"}
{"question": "is it possible to alter the word order to stress the importance of a certain phrase?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes. You can alter the word order to stress", "extracted_entity": null}
{"question": "what is the official language of Turkey?", "answer": "Turkish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turkish.", "Turkish is the official language of Turkey."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish", "extracted_entity": null}
{"question": "when was the Turkish Language Association founded?", "answer": "In 1928.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1928.", "The Turkish Language Association was founded in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "It was founded in.", "extracted_entity": null}
{"question": "what are the most significant foreign contributors to Turkish vocabulary?", "answer": "Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. "], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Arabic and Persian", "extracted_entity": "Turkish_language"}
{"question": "who were the Orkhon inscriptions built for?", "answer": "For the prince Kul Tigin and his brother Emperor Bilge Khan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For the prince Kul Tigin and his brother Emperor Bilge Khan.", "The Orkhon inscriptions were erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Orkhon inscriptions were built for the people", "extracted_entity": "Orkhon_River"}
{"question": "what was one of the results of the introduction of the new Turkish alphabet?", "answer": "A dramatic increase in literacy from Third World levels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A dramatic increase in literacy from Third World levels.", "There was a dramatic increase in literacy from its original Third World levels."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the introduction of the new Turkish alphabet resulted in the", "extracted_entity": null}
{"question": "why does Lewis use \"absolute case\" instead of \"nominative\"?", "answer": "Because it is also used for the indefinite accusative.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is also used for the indefinite accusative."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Lewis uses the absolute case because it is more accurate", "extracted_entity": null}
{"question": "how many people speak Turkish worldwide?", "answer": "There are roughly another 10 million native speakers worldwide.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are roughly another 10 million native speakers worldwide."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "80 to 90 million", "extracted_entity": "Turkish_language"}
{"question": "in Turkish, which syllable usually has the stress?", "answer": "Stress is usually on the last syllable", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stress is usually on the last syllable"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Usually the syllable has the stress, but there", "extracted_entity": null}
{"question": "is the spelling of Turkish phonetic?", "answer": "yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish is phonetic. It is not phonetical", "extracted_entity": null}
{"question": "what percentage of the words in Turkish vocabulary have Turkish origins?", "answer": "86% of the Turkish vocabulary have Turkish origins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["86% of the Turkish vocabulary have Turkish origins."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "t is not a language that has been in", "extracted_entity": "Turkish_language"}
{"question": "after what event was the Turkish Language Association established?", "answer": "After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "After the language reform", "extracted_entity": null}
{"question": "what exception to the rules of vowel harmony do compound words have?", "answer": "In compound words, the vowels need not harmonize between the constituent words of the compound.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In compound words, the vowels need not harmonize between the constituent words of the compound."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Compound words can be made up of words from different", "extracted_entity": null}
{"question": "what are the patterns of vowel harmony?", "answer": "Vowels are characterised by two features: front/back and rounded/unrounded.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowels are characterised by two features: front/back and rounded/unrounded."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the patterns of vowel harmony are backness harmony,", "extracted_entity": null}
{"question": "what are the ways to join groups of nouns?", "answer": "Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The ways to join groups of nouns are conjunction,", "extracted_entity": null}
{"question": "does Vietnamese have a comparatively large number of vowels?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vietnamese_language"}
{"question": "is Vietnamese the mother tongue of the Vietnamese people?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnamese is the mother tongue of the Vietnamese", "extracted_entity": "Vietnamese_language;Vietnamese_people"}
{"question": "is Vietnamese the official administrative language of Vietnam?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnamese_language;Vietnam"}
{"question": "was Vietnamese influenced by Chinese?", "answer": "Much vocabulary has been borrowed from Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much vocabulary has been borrowed from Chinese.", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes. The Chinese ruled Vietnam for over A", "extracted_entity": null}
{"question": "when did written Vietnamese become the official administrative language?", "answer": "the 20th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 20th century", "Vietnamese became the official administrative language by the 20th century."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "in the 10th century", "extracted_entity": "Vietnamese_language"}
{"question": "have consonant clusters been lost?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the Red River Delta traditionally poor?", "answer": "Not compared to Nghe An, Ha Tinh, or Quang Binh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Not compared to Nghe An, Ha Tinh, or Quang Binh", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Red_River_Delta"}
{"question": "what kind of words have been borrowed from Chinese?", "answer": "Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "Vietnamese adopts words from Chinese which express abstract ideas."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "words like tea, silk, porcelain, and k", "extracted_entity": "Chinese_language"}
{"question": "is Vietnam independent from France?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnam;France"}
{"question": "was Vietnamese formally written using the Chinese writing system?", "answer": "Yes, Vietnamese was *formerly* written using the Chinese writing system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese was *formerly* written using the Chinese writing system."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, it was written using the Chinese writing system", "extracted_entity": "Vietnamese_language"}
{"question": "does Vietnamese have a large number of vowels?", "answer": "Yes, Vietnamese has a comparatively large number of vowels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese has a comparatively large number of vowels."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vietnamese_language"}
{"question": "what are the three dialect regions of Vietnamese?", "answer": "The three regions are North, Central, and South.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three regions are North, Central, and South."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Northern, Central and Southern", "extracted_entity": "Vietnamese_language"}
{"question": "when was the reunification of Vietnam?", "answer": "The reunification of Vietnam occurred in 1975-76.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The reunification of Vietnam occurred in 1975-76."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1976", "extracted_entity": "Vietnam"}
{"question": "how many tones do Northern varieties have?", "answer": "Northern Vietnamese has six tones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Vietnamese has six tones."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Four", "extracted_entity": null}
{"question": "does Vietnamese borrow from Latin and Greek?", "answer": "No, Vietnamese does not borrow from Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Vietnamese does not borrow from Latin and Greek."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no, it is a language isolate", "extracted_entity": "Vietnamese_language;Latin;Greek_language"}
{"question": "did Vietnamese come to predominate politically in 2nd century BC?", "answer": "No, Chinese came to predominate politically in the 2nd century B.C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese came to predominate politically in the 2nd century B.C."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vietnam"}
{"question": "do more northern speakers speak the southern regional dialect than southern speakers speak the northern dialect?", "answer": "More southern speakers speak the northern dialect than northerners speak the suthern dialect.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More southern speakers speak the northern dialect than northerners speak the suthern dialect."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes,B: NoC: No one knows", "extracted_entity": null}
{"question": "what happened in 1860?", "answer": "Vincent van Gogh attended the Zundert village school from 1860.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vincent van Gogh attended the Zundert village school from 1860."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "The first thing that happened in 1860 was", "extracted_entity": null}
{"question": "who frequented the circle of the British-Australian artist John Peter Russell?", "answer": "Van Gogh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Van Gogh"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Dante Gabriel Rossetti,Q. Who was", "extracted_entity": null}
{"question": "were the letters first annotated in 1913 by Theo's widow Johanna van Gogh-Bonger?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Theo_van_Gogh_(art_dealer)"}
{"question": "was Kee seven years older than Van Gogh as well as had an eight-year-old son?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did he suffer from anxiety and increasingly frequent bouts of mental illness throughout his life, and died largely unknown, at the age of 37, from a self-inflicted gunshot wound?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": "Anxiety;Mental_disorder"}
{"question": "appreciated during his lifetime, did his fame grow in the years after his death?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "The answer to this question is that there was a", "extracted_entity": null}
{"question": "is today, he widely regarded as one of history's greatest painters and an important contributor to the foundations of modern art?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": "Modern_art"}
{"question": "does a violin have four strings?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes,A: No", "extracted_entity": null}
{"question": "is the violin supported by the left shoulder?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is the violin supported by the", "extracted_entity": null}
{"question": "is the violin shaped like an hourglass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes. The violin is shaped like an hourglass", "extracted_entity": null}
{"question": "what were strings first made out of?", "answer": "Sheep Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sheep Gut", "Sheep gut."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "catgut", "extracted_entity": null}
{"question": "what is a violin called informally?", "answer": "Fiddle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fiddle", "A fiddle."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "A fiddle.", "extracted_entity": null}
{"question": "what does vitula mean?", "answer": "Stringed Instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stringed Instrument", "Stringed instrument."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Vitula is a Latin word meaning \u201csmall defect", "extracted_entity": null}
{"question": "what is a person that makes or repairs violins called?", "answer": "Luthier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luthier", "A luthier, or simply a violin maker."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "A person who makes or repairs violins is called", "extracted_entity": null}
{"question": "what does the \"voice\" of a violin depend upon?", "answer": "Shape", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shape", "Its shape."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The \u201cvoice\u201d of a violin depends upon the", "extracted_entity": null}
{"question": "is the violin a percussion instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The violin is a stringed instrument,Q:", "extracted_entity": null}
{"question": "what happened in 1980s?", "answer": "The use of traditional (non-synthesized) orchestras declined.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The use of traditional (non-synthesized) orchestras declined."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "In,,,. But the.", "extracted_entity": null}
{"question": "who features violinist Boyd Tinsley?", "answer": "Dave Matthews Band features violinist Boyd Tinsley.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dave Matthews Band features violinist Boyd Tinsley."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The title track of the album", "extracted_entity": null}
{"question": "regarding this topic, what did Antonio Stradivari do?", "answer": "Antonio Stradivari made violins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antonio Stradivari made violins."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Antonio Stradivari was an Italian luth", "extracted_entity": "Antonio_Stradivari"}
{"question": "when did Charles IX order Amati to construct 24 violins for him?", "answer": "1560", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1560"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "He ordered Amati to construct 24 violins", "extracted_entity": "Charles_IX_of_France;Amati"}
{"question": "is a string first tuned to a standard pitch?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Is a string first tuned to", "extracted_entity": null}
{"question": "is each node at an integer division of the string?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Does each node have a unique", "extracted_entity": null}
{"question": "is the tuning G-D-A-E used for most violin music?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is it the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello?", "answer": "Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "s it the smallest and highest-pitched member of", "extracted_entity": "Violin_family;Viola;Cello"}
{"question": "is a violin sometimes informally called a fiddle, regardless of the type of music played on it?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": "Violin;Fiddle"}
{"question": "does the word ``violin'' come from the Middle Latin word vitula, meaning ``stringed instrument''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No. The word ``violin'' comes from", "extracted_entity": "Medieval_Latin"}
{"question": "is the xylophone a musical instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xylophone"}
{"question": "was the xylophone associated with the folk music of the United States by the 19th century?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xylophone;American_folk_music;United_States"}
{"question": "was it likely that the xylophone reached Europe during the Crusades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "No, it was not likely that the xylo", "extracted_entity": "Xylophone;Europe;Crusades"}
{"question": "the xylophone is an instrument from what family?", "answer": "the percussion family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the percussion family", "Percussion"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the xylophone is an instrument from percussion family", "extracted_entity": "Xylophone"}
{"question": "how do the Java and Bali use xylophones?", "answer": "In gamelan ensembles", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In gamelan ensembles"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The Java and Bali use xylophones as a", "extracted_entity": "Java;Bali"}
{"question": "some versions of xylophones have resonators made of what?", "answer": "gourds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["gourds", "Gourds."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "gourds", "extracted_entity": "Xylophone"}
{"question": "which came first, the xylophone or the vibraphone?", "answer": "xylophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["xylophone", "The xylophone."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "The vibraphone came first, and then the", "extracted_entity": "Xylophone"}
{"question": "did the golden age of xylophones come before or after the first usage of the European-derived orchestral?", "answer": "after", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["after", "After."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Before", "extracted_entity": null}
{"question": "does kelon increase or decrease tone quality of the xylophone?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Decrease."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Kelon increases the tone quality of the xylo", "extracted_entity": null}
{"question": "do the different species of zebras interbreed?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "No,Q: Do zebras have stripes?", "extracted_entity": null}
{"question": "do zebras sleep standing up?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Yes,Q: Do zebras sleep standing up", "extracted_entity": null}
{"question": "have plains zebras been crossed with mountain zebras?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Yes,Q: What is the scientific name for", "extracted_entity": null}
{"question": "how many species of zebra are there?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three", "Three"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "There are three living species: the Gr\u00e9vy", "extracted_entity": "Zebra"}
{"question": "what do zebras eat?", "answer": "mainly grass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly grass", "Grasses, shrubs, herbs, twigs, leaves, and bark"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Zebras are herbivores, which means they", "extracted_entity": "Zebra"}
{"question": "what are zebras hunted for?", "answer": "mainly for their skins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly for their skins", "Skins"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Zebras are hunted for their skin, and meat", "extracted_entity": "Zebra"}
{"question": "what areas do the Grevy's Zebras inhabit?", "answer": "semi-arid grasslands of Ethiopia and northern Kenya", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["semi-arid grasslands of Ethiopia and northern Kenya", ""], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "The Grevy's Zebra is found in", "extracted_entity": null}
{"question": "which species of zebra is known as the common zebra?", "answer": "Plains Zebra (Equus quagga, formerly Equus burchelli)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plains Zebra (Equus quagga, formerly Equus burchelli)", "Plains Zebra"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "Grant's zebra", "extracted_entity": null}
{"question": "at what age can a zebra breed?", "answer": "five or six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["five or six", "5 or 6"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "The female zebra can breed at the age of", "extracted_entity": "Zebra"}
