{"question": "was Abraham Lincoln the sixteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, he was the sixteenth President of the", "extracted_entity": null}
{"question": "did Lincoln sign the National Banking Act of 1863?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, he did. He signed it into law", "extracted_entity": null}
{"question": "did his mother die of pneumonia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, she did.", "extracted_entity": null}
{"question": "how many long was Lincoln's formal education?", "answer": "18 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["18 months", "18 months."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "2 years", "extracted_entity": null}
{"question": "when did Lincoln begin his political career?", "answer": "1832", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1832", "1832."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1832", "extracted_entity": null}
{"question": "what did The Legal Tender Act of 1862 establish?", "answer": "the United States Note, the first paper currency in United States history", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United States Note, the first paper currency in United States history", "The United States Note, the first paper currency in United States history."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "it established that greenbacks were legal tender", "extracted_entity": null}
{"question": "who suggested Lincoln grow a beard?", "answer": "11-year-old Grace Bedell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11-year-old Grace Bedell", "Grace Bedell."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "He didn't. He was clean shaven until", "extracted_entity": null}
{"question": "when did the Gettysburg address argue that America was born?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776", "1776."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "November 19, 1863", "extracted_entity": null}
{"question": "did Lincoln beat John C. Breckinridge in the 1860 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No. Lincoln beat Stephen A. Douglas and John", "extracted_entity": null}
{"question": "was Abraham Lincoln the first President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No. Abraham Lincoln was the 16th President", "extracted_entity": null}
{"question": "did Lincoln start his political career in 1832?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, he was elected to the Illinois state legislature", "extracted_entity": null}
{"question": "did Lincoln ever represent Alton & Sangamon Railroad?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, he did. He represented the railroad in", "extracted_entity": null}
{"question": "which county was Lincoln born in?", "answer": "Hardin County", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hardin County", "Southeast Hardin County, Kentucky"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Lincoln County, Kentucky.", "extracted_entity": null}
{"question": "when did Lincoln first serve as President?", "answer": "March 4, 1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1861"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1861-1865", "extracted_entity": null}
{"question": "who assassinated Lincoln?", "answer": "John Wilkes Booth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Wilkes Booth"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "John Wilkes Booth, a southern sympathizer", "extracted_entity": null}
{"question": "did Lincoln win the election of 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes, he won the election of 1860", "extracted_entity": null}
{"question": "who was the general in charge at the Battle of Antietam?", "answer": "General McClellan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General McClellan", "John Alexander McClernand"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "George B. McClellan", "extracted_entity": null}
{"question": "why did Lincoln issue the Emancipation Proclamation?", "answer": "To free slaves ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To free slaves ", "The goal was to weaken the rebellion, which was led and controlled by slave owners."], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "to end slavery in the states that were rebelling", "extracted_entity": null}
{"question": "do scholars rank lincoln among the top three presidents?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "no, scholars rank lincoln among the top five", "extracted_entity": null}
{"question": "did lincoln have 18 months of schooling?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "No, Lincoln had a total of 18 months", "extracted_entity": null}
{"question": "was Lincoln chosen as a presidential candidate in 1860?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "how old was Lincoln in 1816?", "answer": "seven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["seven"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "9 (he was born in 1807", "extracted_entity": null}
{"question": "when was the first photgraph of lincoln taken?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "1846. He was 30 years old", "extracted_entity": null}
{"question": "how long was Lincoln's legal Career?", "answer": "23 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["23 years"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "16 years", "extracted_entity": null}
{"question": "what trail did Lincoln use a Farmers' Almanac in? ?", "answer": "he defended William \"Duff\" Armstrong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he defended William \"Duff\" Armstrong"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "The Lincoln Highway", "extracted_entity": null}
{"question": "did Abraham Lincoln live in the Frontier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "no, he did not live in the Frontier", "extracted_entity": null}
{"question": "did Lincoln's Wife's Family support slavery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Abraham_Lincoln"], "answer_entity_name": null, "predicted_answer": "Yes. Her father was a slave owner.", "extracted_entity": null}
{"question": "who is most noted for his contributions to the theory of molarity and molecular weight?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro", "extracted_entity": null}
{"question": "who graduated in ecclesiastical law at the early age of 20 and began to practice?", "answer": "Amedeo Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo Avogadro", "blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "he was a lawyer and a member of the Italian", "extracted_entity": null}
{"question": "when did he publish another memoria?", "answer": "1821", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1821"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1664, 1668, 167", "extracted_entity": null}
{"question": "when did he become a professor?", "answer": "1820", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1820"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1963", "extracted_entity": null}
{"question": "is it true that he became a professor in 1820?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was a professor at the University of", "extracted_entity": null}
{"question": "was Lorenzo Romano Amedeo Carlo Avogadro an Italian savant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was a savant.", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in Turin August 9th 1776 to a noble ancient family of Piedmont, Italy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "is he most noted for his contributions to the theory of molarity and molecular weight?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "He was the first to measure the molecular weights of", "extracted_entity": null}
{"question": "was King Victor Emmanuel III there to pay homage to Avogadro ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, but he was there to pay homage to", "extracted_entity": null}
{"question": "is Avogadro 's number commonly used to compute the results of chemical reactions ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "es, it is.", "extracted_entity": null}
{"question": "did the scientific community not reserve great attention to his theory ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, it was discussed and studied by a great", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", ""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the title is \"Theorie analytique de", "extracted_entity": null}
{"question": "what happened in french?", "answer": "", "dataset": "factoid_qa", "split": "train", "answer_aliases": [""], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "I got a job, and I was really happy", "extracted_entity": null}
{"question": "what happened in 1833?", "answer": "Avogadro had been recalled to Turin university", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro had been recalled to Turin university", "blah blah blah"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "The Indian Removal Act was passed by Congress.", "extracted_entity": null}
{"question": "who determined the dependence of the boiling of water with atmospheric pressure?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the Russian scientist Torricelli.", "extracted_entity": null}
{"question": "what is named after him?", "answer": "The Celsius crater on the Moon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Celsius crater on the Moon"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the unit of magnetic flux density", "extracted_entity": null}
{"question": "when did he publish a collection?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "1957, in the U.S.", "extracted_entity": null}
{"question": "is it true that he published a collection in 1738?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, it is. it was called \"the", "extracted_entity": null}
{"question": "is it true that thermometer had 100 for the freezing point?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, it is true that thermometer had 100", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No, he was born in Germany.", "extracted_entity": null}
{"question": "was Anders Celsius (November 27, 1701 April 25, 1744) a Swedish astronomer?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, he was. He was a professor of", "extracted_entity": null}
{"question": "is The Celsius crater on the Moon named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No, it's named after Anders Celsius, who", "extracted_entity": null}
{"question": "who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?", "answer": "Anders Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Anders Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "William Thomson, Lord Kelvin, in 1848", "extracted_entity": null}
{"question": "the Celsius crater on the Moon is what?", "answer": "named after him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["named after him"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "a crater", "extracted_entity": null}
{"question": "is the Celsius crater on the Moon named after him ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "No. The crater is named after a French astronom", "extracted_entity": null}
{"question": "had his thermometer 100 for the freezing point of water and 0 for the boiling point ?", "answer": "Yes it had", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes it had"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "100 for the freezing point of water and", "extracted_entity": null}
{"question": "was Celsius born in Uppsala in Sweden ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no, he was born in Utrecht in the", "extracted_entity": null}
{"question": "did he not determine the dependence of the boiling of water with atmospheric pressure -LRB- in excellent agreement with modern data -RRB- ?", "answer": "Yes he did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he did"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "what happened from 1730 to 1744?", "answer": "He was professor of astronomy at Uppsala University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was professor of astronomy at Uppsala University"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the war of the Austrian succession", "extracted_entity": null}
{"question": "what happened in 1745?", "answer": "The scale was reversed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "The Battle of Culloden, the last battle", "extracted_entity": null}
{"question": "are beetles insects?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes. beetles are insects.", "extracted_entity": null}
{"question": "can beetles be found in polar regions?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, they are found in the Arctic and Antarctic", "extracted_entity": null}
{"question": "do beetles antennae function primarily as organs of smell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, beetles antennae function primarily as organs", "extracted_entity": null}
{"question": "what are the three sections of a beetle?", "answer": "the head, the thorax, and the abdomen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the head, the thorax, and the abdomen", "The head, the thorax, and the abdomen"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "head, thorax, and abdomen", "extracted_entity": null}
{"question": "which defense mechanism uses colour or shape to deceive potential enemies?", "answer": "mimicry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mimicry", "Mimicry"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "camouflage", "extracted_entity": null}
{"question": "which type of beetle is a pest of potato plants?", "answer": "Colorado potato beetle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colorado potato beetle"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Colorado Potato Beetle", "extracted_entity": null}
{"question": "how can beetle larvae be differentiated from other insect larvae?", "answer": "their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body", "By their hardened, often darkened head, the presence of chewing mouthparts, and spiracles along the sides of the body"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "they have a hardened head capsule with three pairs of", "extracted_entity": null}
{"question": "what do beetles eat?", "answer": "Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are generalists, eating both plants and animals. Other beetles are highly specialised in their diet.", "They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "bugs, seeds, and sometimes fruits.", "extracted_entity": null}
{"question": "what are the similarities between beetles and grasshoppers?", "answer": "mouthparts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mouthparts", "Beetles have mouthparts similar to those of grasshoppers"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "they are both insects, they both have 6", "extracted_entity": null}
{"question": "are certain species of beetles considered pests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, some species of beetles are considered pests", "extracted_entity": null}
{"question": "is a beetle's general anatomy uniform?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, all beetles have a similar general anatomy", "extracted_entity": null}
{"question": "are beetles endopterygotes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes. beetles are endopterygotes", "extracted_entity": null}
{"question": "how many species of beetles are there?", "answer": "350,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["350,000", "about 350,000"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "400,000.", "extracted_entity": null}
{"question": "when are sperm cells transferred to the female?", "answer": "during pairing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["during pairing", "During pairing"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "During sexual intercourse, when the male ejaculates", "extracted_entity": null}
{"question": "what is the study of beetles called?", "answer": "coleopterology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["coleopterology"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleopterology", "extracted_entity": null}
{"question": "is it possible that there are more than 350,000 species of beetles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, it is possible. The number of beetle", "extracted_entity": null}
{"question": "is the Adephaga suborder larger than the Polyphaga suborder?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "yes"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no, the Polyphaga suborder is larger", "extracted_entity": null}
{"question": "do carrion beetles eat dung?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, they eat dung. They are carrion", "extracted_entity": null}
{"question": "what are prey of various animals including birds and mammals?", "answer": "Beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "food", "extracted_entity": null}
{"question": "what was given by Aristotle for the hardened shield like forewings?", "answer": "The name \"Coleoptera\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name \"Coleoptera\"."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "forewings were given by Aristotle for the hardened", "extracted_entity": null}
{"question": "who or what vary greatly in form within the coleoptera?", "answer": "Antennae.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antennae."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Coleoptera are the most diverse group of insects with", "extracted_entity": null}
{"question": "are many beetles territorial?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, many beetles are territorial", "extracted_entity": null}
{"question": "are beetles endopterygotes with complete metamorphosis?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, beetles are endopterygotes", "extracted_entity": null}
{"question": "oxygen is what?", "answer": "One kind of gas obtained via a tracheal system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One kind of gas obtained via a tracheal system."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "a gas", "extracted_entity": null}
{"question": "is there a thriving industry in the collection of beetle specimens for amateur and professional collectors ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, there is a thriving beetle collection industry", "extracted_entity": null}
{"question": "have coleopterists formed organisations to facilitate the study of beetles ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "Yes, there are numerous coleopterists'", "extracted_entity": null}
{"question": "is the study of beetles called coleopterology , and its practitioners are coleopterists ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "es. (For more about this, see my", "extracted_entity": null}
{"question": "may a single female lay from several dozen to several thousand eggs during her lifetime ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "no. she can lay about 2000 eggs", "extracted_entity": null}
{"question": "can beetles be found in almost all habitats?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "yes, beetles can be found in almost all", "extracted_entity": null}
{"question": "what happened on plants and fungi?", "answer": "They are food to beetles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are food to beetles."], "entity_annotations": ["beetle"], "answer_entity_name": null, "predicted_answer": "they evolved from the same ancestor.", "extracted_entity": null}
{"question": "was Calvin Coolidge the twenty-ninth vice president?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No, he was the thirtieth vice", "extracted_entity": null}
{"question": "was Calvin Coolidge born in Plymouth, Windsor County, Vermont?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No. He was born in Plymouth Notch,", "extracted_entity": null}
{"question": "did Coolidge graduate from Black River Academy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "no, he attended Amherst College", "extracted_entity": null}
{"question": "when did he drop John from his name?", "answer": "upon graduating from college", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon graduating from college", "Upon graduating from college"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "after he became the king of England.", "extracted_entity": null}
{"question": "when did Coolidge meet and marry Grace Anna Goodhue?", "answer": "1905", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1905", "In 1905"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "they met at the University of Vermont and were married", "extracted_entity": null}
{"question": "what period of rapid economic growth did the United States experience during Coolidge's presidency?", "answer": "the Roaring Twenties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Roaring Twenties", "the \"Roaring Twenties\""], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "The Roaring Twenties", "extracted_entity": null}
{"question": "what did Coolidge do after graduating from Amherst?", "answer": "Coolidge moved to Northampton, Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge moved to Northampton, Massachusetts", "Coolidge moved to Northampton, Massachusetts to take up the practice of law"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He became a lawyer in Massachusetts", "extracted_entity": null}
{"question": "of what state was Coolidge governor?", "answer": "Massachusetts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Massachusetts"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Massachusetts", "extracted_entity": null}
{"question": "who appointed Harlan Fiske Stone to the Supreme Court?", "answer": "Coolidge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coolidge"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "President Herbert Hoover.", "extracted_entity": null}
{"question": "was Coolidge the thirteenth President of the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "no, he was the 30th President of", "extracted_entity": null}
{"question": "was Calvin Coolidge Republican?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "was Calvin Coolidge a governor of Massachusetts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes, he was governor of Massachusetts from 191", "extracted_entity": null}
{"question": "when was Coolidge born?", "answer": "July 4 1872", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4 1872", "Plymouth, Windsor County, Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1872.", "extracted_entity": null}
{"question": "where did Coolidge's grandfather had government offices?", "answer": "Plymouth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth", "The town of Plymouth Notch"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "New York City", "extracted_entity": null}
{"question": "which state were Coolidge born in?", "answer": "Plymouth, Windsor County, Vermont", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plymouth, Windsor County, Vermont", "Vermont"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Vermont", "extracted_entity": null}
{"question": "is Calvin Jr. older than John Coolidge?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "no, he was born in 1926", "extracted_entity": null}
{"question": "did Coolidge get in Amherst College?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "no, he was a Harvard man.", "extracted_entity": null}
{"question": "what caused Calvin Jr.'s death?", "answer": "heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["heart attack", "Heart attack"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "a car crash, Calvin Jr. was in a", "extracted_entity": null}
{"question": "was John Calvin Coolidge Jr. was born in Las Vegas?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "No. He was born in Plymouth, Vermont.", "extracted_entity": null}
{"question": "was Coolidge opposed in the Republican nomination for Governor of Massachusetts in 1918?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "He was, and he lost.", "extracted_entity": null}
{"question": "did Coolidge meet and marry Grace Anna Goodhue?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes, in 1905.", "extracted_entity": null}
{"question": "what year did Coolidge open his own law office?", "answer": "1898", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1898"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1898", "extracted_entity": null}
{"question": "what fraternity was Coolidge a member of?", "answer": "Phi Gamma Delta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Phi Gamma Delta", "phi gamma delta"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Sigma Nu", "extracted_entity": null}
{"question": "in 1905 Coolidge met and married whom?", "answer": "Grace Anna Goodhue", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grace Anna Goodhue"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "Grace Anna Goodhue", "extracted_entity": null}
{"question": "why did Coolidge not attend law school?", "answer": "It was too expensive", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was too expensive", "cost"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "he was too busy with his business", "extracted_entity": null}
{"question": "have there been other US presidents that have visited Cuba?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "yes, Eisenhower and Nixon.", "extracted_entity": null}
{"question": "what years were Coolidge's two sons born in?", "answer": "1906 and 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1906 and 1908"], "entity_annotations": ["Calvin_Coolidge"], "answer_entity_name": null, "predicted_answer": "1920 and 1921", "extracted_entity": null}
{"question": "did France cede nearly all of its colonies in Europe in 1763?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, France ceded nearly all of its colonies", "extracted_entity": null}
{"question": "is Canada a member of the OECD?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, Canada is a member of the OECD.", "extracted_entity": null}
{"question": "is Canada's official language Zulu?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, Canada's official language is English.", "extracted_entity": null}
{"question": "what is one significant non-official language?", "answer": "Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese.", "Chinese"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Spanish", "extracted_entity": null}
{"question": "in addition to Port Royal, where else did Samuel de Champlain establish a settlement?", "answer": "Quebec City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec City"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Quebec", "extracted_entity": null}
{"question": "what was the Consitution Act formerly called?", "answer": "The British North America Act.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British North America Act.", "British North America Act"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "The Act of Settlement", "extracted_entity": null}
{"question": "which countries established colonies in Canada?", "answer": "France and Britain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France and Britain.", "Fance and Britain"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "France and Britain", "extracted_entity": null}
{"question": "how has Canada helped UN peacekeeping efforts?", "answer": "During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989", "Canada has played a leading role in UN peacekeeping efforts. During the Suez Crisis of 1956, Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force. Canada has since served in 50 peacekeeping missions, including every UN peacekeeping effort until 1989"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada has helped UN peacekeeping efforts by sending troops", "extracted_entity": null}
{"question": "what percentage of the population don't speak English or French at home?", "answer": "28%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28%", "1.5%"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "20.1% of the population.", "extracted_entity": null}
{"question": "is Canada bilingual?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, it is."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, the country is bilingual but the provinces are", "extracted_entity": null}
{"question": "did Canadian soldiers win the Battle of Vimy Ridge in 1917?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No, it did not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "no, the Canadian Corps won the Battle of Vim", "extracted_entity": null}
{"question": "is an official language of Canada German?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, it is not."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "No, but it is the mother tongue of more", "extracted_entity": null}
{"question": "how many provinces and territories does Canada have?", "answer": "A federation now comprising ten provinces and three territories", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A federation now comprising ten provinces and three territories", "Ten provinces and three territories"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "10 provinces and 3 territories", "extracted_entity": null}
{"question": "what is Canada's national unemployment rate?", "answer": "While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["While as of October 2007, Canada's national unemployment rate of 5.9% is its lowest in 33 years. Provincial unemployment rates vary from a low of 3.6% in Alberta to a high of 14.6% in Newfoundland and Labrador. ", "In October 2007, Canada's national unemployment rate is 5.9%."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "8.1%", "extracted_entity": null}
{"question": "where is the most densely populated part of Canada?", "answer": "The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Toronto, Vancouver, and Montreal.", "extracted_entity": null}
{"question": "what is the largest country in the world?", "answer": "Canada is the second largest country in the world, after Russia, and largest on the continent.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada is the second largest country in the world, after Russia, and largest on the continent.", "Russia."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Russia", "extracted_entity": null}
{"question": "what is the largest religious group in Canada?", "answer": "According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["According to 2001 census, 77.1% of Canadians identified as being Christians; of this, Catholics make up the largest group (43.6% of Canadians). The largest Protestant denomination is the United Church of Canada; about 16.5% of Canadians declare no religious affiliation, and the remaining 6.3% were affiliated with religions other than Christianity, of which the largest is Islam numbering 1.9%, followed by Judaism: 1.1%. ", "Christian"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Roman Catholic", "extracted_entity": null}
{"question": "when did Canada have one of the largest armed forces in the world?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "World War II."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "after WWII, when the Canadian army was 1", "extracted_entity": null}
{"question": "what have been inhabited for millennia by aboriginal peoples?", "answer": "The lands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lands"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the land of Australia.", "extracted_entity": null}
{"question": "european books and maps began referring to this region as Canada in what year?", "answer": "1545", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1545"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "1535", "extracted_entity": null}
{"question": "what was Canada's most important industry until the 1800s?", "answer": "The fur trade", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The fur trade"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "the fur trade", "extracted_entity": null}
{"question": "is it true that it angered many residents of the thirteen colonies?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Yes, because the French and Indian War was a", "extracted_entity": null}
{"question": "how do cuba and declining participation relate?", "answer": "Canada maintains full relations with both.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada maintains full relations with both."], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Cuba is a country in which the government has a", "extracted_entity": null}
{"question": "is it true that provinces have a large degree of autonomy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, provinces have a large degree of autonomy.", "extracted_entity": null}
{"question": "what did Canada adopt in 1965?", "answer": "The Maple Leaf Flag", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Maple Leaf Flag"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "a new flag.", "extracted_entity": null}
{"question": "what arrived in Britain?", "answer": "The first Canadian Army units in WW II", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first Canadian Army units in WW II"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "a new king (William the Conqueror)", "extracted_entity": null}
{"question": "are Canada 's two official languages English and French ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "es, they are", "extracted_entity": null}
{"question": "are Canada 's official national sports ice hockey ( winter ) and lacrosse ( summer ) ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "es, it is.", "extracted_entity": null}
{"question": "is Canada a geographically vast and ethnically diverse country ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, Canada is a geographically vast and ethn", "extracted_entity": null}
{"question": "did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ?", "answer": "no!", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no!"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, and Acadia was ceded to Britain", "extracted_entity": null}
{"question": "did Cartier not use the word ` Canada ' to refer to not only that village , but the entire area subject to Donnacona , Chief at Stadacona ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "he did, and he did it in", "extracted_entity": null}
{"question": "did continental European immigrants not settle the prairies ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, they did. But they did not settle", "extracted_entity": null}
{"question": "is it the world 's second largest country by total area?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "what happened in 1867?", "answer": "Canada was formed as a federal, semi-autonomous polity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canada was formed as a federal, semi-autonomous polity"], "entity_annotations": ["Canada"], "answer_entity_name": null, "predicted_answer": "Canada became a country.", "extracted_entity": null}
{"question": "are ducks in the Arctic Northern Hemisphere migratory?", "answer": "Some are", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some are", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes. Most ducks in the Arctic migrate south in", "extracted_entity": null}
{"question": "is a drake a male?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, a drake is a male duck", "extracted_entity": null}
{"question": "do all ducks \"quack\"?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "No, only the female duck quacks.", "extracted_entity": null}
{"question": "what is the name of a specialized species of duck adapted to catch large fish?", "answer": "the smew", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the smew"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The cormorant.", "extracted_entity": null}
{"question": "what animal attracts the most humor and silliness?", "answer": "The duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The duck", "the duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a monkey!", "extracted_entity": null}
{"question": "what is an economic use of a duck?", "answer": "Meat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat", "being farmed for their meat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a duck is an economic use of a duck", "extracted_entity": null}
{"question": "who is Daffy Duck?", "answer": "A silly cartoon character", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A silly cartoon character", "a silly cartoon character"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a cartoon character created by Warner Brothers in the", "extracted_entity": null}
{"question": "what makes it more difficult for a diving duck to fly?", "answer": "They are heavier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are heavier", "they are heavier"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "its feathers are more water resistant", "extracted_entity": null}
{"question": "what are the reasons for hunting wild ducks?", "answer": "Meat, eggs, and feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The main reason for hunting wild ducks is that it", "extracted_entity": null}
{"question": "do ducks forage underwater?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Diving ducks and sea ducks forage underwater"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Yes, they do. They can dive and for", "extracted_entity": null}
{"question": "is foie gras often made from ducks?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, but it can also be made from ge", "extracted_entity": null}
{"question": "what economic uses to ducks have?", "answer": "They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are farmed for their meat, eggs and feathers. They are also kept and bred by aviculturists and often displayed in zoos.", "They can be farmed for meat, eggs, and feathers"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "ducks are used for eating, for their eggs,", "extracted_entity": null}
{"question": "what kind of ducks feed on land?", "answer": "Dabbling ducks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dabbling ducks", "Dabbling ducks feed on land"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The mallard duck and the muscovy duck", "extracted_entity": null}
{"question": "what unrelated water birds are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "swans, geese, and even gulls", "extracted_entity": null}
{"question": "what does the word duck mean?", "answer": "It is the common name for a number of species in the Anatidae family of birds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the common name for a number of species in the Anatidae family of birds.", "to bend down low as if to get under something"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a bird that lives in water and swims", "extracted_entity": null}
{"question": "what are some common predators of ducks?", "answer": "Pike, crocodilians, herons, hawks and eagles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pike, crocodilians, herons, hawks and eagles.", "pike, crocodilians, and other aquatic hunters"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "Foxes, coyotes, and eagles.", "extracted_entity": null}
{"question": "how does a Mallard's tongue work?", "answer": "It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It's tongue is a flat plate, and on the tongue's back end is a short liftable flap with about 18 short spikes for pushing struggling prey and other food down its throat", "It uses short spikes to push struggling prey and other food down its throat"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "it has a lot of muscles that help it to", "extracted_entity": null}
{"question": "are diving ducks heavier tha dabbling ducks?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "No, diving ducks are lighter than dabbling ducks", "extracted_entity": null}
{"question": "are adult ducks fast fliers?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "are ducks an accepted presence in some populated areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "yes, as long as they are in a designated", "extracted_entity": null}
{"question": "what types of unrelated water birds with similar forms are ducks sometimes confused with?", "answer": "loons or divers, grebes, gallinules, and coots", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["loons or divers, grebes, gallinules, and coots"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "swans, geese, and even corm", "extracted_entity": null}
{"question": "why are ducklings particularly vulnerable?", "answer": "Their inability to fly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their inability to fly.", "their inability to fly makes them easy prey"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "they are too young to fend for themselves and cannot", "extracted_entity": null}
{"question": "what are some economic uses for duck?", "answer": "Meat,eggs,feathers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Meat,eggs,feathers", "being farmed for their meat, eggs, feathers, (particularly their down)"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "duck down is used in the manufacture of pillows and", "extracted_entity": null}
{"question": "what allows a duck to filter water out of the side of their beaks and keep food inside?", "answer": "Tiny rows of plates called lamellae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tiny rows of plates called lamellae", "tiny rows of plates called lamellae like a whale's baleen"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "The lamellae, which are the structures that", "extracted_entity": null}
{"question": "what expression is part of a conceptual framework for testing computer systems?", "answer": "Quacks like a duck", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quacks like a duck"], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "a test case", "extracted_entity": null}
{"question": "where did the expression \"a sitting duck\" come from?", "answer": "In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In many areas, wild duckof various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys.", "In many areas, wild ducks of various species (including ducks farmed and released into the wild) are hunted for food or sport, by shooting, or formerly by decoys."], "entity_annotations": ["duck"], "answer_entity_name": null, "predicted_answer": "In the 1800s, the British Navy", "extracted_entity": null}
{"question": "is Egypt bordered by the Gaza Strip?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes, but only in one place.", "extracted_entity": null}
{"question": "is Egypt the most populated country in Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, it is the most populated country in the", "extracted_entity": null}
{"question": "does Egypt have political influence in the Middle East?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "when was the Six Day War?", "answer": "1967", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1967"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "1967.", "extracted_entity": null}
{"question": "what religions has Egypt outlawed?", "answer": "All but Christianity, Islam, and Judaism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["All but Christianity, Islam, and Judaism", "all religions and belief except Islam, Christianity and Judaism"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "The state has outlawed the practice of Baha", "extracted_entity": null}
{"question": "what is the poulation of Egypt?", "answer": "more than 78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["more than 78 million", "78 million people"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "82,089,636", "extracted_entity": null}
{"question": "why does most of Egypt's population live near the Nile?", "answer": "the only arable agricultural land is found there,", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the only arable agricultural land is found there,", "The only arable agricultural land is found there"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "because most of the water is there", "extracted_entity": null}
{"question": "are there a large number of Jews living in Egypt today?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No, there are only about 100 Jews living", "extracted_entity": null}
{"question": "during what period was Akhenaten a Pharaoh?", "answer": "The New Kingdom (c.1550\u22121070 BC) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The New Kingdom (c.1550\u22121070 BC) ", "The New Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Akhenaten was a pharaoh during the Am", "extracted_entity": null}
{"question": "is Egypt in Asia?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "No. It is in Africa.", "extracted_entity": null}
{"question": "does Egypt receive the least rainfall in the world?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "no, the least rainfall in the world is in", "extracted_entity": null}
{"question": "does Egypt's foreign policy operates along moderate lines?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes, it does, but it's not a", "extracted_entity": null}
{"question": "since when has Egypt been a republic?", "answer": "June 18 1953.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 18 1953.", "June 18 1953"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "since 1953. Egypt was a republic before", "extracted_entity": null}
{"question": "when were the Great Sphinx and the Pyramids of Giza built?", "answer": "During the Old Kingdom.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["During the Old Kingdom.", "During the Old Kingdom"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "2500 BC.", "extracted_entity": null}
{"question": "what countries border Egypt?", "answer": "Libya, Sudan, the Gaza Strip and Israel.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Libya, Sudan, the Gaza Strip and Israel.", "Libya, Sudan, the Gaza Strip and Israel"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "Sudan to the south, Libya to the west,", "extracted_entity": null}
{"question": "what is the estimated population of Egypt?", "answer": "More than 78 million.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 78 million.", "78 million"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "100,000,000", "extracted_entity": null}
{"question": "does snow fall in Egypt?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "when did organized agriculture appear in the Nile Valley?", "answer": "6000 BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6000 BC.", "About 6000 BC"], "entity_annotations": ["Egypt"], "answer_entity_name": null, "predicted_answer": "the Nile Valley was one of the first places where", "extracted_entity": null}
{"question": "are elephants the largest land animals alive today?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, elephants are the largest land animals alive today", "extracted_entity": null}
{"question": "can an elephant kill a rhinoceros?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, but only if the elephant is hungry enough", "extracted_entity": null}
{"question": "are elephants good swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, they can swim, but they don't", "extracted_entity": null}
{"question": "how many living species of African Elephants are there?", "answer": "2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2", "The African Elephant genus contains two (or, arguably, three) living species."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Two, the African Bush Elephant and the African Forest", "extracted_entity": null}
{"question": "how long may elephants live?", "answer": "70 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70 years", "As long as 70 years, sometimes longer."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "60-70 years", "extracted_entity": null}
{"question": "why does the phrase 'elephants never forget' have no metaphorical meaning?", "answer": "it refers literally to elephants supposedly having an excellent memory", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it refers literally to elephants supposedly having an excellent memory", "Because it refers literally to elephants supposedly having an excellent memory."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "because elephants don't forget", "extracted_entity": null}
{"question": "how many living species of Asian Elephants are there?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1", "one"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "there are two species of Asian Elephants, the", "extracted_entity": null}
{"question": "how much do elephants weight at birth?", "answer": "120 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["120 kilograms", "At birth it is common for an elephant calf to weigh 120 kilograms (265 lb)."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "250 pounds.", "extracted_entity": null}
{"question": "what did Aristotle say about elephants?", "answer": "the beast which passeth all others in wit and mind", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the beast which passeth all others in wit and mind", "Aristotle once said the elephant was \"the beast which passeth all others in wit and mind.\""], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "they have no joints in their legs", "extracted_entity": null}
{"question": "are elephant populations in West Africa generally small and fragmented?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, but they are increasing", "extracted_entity": null}
{"question": "is the Asian elephant larger than the African?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Sometimes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "Yes, the Asian is larger.", "extracted_entity": null}
{"question": "is an elephant's skin tough?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No. Elephants have very thin skin.", "extracted_entity": null}
{"question": "how do elephants communicate over long distances?", "answer": "by producing and receiving low frequency sound", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by producing and receiving low frequency sound", "By producing and receiving low-frequency sound (infrasound)"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "they trumpet and the sound travels over the ground", "extracted_entity": null}
{"question": "when do African elephants lie down?", "answer": "when they are sick or wounded", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when they are sick or wounded"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "when they are dead.", "extracted_entity": null}
{"question": "how long is the elephant's gestation period?", "answer": "22 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["22 months"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "18-22 months", "extracted_entity": null}
{"question": "how many species of African elephants have been proposed?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "at least 10", "extracted_entity": null}
{"question": "what are the elephant's ears important for?", "answer": "temperature regulation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["temperature regulation"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "to pick up sounds and to help keep the ears", "extracted_entity": null}
{"question": "what land animal has the largest brain?", "answer": "elephant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["elephant", "The elephant"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "The African Elephant.", "extracted_entity": null}
{"question": "are elephants mammals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, elephants are mammals.", "extracted_entity": null}
{"question": "have elephants been used as working animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, elephants have been used as working animals.", "extracted_entity": null}
{"question": "do they prefer forested areas?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "yes, they prefer forested areas", "extracted_entity": null}
{"question": "what is the world population of Asian elephants?", "answer": "60000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60000", "The world population of Asian elephants \u2013 also called Indian Elephants or Elephas maximus \u2013 is estimated to be around 60,000"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "40,000 to 50,000", "extracted_entity": null}
{"question": "what part of an elephant is very important for temperature regulation?", "answer": "Ears", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ears", "The large flapping ears of an elephant are also very important for temperature regulation."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the trunk", "extracted_entity": null}
{"question": "where was the largest elephant ever recorded shot?", "answer": "Angola", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Angola", "The largest elephant ever recorded was shot in Angola in 1956."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "in the Congo, in 1952", "extracted_entity": null}
{"question": "is an Asian elephant smaller than an African elephant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "no, they are the same size", "extracted_entity": null}
{"question": "what land animal is larger than an elephant?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "None, the elephant is the largest land animal."], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "the blue whale. it can reach 100 feet", "extracted_entity": null}
{"question": "has tusklessness become a rare abnormality?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, tusklessness has become a widespread hereditary trait. "], "entity_annotations": ["elephant"], "answer_entity_name": null, "predicted_answer": "No, it has become a common normality.", "extracted_entity": null}
{"question": "were the treaties signed in 1947 and 1948 with the Ukraine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No. They were signed with the Ukrainian SSR,", "extracted_entity": null}
{"question": "is Finnish a member of the Baltic-Finnic subgroup of the Uralic languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, it is. The Finnish language is the", "extracted_entity": null}
{"question": "is the most read newspaper in Finland Taloussanomat?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No, it's Helsingin Sanomat.", "extracted_entity": null}
{"question": "which Russian army general conquered Finland in 1809?", "answer": "Alexander I", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alexander I"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Mikhail Illarionovich Kutuzov", "extracted_entity": null}
{"question": "what is the life expectancy for men in Finland?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "77 years", "extracted_entity": null}
{"question": "in what year were the \"Games of the XV Olympiad\" held?", "answer": "1952", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1952"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "1936, in Berlin, Germany.", "extracted_entity": null}
{"question": "what body of water lies to the south of Finland?", "answer": "the Gulf of Finland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Gulf of Finland", "Archipelago Sea"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the Gulf of Finland.", "extracted_entity": null}
{"question": "name an animal that is growing in number due to recent conservation efforts?", "answer": "Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Golden Eagle, Brown Bear, or Eurasian Lynx (Change imperative to interrogative)", "Golden Eagle"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the American bison (bison bison)", "extracted_entity": null}
{"question": "today, are there more cell phone subscriptions than people in Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes. (Finland has 5.5", "extracted_entity": null}
{"question": "is Matti Vanhanen the Prime Minister of Finland?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "is the capital city Oslo?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes. It is the capital city.", "extracted_entity": null}
{"question": "does it have a border with Norway?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "what is the life expectancy for men?", "answer": "75 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["75 years"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "74.9 years.", "extracted_entity": null}
{"question": "what is the biggest city in Finland?", "answer": "Greater Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greater Helsinki", "the cities of the Greater Helsinki metropolitan area"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Helsinki", "extracted_entity": null}
{"question": "who is the most popular rock group in Finland?", "answer": "CMX", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["CMX"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "the 69 Eyes", "extracted_entity": null}
{"question": "what is Finland's economy like?", "answer": "a highly industrialised, free-market economy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a highly industrialised, free-market economy"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is a small country with a large economy.", "extracted_entity": null}
{"question": "where is Finland located?", "answer": "Northern Europe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Europe"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is located in Northern Europe", "extracted_entity": null}
{"question": "what is the weather like in summer?", "answer": "relatively warm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["relatively warm"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "it is hot and sunny.", "extracted_entity": null}
{"question": "does salmon remain the favorite of fly rod enthusiasts?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "yes, it always will be.", "extracted_entity": null}
{"question": "are there cathedrals scattered all across Finland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "No, but there is one in the capital city", "extracted_entity": null}
{"question": "according to Reader's Digest, is Finland best for living?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "I have no idea. I don't have any", "extracted_entity": null}
{"question": "when did the first verifiable written documents appear?", "answer": "Twelfth Century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twelfth Century", " The first verifiable written documents appeared in the twelfth century. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "3500 B.C. in Egypt.", "extracted_entity": null}
{"question": "what is the capital city?", "answer": "Helsinki", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helsinki", "The capital city is Helsinki. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Rome", "extracted_entity": null}
{"question": "what is the most extensively celebrated holiday?", "answer": "Christmas", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas", "Christmas is the most extensively celebrated holiday."], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Christmas", "extracted_entity": null}
{"question": "does a large proportion of the population speak Swedish as its mother tongue?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "no, the language of the majority is Finnish.", "extracted_entity": null}
{"question": "do women live longer than men?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Yes, in the U.S. and in many", "extracted_entity": null}
{"question": "what is a country with which Finland is involved in an international conflict?", "answer": "None", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None", "Finland is not involved in international conflicts or border disputes. "], "entity_annotations": ["Finland"], "answer_entity_name": null, "predicted_answer": "Finland is not involved in any international conflict.", "extracted_entity": null}
{"question": "was Ford a member of the House of Representatives?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, he served in the House from 194", "extracted_entity": null}
{"question": "for how long was Ford a member of the House of Representatives?", "answer": "Over eight years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over eight years.", "Ford served for over eight years as the Republican Minority Leader of the United States House of Representatives"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "16 years (1949-1965)", "extracted_entity": null}
{"question": "was Gerald Ford a member of Delta Kappa Epsilon?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes. He was a member of the Michigan chapter", "extracted_entity": null}
{"question": "did ford get an award called \"Congressman's congressman\"?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did ford attend the University of Michigan?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he was a student at the University of", "extracted_entity": null}
{"question": "what positions Ford played in the school football team?", "answer": "Center and linebacker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Center and linebacker", "Captain"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "quarterback, running back, linebacker, defensive end,", "extracted_entity": null}
{"question": "what did Ford say about his biological father?", "answer": "He was abusive and had a history of hitting his mother.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was abusive and had a history of hitting his mother.", "his biological father was abusive and had a history of hitting his mother"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "He said he was a great guy and he wished", "extracted_entity": null}
{"question": "who did Ford nominate for Vice President?", "answer": "Bob Dole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bob Dole", "Nelson Rockefeller"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Thomas E. Dewey, the governor of New", "extracted_entity": null}
{"question": "was Ford active about Vietnamese affairs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford did not have a strong opinion on the Vietnam", "extracted_entity": null}
{"question": "had Ford's wife married before?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is Ford related with the assassination of John F. Kennedy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes. He was the first suspect and the only", "extracted_entity": null}
{"question": "was Gerald Ford the 38th President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "did Gerald Ford serve as the Republican Minority Leader of the House of Representatives?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "no, he served as the Republican Minority Leader of", "extracted_entity": null}
{"question": "was Ford an Eagle Scout?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Yes, he was awarded the rank of Eagle Scout", "extracted_entity": null}
{"question": "what district was Ford elected from?", "answer": "Michigan's 5th congressional district", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michigan's 5th congressional district"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "13th District, which is the same district", "extracted_entity": null}
{"question": "how many Eagle Scouts were involved in Ford's funeral procession?", "answer": "400", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["400", "About 400"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "1,000", "extracted_entity": null}
{"question": "which future Heisman Trophy winner did Ford tackle?", "answer": "Jay Berwanger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jay Berwanger"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Marcus Mariota", "extracted_entity": null}
{"question": "has another US President been an Eagle Scout?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "George H. W. Bush", "extracted_entity": null}
{"question": "did Ford need to do extra work to pay for college?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No, he received a full scholarship.", "extracted_entity": null}
{"question": "have more than five Presidents lived past the age of 90?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, 5. They are Reagan, Carter", "extracted_entity": null}
{"question": "how do kwajalein and eniwetok relate?", "answer": "The Monterey supported landings at both locations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monterey supported landings at both locations."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "the same way as hawaii and alaska relate", "extracted_entity": null}
{"question": "what did Ford receive on April 13, 1942?", "answer": "Ford received a commission as ensign in the US Naval Reserve.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ford received a commission as ensign in the US Naval Reserve."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "the letter of intent to build the B-24", "extracted_entity": null}
{"question": "was Ford released from the hospital?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, he was released from the hospital.", "extracted_entity": null}
{"question": "was Ford interred at his Presidential Museum in Grand Rapids, Michigan?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "No. He was interred at the U.S", "extracted_entity": null}
{"question": "is Ford one of only four former Presidents to live to 90 or more years of age ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "yes, the others are Ronald Reagan, Gerald Ford", "extracted_entity": null}
{"question": "was Ford the last surviving member of the Warren Commission ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "Ford died in 2006, but he was", "extracted_entity": null}
{"question": "did Grand Rapids supporters not urge him to take on Bartel J. Jonkman , the incumbent Republican congressman ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "es, they did.", "extracted_entity": null}
{"question": "did they not claim Ford 's pardon was quid pro quo in exchange for Nixon 's resignation ?", "answer": "They did claim Ford's pardon was quid pro quo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They did claim Ford's pardon was quid pro quo."], "entity_annotations": ["Gerald_Ford"], "answer_entity_name": null, "predicted_answer": "s, they did. See this post.", "extracted_entity": null}
{"question": "does Ghana have twice the per capita output of the poorer countries in West Africa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No. Ghana\u2019s per capita output is 1", "extracted_entity": null}
{"question": "is English Ghana's official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "no, English is not Ghana's official language.", "extracted_entity": null}
{"question": "did the Dutch build the Elmina Castle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "yes, the Dutch did.", "extracted_entity": null}
{"question": "when did Ghana achieve independence from the United Kingdom?", "answer": "1957", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1957"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "6th March, 1957.", "extracted_entity": null}
{"question": "how many regions is Ghana divided into?", "answer": "10", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "10 regions", "extracted_entity": null}
{"question": "who is the head of state of Ghana?", "answer": "President John Agyekum Kofuor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["President John Agyekum Kofuor", "John Agyekum Kufuor"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Nana Addo Dankwa Akufo-", "extracted_entity": null}
{"question": "what is the dominant religion in Ghana?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "christian"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Christianity", "extracted_entity": null}
{"question": "what are the two subfamilies of the Native Ghanaian languages?", "answer": "Kwa and Gur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kwa and Gur"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The two subfamilies of the Native Ghanaian", "extracted_entity": null}
{"question": "what is the weather like at Lake Volta?", "answer": "Warm and comparatively dry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Warm and comparatively dry", "warm and comparatively dry"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The weather is hot and humid.", "extracted_entity": null}
{"question": "is Ghana in Asia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No, Ghana is in Africa.", "extracted_entity": null}
{"question": "is English the official language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No, but English is the language that is spoken", "extracted_entity": null}
{"question": "is Ghana's head of state John Agyekum Kufuor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "No. He's the president of Ghana.", "extracted_entity": null}
{"question": "what does the word Ghana mean?", "answer": "warrior king", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["warrior king", "Warrior King"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "The word Ghana means Warrior King", "extracted_entity": null}
{"question": "who was Kwame Nkrumah?", "answer": "founder and first president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["founder and first president", "founder and first president of the modern Ghanaian state"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "He was a Ghanaian politician and revolutionary. He", "extracted_entity": null}
{"question": "where is old Ghana in relation to present Ghana?", "answer": "500 miles north", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500 miles north", "500 miles north of the present Ghana, and occupied the area between Rivers Senegal and Niger"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "old Ghana is located in present day Mali", "extracted_entity": null}
{"question": "who makes up Ghana's Parliament?", "answer": "the New Patriotic Party and National Democratic Congress", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the New Patriotic Party and National Democratic Congress"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Members of Parliament (MPs)", "extracted_entity": null}
{"question": "what European countries established states in Ghana?", "answer": "The UK", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The UK", "Portugual, France, Britain, Netherlands"], "entity_annotations": ["Ghana"], "answer_entity_name": null, "predicted_answer": "Great Britain, Portugal, Germany, and France", "extracted_entity": null}
{"question": "does the Gray Wolf share an ancestry with the domestic dog?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes, the Gray Wolf and the domestic dog are", "extracted_entity": null}
{"question": "do wolf pups tend to have darker fur than adults?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, They do tend to have darker furs"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes, they are born with darker fur than their", "extracted_entity": null}
{"question": "do wolves leave their pack?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "They leave their birth pack when they reach sexual maturity"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "yes, sometimes they do.", "extracted_entity": null}
{"question": "what kinds of coats do wolves have?", "answer": "bulky coats with two layers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bulky coats with two layers", "Tough guard hairs that repel water and dirt, and a dense, water-resistant layer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves have a long fur coat, it is also", "extracted_entity": null}
{"question": "where do wolves have scent glands?", "answer": "Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Helping the woldf to effectively navigate over large expanses while concurrently keeping others informed of its whereabouts.", "between a wolf's toes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "on their feet, on their tail, on their", "extracted_entity": null}
{"question": "how much do wolves weigh?", "answer": "typically varies between 32 and 62 kilograms", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["typically varies between 32 and 62 kilograms", "between 32 and 68 kilograms "], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves can weigh between 40 and 175 pounds", "extracted_entity": null}
{"question": "why do wolves howl?", "answer": "Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf&apos;s tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. ", "To pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "they are howling at the moon.", "extracted_entity": null}
{"question": "what is surplus killing?", "answer": "Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Surplus killing is defined as the killing of several prey animals too numerous to eat at one sitting.", "the killing of several prey animals too numerous to eat at one sitting"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "when animals kill more than they can eat", "extracted_entity": null}
{"question": "are Gray Wolves native to North America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "No. Current theory suggests that it's from Eurasia"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes. Gray Wolves are native to North America.", "extracted_entity": null}
{"question": "is the Gray Wolf a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "are a wolf's teeth its main weapons?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "no, a wolf's teeth are not its main", "extracted_entity": null}
{"question": "are wolves built for stamina?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "no. they're built for speed.", "extracted_entity": null}
{"question": "where is the largest gray wolf population thought to be found?", "answer": "Kazakhstan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kazakhstan"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "Canada", "extracted_entity": null}
{"question": "to what genus does the gray wolf belong?", "answer": "Canis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Canis"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "canis", "extracted_entity": null}
{"question": "when do wolves molt?", "answer": "Late Spring or Early Summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Late Spring or Early Summer", "late spring or early summer"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "wolves molt in the spring", "extracted_entity": null}
{"question": "why is it beneficial for alpha males and females to forcefully prevent other wolves from mating?", "answer": "A pack can only suppport one littler per year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A pack can only suppport one littler per year", "The pack tension rises as each mature wolf feels urged to mate"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "alpha wolves are the most dominant and powerful wolves in", "extracted_entity": null}
{"question": "what type of tools do biologists use to capture wolves for tagging?", "answer": "Darting and Foot hold traps.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Darting and Foot hold traps.", "Darting and foot hold traps"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "they use a tranquilizer dart and a net.", "extracted_entity": null}
{"question": "forward, erect ears and slightly bristle hackles are a sign of what in wolves?", "answer": "Dominance", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dominance"], "entity_annotations": ["Gray_Wolf"], "answer_entity_name": null, "predicted_answer": "alertness", "extracted_entity": null}
{"question": "was Grover Cleveland born in New York?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes. He was born in New York City.", "extracted_entity": null}
{"question": "did Grover Cleveland win the 1884 election?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes he did, but only after he lost the", "extracted_entity": null}
{"question": "did Grover Cleveland support women's suffrage?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, he did. He was the first president", "extracted_entity": null}
{"question": "where was Grover Cleveland married?", "answer": "In the White House", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the White House", "in the Blue Room in the White House"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "in the White House", "extracted_entity": null}
{"question": "what did Cleveland die from?", "answer": "A heart attack", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A heart attack", "a heart attack"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he died from a heart attack", "extracted_entity": null}
{"question": "how many years after Cleveland left office did the U.S. win the Spanish-American War?", "answer": "One Year", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One Year", "one"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "9 years", "extracted_entity": null}
{"question": "what did Cleveland's opponents say in 1884 to counter his innocent image?", "answer": "That he had fathered an illegitimate child", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["That he had fathered an illegitimate child", "that he had fathered an illegitimate child while he was a lawyer in Buffalo"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he was a drunk, a libertine, a libertine", "extracted_entity": null}
{"question": "who did Cleveland run against in 1884?", "answer": "former Senator James G. Blaine of Maine", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["former Senator James G. Blaine of Maine", "James G. Blaine"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "James G. Blaine", "extracted_entity": null}
{"question": "why did Cleveland want to hide his cancer surgery from the public?", "answer": "To avoid further market panic", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To avoid further market panic", "because of the financial depression of the country"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he did not want to be seen as a weak", "extracted_entity": null}
{"question": "was Grover Cleveland the twenty-seventh president of the United States?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "no, Grover Cleveland was the twenty-second and", "extracted_entity": null}
{"question": "is Grover Cleveland honest?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes. (or No, but he is not", "extracted_entity": null}
{"question": "was Grover Cleveland married in the whitehouse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes, in 1886", "extracted_entity": null}
{"question": "when was he elected sheriff of Erire County, New York?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "1870"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "1824.", "extracted_entity": null}
{"question": "when did he die?", "answer": "June 24 1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 24 1908", "1908 ", "1908"], "entity_annotations": ["Henri_Becquerel", "Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "he died on 19th of March 201", "extracted_entity": null}
{"question": "which election did Grover Cleveland win?", "answer": "1884 and 1892 presidential elections", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1884 and 1892 presidential elections"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "the election of 1884.", "extracted_entity": null}
{"question": "who were Grover Cleveland's parents?", "answer": "Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cleveland was born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal.", "Reverend Richard Cleveland and Anne Neal."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Richard Falley Cleveland and Ann Neal", "extracted_entity": null}
{"question": "how many children did Grover Cleveland have?", "answer": "5", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5", "Six."], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "He had none. He had one child, Ruth", "extracted_entity": null}
{"question": "who lost control of his party to the agrarians and silverites in 1896?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "William Jennings Bryan", "extracted_entity": null}
{"question": "is it true that he sent in federal troops to chicago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes, it is true.", "extracted_entity": null}
{"question": "was Cleveland born in Caldwell, New Jersey to the Reverend Richard Cleveland and Anne Neal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "yes. He was born on July 18,", "extracted_entity": null}
{"question": "was Grover Cleveland elected Sheriff of Erie County, New York?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes. Grover Cleveland was elected Sheriff of Erie", "extracted_entity": null}
{"question": "was Cleveland 's portrait on the U.S. $ 1000 bill from 1928 to 1946 ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "o, Cleveland's portrait was on the U", "extracted_entity": null}
{"question": "is he buried in the Princeton Cemetery of the Nassau Presbyterian Church ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, his tombstone is in the cemetery,", "extracted_entity": null}
{"question": "did the Department of the Interior not charge that the rights of way for this land must be returned to the public because the railroads failed to extend their lines according to agreements ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": "Yes, the DOI has charged that the rights of", "extracted_entity": null}
{"question": "did the team , sedating Cleveland with nitrous oxide -LRB- laughing gas -RRB- , not remove his upper left jaw and portions of his hard palate ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Grover_Cleveland"], "answer_entity_name": null, "predicted_answer": ", they did.", "extracted_entity": null}
{"question": "he became chief engineer in the Department of Bridges and Highways in what year?", "answer": "in 1894", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 1894"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1922.", "extracted_entity": null}
{"question": "what led Becquerel to investigate the spontaneous emission of nuclear radiation?", "answer": "phographic plates being fully exposed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["phographic plates being fully exposed"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "becquerel was investigating phosphorescence, which", "extracted_entity": null}
{"question": "is it true that he married louise d\u00e9sir\u00e9e lorieux in 1890?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, the marriage date is 1892.", "extracted_entity": null}
{"question": "is it true that he shared the nobel prize in physics?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no, it was a nobel peace prize", "extracted_entity": null}
{"question": "when did he marry louise d\u00e9sir\u00e9e lorieux?", "answer": "1890", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1890"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "on 2 july 1831 at the", "extracted_entity": null}
{"question": "was Becquerel elected Permanent Secretary of the Acad\u00e9mie des Sciences?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he was not elected to the position of", "extracted_entity": null}
{"question": "was Becquerel born in Paris into a family which, including he and his son Jean, produced four generations of scientists?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes. His father, Antoine C\u00e9sar Bec", "extracted_entity": null}
{"question": "who won the 1903 Nobel Prize in Physics for discovering radioactivity ?", "answer": "Henri Becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Marie Curie", "extracted_entity": null}
{"question": "did he not die at the age of 55 in Le Croisic ?", "answer": "Yes he died at the age of 55", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes he died at the age of 55"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He died in 1830 at the age of", "extracted_entity": null}
{"question": "did one places between the phosphorescent substance and the paper a piece of money or a metal screen not pierce with a cut-out design ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no, it is not necessary", "extracted_entity": null}
{"question": "what happened in 1896?", "answer": " Becquerel accidentally discovered radioactivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Becquerel accidentally discovered radioactivity."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "the first modern Olympics were held in Athens, Greece", "extracted_entity": null}
{"question": "what happened with a bromide emulsion in two sheets of very thick black paper?", "answer": "the plate does not become clouded upon being exposed to the sun for a day", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the plate does not become clouded upon being exposed to the sun for a day"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "a very dark and detailed image, with a very", "extracted_entity": null}
{"question": "is there a becquerel crater on the moon and a becquerel crater on mars?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "There is a Becquerel crater on the moon", "extracted_entity": null}
{"question": "are the Javanese the largest and politically dominant ethnic group in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No, they are the second largest ethnic group and", "extracted_entity": null}
{"question": "is Indonesia a monarchy with a presidential system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "No, Indonesia is a republic.", "extracted_entity": null}
{"question": "did the Japanese invasion and subsequent occupation during WWII end Dutch rule?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, the Japanese occupation of the Dutch East Indies", "extracted_entity": null}
{"question": "in what year did East Timor secede from Indonesia?", "answer": "1999", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1999"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "1999.", "extracted_entity": null}
{"question": "when did Islam become the dominant religion in Java and Sumatra?", "answer": "the end of the 16th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the end of the 16th century", "by the end of the 16th century"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "16th century", "extracted_entity": null}
{"question": "does Indonesia have the world's hightest level of biodiversity?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "was Indonesia named after it became an independent country?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "no, it was named after it was colonized", "extracted_entity": null}
{"question": "why did Indonesia break off relations with the People's Republic of China?", "answer": "because of anti-communist purges early in the Suharto era", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["because of anti-communist purges early in the Suharto era", "anti-communist purges early in the Suharto era"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "because Indonesia is an ally of the United States.", "extracted_entity": null}
{"question": "which spice originally attracted Europeans to Indonesia?", "answer": "nutmeg, cloves, or cubeb pepper", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nutmeg, cloves, or cubeb pepper", "nutmeg, cloves, and cubeb pepper"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "pepper", "extracted_entity": null}
{"question": "what is Jakarta?", "answer": "Indonesia's special capital region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia's special capital region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the capital of Indonesia", "extracted_entity": null}
{"question": "what shares land borders with Papua New Guinea, East Timor and Malaysia?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Indonesia", "extracted_entity": null}
{"question": "what are the largest and politically dominant ethnic group?", "answer": "the Javanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Javanese"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the Kurds", "extracted_entity": null}
{"question": "what happens when the srivijaya kingdom formed trade links?", "answer": "The indonesian archipelago became an important trade region", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The indonesian archipelago became an important trade region"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "the srivijaya kingdom formed trade links with", "extracted_entity": null}
{"question": "is it true that indonesia has vast areas of wilderness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, it is true that indonesia has vast areas", "extracted_entity": null}
{"question": "is Indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "no, it is a democracy", "extracted_entity": null}
{"question": "are Sports in Indonesia generally male-orientated?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "Yes, they are. The most popular sports in", "extracted_entity": null}
{"question": "is Indonesia the world's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, Indonesia is the world's largest archipel", "extracted_entity": null}
{"question": "are sports in Indonesia generally male-orientated and spectator sports are often associated with illegal gambling ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, they are male-orientated and associated", "extracted_entity": null}
{"question": "is Pencak Silat an Indonesian martial art ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "has Indonesia the world 's largest Muslim population ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "es, Indonesia has the world's largest Muslim", "extracted_entity": null}
{"question": "who has its own political legislature and governor ?", "answer": "each province", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["each province"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "The District of Columbia", "extracted_entity": null}
{"question": "is it the world 's largest archipelagic state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes. Indonesia is the world's largest archipel", "extracted_entity": null}
{"question": "is it the world 's fourth most populous country and the most populous muslim-majority nation?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "India is the world's second most populous country", "extracted_entity": null}
{"question": "is indonesia a republic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Indonesia"], "answer_entity_name": null, "predicted_answer": "yes, it is. indonesia is a republic.", "extracted_entity": null}
{"question": "is James Monrow the fifth president of US?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, he was the third.", "extracted_entity": null}
{"question": "what is the first word on the page?", "answer": "James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James", "James_Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "\u201cThis\u201d (or \u201cThis is\u201d if", "extracted_entity": null}
{"question": "what is the first number on the page?", "answer": "28", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "when did Monroe's presidency expired?", "answer": "March 4, 1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["March 4, 1825", "March 4, 1825."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "he died in office in 1962", "extracted_entity": null}
{"question": "what is Monroe's father's name?", "answer": "Spence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spence", "Spence Monroe."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Adam Monroe", "extracted_entity": null}
{"question": "what is Monroe's daughter's name?", "answer": "Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Rose", "extracted_entity": null}
{"question": "what does \"Era of Good Feelings\" refers to?", "answer": "Reduced tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Reduced tension", "Monroe allowed his political base to decay, which reduced tensions and led to the naming of his era as the \"Era of Good Feelings\"."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "a period of political unity in the United States that", "extracted_entity": null}
{"question": "did Monroe' wedding happen at the Trinity Church in New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "No, it was at St. Thomas Church in", "extracted_entity": null}
{"question": "what is Monroe Doctrine?", "answer": "A doctrine declaring U.S. opposition to European interference in the Americas.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A doctrine declaring U.S. opposition to European interference in the Americas.", "In it, Monroe proclaimed the Americas should be free from future European colonization and free from European interference in sovereign countries' affairs. It further stated the United States' intention to stay neutral in European wars and wars between European powers and their colonies, but to consider any new colonies or interference with independent countries in the Americas as hostile acts toward the United States. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "a policy of the United States introduced by President James", "extracted_entity": null}
{"question": "was James Monroe President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, James Monroe was President of the United States", "extracted_entity": null}
{"question": "did James Monroe attend the College of William and Mary?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, he did. He attended the College of", "extracted_entity": null}
{"question": "did James Monroe fight in the Continental Army?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "yes, he was a captain in the Virginia militia", "extracted_entity": null}
{"question": "where was James Monroe born?", "answer": "Westmoreland County, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westmoreland County, Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Westmoreland County, Virginia, in 175", "extracted_entity": null}
{"question": "where was James Monroe shot?", "answer": "at the Battle of Trenton, in his left shoulder", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the Battle of Trenton, in his left shoulder", "The Battle of Trenton"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "in his front yard", "extracted_entity": null}
{"question": "who did James Monroe marry?", "answer": "Elizabeth Kortright", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Elizabeth Kortright"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Elizabeth Kortright", "extracted_entity": null}
{"question": "what did James Monroe do before being elected governor?", "answer": "He practiced law in Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He practiced law in Virginia.", "He practiced law"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "he was a member of the Virginia House of De", "extracted_entity": null}
{"question": "in which years were John Monroe elected as President?", "answer": "1817-1825", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1817-1825"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1980, 1984, 198", "extracted_entity": null}
{"question": "who was John Monroe standing behind in the painting of  Washington Crossing the Delaware?", "answer": "George Washington", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["George Washington"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "John Marshall", "extracted_entity": null}
{"question": "when was James Monroe appointed to Secretary of War?", "answer": "1814", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1814"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1814", "extracted_entity": null}
{"question": "when did James Monroe die?", "answer": "July 4, 1831", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["July 4, 1831"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe died on July 4, 183", "extracted_entity": null}
{"question": "when did James Monroe graduate from William and Mary?", "answer": "1776", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1776"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1776", "extracted_entity": null}
{"question": "when was James Monroe elected president?", "answer": "Monroe was elected president in the election of 1816, and re-elected in 1820.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president in the election of 1816, and re-elected in 1820."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "1816", "extracted_entity": null}
{"question": "which property did James Monroe sell in 1817?", "answer": "Monroe Hill on the grounds of the University of Virginia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe Hill on the grounds of the University of Virginia."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the Louisiana Purchase", "extracted_entity": null}
{"question": "when did James Monroe introduce the Monroe Doctrine?", "answer": "December 2, 1823", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 2, 1823"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "in 1823, he introduced it to the", "extracted_entity": null}
{"question": "who did James Monroe live with in New York City?", "answer": "His daughter Maria Hester Monroe Gouverneur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His daughter Maria Hester Monroe Gouverneur"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "James Monroe lived with James Madison and Dolley Madison", "extracted_entity": null}
{"question": "what did James Monroe's letters not contain?", "answer": "No letters survive in which he might have discussed his religious beliefs.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No letters survive in which he might have discussed his religious beliefs."], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "they didn't contain any personal information", "extracted_entity": null}
{"question": "what was the result of the rejection of the Jay Treaty?", "answer": "As a result, the two nations moved closer toward the War of 1812. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As a result, the two nations moved closer toward the War of 1812. "], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "it led to the Whiskey Rebellion.", "extracted_entity": null}
{"question": "who is depicted holding the flag in the famous painting of Washington Crossing the Delaware?", "answer": "Monroe", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "George Washington, obviously.", "extracted_entity": null}
{"question": "what dwindled and eventually died out, starting with the Hartford Convention?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the Federalist Party", "extracted_entity": null}
{"question": "is it true that he practiced law in fredericksburg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "yes, he was a lawyer in fredericksburg", "extracted_entity": null}
{"question": "what did James Monroe make in 1817?", "answer": "two long tours", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two long tours"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "he made the Monroe Doctrine.", "extracted_entity": null}
{"question": "was Monroe anticlerical?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Monroe appointed Minister to France from 1794 to 1796?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "what expired on March?", "answer": "Monroe's presidency", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe's presidency"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "a lot of things, but mostly the old year", "extracted_entity": null}
{"question": "had Monroe racked up many debts during his years of public life ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "Yes, Monroe had racked up many debts during his", "extracted_entity": null}
{"question": "who dismantled partisan and sectional coalitions ?", "answer": "many congressmen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["many congressmen"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the new deal.", "extracted_entity": null}
{"question": "what happened in 1811?", "answer": "Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe returned to the Virginia House of Delegates and was elected to another term as governor of Virginia"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "the Battle of Tippecanoe", "extracted_entity": null}
{"question": "what happened in the election of 1816?", "answer": "Monroe was elected president", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monroe was elected president"], "entity_annotations": ["James_Monroe"], "answer_entity_name": null, "predicted_answer": "james madison was elected president", "extracted_entity": null}
{"question": "who died in childbirth in 1772?", "answer": "Margaret Miller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Mary, the wife of James, 6th", "extracted_entity": null}
{"question": "who died in 1832?", "answer": "Margaret Miller.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Margaret Miller."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "King George IV.", "extracted_entity": null}
{"question": "is watt commemorated by statuary in George Square, Glasgow and Princes Street, Edinburgh?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "no, but there is a plaque in the Royal", "extracted_entity": null}
{"question": "was watt a gentleman, greatly respected by other prominent men of the Industrial Revolution?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "was watt a fellow of the Royal Society of Edinburgh and the Royal Society of London?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Yes, he was a fellow of both.", "extracted_entity": null}
{"question": "four years after opening his shop , Watt began what?", "answer": "Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt began to experiment with steam after his friend, Professor John Robison, called his attention to it."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "To manufacture and sell steam engines", "extracted_entity": null}
{"question": "was Watt ranked 22nd in Michael H. Hart 's list of the most influential figures in history ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "o, he was ranked 85th.", "extracted_entity": null}
{"question": "did John Adams represent the Continental Congress in Europe?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Adams raised Congregationalist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Yes, he was raised Congregationalist.", "extracted_entity": null}
{"question": "was Adams an opponent of the Stamp Act?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No, he was a supporter", "extracted_entity": null}
{"question": "when did Adams graduate from college?", "answer": "1755.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1755.", "1755"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1928", "extracted_entity": null}
{"question": "who was on the committee with Adams to draft  a Declaration of Independence?", "answer": "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.", "Thomas Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Thomas Jefferson and Benjamin Franklin", "extracted_entity": null}
{"question": "what did Jefferson call John Adams?", "answer": "The \"Colossus of Independence\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The \"Colossus of Independence\".", "Colossus of Independence"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "his \"little fat friend\"", "extracted_entity": null}
{"question": "what was Adams' political party?", "answer": "The Federalist Party.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party.", "Federalist"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Federalist", "extracted_entity": null}
{"question": "was Adams the first to introduce a bicameral legislature?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "no. The first bicameral legislature was the", "extracted_entity": null}
{"question": "did John Adams get along with Alexander Hamilton?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No, they hated each other.", "extracted_entity": null}
{"question": "did John Adams go to Harvard? ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No. He went to Harvard, but he didn", "extracted_entity": null}
{"question": "did John Adams support the Stamp Act of 1765?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "No. He was one of the first people to", "extracted_entity": null}
{"question": "is Adams' birthplace part of a national park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "no, it's a private property", "extracted_entity": null}
{"question": "when did John Adams serve as Vice President?", "answer": "1789-1797", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1789-1797"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "1797-1801", "extracted_entity": null}
{"question": "with what party did Adams run for presidency?", "answer": "The Federalist Party", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Federalist Party"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Federalist Party", "extracted_entity": null}
{"question": "where is Adams buried?", "answer": "United First Parish Church", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["United First Parish Church"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "In the National Cemetery in Arlington, Virginia", "extracted_entity": null}
{"question": "who were the midnight judges?", "answer": "They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They were a series of judges, so called because most of them were formally appointed days before Adams' presidential term expired", "They were judges formally appointed days before Adams term expired"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "a 70's band that did a great version", "extracted_entity": null}
{"question": "in what ways was Adams opposed by Anderw Hamilton?", "answer": "Hamilton wanted to control the army differently than Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hamilton wanted to control the army differently than Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Adams opposed Hamilton because he was a Federalist,", "extracted_entity": null}
{"question": "what information did he record in his diary?", "answer": "Descriptions of events and ompressions of men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Descriptions of events and ompressions of men", "He wrote descriptions of events and impressions of men"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "he recorded the date, the time, and the", "extracted_entity": null}
{"question": "who was defeated for re-election in the`` Revolution of 1800'' by Thomas Jefferson?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "John Adams", "extracted_entity": null}
{"question": "who represented the Continental Congress in Europe?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Benjamin Franklin, John Adams, John Jay, and", "extracted_entity": null}
{"question": "what is now part of Adams National Historical Park?", "answer": "John Adams' birthplace", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams' birthplace"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the John Quincy Adams house in Quincy", "extracted_entity": null}
{"question": "is it true that adams had spent some time as the ambassador?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "yes, he had spent some time as the ambassador", "extracted_entity": null}
{"question": "is it true that massachusetts sent him in 1774?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "It is true that the Massachusetts Provincial Congress sent Adams", "extracted_entity": null}
{"question": "who did Massachusetts send in 1774?", "answer": "John Adams", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Adams"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "Samuel Adams, John Adams, and James Otis", "extracted_entity": null}
{"question": "are his last words often quoted as \" Thomas Jefferson survives \" . ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "o, but he did say something like \" Thomas", "extracted_entity": null}
{"question": "the John Adams Library , housed at the Boston Public Library , contains what?", "answer": "Adams's personal collection of more than 3,500 volumes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams's personal collection of more than 3,500 volumes"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "he second-largest collection of the writings of John Adams", "extracted_entity": null}
{"question": "adams ' opponents were what?", "answer": "Democratic Republicans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Democratic Republicans"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "hey were the federalists, who were the people", "extracted_entity": null}
{"question": "did the election of 1800 not become a bitter and volatile battle , with each side expressing extraordinary fear of the other party and its policies ?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "es, it did become a bitter and volatile battle", "extracted_entity": null}
{"question": "what happened in 1764?", "answer": "Adams married Abigail Smith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adams married Abigail Smith"], "entity_annotations": ["John_Adams"], "answer_entity_name": null, "predicted_answer": "the British Parliament passed the Sugar Act, which imposed", "extracted_entity": null}
{"question": "is a kangaroo a reptile?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No. A kangaroo is a marsupial", "extracted_entity": null}
{"question": "is a kangaroo on the Australian coat of arms?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no. the kangaroo is on the Australian flag", "extracted_entity": null}
{"question": "was James Cook the first to record the name \"Kangooroo?\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no, Cook did not name the kangaroo.", "extracted_entity": null}
{"question": "what is a collective noun for kangaroos?", "answer": "mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mob, troop, or court", "Mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a mob of kangaroos.", "extracted_entity": null}
{"question": "where do joeys complete postnatal development?", "answer": "marsupium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["marsupium", "A pouch called a marsupium"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "in the pouch of the mother", "extracted_entity": null}
{"question": "what do kangaroos use for \"crawl-walking?\"?", "answer": "its hind feet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["its hind feet", "Their hind feet and their tails"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "their back legs.", "extracted_entity": null}
{"question": "why do kangaroos have a wide bite?", "answer": "The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two sides of the lower jaw are not joined together and the lower incisors are farther apart.", "Because of grazing"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "to eat more food.", "extracted_entity": null}
{"question": "what is responsible for converting the hydrogen byproduct of fermentation into acetate?", "answer": "bacteria", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bacteria", "The digestive system of a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Acetate kinase", "extracted_entity": null}
{"question": "are wild kangaroos shot for meat?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, they are, but not for food,", "extracted_entity": null}
{"question": "have kangaroos fared well since European settlement?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "they have. They were never in danger of extinction", "extracted_entity": null}
{"question": "do kangaroos have many natural predators?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, they have many predators, including dingoes", "extracted_entity": null}
{"question": "what is the average life expectancy of a kangaroo?", "answer": "4.6 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["4.6 years", "about 4.6 years", "4 to 6 years"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "8 years", "extracted_entity": null}
{"question": "when did the first official report of kangaroo blindness take place?", "answer": "1994", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1994"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "1964.", "extracted_entity": null}
{"question": "what are vehicles that frequent isolated roads often fitted with?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a bull bar", "extracted_entity": null}
{"question": "do kangaroos eat plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "yes, kangaroos eat plants", "extracted_entity": null}
{"question": "what are some extinct predators of the kangaroo?", "answer": "Thylacine, Marsupial Lion, Maagalania, and Wonambi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Thylacine, Marsupial Lion, Maagalania, and Wonambi", "The Thylacine"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "none, the kangaroo has no predators", "extracted_entity": null}
{"question": "is a Kangaroo a Marsupial?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No. A Kangaroo is a Marsupial", "extracted_entity": null}
{"question": "is the kangaroo an herbivour?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no, kangaroos are omnivores.", "extracted_entity": null}
{"question": "are Kangaroos Shy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "No, they are not. They are bold and", "extracted_entity": null}
{"question": "what method of locomotion do Kangaroos Use?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "They hop. They are the most efficient of all", "extracted_entity": null}
{"question": "what is a collective noun for a kangaroo?", "answer": "a mob, troop, or court", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a mob, troop, or court"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a mob", "extracted_entity": null}
{"question": "what is a roo?", "answer": "a kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "a roo is a type of marsupial", "extracted_entity": null}
{"question": "what is used to protect a vehicale from a Kangaroo?", "answer": "roo bars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roo bars"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "A kangaroo guard", "extracted_entity": null}
{"question": "what method is used by Kangaroos to travel?", "answer": "hopping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hopping"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "hopping", "extracted_entity": null}
{"question": "who asked a nearby local what the creatures were called?", "answer": "Captain James Cook and naturalist Sir Joseph Banks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Captain James Cook and naturalist Sir Joseph Banks"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "the man", "extracted_entity": null}
{"question": "what are four species that are commonly referred to as kangaroos?", "answer": "The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Red Kangaroo, The Eastern Grey Kangaroo, The Western Grey Kangaroo, and The Antilopine Kangaroo"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "the red kangaroo, eastern grey kangaroo,", "extracted_entity": null}
{"question": "are kangaroos and wallabies adept swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are kangaroos farmed to any extent?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "no. there are no kangaroo farms in australia", "extracted_entity": null}
{"question": "who also discovered that less than three percent of kangaroos exposed to the virus developed blindness ?", "answer": "Veterinarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Veterinarians"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Dr. David H. Spector, a researcher", "extracted_entity": null}
{"question": "different species of kangaroos eat what?", "answer": "different diets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different diets"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "grass, leaves, and shrubs", "extracted_entity": null}
{"question": "have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes. Kangaroos have been known to leap", "extracted_entity": null}
{"question": "is a collision with a vehicle capable of killing a kangaroo ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, if the kangaroo is a baby kang", "extracted_entity": null}
{"question": "was the game of Marn grook played using a ball made from kangaroo by the Kurnai people ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, the game was played using a ball made", "extracted_entity": null}
{"question": "have kangaroos large , powerful hind legs , large feet adapted for leaping , a long muscular tail for balance , and a small head ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "they do.", "extracted_entity": null}
{"question": "is a kangaroo a marsupial from the family macropodidae -LRB- macropods , meaning (`` ` large foot ('' ' -RRB-?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": ", it is a marsupial from the", "extracted_entity": null}
{"question": "what kinds of changes have larger kangaroos adapted much better to?", "answer": "Changes wrought to the Australian landscape by humans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Changes wrought to the Australian landscape by humans."], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "They are adapted to changes in their environment. This", "extracted_entity": null}
{"question": "have larger kangaroos adapted much better to changes?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["kangaroo"], "answer_entity_name": null, "predicted_answer": "Yes, larger kangaroos have adapted much better", "extracted_entity": null}
{"question": "is the leopard smaller than the other members of Panthera?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No, it is the second largest member of the", "extracted_entity": null}
{"question": "is a leopard larger and less lanky than a cheetah?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no, it is the opposite.", "extracted_entity": null}
{"question": "are large pythons potential prey for leopards?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no, they are too large for leopards", "extracted_entity": null}
{"question": "what may a leopard be mistaken for?", "answer": "A cheetah or a jaguar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cheetah or a jaguar"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a spotted lion.", "extracted_entity": null}
{"question": "what is a hybrid animal resulting from a union between a leopard and a puma?", "answer": "a pumapard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pumapard", "A pumapard"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a leopuma", "extracted_entity": null}
{"question": "where do leopards often hide their kills?", "answer": "in dense vegetation", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in dense vegetation", "in dense vegetation or in trees"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "in a tree", "extracted_entity": null}
{"question": "how did early naturalists distinguish between leopards and panthers?", "answer": "by the length of their tails", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by the length of their tails", "By the length of the tail, panthers having longer tails than leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "by their size and color", "extracted_entity": null}
{"question": "what are dark leopards known as colloquially?", "answer": "Black Panthers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Black Panthers", "black panthers"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "\"leopards\" (or \"leos", "extracted_entity": null}
{"question": "what shape are a leopard's black rosettes in East Africa?", "answer": "circular", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["circular"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "oval", "extracted_entity": null}
{"question": "is the leopard an Old World mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is the leopard of the Felidae family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is the leopard one of the four 'big cats'?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "where is the leopard distributed?", "answer": "southern Eurasia and Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["southern Eurasia and Africa", "southern Eurasia and Africa, from Korea to South Africa and Spain"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard is distributed in the central and southern parts", "extracted_entity": null}
{"question": "how long is a leopard's tail?", "answer": "60 to 110cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 to 110cm", "60-110cm"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "A leopard's tail is about 3 feet long", "extracted_entity": null}
{"question": "how much does a leopard weigh?", "answer": "Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. ", "(because of ambiguity) - 28-90kg "], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "about 100 pounds.", "extracted_entity": null}
{"question": "does a male leopard weigh more than a female leopard?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Yes, the male leopard is heavier than the female", "extracted_entity": null}
{"question": "what is the difference between leopards and cheetahs?", "answer": "The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard has rosettes rather than cheetah's simple spots and the leopard is larger and less lanky than the cheetah.", "The leopard has rosettes rather than cheetah's simple spots, but they lack internal spots, unlike the jaguar. The leopard is larger and less lanky than the cheetah but smaller than the jaguar."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "Leopards have spots and cheetahs", "extracted_entity": null}
{"question": "how does the distribution size of the leopard compare to the distribution of other wild cats?", "answer": "As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As of 1996, the leopard had the largest distribution of any wild cat, although populations before and since have shown a declining trend and are fragmented outside of subsaharan Africa.", "the leopard had the largest distribution of any wild cat."], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard has a much larger distribution than other wild", "extracted_entity": null}
{"question": "what resembles that of the similarly-sized cougar in the Americas?", "answer": "The leopard's ecological role", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard's ecological role"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the jaguarundi, which is a wildcat", "extracted_entity": null}
{"question": "what was one of the many species described in Linnaeus's 18th-century work, Systema Naturae?", "answer": "Felis pardus", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felis pardus"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the common name for a group of mammals with ho", "extracted_entity": null}
{"question": "what sort of cats are solitary?", "answer": "Leopards", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leopards"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The cats that live in the desert.", "extracted_entity": null}
{"question": "are leopards circular?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no, they are not circular.", "extracted_entity": null}
{"question": "is the leopard solitary?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No. It is a social animal, living in", "extracted_entity": null}
{"question": "what centred in Sierra?", "answer": "The leopard men", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The leopard men"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "The Sierra Nevada", "extracted_entity": null}
{"question": "felis pardus was what?", "answer": "One of the many species described in Linnaeus's 18th-century work, Systema Naturae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of the many species described in Linnaeus's 18th-century work, Systema Naturae"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "a genus of the cat family", "extracted_entity": null}
{"question": "has the New Zealand Rugby League featured the Otahuhu Leopards and then the Tamaki Leopards ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No, it hasn't.", "extracted_entity": null}
{"question": "were the Leopard men a West African secret society who practised cannibalism ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "No, they were a West African secret society who", "extracted_entity": null}
{"question": "was the leopard tank a German designed tank which entered service in 1965 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, it was designed in 1965 and", "extracted_entity": null}
{"question": "is the black color heritable and caused by only one recessive gene locus ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "no, there are several genes involved in melanin", "extracted_entity": null}
{"question": "is the leopard -LRB- panthera pardus -RRB- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "the leopard is an old world mammal", "extracted_entity": null}
{"question": "can a panther be any of several species of large felid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes, panther is a term used for the", "extracted_entity": null}
{"question": "is the leopard an agile and graceful predator?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["leopard"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Liechtenstein bordered by Switzerland?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, Liechtenstein is bordered by Switzerland.", "extracted_entity": null}
{"question": "is Liechtenstein the smallest German-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "was Liechtenstein part of the ancient Roman province of Raetia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no. Liechtenstein was not part of the", "extracted_entity": null}
{"question": "when was the first factory opened?", "answer": "1836", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1836"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1857.", "extracted_entity": null}
{"question": "how many municipalities is Liechtenstein divided into?", "answer": "11", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["11"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "11.", "extracted_entity": null}
{"question": "what is the national currency of Liechtenstein?", "answer": "Swiss franc", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swiss franc"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Swiss franc", "extracted_entity": null}
{"question": "has Leichtenstein worked to promote the county's image by prosecuting international money-laundering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, he has not. His office has been", "extracted_entity": null}
{"question": "what company administers Leichtenstein's railways?", "answer": "Austrian Federal Railways", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austrian Federal Railways"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "It's called the 'National Railway Company of Le", "extracted_entity": null}
{"question": "what percentage of Liechtenstein's population is foreign-born?", "answer": "two-thirds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two-thirds", "67%"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "21.7% (2009 est.)", "extracted_entity": null}
{"question": "is there an airport in Liechtenstein?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no, there isn\u2019t, but the airport of", "extracted_entity": null}
{"question": "is Liechtenstein heavily urbanized?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein is heavily urbanized.", "extracted_entity": null}
{"question": "is Liechtenstein doubly landlocked?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, it is landlocked and it is surrounded", "extracted_entity": null}
{"question": "does Liechtenstein have an army?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Yes, but it's so small, they don", "extracted_entity": null}
{"question": "when was Liechtenstein's current constitution adopted?", "answer": "October 1921.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["October 1921.", "October 1921"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "15 May 2003", "extracted_entity": null}
{"question": "what is the official language of Liechtenstein?", "answer": "German.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German.", "German"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "German.", "extracted_entity": null}
{"question": "what countries border Liechtenstein?", "answer": "Switzerland and Austria.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Switzerland and Austria.", "Switzerland and Austria"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Switzerland and Austria.", "extracted_entity": null}
{"question": "what Roman province was Liechtenstein part of?", "answer": "Raetia.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Raetia.", "Raetia"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "It was part of the Roman province of Raet", "extracted_entity": null}
{"question": "how many municipalities are within Oberland?", "answer": "6.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.", "6"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "10", "extracted_entity": null}
{"question": "what is the smallest German-speaking country in the world?", "answer": "Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Liechtenstein, with an area of 160", "extracted_entity": null}
{"question": "the Savings and Loans Bank was founded, as was the first cotton-weaving mill in what year?", "answer": "1861", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1861"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "1816", "extracted_entity": null}
{"question": "what enjoys one of the world's highest standards of living?", "answer": "Liechtenstein's population", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Liechtenstein's population"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the world's richest man, Bill Gates.", "extracted_entity": null}
{"question": "does the state court rule on the conformity of laws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, but the federal courts have the final say", "extracted_entity": null}
{"question": "what do most recognizable international company and largest employer have in common?", "answer": "They are Hilti.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are Hilti."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "They both have a common goal: to make the", "extracted_entity": null}
{"question": "was Liechtenstein completed in November 2000?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, Liechtenstein was completed in November", "extracted_entity": null}
{"question": "what completed in November?", "answer": "the Kunstmuseum Liechtenstein", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Kunstmuseum Liechtenstein"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "10,000 hours of practice", "extracted_entity": null}
{"question": "the State Court rules what?", "answer": "on the conformity of laws with the constitution", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["on the conformity of laws with the constitution"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the State Court rules what?", "extracted_entity": null}
{"question": "the Historical Society of the Principality of Liechtenstein plays what?", "answer": "a role in preserving the culture and history of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a role in preserving the culture and history of the country."], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "the Historical Society of the Principality of Liechten", "extracted_entity": null}
{"question": "is the museum collection also the national art collection of Liechtenstein ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, the museum collection is the national art collection", "extracted_entity": null}
{"question": "is there a small heliport at Balzers in Liechtenstein available for charter helicopter flights ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, there is a heliport at Bal", "extracted_entity": null}
{"question": "are nationals referred to by the plural : Liechtensteiners ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "No, Liechtensteiners is not a word", "extracted_entity": null}
{"question": "is it a winter sports resort , although it is perhaps best known as a tax haven ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "iechtenstein", "extracted_entity": null}
{"question": "is it a winter sports resort?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "yes, it is. (it's a winter", "extracted_entity": null}
{"question": "is it the smallest german-speaking country in the world?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "no, it\u2019s not, because Liechtenstein", "extracted_entity": null}
{"question": "what happened in moravia , lower austria , silesia , and styria , though in all cases , these territories were held in fief under other more senior feudal lords?", "answer": "The Liechtenstein Dynasty acquired vast swaths of land", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Liechtenstein Dynasty acquired vast swaths of land"], "entity_annotations": ["Liechtenstein"], "answer_entity_name": null, "predicted_answer": "Austrian lands were the lands that the Habs", "extracted_entity": null}
{"question": "did Webster write, \"I can now sleep of nights\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No. It's \"I can now sleep nights", "extracted_entity": null}
{"question": "was the arrival of Louis Kossuth (exiled leader of a failed Hungarian revolution) another issue that presented itself during Fillmore's presidency?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes, and it was a big issue, and", "extracted_entity": null}
{"question": "did he die at 11:10 p.m. on March 8, 1874, of the after-effects of a stroke?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "he died at 11:10 p.m.", "extracted_entity": null}
{"question": "who got Seward elected to the senate?", "answer": "Weed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Weed", "Weed ultimately got Seward elected to the senate"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "New York\u2019s Tammany Hall", "extracted_entity": null}
{"question": "where was Commodore Matthew C. Perry sent to open Japan to Western trade?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "Commodore Matthew C. Perry send to Japan"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Edo, Japan", "extracted_entity": null}
{"question": "what is The Remarkable Millard Fillmore?", "answer": "A fake biography based on real events that happened in Fillmore's life.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A fake biography based on real events that happened in Fillmore's life.", "In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events that happened in Fillmore's life"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "a book about Millard Fillmore.", "extracted_entity": null}
{"question": "did Fillmore form a law partnership before or after he founded the private University of Buffalo?", "answer": "Before.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Before.", "Law partnership founded before University of Buffalo"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "after", "extracted_entity": null}
{"question": "did Fillmore run for President a second time?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "I don't know (semantic ambiguity)"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, he did not run for president a second", "extracted_entity": null}
{"question": "how long after the death of his first wife did Fillmore marry Caroline McIntosh?", "answer": "where is the death date of his first wife?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where is the death date of his first wife?"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "3 months, 2 days", "extracted_entity": null}
{"question": "was Millard Fillmore the thirteenth President of the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes. He served from 1850 to", "extracted_entity": null}
{"question": "was Millard Fillmore born on January 7, 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, he was born on January 7,", "extracted_entity": null}
{"question": "did Millard Fillmore die on March 8, 1974?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, he died on March 8,", "extracted_entity": null}
{"question": "where was Millard Fillmore born?", "answer": "in a log cabin in Summerhill, New York", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in a log cabin in Summerhill, New York"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Cayuga County, New York", "extracted_entity": null}
{"question": "to whom was Millard Fillmore born to?", "answer": "to Nathaniel and Phoebe Millard Fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to Nathaniel and Phoebe Millard Fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "a farmer and a farmer\u2019s daughter", "extracted_entity": null}
{"question": "who did Millard Fillmore fall in love with?", "answer": "He fell in love with Abigail Powers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He fell in love with Abigail Powers", "Abigail Powers"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Millard Fillmore fell in love with the", "extracted_entity": null}
{"question": "what did Taylor and Fillmore disagree upon?", "answer": "slavory issues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slavory issues", "Taylor and Fillmore disagreed on the slavery issue in the new western territories taken from Mexico in the Mexican-American War"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Fillmore believed that the only way to keep the", "extracted_entity": null}
{"question": "how did Fillmore ascend to the presidency?", "answer": "upon the death of the sitting president, Taylor", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["upon the death of the sitting president, Taylor", "Official White House portrait of Millard FillmoreFillmore ascended to the presidency upon the sudden and unexpected death of President Taylor in July 1850."], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "after the death of Zachary Taylor", "extracted_entity": null}
{"question": "how did the supporters of Henry Clay feel about Fillmore in 1848?", "answer": "they were ok with him", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they were ok with him", "angry"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "they felt that he was too moderate on the slavery", "extracted_entity": null}
{"question": "who was first apprenticed to a fuller to learn the cloth-making trade?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Thomas Paine.", "extracted_entity": null}
{"question": "he founded the private university of buffalo on what date?", "answer": "1846", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1846"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "1887", "extracted_entity": null}
{"question": "who or what fell in love with abigail powers?", "answer": "millard fillmore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["millard fillmore"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "abigail powers", "extracted_entity": null}
{"question": "was Fillmore one of the founders of the University of Buffalo?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "Yes, he was a founder of the University of", "extracted_entity": null}
{"question": "was Another primary objective of Fillmore to preserve the Union from the intensifying slavery debate?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes. Fillmore was a supporter of the Comp", "extracted_entity": null}
{"question": "was Fillmore the second Chancellor, a position he maintained while both Vice President and President?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, Fillmore was the second Vice President.", "extracted_entity": null}
{"question": "is Fillmore the first of two presidents to have been an indentured servant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, James K. Polk was an indent", "extracted_entity": null}
{"question": "is the comic strip Mallard Fillmore named after the president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "No, it's named after a duck.", "extracted_entity": null}
{"question": "was Fillmore the first U.S. President born after the death of a former president ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "yes. Millard Fillmore was born in", "extracted_entity": null}
{"question": "did Fillmore not turn down the honor , explaining that he had neither the `` literary nor scientific attainment '' to justify the degree ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "es, he did.", "extracted_entity": null}
{"question": "what happened in a log cabin in summerhill , new york?", "answer": "Fillmore was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fillmore was born"], "entity_annotations": ["Millard_Fillmore"], "answer_entity_name": null, "predicted_answer": "A man by the name of Samuel Wilson was living", "extracted_entity": null}
{"question": "who was born precisely at midnight during an electrical storm , to a Serbian family in the village of Smiljan near Gospi\u0107 , in the Lika region of the Croatian Krajina in Military Frontier ( part of the Austrian Empire ) , in the present-day Croatia . ?", "answer": "Nikola Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "la Tesla", "extracted_entity": null}
{"question": "are there at least two films describing Tesla 's life ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "es, there are two films.", "extracted_entity": null}
{"question": "have a number of live theatrical plays based on Tesla 's life been produced and staged worldwide ?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes."], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "es, many, and many more are in the", "extracted_entity": null}
{"question": "do sea otters have long muscular tails?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no they have short muscular tails", "extracted_entity": null}
{"question": "can otters survive in cold water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they have a thick layer of fat and", "extracted_entity": null}
{"question": "does the giant otter inhabit South Africa?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no, the giant otter is found in South", "extracted_entity": null}
{"question": "how many species of otter are there?", "answer": "13", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "13 (including the giant otter)", "extracted_entity": null}
{"question": "what do river otters eat?", "answer": "a variety of fish and shellfish, as well as small land mammals and birds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a variety of fish and shellfish, as well as small land mammals and birds", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds. ", "River otters eat a variety of fish and shellfish, as well as small land mammals and birds."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish, frogs, crayfish, birds, and", "extracted_entity": null}
{"question": "how much do sea otters weigh?", "answer": "30 kg (about 65 pounds)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["30 kg (about 65 pounds)"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "30-100 pounds", "extracted_entity": null}
{"question": "how do sea otters insulate themselves?", "answer": "a layer of air trapped in their fur(!)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a layer of air trapped in their fur(!)", "As with other species of otter, they rely on a layer of air trapped in their fur, which they keep topped up by blowing into the fur from their mouths."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they have a thick fur coat", "extracted_entity": null}
{"question": "why are sea otters nearly extinct?", "answer": "They've been hunted for their fur", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They've been hunted for their fur", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "because of the sea otter fur trade.", "extracted_entity": null}
{"question": "what is one of the challenges of re-establishing a population of Eurasian otters in the UK?", "answer": "roadkill deaths", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["roadkill deaths", "Roadkill deaths have become one of the significant threats to the success of their re-establishment. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "the otters are not used to living in the", "extracted_entity": null}
{"question": "is otter a kind of mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, otters are mammals.", "extracted_entity": null}
{"question": "how many species and genera does otter have?", "answer": "13 species and 7 genera", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["13 species and 7 genera", "13 species in 7 genera."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "otter has 13 species and 7 genera", "extracted_entity": null}
{"question": "do otters live in water?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they live in water", "extracted_entity": null}
{"question": "where do sea otters live?", "answer": "Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sea otters (Enhydra lutris) live along the Pacific coast of North America. ", "The sea otter lives actually in the sea. ", "Beside water", "Pacific coast of North America"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "in the sea.", "extracted_entity": null}
{"question": "where does the word \"otter\" derive from?", "answer": "The word \"otter\" derives from the Old English word otr, otor or oter. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The word \"otter\" derives from the Old English word otr, otor or oter. ", "The word \"otter\" derives from the Old English word otr, otor or oter. This and cognate words in other Indo-European languages ultimately stem from a root which also gave rise to the English words \"water\", \"wet\" and \"winter\". "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Old English \"\u014dtor\" (compare Old High", "extracted_entity": null}
{"question": "does otter give birth or lay egg?", "answer": "give birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["give birth"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Otter gives birth.", "extracted_entity": null}
{"question": "what drives sea otter almost to extinction?", "answer": "Humans hunted them almost to extinction. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Humans hunted them almost to extinction. ", "Sea otters have some 200,000 hairs per square cm of skin, a rich fur for which humans hunted them almost to extinction. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "overfishing for their fur", "extracted_entity": null}
{"question": "why otters are considered as totem animals?", "answer": "The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The time of year associated with this is also associated with the Aquarius sign of the Zodiac, through which the sun passes January 20-February 19. "], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Because they are playful and they like to have fun", "extracted_entity": null}
{"question": "do sea otters live along the Pacific coast?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they live along the Pacific coast of North", "extracted_entity": null}
{"question": "are otters totem animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they are. Otters are a to", "extracted_entity": null}
{"question": "are otters herbivores?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "No, they are carnivores.", "extracted_entity": null}
{"question": "what is the primary item in an otter's diet?", "answer": "Fish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fish.", "fish"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "fish.", "extracted_entity": null}
{"question": "what is an otter's den called?", "answer": "Holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holt", "holt", "A holt"], "entity_annotations": ["otter", "Otter"], "answer_entity_name": null, "predicted_answer": "a holt", "extracted_entity": null}
{"question": "why is the giant otter becoming increasingly rare?", "answer": "Poaching, habitat loss, and toxins in gold mining.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Poaching, habitat loss, and toxins in gold mining.  ", "poaching, habitat loss, and the use of mercury in illegal alluvial gold mining"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "it is being hunted for its fur, and its", "extracted_entity": null}
{"question": "how do otters keep themselves warm without blubber?", "answer": "A layer of air trapped in their fur.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A layer of air trapped in their fur.  ", "a layer of air trapped in their fur"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they are small, and have a high metabolic rate", "extracted_entity": null}
{"question": "how are otters playful animals?", "answer": "The slide down snowy slopes, apparently for sheer enjoyment.  ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The slide down snowy slopes, apparently for sheer enjoyment.  ", "They slide repeatedly down snowy slopes for sheer enjoyment."], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "they are playful animals because they like to play with", "extracted_entity": null}
{"question": "what animals are related to otters?", "answer": "weasels, polecats, and badgers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["weasels, polecats, and badgers"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "They are related to weasels, mink", "extracted_entity": null}
{"question": "what traps a layer of air, and keeps them dry and warm under water?", "answer": "Long guard hair", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Long guard hair"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "the water strider.", "extracted_entity": null}
{"question": "the collective noun romp is sometimes used for a group of what?", "answer": "Otters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Otters"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "monkeys", "extracted_entity": null}
{"question": "is it true that otters eat a variety of fish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "Yes, otters eat a variety of fish.", "extracted_entity": null}
{"question": "are otters playful animals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, otters are playful animals.", "extracted_entity": null}
{"question": "are otters very active?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "yes, they are very active", "extracted_entity": null}
{"question": "are male otters dog-otters?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "no, they're not. Male otters are", "extracted_entity": null}
{"question": "an otter 's den is what?", "answer": "A holt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A holt"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "a holt", "extracted_entity": null}
{"question": "sea otters eat what?", "answer": "Shellfish and other invertebrates", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shellfish and other invertebrates"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "sea urchins", "extracted_entity": null}
{"question": "is the myth of Otter 's Ransom the starting point of the Volsunga saga ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "o, the story of Otter's R", "extracted_entity": null}
{"question": "are Male otters dog-otters , females are bitches and babies are cubs or pups ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "o, male otters are called dogs, females", "extracted_entity": null}
{"question": "is an otter 's den called a holt ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "es it is, and otters can hold their", "extracted_entity": null}
{"question": "have most otters fish as the primary item in their diet , supplemented by frogs , crayfish and crabs ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["otter"], "answer_entity_name": null, "predicted_answer": "s, the diet of the Eurasian otter", "extracted_entity": null}
{"question": "do penguins feed on krill?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "what is the largest living species of penguin?", "answer": "Emperor Penguin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Emperor Penguin", "the Emperor Penguin (Aptenodytes forsteri)", "Emperor"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "the emperor penguin.", "extracted_entity": null}
{"question": "do penguins live almost exclusively in the Southern Hemisphere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "Yes, penguins live in the southern hemisphere.", "extracted_entity": null}
{"question": "what is \"tobogganing\"?", "answer": "when penguins slide on their bellies across the snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when penguins slide on their bellies across the snow", "Tobogganing is when penguins slide on their bellies across the snow."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "sliding down a snow-covered hill on a wooden sled", "extracted_entity": null}
{"question": "why are penguins countershaded?", "answer": "for camouflage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["for camouflage"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "to camouflage against the white ice and the dark water", "extracted_entity": null}
{"question": "are penguins afraid of humans?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, penguins are afraid of humans", "extracted_entity": null}
{"question": "how much time to penguins spend on land?", "answer": "half of their life", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["half of their life", "They spend half of their life on land."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "about 1/3 of their time", "extracted_entity": null}
{"question": "how many species of penguins are there?", "answer": "between 17 and 20 living species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["between 17 and 20 living species", "Penguin biodiversity varies between 17 and 20 living species, all in the subfamily Spheniscinae."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "17", "extracted_entity": null}
{"question": "how are Isabelline penguins different from most penguins?", "answer": "they have brown rather than black plumage", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they have brown rather than black plumage", "Because they are born with brown rather than black plumage."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "They are not black and white, they are tan", "extracted_entity": null}
{"question": "are penguins birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, penguins are birds.", "extracted_entity": null}
{"question": "do penguins have a better than average sense of hearing for birds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "no, but they have good hearing for their size", "extracted_entity": null}
{"question": "are penguins considered \"higher waterbirds\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, penguins are not considered \"higher water", "extracted_entity": null}
{"question": "what order do penguins belong to?", "answer": "Sphenisciformes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sphenisciformes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "penguins belong to the order of Sphenisc", "extracted_entity": null}
{"question": "how tall were the tallest prehistoric penguins?", "answer": "1.80 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.80 meters", "as tall as an adult human"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "6 feet (1.8 meters) tall", "extracted_entity": null}
{"question": "what makes penguins so agile in the water?", "answer": "Their wings have become flippers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Their wings have become flippers", "Smooth plumage preserves a layer of air, ensuring buoyancy, wings are flippers"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they have flippers instead of feet", "extracted_entity": null}
{"question": "why do penguins \"tobaggan\"?", "answer": "It conserves energy while moving quickly", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It conserves energy while moving quickly", "Conserves energy while moving quickly"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "to avoid predators", "extracted_entity": null}
{"question": "who spend half of their life on land and half in the oceans?", "answer": "Penguins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "whales!", "extracted_entity": null}
{"question": "what is also the distance that Antarctic tourists are told to keep from penguins?", "answer": "3 meters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 meters"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "1,000 feet", "extracted_entity": null}
{"question": "what have become flippers, useless for flight in the air?", "answer": "Penguins' wings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Penguins' wings"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "wings of the pterosaurs.", "extracted_entity": null}
{"question": "what happens when mothers lose a chick?", "answer": "They sometimes attempt to \"steal\" another chick.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes attempt to \"steal\" another chick."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they sometimes lay another egg", "extracted_entity": null}
{"question": "is it true that each penguin gets a turn in the center?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, it is true that each penguin gets", "extracted_entity": null}
{"question": "are all penguins countershaded?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No, some penguins are not countershaded", "extracted_entity": null}
{"question": "are penguins astonishingly agile?", "answer": "In the water they are.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the water they are."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, they can turn on a dime.", "extracted_entity": null}
{"question": "are penguins at risk?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, they are. They are at risk of", "extracted_entity": null}
{"question": "is it also possible that penguin comes from the Latin pinguis , fat ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "o, penguin is a loan word from the", "extracted_entity": null}
{"question": "have penguins an average sense of hearing for birds ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "they have a sense of hearing, but it is", "extracted_entity": null}
{"question": "can larger penguins dive deep in case of need ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "no, they have to stay in shallow water", "extracted_entity": null}
{"question": "is the smallest penguin species the Little Blue Penguin -LRB- also known as the Fairy Penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2.2 lb -RRB- ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "es, the Little Blue Penguin is the smallest p", "extracted_entity": null}
{"question": "is it not even known whether the gigantic palaeeudyptines constitute a monophyletic lineage , or whether gigantism was evolved independently in a much restricted Palaeeudyptinae and the Anthropornithinae ?", "answer": "It is not even known.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not even known."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "o, it is not even known whether the gigantic", "extracted_entity": null}
{"question": "is the largest living species the emperor penguin -LRB- aptenodytes forsteri -RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "No. The largest living species is the African elephant", "extracted_entity": null}
{"question": "is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "yes, the little blue penguin -LRB", "extracted_entity": null}
{"question": "what happened in a region not quite 2000 km south of the equator 35 mya?", "answer": "At least one giant penguin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At least one giant penguin."], "entity_annotations": ["penguin"], "answer_entity_name": null, "predicted_answer": "the Gondwana supercontinent began to break", "extracted_entity": null}
{"question": "what is the last word on the page?", "answer": "Connecticut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Connecticut", "(SSN-22)"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "the word is \"and\" on the last line", "extracted_entity": null}
{"question": "is polar bear a mammal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, polar bear is a mammal. Polar", "extracted_entity": null}
{"question": "is polar bear a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No, Polar bears are omnivores", "extracted_entity": null}
{"question": "what is largest polar bear on record?", "answer": "2200 lb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2200 lb", "A huge male, allegedly weighing 1002 kg (2200 lb) shot at Kotzebue Sound in northwestern Alaska in 1960."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "2,200 pounds", "extracted_entity": null}
{"question": "what is polar bear's skin color?", "answer": "white or cream", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["white or cream", "Black"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "white", "extracted_entity": null}
{"question": "how long is polar bear's guard hair?", "answer": "5-15 cm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["5-15 cm"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "3 to 4 inches", "extracted_entity": null}
{"question": "what is cause of polar bear's skin diseases?", "answer": "mites or other parasites", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mites or other parasites", "Mites or other parasites"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "The polar bear's skin is covered in a thick", "extracted_entity": null}
{"question": "why polar bear is a special species?", "answer": "The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union listed polar bears as a vulnerable species, one of three sub-categories of threatened status, in May 2006. Their latest estimate is that 7 out of 19 subpopulations are declining or already severely reduced."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "because it is the only bear that can swim", "extracted_entity": null}
{"question": "what do fossil and DNA evidence tell us?", "answer": "The polar bear diverged from the brown bear about 200 thousand years ago.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The polar bear diverged from the brown bear about 200 thousand years ago.", "The polar bear diverged from the brown bear roughly 200 thousand years ago."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they tell us that we are all related to each", "extracted_entity": null}
{"question": "does a polar bear live in the Arctic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "is a polar bear white in color?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes, it is white in color.", "extracted_entity": null}
{"question": "is a polar bear at high risk of extinction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "how heavy is a male polar bear?", "answer": "300-600 kg (660-1320 lb)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300-600 kg (660-1320 lb)", "Most adult males weigh 350-650 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "700-1000 pounds", "extracted_entity": null}
{"question": "how heavy was the largest polar bear on record?", "answer": "1002 kg (2200 lb) ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1002 kg (2200 lb) ", "He allegedly weighed 1002 kg"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "1,002 kg (2,208 lbs", "extracted_entity": null}
{"question": "what does a polar bear's fur provide?", "answer": "It provides the animal with effective camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It provides the animal with effective camouflage.", "A polar bear's fur provides camouflage and insulation"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "insulation.", "extracted_entity": null}
{"question": "do female polar bears weight more than the male?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes they are bigger and weigh more than the male", "extracted_entity": null}
{"question": "how much weight do female polar bears gain during pregnancy?", "answer": "They gain double their weight.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They gain double their weight.", "They double their weight during pregnancy"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "10-20% of their body weight", "extracted_entity": null}
{"question": "can polar bears be seen under infrared photography?", "answer": "Polar bears are nearly invisible under infrared photography.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Polar bears are nearly invisible under infrared photography.", "Only their breath and muzzles can be easily seen"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "yes, polar bears are warm-blooded, so", "extracted_entity": null}
{"question": "what is actually black in color?", "answer": "A polar bear's skin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A polar bear's skin."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "not much. I mean, there is black ink", "extracted_entity": null}
{"question": "what insulate it against the cold?", "answer": "Its think blubber and fur.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its think blubber and fur."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a thick layer of air.", "extracted_entity": null}
{"question": "what includes a lengthy justification of why this species is listed as vulnerable?", "answer": "Database entry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Database entry."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a species profile", "extracted_entity": null}
{"question": "what do with greenpeace and the natural resources defense council have in common?", "answer": "They filed lawsuits in California.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They filed lawsuits in California."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they are both environmental groups that are opposed to the", "extracted_entity": null}
{"question": "are studies insufficient evidence for global protection?", "answer": "It is arguable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is arguable."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Yes, because it\u2019s not a good idea to", "extracted_entity": null}
{"question": "are polar bears excellent swimmers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "they can swim for 100 km without stopping", "extracted_entity": null}
{"question": "a semi-aquatic marine mammal , the polar bear has what?", "answer": "It has adapted for life on a combination of land, sea, and ice.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It has adapted for life on a combination of land, sea, and ice."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a white coat", "extracted_entity": null}
{"question": "garbage is what?", "answer": "Garbage is now recycled or transported to Thompson, Manitoba.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Garbage is now recycled or transported to Thompson, Manitoba."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "a waste product or by-product of a manufacturing process", "extracted_entity": null}
{"question": "the bears sometimes what?", "answer": "They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They sometimes have problems with various skin diseases with dermatitis caused sometimes by mites or other parasites."], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "hibernate", "extracted_entity": null}
{"question": "is the Polar Bear the mascot of Bowdoin college ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "no, the polar bear is the mascot of the", "extracted_entity": null}
{"question": "have thumbpolar bears been made both controversial and famous for their distinctive white fur and their habitat ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "Thumbpolar bears have been made both controversial and", "extracted_entity": null}
{"question": "did Mitchell Taylor , the Nunavut Government Manager of Wildlife Research , not write a letter to the U.S. Fish and Wildlife Service arguing that local studies are insufficient evidence for global protection at this time ?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "s, he did write such a letter.", "extracted_entity": null}
{"question": "are Cubs born in December without awakening the mother ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["polar_bear"], "answer_entity_name": null, "predicted_answer": "No. Cubs are born in December and are born", "extracted_entity": null}
{"question": "does Qatar rank as the eighth richest country in the world per capita?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "do nearly all Qataris profess Islam?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes, the constitution of Qatar is based on Islam", "extracted_entity": null}
{"question": "can women legally drive in Qatar?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "when did Qatar become an independent state?", "answer": "September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 3, 1971.", "September 3, 1971"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "3 September 1971", "extracted_entity": null}
{"question": "where is Qatar's telecommunication system centered?", "answer": "Doha.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Doha.", "Doha"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Doha, the capital of Qatar.", "extracted_entity": null}
{"question": "where did a suicide-bombing kill a teacher in 2005?", "answer": "At the Doha Players Theatre.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At the Doha Players Theatre.", "Doha Players Theatre"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "in Iraq", "extracted_entity": null}
{"question": "does the native pronunciation of Qatar sound like 'cutter'?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "no"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes, it does. (The 'Q'", "extracted_entity": null}
{"question": "what is the Arabic word for municipalities?", "answer": "baladiyah.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["baladiyah.", "baladiyah"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Baladiyeh", "extracted_entity": null}
{"question": "what universities are in Education City?", "answer": "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University's Weill Medical College.", "Carnegie Mellon University, Georgetown University School of Foreign Service, Texas A&M University, Virginia Commonwealth University, and Cornell University"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Qatar University, Weill Cornell Medical College, Texas", "extracted_entity": null}
{"question": "what carried with it a tacit recognition of Qatar's status as distinct from Bahrain?", "answer": "The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British Protectorate (per Colonel Lewis Pelly) asking to negotiate with a representative from Qatar after Bahrain's violation of the 1820 Anglo-Bahraini Treaty."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "The Treaty of Jeddah", "extracted_entity": null}
{"question": "qatar became an independent sovereign state in what year?", "answer": "On September 3, 1971.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On September 3, 1971."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "qatar became an independent sovereign state in 197", "extracted_entity": null}
{"question": "what consists of a low, barren plain, covered with sand?", "answer": "Much of the country.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much of the country."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "the sahara desert", "extracted_entity": null}
{"question": "how do qatar and the wider region relate?", "answer": "Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar is aiming to become a role model for economic and social transformation in the region. Large scale investment in all social and economic sectors will also lead to the development of a strong financial market."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "they are related because qatar is a small country", "extracted_entity": null}
{"question": "was Qatar University founded in 1973?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No, Qatar University was founded in 1977", "extracted_entity": null}
{"question": "is Qatar bordered by Saudi Arabia to the south?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "what bordered by Saudi?", "answer": "Qatar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yemen", "extracted_entity": null}
{"question": "who served as the headquarters and one of the main launching sites of the US invasion of Iraq in 2003 ?", "answer": "Qatar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Qatar."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "The USS Abraham Lincoln", "extracted_entity": null}
{"question": "has Qatar a modern Telecommunication system centered in Doha ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "was Qatar University founded in 1973 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "Yes, Qatar University was founded in 1973", "extracted_entity": null}
{"question": "did the Qataris not choose as their negotiator the respected entrepreneur and long-time resident of Doha , Muhammed bin Thani ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, they did.", "extracted_entity": null}
{"question": "will oil and gas probably remain the backbone of Qatar 's economy for some time to come ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es, it will", "extracted_entity": null}
{"question": "has it been created with a long term perspective to support the development of Qatar and the wider region , develop local and regional markets , and strengthen the links between the energy based economies and global financial markets ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "s, the exchange is a public limited company owned", "extracted_entity": null}
{"question": "what happened in these positions in english?", "answer": "These allophones cannot occur there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["These allophones cannot occur there."], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "I was the captain of the team", "extracted_entity": null}
{"question": "can these allophones not occur in these positions in english?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "yes. they can.", "extracted_entity": null}
{"question": "is -RRB- , officially the state of qatar -LRB- arabic (: : transliterated as dawlat qatar -RRB- , an arab emirate in southwest asia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Qatar"], "answer_entity_name": null, "predicted_answer": "es -LRB- it is -RRB", "extracted_entity": null}
{"question": "does Romania border Hungary?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, and it's the only country it borders", "extracted_entity": null}
{"question": "is Romania a secular state?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes. The constitution of Romania is secular. Article", "extracted_entity": null}
{"question": "is the president elected by popular vote?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no. He is elected by the electoral college,", "extracted_entity": null}
{"question": "how many counties is Romania divided into?", "answer": "41.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["41.", "forty-one"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "41.", "extracted_entity": null}
{"question": "what is the highest mountain in Romania?", "answer": "Moldoveanu Peak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moldoveanu Peak.", "Moldoveanu Peak"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Moldoveanu, 2544 m", "extracted_entity": null}
{"question": "what is the official language of Romania?", "answer": "Romanian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romanian.", "Romanian"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanian", "extracted_entity": null}
{"question": "is the Romanian economy doing well?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no, it\u2019s not, but it\u2019s better", "extracted_entity": null}
{"question": "are there many Roma in Romania?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, around 3 million. They are the", "extracted_entity": null}
{"question": "how many territories joined to form Romania?", "answer": "2.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2.", "2"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "3 (Transylvania, Bukovina,", "extracted_entity": null}
{"question": "does Romania share a border with Ukraine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "did Romania declare neutrality during World War I?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, in 1914", "extracted_entity": null}
{"question": "does Romania share the same language with Moldova?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Practically"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no, they have their own language.", "extracted_entity": null}
{"question": "who was the first gymnast to score a perfect \"ten\"?", "answer": "Nadia Com\u0103neci", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nadia Com\u0103neci"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Olga Korbut, in 1972.", "extracted_entity": null}
{"question": "what is the largest city in Romania?", "answer": "Bucharest", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bucharest"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Bucharest", "extracted_entity": null}
{"question": "what is the largest ethnic minority in Romania?", "answer": "Hungarians", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hungarians"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romanians (who are not part of the ethnic", "extracted_entity": null}
{"question": "how many countries in Europe are bigger than Romania?  ?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven", "Eleven"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "why doesn't Romania have a state religion?  ?", "answer": "Romania is a secular state", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Romania is a secular state"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "because it is a secular state.", "extracted_entity": null}
{"question": "why wasn't Romania neutral during World War II?", "answer": "it received a Soviet ultimatum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it received a Soviet ultimatum", "The Soviets threatened invasion."], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Romania was allied with Germany and Italy during World War", "extracted_entity": null}
{"question": "sibiu, a large city in Transylvania, was chosen as European Capital of Culture in what year?", "answer": "2007", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2007"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "2007", "extracted_entity": null}
{"question": "what are approximately 42,000 years old and have been nicknamed`` John of Anina''?", "answer": "the remains (the lower jaw) of the oldest modern human", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the remains (the lower jaw) of the oldest modern human"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "the oldest known rock paintings in the Americas. They", "extracted_entity": null}
{"question": "is it true that romania has a population of 21,698,181?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, this is the 2011 population estimate", "extracted_entity": null}
{"question": "what do economy and law have in common?", "answer": "(not sure how to answer this)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(not sure how to answer this)"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "they are both fields of study that are based on", "extracted_entity": null}
{"question": "what did Tourism in Romania attract in 2005?", "answer": "investments worth 400 million euros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["investments worth 400 million euros"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "5.5 million tourists.", "extracted_entity": null}
{"question": "is Romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes. (the president is the head of state", "extracted_entity": null}
{"question": "is Romania a semi-presidential democratic republic where executive functions are shared between the president and the prime minister ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "Yes. (The prime minister is head of the", "extracted_entity": null}
{"question": "was Gellu Naum the leader of the surrealist movement in Romania ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no, he was an eccentric artist who was a", "extracted_entity": null}
{"question": "was the Bucharest Metro only opened in 1979 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "No, it was opened in 1979.", "extracted_entity": null}
{"question": "is romania -LRB- , -RRB- a country in southeastern europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is romania a semi-presidential unitary state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Romania"], "answer_entity_name": null, "predicted_answer": "no, it is a parliamentary republic.", "extracted_entity": null}
{"question": "do singaporeans enjoy squid?", "answer": "Yes, they do.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they do.", "yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did Singapore's standard of living decrease since independence?", "answer": "No, the standard living was raised.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the standard living was raised.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "no. the standard of living has increased since independence", "extracted_entity": null}
{"question": "did the media contribute 15.6% to Singapore's GDP in 2001?", "answer": "No, it contributed 1.56% to GDP in 2001.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, it contributed 1.56% to GDP in 2001.", "no"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "no. It contributed 5.6% to", "extracted_entity": null}
{"question": "how many people use the bus network daily?", "answer": "More than 2.78 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More than 2.78 million people.", "2.78 million people"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1.5 million people per day", "extracted_entity": null}
{"question": "where did Orang Laut people live?", "answer": "Orang Laut people lived around the coast, rivers and smaller islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Orang Laut people lived around the coast, rivers and smaller islands.", "Several hundred indigenous Orang Laut people also lived around the coast, rivers and smaller islands."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "they lived in the forest and rivers of Sumatra", "extracted_entity": null}
{"question": "what is the MRT?", "answer": "One of three heavy rail passenger transport lines in Singapore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["One of three heavy rail passenger transport lines in Singapore.", "Mass Rapid Transit system"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "a type of rapid transit system", "extracted_entity": null}
{"question": "how is Singapore's climate characterized?", "answer": "Singapore has a tropical rainforest climate with no distinctive seasons.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore has a tropical rainforest climate with no distinctive seasons.", "Its climate is characterized by uniform temperature and pressure, high humidity, and abundant rainfall"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore's climate is characterized by high temperatures and humidity", "extracted_entity": null}
{"question": "how much increase in the media's contribution to GDP is the government seeking?", "answer": "The government seeks to increase its GDP contribution to 3% by 2012. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government seeks to increase its GDP contribution to 3% by 2012. ", "the government seeks to increase its GDP contribution to 3% by 2012. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1.5%", "extracted_entity": null}
{"question": "how many days did it take the Imperial Japanese Army to win the Battle of Singapore?", "answer": "Six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six days", "6 days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "3 days.", "extracted_entity": null}
{"question": "is Singapore located at the southern tip of the Korean Penisula?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "No. It is located at the southern tip of", "extracted_entity": null}
{"question": "is Singapore a city-state?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, it is a city-state.", "extracted_entity": null}
{"question": "has Sang Nila Utama visited Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, he has. He was the founder of", "extracted_entity": null}
{"question": "what does \"singa\" mean?", "answer": "lion", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lion"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "lion", "extracted_entity": null}
{"question": "when is the first record of settlement in Singapore?", "answer": "second century AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["second century AD", "The first records of settlement in Singapore are from the second century AD."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1819", "extracted_entity": null}
{"question": "which nation invaded Singapore during World War II?", "answer": "Japan", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japan", "During World War II, the Imperial Japanese Army invaded Malaya, culminating in the Battle of Singapore."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Japan", "extracted_entity": null}
{"question": "why Churchill called the occupation of Singapor by Janpan during WWII \"Britain's greatest defeat\"?", "answer": "The British were defeated in six days", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The British were defeated in six days"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "because he had to accept the fact that he could", "extracted_entity": null}
{"question": "was Lee Kuan Yew a successful leader of Singapore?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "when did Goh Chok Tong succeed Lee as Prime Minister?", "answer": "In 1990, Goh Chok Tong succeeded Lee as Prime Minister.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1990, Goh Chok Tong succeeded Lee as Prime Minister."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "28 November 2004", "extracted_entity": null}
{"question": "what is the primary domestic source of water supply?", "answer": "The primary domestic source of water supply in Singapore is rainfall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The primary domestic source of water supply in Singapore is rainfall."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the aquifer", "extracted_entity": null}
{"question": "what is the national language of Singapore?", "answer": "The national language of Singapore is Malay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The national language of Singapore is Malay."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The national language of Singapore is Malay", "extracted_entity": null}
{"question": "where is the National Orchid Garden?", "answer": "Singapore Botanic Gardens.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore Botanic Gardens."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore Botanic Gardens.", "extracted_entity": null}
{"question": "why are relations with Malaysia and Indonesia important?", "answer": "Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Due to obvious geographical reasons, relations with Malaysia and Indonesia are most important."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "they are important because they are our closest neighbors and", "extracted_entity": null}
{"question": "what is the most densely populated independent country in the world?", "answer": "Monaco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monaco"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Vatican City (about 1,000 people per", "extracted_entity": null}
{"question": "who did Sir Thomas Stamford Raffles work for?", "answer": "British East India Company", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The British East India Company", "extracted_entity": null}
{"question": "when was Lee Kuan Yew prime minister of Singapore?", "answer": "1959", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1959"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "1959 to 1990", "extracted_entity": null}
{"question": "what is the punishment for first-degree murder?", "answer": "There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are laws which allow capital punishment in Singapore for first-degree murder and drug trafficking."], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "life in prison without parole.", "extracted_entity": null}
{"question": "what established a trading post on the island in 1819?", "answer": "British East India Company ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British East India Company "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "John Jacob Astor", "extracted_entity": null}
{"question": "what used Singapore as a strategic trading post along the spice route?", "answer": "British", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "The Dutch", "extracted_entity": null}
{"question": "what became one of the most important commercial and military centres of the British Empire?", "answer": "Singapore", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Singapore"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Singapore", "extracted_entity": null}
{"question": "who boasted a sizeable community of 100,000 by 1869?", "answer": "British India office", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["British India office"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "the Mormons", "extracted_entity": null}
{"question": "how do reservoirs and water relate?", "answer": "By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By 2001, there were 19 raw water reservoirs, 9 treatment works and 14 storage or service reservoirs locally to serve domestic needs. "], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "water reservoirs are used to store water.", "extracted_entity": null}
{"question": "is it true that singapore has several long - standing disputes with malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "singapore and malaysia have several long - standing disputes,", "extracted_entity": null}
{"question": "has Singapore Changi Airport a network of 81 airlines connecting Singapore to 185 cities in 58 countries ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, Singapore Changi Airport has a network of", "extracted_entity": null}
{"question": "has Singapore a network of reservoirs and water catchment areas ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "Yes, there is a network of reservoirs and", "extracted_entity": null}
{"question": "has Urbanization eliminated many areas of former primary rainforest , with the only remaining area of primary rainforest being Bukit Timah Nature Reserve ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "es, Urbanization has eliminated many areas of former", "extracted_entity": null}
{"question": "does the economy not depend heavily on exports refining imported goods , especially in manufacturing ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "t does", "extracted_entity": null}
{"question": "is singapore -LRB- ; (: ; , -RRB- , officially the republic of singapore -LRB- ; (: ; , -RRB- , an island nation located at the southern tip of the malay peninsula?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "republic of singapore -LRB- ; (:", "extracted_entity": null}
{"question": "is singapore the 17th wealthiest country in the world in terms of gdp per capita?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Singapore"], "answer_entity_name": null, "predicted_answer": "no, its the 5th wealthiest", "extracted_entity": null}
{"question": "was Thedore Roosevelt  a member of the Republican Party?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Theodore Roosevelt have a brother?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "no, but he has a sister.", "extracted_entity": null}
{"question": "did Theodore Roosevelt go to Harvard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "No, he went to Harvard Law School.", "extracted_entity": null}
{"question": "where was Theodore Roosevelt born?", "answer": "28 East 20th Street in the modern-day Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["28 East 20th Street in the modern-day Gramercy section of New York City", "a four-story brownstone at 28 East 20th Street, in the modern-day Gramercy section of New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York City", "extracted_entity": null}
{"question": "what did Roosevelt do to improve his physical condition?", "answer": "Roosevelt took up exercise", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roosevelt took up exercise", "He started boxing"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Roosevelt took a two week vacation to Oyster Bay", "extracted_entity": null}
{"question": "why did Roosevelt start Boxing lessons?", "answer": "To deal with bullies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To deal with bullies"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "he wanted to learn how to punch Hitler in the", "extracted_entity": null}
{"question": "what was the nickname of Theodore Roosevelt's sister Anna?", "answer": "Bamie as a child and Bye as an adult", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bamie as a child and Bye as an adult", "Bamie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Bamie", "extracted_entity": null}
{"question": "what was Roosevelt's justification for supporting desegregation of schools in New York?", "answer": "Noting that his children had been educated with other races and there was nothing wrong with it", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Noting that his children had been educated with other races and there was nothing wrong with it", "that his children had been educated with other races and there was nothing wrong with it"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "he wanted to help the poor.", "extracted_entity": null}
{"question": "how did newspapers respond to Roosevelt inviting Brooker T. Washington to dinner at the White House?", "answer": "mixing of the races on social occassions", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mixing of the races on social occassions", "They outraged."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "They were split. Some thought it was a good", "extracted_entity": null}
{"question": "did Theodore Roosevelt study Judo?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No clue"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "no, he studied Jujitsu. Judo", "extracted_entity": null}
{"question": "did Roosevelt receive a Medal of Honor?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "nein"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "no. he was awarded the Medal of Honor for", "extracted_entity": null}
{"question": "did Roosevelt study biology?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He was strong in biology."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "no, he did not. He studied law.", "extracted_entity": null}
{"question": "where was Theodore Roosevelt from?", "answer": "Gramercy section of New York City", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gramercy section of New York City", "New York City"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "New York", "extracted_entity": null}
{"question": "who was Roosevelt's presidential hero?", "answer": "Lincoln was Roosevelt's presidential hero.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lincoln was Roosevelt's presidential hero.", "Lincoln was Roosevelt's presidential hero"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Teddy Roosevelt", "extracted_entity": null}
{"question": "when did Roosevelt die?", "answer": "On January 6, 1919, Roosevelt died in his sleep.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 6, 1919, Roosevelt died in his sleep."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "April 12, 1945.", "extracted_entity": null}
{"question": "who helped to fund Roosevelt's African safari?", "answer": "Financed by Andrew Carnegie and his own proposed writings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Financed by Andrew Carnegie and his own proposed writings", "Andrew Carnegie"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "the DuPont family", "extracted_entity": null}
{"question": "did Roosevelt support racial integration in schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "He supported a bill allowing school desegregation in New York State."], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "No, he did not support racial integration in schools", "extracted_entity": null}
{"question": "was Roosevelt's family rich?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Theodore_Roosevelt"], "answer_entity_name": null, "predicted_answer": "Yes, they were rich.", "extracted_entity": null}
{"question": "are turtles ectothermic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes they are ectothermic because they cannot regulate", "extracted_entity": null}
{"question": "does the mother care for the young?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "are turtles pets?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, turtles are pets", "extracted_entity": null}
{"question": "what shape are the eggs of the larest species of turtle?", "answer": "Spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spherical", "spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "spherical", "extracted_entity": null}
{"question": "how often do turtles breed?", "answer": "Every few years or more", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Every few years or more"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles breed once a year.", "extracted_entity": null}
{"question": "what do all turtles and tortoises breathe?", "answer": "air", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["air"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "through their nostrils", "extracted_entity": null}
{"question": "how do turtles reproduce?", "answer": "They lay eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They lay eggs", "they lay eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles lay eggs on the beach and bury them in", "extracted_entity": null}
{"question": "what has been discovered about turtles organs?", "answer": "They do not gradually break down or become less efficient over time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They do not gradually break down or become less efficient over time", "they do not break down"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "nothing", "extracted_entity": null}
{"question": "what suborder of turtle draws its head into its shell?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "cryptodira", "extracted_entity": null}
{"question": "do sea turtles lay eggs on dry sandy beaches?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no, they lay eggs in the sand", "extracted_entity": null}
{"question": "are Pleurodira known as side-necked turtles?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, they are known as side-necked turtles", "extracted_entity": null}
{"question": "are turtles a part of the Flying Spaghetti Monster?", "answer": "*shrug*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["*shrug*"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No, because the FSM is not a part of", "extracted_entity": null}
{"question": "how many suborders are turtles divided into?", "answer": "Three.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three.", "three"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "1. Cryptodira, which are the", "extracted_entity": null}
{"question": "name a turtle popularly kept as a pet.?", "answer": "Russian Tortoises", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Russian Tortoises"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Red-eared Slider. (Trachemys", "extracted_entity": null}
{"question": "name four related articles for the article on turtles.?", "answer": "Triassic, Ernst Haeckel, Kunstformen der Natur, Animal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Triassic, Ernst Haeckel, Kunstformen der Natur, Animal"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtles are reptiles. Reptiles are verte", "extracted_entity": null}
{"question": "if a turtle egg was kept warm, what would likely hatch?", "answer": "A female turtle.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A female turtle.", "A female turtle"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "a turtle", "extracted_entity": null}
{"question": "what do turles use to breathe in the water?", "answer": "Papillae", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Papillae"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "gills.", "extracted_entity": null}
{"question": "why are side-necked turtles called side-necked turtles?", "answer": "Because of the way they withdraw their heads into their shells.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because of the way they withdraw their heads into their shells.", "The way they withdraw their heads into their shells."], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "they retract their necks to the side of their", "extracted_entity": null}
{"question": "do turtles lay eggs underwater?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes, sea turtles lay eggs on beaches.", "extracted_entity": null}
{"question": "are turtle eggs leathery and soft?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, turtle eggs are leathery and soft", "extracted_entity": null}
{"question": "are turtles being examined for longevity genes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "no, they are being examined for longevity genes.", "extracted_entity": null}
{"question": "what suborder of turtles is extinct?", "answer": "Paracryptodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paracryptodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the cryptodira", "extracted_entity": null}
{"question": "what do turtle eggs preared to eat consist mainly of?", "answer": "Yolk", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yolk"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "turtle eggs", "extracted_entity": null}
{"question": "where do sea turtles lay their eggs?", "answer": "Holes Dug into the Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Holes Dug into the Mud or Sand", "holes dug into mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "on the beach", "extracted_entity": null}
{"question": "what is the smallest suborder of turtles?", "answer": "Pleurodira", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pleurodira"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Trionychia (soft-shelled turtles)", "extracted_entity": null}
{"question": "what are turtle eggs covered in when they incubate?", "answer": "Mud or Sand", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Mud or Sand", "mud or sand"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "a soft, leathery shell", "extracted_entity": null}
{"question": "what type of creatures breathe air and don't lay eggs underwater?", "answer": "Amniotes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amniotes", "amnoites"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "amphibians (frogs, toads,", "extracted_entity": null}
{"question": "what are deposited in holes dug into mud or sand?", "answer": "eggs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eggs"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "fossil", "extracted_entity": null}
{"question": "what sort of turtles are ectothermic?", "answer": "all of them", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["all of them"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "they are cold blooded, meaning that their body", "extracted_entity": null}
{"question": "are the largest turtles aquatic?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "No, the largest turtles are terrestrial", "extracted_entity": null}
{"question": "is aquatic respiration in Australian freshwater turtles being studied?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes, in our lab. We are studying how", "extracted_entity": null}
{"question": "these projections , called papillae , have what?", "answer": "a rich blood supply", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a rich blood supply"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the projections are papillae, which", "extracted_entity": null}
{"question": "the eggs of the largest species are what?", "answer": "spherical", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["spherical"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "the largest species of bird", "extracted_entity": null}
{"question": "has this inspired genetic researchers to begin examining the turtle genome for longevity genes ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "yes, it has", "extracted_entity": null}
{"question": "can turtles take many years to reach breeding age ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes. Some species may take up to 15", "extracted_entity": null}
{"question": "are Immature sea turtles not cared for by the adults ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "Yes, immature sea turtles are not cared for by", "extracted_entity": null}
{"question": "are the Pleurodira sometimes known as the side-necked turtles , a reference to the way they withdraw their heads into their shells ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "es, they are.", "extracted_entity": null}
{"question": "does the Order Testudines not include both extant -LRB- living -RRB- and extinct species , the earliest known turtles being from around 215 million years ago ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["turtle"], "answer_entity_name": null, "predicted_answer": "oes the Order Testudines not include both ext", "extracted_entity": null}
{"question": "was Ulysses Grant a general in the American Civil War?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "yes, he was the general of the Union army", "extracted_entity": null}
{"question": "was Grant's father-in-law a Democrat?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yup"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes, Jesse Root Grant, the father of U", "extracted_entity": null}
{"question": "did Ulysses win the Battle of Champion Hill?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "you betcha"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "no, the Battle of Champion Hill was a Confederate", "extracted_entity": null}
{"question": "who was Grant's brother in law?", "answer": "Fred Dent", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fred Dent"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "John Usher", "extracted_entity": null}
{"question": "where was Grant born?", "answer": "Point Pleasant, Ohio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Point Pleasant, Ohio", "A log cabin in Point Pleasant, Clermont County, Ohio"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Point Pleasant, Ohio", "extracted_entity": null}
{"question": "what was Grant's political affiliation?", "answer": "Republican", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Republican"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Republican", "extracted_entity": null}
{"question": "why did Grant say \"Damn, I had nothing to do with this batte.\"?", "answer": "It went well.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It went well.", "Because the generals under him acted on their own."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Grant was not in command of the Union army at", "extracted_entity": null}
{"question": "did Horace Greenley lose in the presidential elections of 1872?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "si"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "no, he was not a candidate in 187", "extracted_entity": null}
{"question": "why did Grant think that war was unjust?", "answer": "He thought it was designed to gain land open to slavery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He thought it was designed to gain land open to slavery.", "He accepted the theory that it was designed to gain land open to slavery."], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "he thought that war was unjust because it would cost", "extracted_entity": null}
{"question": "who achieved international fame as the leading Union general in the American Civil War?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Ulysses S. Grant", "extracted_entity": null}
{"question": "grant was elected president as a Republican in what year?", "answer": "1868", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1868"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1868, after the 14th Amendment", "extracted_entity": null}
{"question": "who took a hard line that reduced violence by groups like the Ku Klux Klan?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "president Theodore Roosevelt", "extracted_entity": null}
{"question": "when did he vote for democrat james buchanan?", "answer": "1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1856"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "1856", "extracted_entity": null}
{"question": "who did President Lincoln promote of major general in the regular army, effective July 4?", "answer": "Grant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grant"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "George B. McClellan", "extracted_entity": null}
{"question": "was Grant elected president as a Republican?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "no, he was elected as a Democrat", "extracted_entity": null}
{"question": "what was from Pennsylvania?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Philadelphia", "extracted_entity": null}
{"question": "was it a two-sentence description that completely caught the essence of Ulysses S. Grant ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "yes, it was, and it was by a", "extracted_entity": null}
{"question": "was Grant 's favorite brand of bourbon whiskey Old Crow ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "es. It was his favorite brand of whiskey.", "extracted_entity": null}
{"question": "had he smoked only sporadically ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "No, he smoked daily.", "extracted_entity": null}
{"question": "did Grant & Perkins not sell harnesses , saddles , and other leather goods and purchase hides from farmers in the prosperous Galena area ?", "answer": "they did", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["they did"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "s they did.", "extracted_entity": null}
{"question": "is the famous dragon gate at the entrance to the district at the corner of Grant and Bush Street ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Yes, the dragon gate is a famous landmark in", "extracted_entity": null}
{"question": "what happened in 1865?", "answer": "he accepted the surrender of Robert E. Lee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he accepted the surrender of Robert E. Lee"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was assassinated.", "extracted_entity": null}
{"question": "what happened in recent years?", "answer": "his reputation as president has improved", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["his reputation as president has improved"], "entity_annotations": ["Ulysses_S._Grant"], "answer_entity_name": null, "predicted_answer": "the number of people in the world has increased by", "extracted_entity": null}
{"question": "is Uruguay located in the northwesten part of Africa?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, it is located in the Southern part of", "extracted_entity": null}
{"question": "is Uruguay's capital Montevideo?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Montevideo is the capital of Uruguay", "extracted_entity": null}
{"question": "does Uruguay have cold summers?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "no"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no, it is hot all year long, even", "extracted_entity": null}
{"question": "who founded Montevideo?", "answer": "The Spanish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Spanish.", "the Spanish", "By the Spanish, in the early 18th century", "Montevideo was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Jos\u00e9 Gervasio Artigas", "extracted_entity": null}
{"question": "where is Uruguay's oldest church?", "answer": "San Carlos, Maldonado.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Carlos, Maldonado.", "San Carlos, Maldonado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the Iglesia Matriz in Colonia del Sacramento", "extracted_entity": null}
{"question": "who heavily influenced the architecture and culture of Montevideo?", "answer": "European immigrants.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["European immigrants.", "European immigrants"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the spanish", "extracted_entity": null}
{"question": "what are poor neighborhoods called informally?", "answer": "Cantegriles.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cantegriles.", "Cantegriles"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "ghettos", "extracted_entity": null}
{"question": "is uruguay's landscape mountainous?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "not really?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay's landscape is not mountainous, but the", "extracted_entity": null}
{"question": "what religion do most Uruguayans profess?", "answer": "None.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None.", "Roman Catholic"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic.", "extracted_entity": null}
{"question": "is Uruguay located in South America?", "answer": "Yes. It's located in the southeastern part", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. It's located in the southeastern part", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, it is located in South America.", "extracted_entity": null}
{"question": "can citizens propose changes to the Constitution?", "answer": "Yes. People are allowed to challenge laws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. People are allowed to challenge laws.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, they can. They can also propose changes", "extracted_entity": null}
{"question": "did Uruguay host the first ever World Cup?", "answer": "Yes, in 1930.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, in 1930.", "Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No, the first World Cup was held in Uruguay", "extracted_entity": null}
{"question": "how many square kilometres of continental land is Uruguay?", "answer": "176.215 km\u00b2 ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["176.215 km\u00b2 ", "176,214 square kilometres"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "176,215 square kilometres.", "extracted_entity": null}
{"question": "how much of the population is of white European descent?", "answer": "88%", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88%", "Approximately 88% of its population are of prevalently white European descent."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "6.2% of the population is of", "extracted_entity": null}
{"question": "why was the capital of Uruguay founded?", "answer": "For a military stronghold.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For a military stronghold.", "Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century as a military stronghold."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Because of the war between Brazil and Argentina.", "extracted_entity": null}
{"question": "how many times has Uruguay won the World Cup?", "answer": "Twice. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Twice. ", "On two or more occasions."], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "0.", "extracted_entity": null}
{"question": "how much is the average income of an African woman compared to a European man?", "answer": "African women earns 0.65 * 0.718  = 46.67% of a European man earns in average", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["African women earns 0.65 * 0.718  = 46.67% of a European man earns in average"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "about the same.", "extracted_entity": null}
{"question": "does Uruguay recognize same-sex civil unions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "yes, since 2008. Uruguay is the", "extracted_entity": null}
{"question": "does Uruguay border French Guiana?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no, but French Guiana is an overseas department", "extracted_entity": null}
{"question": "is Uruguay's warmest month June?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, the warmest month is June. The", "extracted_entity": null}
{"question": "what does a citizen use to propose changes to the Constitution?", "answer": "Referendum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Referendum", "Plebiscite"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a constitutional amendment.", "extracted_entity": null}
{"question": "what religions are found in Uruguay?", "answer": "Roman Catholic, Protestant, Jewish, and nonprofessing.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Roman Catholic, Protestant, Jewish, and nonprofessing.", "Roman Catholic, Protestant, Jewish"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Roman Catholic 66%, non-Catholic Christian", "extracted_entity": null}
{"question": "what has Uruguay done to be competitive in agriculture?", "answer": "Labeling as \"Natural\" or \"Ecological\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Labeling as \"Natural\" or \"Ecological\"", "Use low inputs of labour, technology, and capital, which results in lower yields but also opens the door for Uruguay to market its products as \"natural\" or \"ecological\""], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "nothing, it is a small country that cannot compete", "extracted_entity": null}
{"question": "what are the names of Uruguay's political parties?", "answer": "Partido Colorado and Partido Blanco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Partido Colorado and Partido Blanco", "Partido Blanco and Partido Colorado"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "the names of Uruguay's political parties are the Colorado", "extracted_entity": null}
{"question": "is Uruguay the smallest soverign nation in South America?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "No. Suriname is smaller.", "extracted_entity": null}
{"question": "what was founded by the Spanish in the early 18th century as a military stronghold?", "answer": "Montevideo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montevideo"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "St. Augustine, Florida", "extracted_entity": null}
{"question": "what has many possible meanings?", "answer": "(What?)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(What?)"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "a word.", "extracted_entity": null}
{"question": "how do india and latin america relate?", "answer": "Uruguay exports architectural services to India and Latin America", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uruguay exports architectural services to India and Latin America"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "they have similar cultures and the same problems", "extracted_entity": null}
{"question": "what did Uruguay win in 1828?", "answer": "Its independence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its independence"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay won its independence from Brazil in 1828", "extracted_entity": null}
{"question": "is Uruguay very common?", "answer": "No?", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No?"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no, it is not common at all.", "extracted_entity": null}
{"question": "is The climate in Uruguay temperate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Uruguay has a temperate climate, with a rainy", "extracted_entity": null}
{"question": "is the main sport in Uruguay football ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, the main sport in Uruguay is football.", "extracted_entity": null}
{"question": "is Uruguay 's oldest church in San Carlos , Maldonado ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "n Carlos is in the department of Maldon", "extracted_entity": null}
{"question": "is Uruguay a member of the Cairns Group of exporters of agricultural products ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Yes, Uruguay is a member of the Cairns", "extracted_entity": null}
{"question": "is it widely considered the most secular nation in Latin America ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "Chile", "extracted_entity": null}
{"question": "is it the second smallest independent country in south america , larger only than suriname and the french overseas department of french guiana?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no, french guiana is an overseas department of", "extracted_entity": null}
{"question": "is it a constitutional democracy , where the president fulfills the roles of both head of state and head of government?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Uruguay"], "answer_entity_name": null, "predicted_answer": "no, it is not. the president is head", "extracted_entity": null}
{"question": "was Woodrow Wilson the thirtieth President of the United States?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "did Woodrow Wilson create the League of Nations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, Wilson was a US president who supported the", "extracted_entity": null}
{"question": "did the U.S. join the League of Nations?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, the U.S. did not join the", "extracted_entity": null}
{"question": "where was the League of Nations created?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Versailles, France", "extracted_entity": null}
{"question": "when was Woodrow Wilson born?", "answer": "December 28, 1856", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 28, 1856"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "1856.", "extracted_entity": null}
{"question": "who was President when Wilson finished Congressional Government?", "answer": "Grover Cleveland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Grover Cleveland"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt", "extracted_entity": null}
{"question": "what field did Woodrow Wilson leave law practice to study?", "answer": "history and political science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["history and political science", "Government"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "history", "extracted_entity": null}
{"question": "what caused Wilson to ask Congress to declare war on the Central Powers?", "answer": "German began unrestricted submarine warfare", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["German began unrestricted submarine warfare", "World War I"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the sinking of the Lusitania", "extracted_entity": null}
{"question": "what was more damaging than moving students into colleges?", "answer": "His confrontation with Andrew Fleming West", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His confrontation with Andrew Fleming West"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "moving students out of colleges.", "extracted_entity": null}
{"question": "was Wilson a member of the Phi Kappa Psi fraternity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Totally"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes. He was a member of the Alpha Chapter", "extracted_entity": null}
{"question": "was Wilson an automobile enthusiast?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "ouai"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, he was a train enthusiast.", "extracted_entity": null}
{"question": "did Wilson's father own slaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yeah"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson's father was a poor man who had no", "extracted_entity": null}
{"question": "where is Wilson buried?", "answer": "He was buried in Washington National Cathedral", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was buried in Washington National Cathedral", "Washington National Cathedral"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "in the churchyard of the old parish church,", "extracted_entity": null}
{"question": "where did Wilson attend law school?", "answer": "Wilson attended law school at University of Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson attended law school at University of Virginia", "University of Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yale", "extracted_entity": null}
{"question": "where was Woodrow Wilson born?", "answer": "Woodrow Wilson was born in Staunton, Virginia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Woodrow Wilson was born in Staunton, Virginia", "Staunton, Virginia"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Staunton, Virginia", "extracted_entity": null}
{"question": "did Wilson support desegregation?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Not in the slightest."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, he was a racist", "extracted_entity": null}
{"question": "did Wilson support the committee system?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "no, he did not support the committee system,", "extracted_entity": null}
{"question": "did Wilson have any siblings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No, Wilson was an only child.", "extracted_entity": null}
{"question": "what was Scots-Irish and Scottish?", "answer": "His ancestry", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His ancestry"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "they were both the same", "extracted_entity": null}
{"question": "what defended slavery, owned slaves and set up a Sunday school for them?", "answer": "His father", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the Quakers", "extracted_entity": null}
{"question": "who did Wilson win in 1917?", "answer": "Irish Americans", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish Americans"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the 1917 World Series", "extracted_entity": null}
{"question": "was Wilson awarded the 1919 Nobel Peace Prize?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No. The 1919 Nobel Peace Prize was", "extracted_entity": null}
{"question": "was Wilson a remarkably effective writer and thinker?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Wilson was a remarkably effective writer and thinker.", "extracted_entity": null}
{"question": "what lived in Columbia?", "answer": "Wilson", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilson"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the Columbia River", "extracted_entity": null}
{"question": "was Wilson president of the American Political Science Association in 1910 ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "No. Wilson was elected president of the APSA", "extracted_entity": null}
{"question": "did he not cast his ballot for John M. Palmer , the presidential candidate of the National Democratic Party , or Gold Democrats , a short-lived party that supported a gold standard , low tariffs , and limited government ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "He voted for John M. Palmer,", "extracted_entity": null}
{"question": "did Wilson not spend 1914 through the beginning of 1917 trying to keep America out of the war in Europe ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Wilson , a staunch opponent of antisemitism , sympathetic to the plight of Jews , especially in Poland and in France ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "was a staunch opponent of antisemitism.", "extracted_entity": null}
{"question": "what happened in 1917?", "answer": "raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["raised billions through Liberty loans, imposed an income tax, set up the War Industries Board, promoted labor union growth, supervised agriculture and food production through the Lever Act, took over control of the railroads, and suppressed anti-war movements"], "entity_annotations": ["Woodrow_Wilson"], "answer_entity_name": null, "predicted_answer": "the Russian revolution. It was a revolt against the", "extracted_entity": null}
{"question": "was Volta an Italian physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was an Italian physicist.", "extracted_entity": null}
{"question": "is Volta buried in the city of Pittsburgh?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, in the cemetery of the Holy Sepul", "extracted_entity": null}
{"question": "did Volta have a passion for the study of electricity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "he had a passion for the study of electricity because", "extracted_entity": null}
{"question": "what is the battery made by Volta credited to be?", "answer": "the first cell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the first cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "voltaic cell", "extracted_entity": null}
{"question": "what important electrical unit was named in honor of Volta?", "answer": "the volt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the volt", "volt"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The volt", "extracted_entity": null}
{"question": "where did Volta enter retirement?", "answer": "Spain", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spain"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He retired from the post of Prime Minister in", "extracted_entity": null}
{"question": "is it a disadvantage for something to be unsafe to handle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, it is not a disadvantage for something to", "extracted_entity": null}
{"question": "was Lombardy under Napoleon's rule in 1800?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, it was. Napoleon had conquered it in", "extracted_entity": null}
{"question": "was the Italian 10.000 lira banknote created before the euro?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, it was created in 2002,", "extracted_entity": null}
{"question": "for how many years did Alessandro Volta live?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta lived 29 years", "extracted_entity": null}
{"question": "did Alessandro Volta live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, he was 82 when he died.", "extracted_entity": null}
{"question": "what was Alessandro Volta`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "a physicist", "extracted_entity": null}
{"question": "how old was Alessandro Volta when he died?", "answer": " 82", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" 82"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "82 years old", "extracted_entity": null}
{"question": "how many years ago was it when Volta married the daughter of Count Ludovico Peregrini , Teresa , with whom he raised three sons?", "answer": "215", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["215"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "745, 1750, 175", "extracted_entity": null}
{"question": "is the electrolyte sulphuric acid?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, it is an aqueous solution of potassium", "extracted_entity": null}
{"question": "is volta buried in the city  of Como?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was his 1800  paper written in French?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, it was written in French", "extracted_entity": null}
{"question": "before 1796, was Lombardy ruled by Austria?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. Lombardy was ruled by Napoleon. (", "extracted_entity": null}
{"question": "did he receive the Society`s 1794 Copley Medal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "He did. He was awarded the Copley", "extracted_entity": null}
{"question": "did he experiment with individual cells?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "when did lombardy come under Napoleon`s rule?", "answer": "From 1796 to 1815", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1796 to 1815"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1796", "extracted_entity": null}
{"question": "where did he publish his invention of the Voltaic pile battery?", "answer": "the Philosophical Transactions of the Royal Society", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Philosophical Transactions of the Royal Society"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "in the journal Philosophical Transactions of the Royal Society", "extracted_entity": null}
{"question": "did he become professor of experimental physics at the University of Pavia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he became professor of experimental physics at the", "extracted_entity": null}
{"question": "is it true that his passion been always the study of electricity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, he studied electricity as a hobby and as", "extracted_entity": null}
{"question": "his passion been always the study of what?", "answer": "Electricity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Electricity"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the study of what?", "extracted_entity": null}
{"question": "is it true that Volta married the daughter of Count Ludovico Peregrini?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. Volta married Teresa Peregrini,", "extracted_entity": null}
{"question": "is it true that he published his invention of the Voltaic pile battery?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, he published it in 1799", "extracted_entity": null}
{"question": "was Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, he was Italian.", "extracted_entity": null}
{"question": "did Amedeo Avogadro graduate?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he did not graduate.", "extracted_entity": null}
{"question": "did Avogadro live in England?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he was Italian.", "extracted_entity": null}
{"question": "where was Avogadro a professor of physics?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin, Italy", "extracted_entity": null}
{"question": "how many children did Avogadro have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "6.022x10^23", "extracted_entity": null}
{"question": "what is Avogadro most noted for?", "answer": "contributions to the theory of molarity and molecular weight", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["contributions to the theory of molarity and molecular weight"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "He was the first to notice that the weight of", "extracted_entity": null}
{"question": "what is Avogadro's number?", "answer": "6.02214199x10 23", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["6.02214199x10 23"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the number of particles in a mole", "extracted_entity": null}
{"question": "in what year did Avogadro stop teaching at Turin University?", "answer": "1853", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1853", "1823"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1843 (he was 64 years old", "extracted_entity": null}
{"question": "what subject did Avogadro study first?", "answer": "ecclesiastical law", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ecclesiastical law"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "chemistry", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s birthplace?", "answer": "Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was born in Tur", "extracted_entity": null}
{"question": "what was Amedeo Avogadro`s profession?", "answer": "professor of physics", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of physics"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro was a scientist.", "extracted_entity": null}
{"question": "how many years ago was it when he became a professor of physics at the University of Turin?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "1975.", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s first name?", "answer": "Amedeo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Amedeo"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Lorenzo", "extracted_entity": null}
{"question": "what is Amedeo Avogadro`s last name?", "answer": "Avogadro", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro", "extracted_entity": null}
{"question": "was Amedeo Avogadro born in North America?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he was born in Italy", "extracted_entity": null}
{"question": "was King Victor  Emmanuel  III there to pay  homage  to Avogadro?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he was there to pay homage to the", "extracted_entity": null}
{"question": "in fact, was Avogadro `s  famous  1811  paper written in French . )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes,  in  fact  it", "extracted_entity": null}
{"question": "is avogadro `s  number commonly used to compute  the results  of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes.  the number of moles of a", "extracted_entity": null}
{"question": "hence, can relative molecular  masses be calculated from the masses  of gas samples?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Relative molecular masses can be calculated from the masses of", "extracted_entity": null}
{"question": "did he publish his work?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, in the journal Science.", "extracted_entity": null}
{"question": "did Avogadro submit his poem?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is it true that loschmidt calculated first the value of Avogadro number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No. It was Avogadro who first calculated", "extracted_entity": null}
{"question": "loschmidt calculated first the value of what?", "answer": "Avogadro's number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro's number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the number of molecules in a given volume", "extracted_entity": null}
{"question": "hence , can relative molecular masses be calculated from the masses of gas samples ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "elative molecular mass is a measure of the mass of", "extracted_entity": null}
{"question": "can the title of this famous 1811 paper be roughly translated into English as \"essay on determining the Relative Masses of the Elementary Molecules of Bodies\" ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": null}
{"question": "what is Amedeo Avogadro?", "answer": "A person.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A person."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro was a scientist who", "extracted_entity": null}
{"question": "where is Amedeo Avogadro from?", "answer": "Italy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Italy."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin, Italy", "extracted_entity": null}
{"question": "when did Anders Celcius publish his observations on the aurora borealis?", "answer": "1733", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1733", "1733 "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "in 1733, the year of the great", "extracted_entity": null}
{"question": "what did Anders Celsius determine about the boiling of water?", "answer": "He determined the dependence of the boiling of water with atmospheric pressure.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He determined the dependence of the boiling of water with atmospheric pressure.", "dependence with atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius determined that water boiled at 100 degrees Celsius", "extracted_entity": null}
{"question": "which temperature scale did Celsius propose?", "answer": "Celcius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celcius", " the Celsius temperature scale "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "centigrade scale", "extracted_entity": null}
{"question": "what is the boiling point of water dependent on?", "answer": "atmospheric pressure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atmospheric pressure"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "the amount of pressure exerted on the water", "extracted_entity": null}
{"question": "where was Celsius born?", "answer": "Uppsala in Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Uppsala in Sweden", "Uppsala, Sweden"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Sweden", "extracted_entity": null}
{"question": "whad did Celsius report in his paper \"Observations of two persistent degrees on a thermometer\"?", "answer": "the freezing point is independent of latitude", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the freezing point is independent of latitude"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "He reported that he had observed two persistent degrees on", "extracted_entity": null}
{"question": "how old was Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "47", "extracted_entity": null}
{"question": "what is temperature would water have to be to be halfway between its standard boiling and freezing point?", "answer": "50", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["50"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "273.15 + 273.15 /", "extracted_entity": null}
{"question": "what is the difference between Celsius' original temperature scale and the one we use today?", "answer": "The scale was reversed by Carolus Linnaeus. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The scale was reversed by Carolus Linnaeus. ", "scale was reversed "], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius' original scale was based on the freezing and", "extracted_entity": null}
{"question": "what was Anders Celsius`s profession?", "answer": "professor of astronomy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["professor of astronomy"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "He was a scientist.", "extracted_entity": null}
{"question": "how old was Anders Celsius when he died?", "answer": "42", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "49 years old", "extracted_entity": null}
{"question": "what is Anders Celsius`s last name?", "answer": "Celsius", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Celsius"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Celsius is a surname of Swedish origin. It is", "extracted_entity": null}
{"question": "was celsius born in Uppsala  in Sweden?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Uppsala in", "extracted_entity": null}
{"question": "is The Celsius  crater  on the Moon named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "was Anders Celsius  ( November 27 ,  1701  April 25 ,  1744  ) a Swedish  astronomer?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Anders Celsius the first  to perform  and  publish  careful experiments  aiming at the definition  of an international  temperature  scale  on scientific grounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no, but he was the first to publish a", "extracted_entity": null}
{"question": "was he professor at Uppsala University?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "no, he was professor at Uppsala University", "extracted_entity": null}
{"question": "is it true that he published a collection of 316 observations?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "Yes, it is. In 1918,", "extracted_entity": null}
{"question": "he published a collection of what?", "answer": "observations", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["observations"], "entity_annotations": ["Anders_Celsius"], "answer_entity_name": null, "predicted_answer": "poetry", "extracted_entity": null}
{"question": "can syllables begin with a vowel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, syllables can begin with a vowel.", "extracted_entity": null}
{"question": "are calligraphers held in great esteem?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes. calligraphy is the art of writing.", "extracted_entity": null}
{"question": "have many European languages borrowed numerous words from it?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, many European languages have borrowed numerous words from", "extracted_entity": null}
{"question": "are the most active in Damascus and Cairo?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "The Muslim Brotherhood.", "extracted_entity": null}
{"question": "do arabs consider literary Arabic?", "answer": "(bad question)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["(bad question)"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, they consider it a dead language, a", "extracted_entity": null}
{"question": "the sociolinguistic situation of Arabic provides a prime example of what?", "answer": "the linguistic phenomenon of diglossia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the linguistic phenomenon of diglossia"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "diglossia", "extracted_entity": null}
{"question": "are pronouns in Literary Arabic marked for person , number and gender ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "es, they are marked for all three", "extracted_entity": null}
{"question": "however , are non-human plural nouns grammatically considered to be feminine singular ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "he answer is no, they are not. The", "extracted_entity": null}
{"question": "is a bee an insect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "yes, a bee is an insect.", "extracted_entity": null}
{"question": "are bees related to ants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "yes, they are both in the order hymen", "extracted_entity": null}
{"question": "have managed populations of European honey bees experienced substantial declines?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "Yes, according to a study by the United States", "extracted_entity": null}
{"question": "where do bees live?", "answer": "in colonies", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in colonies"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "in a hive", "extracted_entity": null}
{"question": "when do bumblebee colonies reach peak population?", "answer": "mid to late summer", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mid to late summer"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "July (in the northern hemisphere)", "extracted_entity": null}
{"question": "what do people commonly call cleptoparasitic bees?", "answer": "cuckoo bees", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cuckoo bees"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "mites", "extracted_entity": null}
{"question": "how did vespoid wasps evolve?", "answer": "from predatory ancestors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from predatory ancestors"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "by becoming parasitic wasps, and then,", "extracted_entity": null}
{"question": "what do humans use bees for?", "answer": "honey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["honey"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "bees are used for honey, wax, and poll", "extracted_entity": null}
{"question": "why do honey bee queens continue to lay eggs during the foraging season?", "answer": "to replace daily casualties", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to replace daily casualties"], "entity_annotations": ["Bee"], "answer_entity_name": null, "predicted_answer": "so the colony can produce bees to replace those that", "extracted_entity": null}
{"question": "does Beijing mean \"northern capital\"?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "no, it means \"northern palace\".", "extracted_entity": null}
{"question": "are the caves of Dragon Bone Hill near the village of Zhoukoudian in Fangshan District?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "No, they are not. The caves are in", "extracted_entity": null}
{"question": "are hutongs disappearing?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Yes, but not as fast as before.", "extracted_entity": null}
{"question": "when did Beijing host the Olympic Games?", "answer": "2008", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2008"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "2008", "extracted_entity": null}
{"question": "when was the An Shi Rebellion launched?", "answer": "in 755 AD", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["in 755 AD"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "755", "extracted_entity": null}
{"question": "in 1949, where did Communist forces enter without a fight?", "answer": "Beiping", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Beiping"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "Taiwan", "extracted_entity": null}
{"question": "when did Yuan Shikai die?", "answer": "1916", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1916"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "November 6, 1916 (aged", "extracted_entity": null}
{"question": "where does air pollution in Beijing come from?", "answer": "surrounding cities and provinces", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["surrounding cities and provinces"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "there are 2 main sources of air pollution in", "extracted_entity": null}
{"question": "as of August 1st, 2006, how many trains stop daily at the Beijing Railway Station or the Beijing West Railway Station?", "answer": "167", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["167"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "1,300. That\u2019s 1,", "extracted_entity": null}
{"question": "what is the name of a university (or similar institution for imparting higher education) in Beijing?", "answer": "Tsinghua University", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tsinghua University"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "\u5317\u4eac\u5927\u5b66 (Beijing University)", "extracted_entity": null}
{"question": "are famous middle  schools  in Beijing :?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Beijing"], "answer_entity_name": null, "predicted_answer": "yes,there are some famous middle schools in Beijing", "extracted_entity": null}
{"question": "was Blaise Pascal a mathematician of the first order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, he was. He was a mathematician", "extracted_entity": null}
{"question": "could Blaise Pascal move without crutches?", "answer": "He could move without crutches until a paralytic attack in 1647.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He could move without crutches until a paralytic attack in 1647.", "no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "No, he was paralyzed from the waist down", "extracted_entity": null}
{"question": "has the name Pascal been given to the SI unit of pressure?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "No. The SI unit of pressure is the p", "extracted_entity": null}
{"question": "from what did Pascal suffer throughout his life?", "answer": "ill health", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["ill health", "poor health"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "from the illness of the time", "extracted_entity": null}
{"question": "what did Pascal argue was as perfect as possible?", "answer": "the procedure used in geometry ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the procedure used in geometry ", "the procedure used in geometry"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the mathematical triangle (pascal's triangle)", "extracted_entity": null}
{"question": "what is the best physician?", "answer": "Time", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Time"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the one who is most in love with truth", "extracted_entity": null}
{"question": "who was the eldest sibling?", "answer": "Gilberte", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gilberte"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "my brother, by a few minutes", "extracted_entity": null}
{"question": "aside from the Provincial Letters' religious influence, were they popular as a literary work?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, they were. they were even translated into", "extracted_entity": null}
{"question": "of whose continual poor health was the cause never precisely determined?", "answer": "Blaise Pascal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Blaise Pascal"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "The wife of the Duke of Gloucester", "extracted_entity": null}
{"question": "is the violin cello a bowed string instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes, the violin cello is a bowed string", "extracted_entity": null}
{"question": "is the purfling just for decoration?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "no, it's to reduce the vibration of the", "extracted_entity": null}
{"question": "are the z-holes used as access points to the interior of the cello? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Yes, the holes are used as access points to", "extracted_entity": null}
{"question": "what does violincello mean?", "answer": "Violincello means \"little violene\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Violincello means \"little violene\"."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "it means a cello that has been smashed to", "extracted_entity": null}
{"question": "what are the two materials that bows are made of traditionally? ?", "answer": "Traditionally, bows are made from pernambuco or brazilwood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Traditionally, bows are made from pernambuco or brazilwood."], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "wood and animal sinew", "extracted_entity": null}
{"question": "what is used to tune a cello?", "answer": "Pegs are used to tune a cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pegs are used to tune a cello.", "the pegs"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a bow", "extracted_entity": null}
{"question": "which harmonics can produce any notes above middle C? ?", "answer": "Artificial harmonics can produce any notes above middle C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Artificial harmonics can produce any notes above middle C.", "Artificial harmonics"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "1st, 2nd, 3", "extracted_entity": null}
{"question": "who referred to the cello as \"basso de viola da braccio\"?", "answer": "Monteverdi referred to the cello as \"basso de viola da braccio\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Monteverdi referred to the cello as \"basso de viola da braccio\".", "Monteverdi"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "Vincenzo Galilei, father of", "extracted_entity": null}
{"question": "where is the violincello held?", "answer": "The violincello is held on the shoulder.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The violincello is held on the shoulder.", "on the shoulder"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "the violincello is held between the knees.", "extracted_entity": null}
{"question": "is there cello in performances by Rihanna?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "no, there is no cello in performances by", "extracted_entity": null}
{"question": "are cellos constructed with glue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes, cellos are constructed with glue.", "extracted_entity": null}
{"question": "is the cello a stringed instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "yes, the cello is a stringed instrument", "extracted_entity": null}
{"question": "what is cello an abbreviation of?", "answer": "violoncello", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["violoncello"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "cello is short for violoncello which", "extracted_entity": null}
{"question": "what position is used to play the cello?", "answer": "a gamba", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a gamba", "seated"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a chair", "extracted_entity": null}
{"question": "what is a person who plays the cello called?", "answer": "cellist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cellist"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "a cellist", "extracted_entity": null}
{"question": "why is there purling on a cello?", "answer": "to stop cracks from forming ****misspelled****", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to stop cracks from forming ****misspelled****"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "because it is a string instrument", "extracted_entity": null}
{"question": "when did the first educational works appear for the cello?", "answer": "the 18th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 18th century", "18th century"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "in the late 18th century, when the", "extracted_entity": null}
{"question": "what cello manufacturer should I buy from if I want to play outside?", "answer": "Luis & Clark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luis & Clark"], "entity_annotations": ["Cello"], "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I think it\u2019s", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a member of the National Institute?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was elected on 17 June", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb find any relationship between electric charges and magnetic poles? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No. He found a relationship between electric charges and", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb's father's family in Montpellier? ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he was born in Angoul\u00eame,", "extracted_entity": null}
{"question": "where did Charles-Augustin de Coulomb die?", "answer": "Charles-Augustin de Coulomb died in Paris.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb died in Paris.", "Paris, France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Paris", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb permanently stationed in Paris?", "answer": "Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb was permanently stationed in Paris in 1781.", "Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1787", "extracted_entity": null}
{"question": "what contribution did Charles-Augustin de Coulomb make to the field of geotechnical engineering?", "answer": "Retaining wall design", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Retaining wall design"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Coulomb friction law. It is the basis", "extracted_entity": null}
{"question": "where did the construction of Fort Bourbon take place?", "answer": "The construction of Fort Bourbon took place in Martinique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The construction of Fort Bourbon took place in Martinique.", "Martinique "], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "at the mouth of the Mississippi River", "extracted_entity": null}
{"question": "what is the definition of the electrostatic force of attraction and repulsion? ?", "answer": "Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb's law is the definition of the electrostatic force of attraction and repulsion.", "Coulomb's Law"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the force of attraction and repulsion between two charged", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb retire to a small estate he possessed at Blois?", "answer": "Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb retired to a small estate he possessed at Blois on the outbreak of the revolution in 1789.", "1789"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1788", "extracted_entity": null}
{"question": "if Charles-Augustin de Coulomb was alive today, how old would he have been?", "answer": "273", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["273"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "284 years old", "extracted_entity": null}
{"question": "for how many years did Charles-Augustin de Coulomb live?", "answer": "70", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["70"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "He lived from 1736 to 1806", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he died at the age of 57", "extracted_entity": null}
{"question": "how old was Charles-Augustin de Coulomb when he died?", "answer": "1806", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1806"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "60 years old.", "extracted_entity": null}
{"question": "how many years ago did he resign his appointment as intendant de eaux et fontaine ?", "answer": "220", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["220"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "15 years ago.", "extracted_entity": null}
{"question": "how many years ago did Charles-Augustin de Coulomb die?", "answer": "203", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["203"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1806", "extracted_entity": null}
{"question": "was The SI  unit  of charge , the  coulomb , named after him?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": null}
{"question": "was coulomb born in Angoul\u00eame,  France ,  to a well  to do  family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was his father ,  Henri  Coulomb , inspector of the Royal  Fields  in Montpellier?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes,  he was.", "extracted_entity": null}
{"question": "is coulomb distinguished in the history  of mechanics  and  of electricity and  magnetism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes,  coulomb  is  distinguished", "extracted_entity": null}
{"question": "was Charles Augustin  de Coulomb  ( born  June  14 ,  1736 ,  Angoul\u00eame ,  France  -  died August 23 ,  1806 ,  Paris ,  France  ) a French  physicist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "he was.", "extracted_entity": null}
{"question": "does Coulomb leave a legacy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes, he has a legacy", "extracted_entity": null}
{"question": "when was Charles-Augustin de Coulomb born?", "answer": "June 14, 1736", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["June 14, 1736"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "14 June 1736.", "extracted_entity": null}
{"question": "did he publish an important investigation of the laws of friction?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes, he published a very important investigation of the", "extracted_entity": null}
{"question": "was Coulomb born in Angoul\u00eame, France, to a well to do family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in Angoul\u00eame,", "extracted_entity": null}
{"question": "was his father inspector of the Royal Fields?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, his father was a farmer.", "extracted_entity": null}
{"question": "he discovered an inverse relationship of what?", "answer": "distance and electric force", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["distance and electric force"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the relationship between the number of years of education and", "extracted_entity": null}
{"question": "is it true that coulomb leaves a legacy as a hero?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "yes it is true", "extracted_entity": null}
{"question": "where is Charles-Augustin de Coulomb from?", "answer": "France", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["France"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb was a French physicist and engineer.", "extracted_entity": null}
{"question": "was Old Chinese wholly uninflected?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No. The most important source of evidence for this", "extracted_entity": null}
{"question": "are The modern Chinese dialects more like a family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No, they are not. Because they are not", "extracted_entity": null}
{"question": "is it true that one-fifth of world population speak some form of Chinese?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "it is true that one-fifth of world population", "extracted_entity": null}
{"question": "one-fifth of world population speak some form of what?", "answer": "Chinese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "English", "extracted_entity": null}
{"question": "is mandarin spoken Chinese distinguished by its high level?", "answer": "no, it's distinguished by diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no, it's distinguished by diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes, mandarin spoken Chinese is distinguished by its", "extracted_entity": null}
{"question": "copenhagen is the capital of what country?", "answer": "Denmark", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Denmark"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "copenhagen is the capital of denmark", "extracted_entity": null}
{"question": "what is the population of Copenhagen?", "answer": "1,161,063", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,161,063", "1,153,615"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "1.2 million", "extracted_entity": null}
{"question": "what transnational bridge was completed in 2000?", "answer": "Oresund Bridge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oresund Bridge"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "the Akashi Kaikyo Bridge in Japan", "extracted_entity": null}
{"question": "why is Copenhagen a regional hub?", "answer": "Its strategic location and excellent infrastructure", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Its strategic location and excellent infrastructure", "Its strategic location and excellent infrastructure with the largest airport in Scandinavia[6] located 14 minutes by train from the city centre"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "Because of its central location in the world.", "extracted_entity": null}
{"question": "what happened to Copenhagen between 1251 and 1255?", "answer": "a bunch of things", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a bunch of things"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "The city was burned down in 1251,", "extracted_entity": null}
{"question": "is Copenhagen completely surrounded with water? ?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "yes, but not completely.?", "extracted_entity": null}
{"question": "copenhagen is ranked number one worldwide for which things?", "answer": "Most Livable City in the World, ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most Livable City in the World, ", "Most Livable City, Location Ranking Survey"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "being green and being a great place to live.", "extracted_entity": null}
{"question": "why did German troops occupy Copenhagen?", "answer": "Because it was WW2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it was WW2"], "entity_annotations": ["Copenhagen"], "answer_entity_name": null, "predicted_answer": "the danish government refused to surrender to the Germans", "extracted_entity": null}
{"question": "have cymbals been used historically to suggest bacchanal?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes. There is a cymbal in the", "extracted_entity": null}
{"question": "can the origins of cymbals be traced to prehistoric times?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes. the earliest known cymbals are made", "extracted_entity": null}
{"question": "are cymbals used in moden orchestras?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, they are used in classical orchestras", "extracted_entity": null}
{"question": "what are the most common Cymbals?", "answer": " The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The most common Cymbals are the Hi-Hats, Crash, Splash, Ride, and China.", "Hi-Hats, Crash, Splash, Ride, and China"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Hi-Hats, Ride Cymbals, Crash", "extracted_entity": null}
{"question": "what is the second main orchestral use of cymbals?", "answer": "The suspended cymbal is the second main orchestral use of symbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The suspended cymbal is the second main orchestral use of symbals.", "The suspended cymbal", "the suspended cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "for the \"roll\" and \"crash\"", "extracted_entity": null}
{"question": "what have clash cymbals traditionally been accompanied by?", "answer": "Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Clash cymbals have traditionally been accompanied by the bass drum playing an identical part.", "The bass drum playing an identical part"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "a bass drum.", "extracted_entity": null}
{"question": "what happens when the center of a Cymbal is hit?", "answer": "It causes a sound which is in a higher register than the rest of the symbal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It causes a sound which is in a higher register than the rest of the symbal.", "It causes a sound which is in a higher register than the rest of the cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "it vibrates and makes a sound", "extracted_entity": null}
{"question": "what is an effective way to accentuate a note?", "answer": "Playing a bass drum playing an identical part is an effective way to accentuate a note.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Playing a bass drum playing an identical part is an effective way to accentuate a note.", "The combination of a clash cymbol being accompanied by a bass drum playing an identical part, played loudly"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "a trill, a turn, or a m", "extracted_entity": null}
{"question": "from what type of Cymbals can a expert player obtain an enormous dynamic range?", "answer": "An expert player can obtain an enormous dynamic range from crash cymbals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An expert player can obtain an enormous dynamic range from crash cymbals.", "Crash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "from a set of Hi-hat Cymbals", "extracted_entity": null}
{"question": "can the origins of cymbals be traced back to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the oldest known cymbals were found in Turkey", "extracted_entity": null}
{"question": "did the modern hi-hat evolve from clash cymbals?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "yes. hi-hats were originally called clash c", "extracted_entity": null}
{"question": "is the word cymbal derived from a Germanic word?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No. The word is derived from the Greek word", "extracted_entity": null}
{"question": "what is the center of a cymbal called?", "answer": "Bell", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bell", "bell"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the bell", "extracted_entity": null}
{"question": "what part of the cymbal gets the best \"crash\"?", "answer": "Edge or rim", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Edge or rim", "edge or rim"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the edge", "extracted_entity": null}
{"question": "what two instruments form the modern drum kit?", "answer": "the cymbals and the drum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cymbals and the drum"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the bass drum and the snare drum", "extracted_entity": null}
{"question": "what Greek mythical creatures were depicted holding cymbals?", "answer": "fauns and satyrs", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["fauns and satyrs"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Satyrs and fauns.", "extracted_entity": null}
{"question": "what are orchestral crash cymbals technically called?", "answer": "a pair of clash cymbals", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a pair of clash cymbals", "clash cymbals"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "crash cymbals", "extracted_entity": null}
{"question": "is Dhaka the capital of Bangladesh?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is Dhaka located on the eastern banks of the Buriganga River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes, Dhaka is located on the eastern banks", "extracted_entity": null}
{"question": "is University of Dhaka the largest public university in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "No, it is not. Bangladesh University of Engineering", "extracted_entity": null}
{"question": "how much area does Dhaka cover?", "answer": "1462.60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1462.60 square kilometers"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "1,300 sq km.", "extracted_entity": null}
{"question": "how many people did the 1970 Bhola cyclone kill?", "answer": "500,000", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["500,000"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "300,000 to 500,000", "extracted_entity": null}
{"question": "when is the end of the independence war of Bangladesh?", "answer": "December 16, 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 16, 1971"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "16 December 1971", "extracted_entity": null}
{"question": "did the Mughals arrive in Dhaka?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Yes. In 1610, Mughal", "extracted_entity": null}
{"question": "is University of Dhaka older than the Dhaka College?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Dhaka"], "answer_entity_name": null, "predicted_answer": "Dhaka College was founded in 1921.", "extracted_entity": null}
{"question": "what is given for the number of native speakers?", "answer": "No figure is given for the number of native speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No figure is given for the number of native speakers."], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "1.5 billion", "extracted_entity": null}
{"question": "is english an intonation  language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is english a de  jure  official  language  of Israel?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the stressed  syllable called the nuclear  syllable?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does english grammar have minimal inflection?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, it does, and it is a fact", "extracted_entity": null}
{"question": "did old English develop into Middle English?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "No, Middle English developed from Old English.", "extracted_entity": null}
{"question": "are books, magazines, and newspapers written in English available?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are many books, magazines and newspapers", "extracted_entity": null}
{"question": "is the syntax of German different with different rules?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "yes, the syntax of German is different with different", "extracted_entity": null}
{"question": "is it true that semantics causes a number of false friends?", "answer": "true", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["true"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "No. There are many false friends in English and", "extracted_entity": null}
{"question": "here , are all syllables unstressed , except the syllables/words best and done , which are stressed ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": ", they are all unstressed, except the", "extracted_entity": null}
{"question": "are many words describing the navy , types of ships , and other objects or activities on the water of dutch origin ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "s, many words describing the navy, types of", "extracted_entity": null}
{"question": "are local variations in the formal written version of the language quite limited , being restricted largely to the spelling differences between British and American English ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["English_language"], "answer_entity_name": null, "predicted_answer": "es, but there are also some spelling differences between", "extracted_entity": null}
{"question": "is the flute a musical instrument?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, the flute is a musical instrument.", "extracted_entity": null}
{"question": "is it possible to open flutes at one or both ends?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, but it is not common, and if", "extracted_entity": null}
{"question": "are Indian concert flutes available in standard pitches?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, but the standard pitches are usually Western concert", "extracted_entity": null}
{"question": "what do we refer musicians who play flute?", "answer": "A flute player, a flautist or a flutist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute player, a flautist or a flutist. ", "A flute player, a flautist, or a flutist"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "flute players", "extracted_entity": null}
{"question": "when was a three-holed flute made from a mammoth tusk discovered?", "answer": "2004.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2004.", "In 2004"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "in the 1980s, in the Czech", "extracted_entity": null}
{"question": "when did the tin whistle first appear?", "answer": "12th century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["12th century.", "In the 12th century"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the tin whistle is a very old instrument, dating", "extracted_entity": null}
{"question": "what are the oldest known musical instruments?", "answer": "A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A three holed-flute made from a mammoth tusk and two flutes made from swan's bones.", "A three-holed flute made from a mammoth tusk and two flutes made from swans' bones are among the oldest known musical instruments"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The oldest known musical instruments are the flute and the", "extracted_entity": null}
{"question": "when was the pan flute spread to other parts of Europe?", "answer": "7th century BC.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7th century BC.", "After the 7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "it was spread to other parts of Europe in the", "extracted_entity": null}
{"question": "what does the air stream across this hole create?", "answer": "A Bernoulli, or siphon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A Bernoulli, or siphon."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "a vacuum. The air stream sucks the water up", "extracted_entity": null}
{"question": "are foxes wary of humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes, but if they are hungry enough, they", "extracted_entity": null}
{"question": "are fennec foxes endangered?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "no, fennec foxes are not endangered", "extracted_entity": null}
{"question": "does the diet of foxes include reptiles?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "Yes, foxes do eat reptiles. In", "extracted_entity": null}
{"question": "how long do most foxes live?", "answer": "Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most foxes live 2 to 3 years, but they can survive for up to 10 years or even longer in captivity.", "2 to 3 years"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "1-3 years", "extracted_entity": null}
{"question": "in what country did fox hunting originate?", "answer": "the United Kingdom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the United Kingdom", "in the United Kingdom"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "England", "extracted_entity": null}
{"question": "what is the smallest species of fox?", "answer": "The Fennec Fox is the smallest species of fox. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Fennec Fox is the smallest species of fox. ", "the Fennec Fox"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "the fennec fox", "extracted_entity": null}
{"question": "what are female foxes called?", "answer": "vixens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["vixens"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "vixens", "extracted_entity": null}
{"question": "what is caching food?", "answer": "Caching is burying it for later consumption, usually under leaves, snow, or soil.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Caching is burying it for later consumption, usually under leaves, snow, or soil.", "burying it for later consumption"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "storing food for later use.", "extracted_entity": null}
{"question": "do foxes damage fruit on farms?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Fox"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "how many years ago did African people spread across 31 francophone African countries can speak French either as a first or second language?", "answer": "2 years ago", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2 years ago"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "50 years ago.", "extracted_entity": null}
{"question": "in Old French, was the plural  for animal animals?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, it was. It is still used in", "extracted_entity": null}
{"question": "is catalan the only  official  language  of Andorra?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in Belgium, however, is quatre-vingts universally used?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, but not in the plural. The plural", "extracted_entity": null}
{"question": "is french taught in many schools as a primary language along with Arabic?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "what is widely used by the Lebanese, especially for administrative purposes?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "the word \"shou\" (\u0634) is", "extracted_entity": null}
{"question": "is french descendant of the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, french is descendant of the Latin language.", "extracted_entity": null}
{"question": "are there also a variety of regional languages In addition?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "yes, the regional languages \u200b\u200bare also different.", "extracted_entity": null}
{"question": "is it true that france signed the European Charter for Regional Languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "Yes, France signed the European Charter for Regional Languages", "extracted_entity": null}
{"question": "are the prefixes en- and em- always nasalized ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["French_language"], "answer_entity_name": null, "predicted_answer": "no, not always. For example, in English", "extracted_entity": null}
{"question": "how many years ago was the Luther Bible by Martin Luther printed?", "answer": "475", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["475"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "500 years ago.", "extracted_entity": null}
{"question": "is german an inflected  language?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "in Switzerland, is \u00df used at all?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes, but only in loan words from German.", "extracted_entity": null}
{"question": "are there two common  word  orders?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are feminines declined in the singular?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes. They are declined as follows:", "extracted_entity": null}
{"question": "is 9 %  of the Internet  population German?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "No. Germany accounts for 1.9%", "extracted_entity": null}
{"question": "is german written using the Latin  alphabet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes,  Latin  alphabet.", "extracted_entity": null}
{"question": "was the Luther Bible by Martin Luther printed?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "Yes. It was the first Bible printed in Germany", "extracted_entity": null}
{"question": "does a  tz indicate that the preceding vowel is short?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are there 622,650 speakers of German In Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "no, there are 622,650 speakers of", "extracted_entity": null}
{"question": "are there also large populations of German ancestry In Mexico?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "There are some German immigrants in Mexico, but the", "extracted_entity": null}
{"question": "would Bundesl\u00e4nder not accept North Rhine Westphalia and Bavaria ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "yes, they would. they would accept them.", "extracted_entity": null}
{"question": "have adverbs of time to appear in the third place in the sentence , just after the predicate ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["German_language"], "answer_entity_name": null, "predicted_answer": "o. They can appear in the second place,", "extracted_entity": null}
{"question": "do both sexes of giraffe have horns?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, but only the males have ossicones", "extracted_entity": null}
{"question": "do giraffes give birth standing up?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "no, they give birth on their knees.", "extracted_entity": null}
{"question": "are giraffes hunted for their hides?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "yes, their hides are used for leather.", "extracted_entity": null}
{"question": "what areas can giraffes inhabit?", "answer": "savannas, grasslands, or open woodlands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["savannas, grasslands, or open woodlands", "inhabit savannas, grasslands, or open woodlands"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "giraffes can inhabit areas that have enough food", "extracted_entity": null}
{"question": "what will a giraffe use to clean off any bugs that appear on its face?", "answer": "extremely long tongue ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["extremely long tongue ", "It's extremely long tongue"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "its tongue", "extracted_entity": null}
{"question": "how do giraffes defend themselves?", "answer": "with a powerful kick", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["with a powerful kick", "With powerful kicks"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "they use their long necks to reach tall trees", "extracted_entity": null}
{"question": "do male giraffes weigh more than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "No, they are about the same weight.", "extracted_entity": null}
{"question": "do male giraffes have larger horns than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "No, both male and female giraffes have", "extracted_entity": null}
{"question": "are male females generally taller than female giraffes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giraffe"], "answer_entity_name": null, "predicted_answer": "No, female giraffes are usually taller than", "extracted_entity": null}
{"question": "was Henri Becquerel one of the discoverers of radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he was not one of the discoverers", "extracted_entity": null}
{"question": "is the SI unit for radioactivity named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes. The SI unit for radioactivity is the", "extracted_entity": null}
{"question": "was Henri Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, he was a French physicist.", "extracted_entity": null}
{"question": "in what year did Henri Becquerel die?", "answer": "1908", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1908", "August 24, 1908"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1908", "extracted_entity": null}
{"question": "where was Henri Becquerel born?", "answer": " Paris ", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" Paris ", "Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "France", "extracted_entity": null}
{"question": "in what year did Henri Becquerel win the Nobel Prize in Physics?", "answer": "1903", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1903"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1903", "extracted_entity": null}
{"question": "was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no. His father was.", "extracted_entity": null}
{"question": "was Henri Becquerel the sole winner of the 1903 Nobel Prize in Physics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, Henri Becquerel was the sole winner", "extracted_entity": null}
{"question": "did Henri Becquerel intentionally discover radioactivity?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "He discovered it accidentally"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no. He discovered radioactivity when he noticed that", "extracted_entity": null}
{"question": "if Henri Becquerel was alive today, how old would he have been?", "answer": "157", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["157"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "153 years old.", "extracted_entity": null}
{"question": "for how many years did Henri Becquerel live?", "answer": "56", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["56"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel lived for 54 years.", "extracted_entity": null}
{"question": "did Henri Becquerel live to be 80 years old?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no, he died at age 54", "extracted_entity": null}
{"question": "what was Henri Becquerel`s profession?", "answer": "physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["physisist"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He was a physicist.", "extracted_entity": null}
{"question": "how old was Henri Becquerel when he died?", "answer": "53", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["53"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "He died at the age of 55.", "extracted_entity": null}
{"question": "how many years ago was it when he became the third in his family to occupy the physics chair at the Musum National d`Histoire Naturelle?", "answer": "117", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["117"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "1932, 1928, 191", "extracted_entity": null}
{"question": "in 1908, the year  of his death, was Becquerel elected Permanent Secretary  of the Acad\u00e9mie  des  Sciences?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "when was Henri Becquerel born?", "answer": "December 15, 1852", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["December 15, 1852"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was born on December 15", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did he share the Nobel Prize in Physics?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "is it true that Becquerel wrapped a fluorescent substance in photographic plates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes, it is true.", "extracted_entity": null}
{"question": "becquerel wrapped a fluorescent substance in what?", "answer": "photographic plates and black material", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["photographic plates and black material"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "paper", "extracted_entity": null}
{"question": "is there a Becquerel crater on the Moon for radioactivity?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, there is not.", "extracted_entity": null}
{"question": "must \u2026 One conclude from these experiments that the phosphorescent substance in question emits rays which pass through the opaque paper and reduces silver salts ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "yes, one can conclude from these experiments that the", "extracted_entity": null}
{"question": "what is Henri Becquerel?", "answer": "Henri Becquerel was a famous physicist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henri Becquerel was a famous physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was a French physicist, Nobel", "extracted_entity": null}
{"question": "where is Henri Becquerel from?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel was from Paris, France", "extracted_entity": null}
{"question": "what does Henri Becquerel do?", "answer": "was a physisist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["was a physisist", "Henri Becquerel was a physicist."], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he discovered radioactivity.", "extracted_entity": null}
{"question": "was Newton a English physicist?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton was an English physicist", "extracted_entity": null}
{"question": "did Newton reject the church's doctrine of the Trinity?", "answer": "Newton may have rejected the church's doctrine of the Trinity.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newton may have rejected the church's doctrine of the Trinity.", "Maybe"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "he did not, he was a trinitarian", "extracted_entity": null}
{"question": "was Newton admitted into Havard College?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "Maybe, but it doesn't say so in the article"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no, he was a student at Trinity College in", "extracted_entity": null}
{"question": "when was Newton's date of birth recorded?", "answer": "Christmas Day, Decembeer 25, 1642.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christmas Day, Decembeer 25, 1642.", "his date of birth was recorded as Christmas Day, December 25, 1642"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "25th of December 1642", "extracted_entity": null}
{"question": "how many laws of motion did Netwon have?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "who did Newton see as the master creator?", "answer": "God", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["God", "Newton saw God as the master creator whose existence could not be denied in the face of the grandeur of all creation"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "God", "extracted_entity": null}
{"question": "when did Netwon investigate the refraction of light?", "answer": "From 1670 to 1672", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1670 to 1672", "1670-1672"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "in 1670", "extracted_entity": null}
{"question": "what principles did Newton explain for mechanics?", "answer": "The principles of conservation of momentum and angular momentum", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The principles of conservation of momentum and angular momentum", "In mechanics, Newton enunciated the principles of conservation of momentum and angular momentum"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Newton's laws of motion, which are:", "extracted_entity": null}
{"question": "interaction with what man stirred up Newton's interest in alchemy?", "answer": "Henry More", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Henry More", "The contact with the theosophist Henry More, revived his interest in alchemy"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Boyle", "extracted_entity": null}
{"question": "james Watt's improvements of what were fundamental to the changes wrought by the Industrial Revolution?", "answer": "The steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The steam engine.", "steam engine"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "the steam engine, and the development of iron making", "extracted_entity": null}
{"question": "james Watt was born where?", "answer": "Greenock, a seaport on the Firth of Clyde", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Greenock, a seaport on the Firth of Clyde"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Scotland", "extracted_entity": null}
{"question": "what was the name of James Watt's mother?", "answer": "Agnus Muirhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Agnus Muirhead", "Agnes Muirhead"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Agnes Watt", "extracted_entity": null}
{"question": "how many of James Watt's children did not live to adulthood?", "answer": "3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1 of 7 (his youngest son died", "extracted_entity": null}
{"question": "james Watt ranked first among how many people in Charles Murray's survey of historiometry?", "answer": "229", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["229"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "10,000", "extracted_entity": null}
{"question": "in what year did James travel to Lodon to study instrument-making?", "answer": "1753", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1753"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1740", "extracted_entity": null}
{"question": "for how many years of James Watt's life was his mother alive?", "answer": "17 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["17 years"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "1 year", "extracted_entity": null}
{"question": "what is James Watt most famous for?", "answer": "Steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "inventing the steam engine", "extracted_entity": null}
{"question": "do most Japanese people employ politeness?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "I think it depends on the situation, but yes", "extracted_entity": null}
{"question": "is it true that Japanese borrowed a considerable number of words?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "Yes, they borrowed a lot of words from Chinese", "extracted_entity": null}
{"question": "are the several dialects of Kagoshima famous to speakers?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Japanese_language"], "answer_entity_name": null, "predicted_answer": "yes, some of them are.", "extracted_entity": null}
{"question": "is Lima the capital of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes. Lima is the capital of Peru.", "extracted_entity": null}
{"question": "is the Museum of Gold in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No. The Museum of Gold is in the city", "extracted_entity": null}
{"question": "does Lima have volleyball venues?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "yes, Lima has 2 venues for volleyball,", "extracted_entity": null}
{"question": "when was Lima founded?", "answer": "January 18,1535.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["January 18,1535."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "1535", "extracted_entity": null}
{"question": "where is the Park of the Reserve located?", "answer": "Near the downtown area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Near the downtown area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "In the area of the former city of Aix", "extracted_entity": null}
{"question": "what is a colectivo?", "answer": "Automobiles that renders express service on some major roads of the Lima Metropolitan Area.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Automobiles that renders express service on some major roads of the Lima Metropolitan Area."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "a shared taxi, like a mini-bus", "extracted_entity": null}
{"question": "is the National University of San Marcos in Lima?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, it is the oldest university in America and", "extracted_entity": null}
{"question": "has the San Lorenzo Megaport Project been completed?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "No, it is still in the planning stages.", "extracted_entity": null}
{"question": "how many years ago was the Lima Stock Exchange the most profitable in the world?", "answer": "Three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Three"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "200 years ago.", "extracted_entity": null}
{"question": "is rainfall very low?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, it is very low", "extracted_entity": null}
{"question": "is lima the capital  and  largest  city  of Peru?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Yes, Lima is the capital and largest city of", "extracted_entity": null}
{"question": "is The GDP  per capita in Lima $ 7,600  /  ref )?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "The GDP per capita in Lima is $ 7", "extracted_entity": null}
{"question": "the Spanish Crown named Pizarro governor of what?", "answer": "Hemming.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hemming."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Peru", "extracted_entity": null}
{"question": "is it true that the ideas of the Enlightenment shaped the development of the city?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "Enlightenment ideas and the industrial revolution are two of the", "extracted_entity": null}
{"question": "the ideas of the Enlightenment shaped the development of what?", "answer": "the city, Lima", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the city, Lima"], "entity_annotations": ["Lima"], "answer_entity_name": null, "predicted_answer": "the United States", "extracted_entity": null}
{"question": "is London the capital of the United Kingdom?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does London's population draw from a wide range of religions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "yes, it does, and many of the religions", "extracted_entity": null}
{"question": "did Caunte take control of the English throne in 101?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "No, he died before he could do so.", "extracted_entity": null}
{"question": "over how many languages are spoken in London?", "answer": "Over 300 languages are spoken in london", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over 300 languages are spoken in london", "Over 300"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "Over 300", "extracted_entity": null}
{"question": "what had the Anglo-Saxons created by the 600s?", "answer": "By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By the 600s, the Anglo-Saxons had created a new settlement called Lundenwic.", "a new settlement called Lundenwic"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "a unique culture that combined Germanic and Celtic elements", "extracted_entity": null}
{"question": "what city in the UK has been subjected to bouts of terrorism?", "answer": "London has been subjected to bouts of terrorism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London has been subjected to bouts of terrorism.", "London"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what countries did James VI of Scotland unite?", "answer": "James VI of Scotland united Scotland and England.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James VI of Scotland united Scotland and England.", "England and Scotland"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "England and Scotland", "extracted_entity": null}
{"question": "how did civil wars affect England during the Middle Ages?", "answer": "London remained relatively untouched", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["London remained relatively untouched"], "entity_annotations": ["London"], "answer_entity_name": null, "predicted_answer": "the civil war affected England by killing many people and", "extracted_entity": null}
{"question": "are the strings of a classical lyre made of gut?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes, or at least they were.", "extracted_entity": null}
{"question": "does a classical lyre have a sound-chest?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes. it does. it's called a sound", "extracted_entity": null}
{"question": "is lyre played in mordern Greece?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "which part of the strings does the left hand touch?", "answer": "The lower strings", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lower strings"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the fingerboard.", "extracted_entity": null}
{"question": "in which place is lyre still played?", "answer": "North-eastern parts of Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North-eastern parts of Africa"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "in the middle east", "extracted_entity": null}
{"question": "does a classical lyre have a fingerboard?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No. The strings are not fretted, but", "extracted_entity": null}
{"question": "is a guitar played with a plectrum like a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "no, a guitar is plucked with the fingers", "extracted_entity": null}
{"question": "was Michael Faraday an English chemist?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Faraday a devout Christian?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes. He was a devout Christian.", "extracted_entity": null}
{"question": "was Faraday considered a gentleman?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No, he was a chemist, and a", "extracted_entity": null}
{"question": "who was a member of the Sandemanian sect of Christianity?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "William Blake", "extracted_entity": null}
{"question": "what did Joseph Henry likely discover?", "answer": "self-induction", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["self-induction"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the fact that the strength of a magnet depends on", "extracted_entity": null}
{"question": "what did the University of Oxford grant Faraday?", "answer": "a Doctor of Civil Law degree (honorary)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Doctor of Civil Law degree (honorary)", "Doctor of Civil Law degree (honorary)"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "a Doctorate in Chemistry in 1835.", "extracted_entity": null}
{"question": "who was made to travel outside the coach?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the coachman", "extracted_entity": null}
{"question": "did Faraday construct the ancestor of modern power generators?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, he was a scientist and he did not", "extracted_entity": null}
{"question": "did Faraday lecture on education in 1854?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, at the Royal Institution", "extracted_entity": null}
{"question": "did Faraday receive an extensive formal education?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "No. He was largely self-educated.", "extracted_entity": null}
{"question": "did Faraday discover the chemical substance benzene?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, benzene was discovered by Michael Faraday", "extracted_entity": null}
{"question": "was Faraday born in England?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "what do some historians of science refer to Faraday as?", "answer": "the best experimentalist in the history of science", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the best experimentalist in the history of science", "best experimentalist in the history of science"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "\"the greatest experimentalist in the history of science", "extracted_entity": null}
{"question": "who was the sponsor and mentor of Faraday?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was the son of a blacksmith", "extracted_entity": null}
{"question": "what did Faraday invent that is used almost universally in science laboratories?", "answer": "the Bunsen burner", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Bunsen burner", "Bunsen burner"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the Faraday cage, which is a metal cage", "extracted_entity": null}
{"question": "does the production of chemical weapons raise an ethical issue?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, chemical weapons are banned by international law,", "extracted_entity": null}
{"question": "what was the Faraday effect first called?", "answer": "diamagnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["diamagnetism"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "The Faraday effect was first called the Rotatory", "extracted_entity": null}
{"question": "can you meet someone through attending a church?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, I can meet someone through attending a church", "extracted_entity": null}
{"question": "what was Michael Faraday`s birthplace?", "answer": "Newington Butts", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what was Michael Faraday`s profession?", "answer": "chemist and physicist", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["chemist and physicist"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was a scientist", "extracted_entity": null}
{"question": "how many years ago did Faraday report on the first synthesis of compound make from carbon and chlorine , c 2 h 6 and c 2 h 4 ?", "answer": "189", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["189"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "840s", "extracted_entity": null}
{"question": "how many years ago was it when he discovered the phenomenon that he named diamagnetism , and what is now called the Faraday effect?", "answer": "164", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["164"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1845. The Faraday effect is the", "extracted_entity": null}
{"question": "how many years ago was it when Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames , which resulted in an oft-reprinted cartoon in Punch?", "answer": "154", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["154"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1855.", "extracted_entity": null}
{"question": "how many years ago was it when he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal?", "answer": "162", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["162"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "1897.", "extracted_entity": null}
{"question": "was education another area  of service  for Faraday?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he did a lot of work on the", "extracted_entity": null}
{"question": "was faraday `s  earliest  chemical  work as an assistant  to Davy?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "He was assigned to work on the preparation of chlorine", "extracted_entity": null}
{"question": "who died at his house at Hampton Court on August 25, 1867?", "answer": "Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "His name was Thomas Hardy and he was the author", "extracted_entity": null}
{"question": "when was Michael Faraday born?", "answer": "September 22, 1791", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["September 22, 1791"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Michael Faraday was born on September 22,", "extracted_entity": null}
{"question": "where did he serve two terms?", "answer": "the Church of Scotland", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Church of Scotland"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "in the House of Representatives", "extracted_entity": null}
{"question": "who was born in Newington Butts, near present-day South London, England?", "answer": "Michael Faraday", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "William Shakespeare", "extracted_entity": null}
{"question": "is it true that institution formed the foundation of electric motor technology?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "it is true that institution formed the foundation of electric", "extracted_entity": null}
{"question": "institution formed the foundation of what?", "answer": "modern electromagnetic technology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern electromagnetic technology"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the foundation of the family", "extracted_entity": null}
{"question": "was his father member of the Sandemanian sect?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was. His father was a member", "extracted_entity": null}
{"question": "was faraday Christian of the small Sandemanian denomination?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is Nassau Sound a body of water in Maine?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No. Nassau Sound is a body of water", "extracted_entity": null}
{"question": "was Nassau placed in operation by Robert Fulton?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No, Nassau was placed in operation by Robert", "extracted_entity": null}
{"question": "who was Fort Nassau built by?", "answer": "the Dutch", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Dutch"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "the Dutch West India Company", "extracted_entity": null}
{"question": "what is Nassau Coliseum?", "answer": "an arena in Uniondale, New York, USA", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["an arena in Uniondale, New York, USA"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "a sports arena in Uniondale, New York.", "extracted_entity": null}
{"question": "what was the Dead or Alive 4 fighting arena modeled after?", "answer": "a Magnetic Accelerator Cannon station from Halo 2", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Magnetic Accelerator Cannon station from Halo 2"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "a prison cell.", "extracted_entity": null}
{"question": "is Nassau Range the highest mountain range in the world?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "no, the highest mountain range in the world is", "extracted_entity": null}
{"question": "is Nassau County named after a German town?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "No. Nassau County is named after the Dutch", "extracted_entity": null}
{"question": "does the United States have a base near Glasgow?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nassau"], "answer_entity_name": null, "predicted_answer": "yes, RAF Mildenhall", "extracted_entity": null}
{"question": "did Tesla study electrical engineering?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Tesla born in the United States?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "no, he was born in Croatia.", "extracted_entity": null}
{"question": "was Tesla hired by Edison?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No. Tesla was hired by Westinghouse.", "extracted_entity": null}
{"question": "when did Tesla demonstrate wireless communication (radio)?", "answer": "1893", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1893"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1899", "extracted_entity": null}
{"question": "what is the SI unit measuring magnetic flux density or magnetic induction?", "answer": "the tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla", "extracted_entity": null}
{"question": "was Tesla regarded as a mad scientist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes he was, he was a visionary, and", "extracted_entity": null}
{"question": "what type of current did Tesla invent?", "answer": "AC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["AC"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "AC (alternating current)", "extracted_entity": null}
{"question": "who was the victor of the \"War of Currents\"?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla", "Nikola Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison, who invented the AC motor, which", "extracted_entity": null}
{"question": "where did Tesla study electrical engineering?", "answer": "the Austrian Polytechnic in Graz ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Austrian Polytechnic in Graz ", "Austrian Polytechnic in Graz", "The Austrian Polytechnic in Graz"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "University of Prague", "extracted_entity": null}
{"question": "is Ottawa the capital of Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "does Ottawa sit on the Ottowa River?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "No, Ottawa sits on the Ottawa River", "extracted_entity": null}
{"question": "do small tremors occur in Ottawa?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes, and they are felt by a few people", "extracted_entity": null}
{"question": "what is Ottawa's population?", "answer": "1,190,982", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1,190,982"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "1,236,324", "extracted_entity": null}
{"question": "what are Ottawa's primary industries?", "answer": "Logging and lumber", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Logging and lumber"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "government, high tech, education, and tourism", "extracted_entity": null}
{"question": "what is Ottawa's junior ice hockey team?", "answer": "the Ottawa 67's", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa 67's"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "the Ottawa 67s", "extracted_entity": null}
{"question": "what is Ottawa's major league hockey team?", "answer": "the Ottawa Senators", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Ottawa Senators"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "the Senators", "extracted_entity": null}
{"question": "what are Carleton University's athletic teams called?", "answer": "Carleton Ravens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Carleton Ravens"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Carleton University Ravens", "extracted_entity": null}
{"question": "is Ottawa colder than Moscow in January?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes, it is. Moscow is in the mid", "extracted_entity": null}
{"question": "is Ottawa on a river?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes. The Ottawa River.", "extracted_entity": null}
{"question": "does it snow in Ottawa?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "no, but it does snow in the winter", "extracted_entity": null}
{"question": "was there a cholera outbreak in 1832?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Yes, there was a cholera outbreak in", "extracted_entity": null}
{"question": "who is the mayor of Ottawa?", "answer": "Larry O'Brien", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Larry O'Brien"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Jim Watson.", "extracted_entity": null}
{"question": "how many townships are in Ottawa?", "answer": "eleven", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["eleven"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "12", "extracted_entity": null}
{"question": "what's the main highway in Ottawa?", "answer": "Highway 417, The Queensway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Highway 417, The Queensway", "provinical Highway 417"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "The Queensway (Highway 417)", "extracted_entity": null}
{"question": "what was Ottawa's name in 1850?", "answer": "Bytown", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bytown"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "Bytown", "extracted_entity": null}
{"question": "based on the average, in what year will the next small tremor occur in Ottawa?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "2013", "extracted_entity": null}
{"question": "where were immigrants from in the 1800s?", "answer": "Irish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Irish", "Ireland"], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "immigrants were from Germany, Ireland, Italy, and", "extracted_entity": null}
{"question": "are there also discussions of having OC Transpo Light Rail and STO Transitway networks linking on the Gatineau side of the Ottawa River ?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ottawa"], "answer_entity_name": null, "predicted_answer": "yes, and also for the other side of the", "extracted_entity": null}
{"question": "does the giant otter live in South America?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes the giant otter lives in South America.", "extracted_entity": null}
{"question": "do otters have claws?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "no, they do not have claws.", "extracted_entity": null}
{"question": "do otters eat fish?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "Yes, they do. They eat fish, m", "extracted_entity": null}
{"question": "why are otters vulnerable to prey depletion?", "answer": "Prey-dependence", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Prey-dependence", "prey-dependency"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "because they have a low reproductive rate and a long", "extracted_entity": null}
{"question": "do otters enjoy playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "yes they do.", "extracted_entity": null}
{"question": "do sea otters have a layer of fat like whales?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "no"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "No, they have a layer of fur.", "extracted_entity": null}
{"question": "is the otter in the same family as the badger?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Otter"], "answer_entity_name": null, "predicted_answer": "No, the otter is in the family Must", "extracted_entity": null}
{"question": "are there any cities named Santiago in the United States?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, there are two cities named Santiago in the", "extracted_entity": null}
{"question": "was Santiago the name of an indie/punk band?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "yes. I saw them play at the Middle East", "extracted_entity": null}
{"question": "is Santiago a name in Spanish?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, it is the Spanish version of the name", "extracted_entity": null}
{"question": "what is the origin of the name Santiago?", "answer": "Spanish for St. James", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Spanish for St. James", "sanit + iago, James"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Santiago is the Spanish and Portuguese language name for St", "extracted_entity": null}
{"question": "who has released an album or song named Santiago?", "answer": "The Chieftains; Loreena McKennitt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Chieftains; Loreena McKennitt", "The Chieftains and Loreena McKennitt"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "the artist known as Prince", "extracted_entity": null}
{"question": "what is the Order of Santiago?", "answer": "a Spanish knightly order", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a Spanish knightly order", "a Spanish Knightly order"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "the Order of Santiago is a Catholic religious order that", "extracted_entity": null}
{"question": "what notable people have been named Santiago?", "answer": "Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Benito Santiago, Hugo Santiago, Joey Santiago, Liliana Santiago, Rafael Santiago Maria, Santiago Caballero, Santiago Cabrera; James, son of Zebedee", "Benito, Hugo, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "There are two notable people who have been named Santiago", "extracted_entity": null}
{"question": "what fictional stories include a main character named Santiago?", "answer": "Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gears of War, Rainbow Six: Covert Operations Essentials, Death Foretold by Gabriel Garc\u00eda M\u00e1rquez, Ernest Hemingway's The Old Man and the Sea, the television series Babylon 5, Anne Rice's novel Interview with the Vampire, aulo Coelho's The Alchemist (novel), the television show Friday Night Lights", "Interview with the Vampire, The Alchemist, and others"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "The Alchemist", "extracted_entity": null}
{"question": "is Santiago the national capital of a country?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "Yes, Santiago is the capital of Chile", "extracted_entity": null}
{"question": "is santiago Spanish for St. James  ( from santo ,  saint  +  iago ,  James  )?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "o. Santiago is a contraction of San Tiago", "extracted_entity": null}
{"question": "is santiago Spanish for St. James?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "No. Santiago is a city in Chile, and", "extracted_entity": null}
{"question": "what is Santiago?", "answer": "Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Santiago is Spanish for St. James. It is also the name of places in Argentina, Brazil, Cape Verde, Chile, Colombia, Costa Rica, Cuba, the Dominican Republic, Ecuador, Guatemala, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, the Philippines, Portugal, Spain, the United States, Venezuela, and the Amazon. It is also the name of a baseball catcher, a film director, a guitarist, an economist, a footballer, a DJ, an actor, and James, son of Zebedee. Fictional people with this name include a character from Gears of War, a character from Chronicle of a Death Foretold, a character in Old Man and the Sea, a character in Babylon 5, a character in Interview with the Vampire, the main character of The Alchemist, and a character on Friday Night Lights. There has also been a battle of Santiago de Cuba, a couple of bands named Santiago, an Order of Santiago, and \"Santiago\" as a shortened form of the Reconquista battle cry."], "entity_annotations": ["Santiago"], "answer_entity_name": null, "predicted_answer": "he is a shepherd.", "extracted_entity": null}
{"question": "has swahili no diphthongs?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "no, but it has vowels that are pronounced as", "extracted_entity": null}
{"question": "is a third  prefix the object  prefix?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is      / ref Swahili unusual among sub-Saharan languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are vowels never reduced , regardless of stress ?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "es. The only time a vowel is reduced is", "extracted_entity": null}
{"question": "does the Australian Black Swan have white feathers on its wings?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t. It is a Black", "extracted_entity": null}
{"question": "do swans belong to the family Anatidae?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes, they belong to the family Anatidae", "extracted_entity": null}
{"question": "can black swans swim with only one leg?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No. They need two legs to swim.", "extracted_entity": null}
{"question": "what are young swans known as?", "answer": "cygnets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["cygnets", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "what is the Irish legend of the Children of Lir about?", "answer": "a stepmother transforming her children into swans for 900 years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a stepmother transforming her children into swans for 900 years", "A stepmother transforming her children into swans for 900 years"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "it is about four children who were turned into sw", "extracted_entity": null}
{"question": "what is the Sanskrit word for swan?", "answer": "hamsa or hansa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["hamsa or hansa", "Hamsa or hansa"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "hamsa", "extracted_entity": null}
{"question": "which album was the song \"The Bonny Swans\" from?", "answer": "The Mask and Mirror", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Mask and Mirror"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "\"The Bonny Swans\" is on the", "extracted_entity": null}
{"question": "what is the Polish word for swan?", "answer": "\u0141ab\u0119d\u017a", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["\u0141ab\u0119d\u017a"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "lebed\u017a.", "extracted_entity": null}
{"question": "which ballet by Pyotr Tchaikovsky is partially based on an Ancient German legend about a princess who was turned into a swan?", "answer": "Swan Lake", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swan Lake"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Swan Lake", "extracted_entity": null}
{"question": "are swan birds of the family Anatidae?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "do swans feature strongly in mythology?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "yes, they are associated with love and beauty.", "extracted_entity": null}
{"question": "is the Trumpeter Swan the largest bird?", "answer": "It is the largest bird of North America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is the largest bird of North America.", "The Trumpeter Swan is the largest bird of North America"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "No. The Trumpeter Swan is the largest native", "extracted_entity": null}
{"question": "what is the name of young swans?", "answer": "Cygnets.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cygnets.", "Cygnets"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "cygnets", "extracted_entity": null}
{"question": "where is the word \"swan\" derived from?", "answer": "Old English swan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old English swan.", "The Old English word \"swan,\" derived from the Indo-European root \"*swen\", meaning to sound or to sing"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "from the Latin \"swan\" which means \"", "extracted_entity": null}
{"question": "what is the best known story about a swan?", "answer": "The Ugly Duckling fable.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Ugly Duckling fable.", "The Ugly Duckling"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "the ugly duckling", "extracted_entity": null}
{"question": "are swans and ducks birds of the same family?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "Yes, swans and ducks are birds of the", "extracted_entity": null}
{"question": "what is the name of an adult female?", "answer": "Pen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pen.", "An adult female swan is called a pen."], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "a woman", "extracted_entity": null}
{"question": "what is the color of the Australian Black Swan?", "answer": "Completely black except for the white flight feathers on its wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Completely black except for the white flight feathers on its wings.", "Completely black except for the white feathers on its wings"], "entity_annotations": ["Swan"], "answer_entity_name": null, "predicted_answer": "white", "extracted_entity": null}
{"question": "is the Tiger a member of the Felidar family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes. The Tiger is a Felidar.", "extracted_entity": null}
{"question": "are tigers solitary animals?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Yes, tigers are solitary animals", "extracted_entity": null}
{"question": "are panthers a type of tiger?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "no, panthers are a type of leopard.", "extracted_entity": null}
{"question": "what do tigers' powerfully built legs and shoulders allow tigers to do?", "answer": "They have the ability to pull down prey substantially heavier than themselves.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They have the ability to pull down prey substantially heavier than themselves.", "have the ability to pull down prey substantially heavier than themselves"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "they allow tigers to move fast and jump high", "extracted_entity": null}
{"question": "what would a tiger do when seized by a crocodile?", "answer": "A tiger will strike at the reptile's eyes with its paws.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A tiger will strike at the reptile's eyes with its paws.", "strike at the reptile's eyes with its paws"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "it would try to escape.", "extracted_entity": null}
{"question": "what are the typical features of a tiger country?", "answer": "It will always have good cover, it will always be close to water and plenty of prey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It will always have good cover, it will always be close to water and plenty of prey.", "Good cover, close to water, and plenty of prey "], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "A tiger country is a country that is ruled by", "extracted_entity": null}
{"question": "would it be common for tigers to mate in January?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "Tigers usually mate in December, but it's possible", "extracted_entity": null}
{"question": "about how many square kilometers would be needed to house 3 female tigers?", "answer": "60 square kilometers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["60 square kilometers", "60"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "3 tigers in 3 acres =", "extracted_entity": null}
{"question": "would a tiger be likely to feedon smaller animals, such as mice?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Tiger"], "answer_entity_name": null, "predicted_answer": "yes, because the tiger is a carnivore", "extracted_entity": null}
{"question": "did the predecessors to trumpets have valves?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, they didn\u2019t. The valves on modern", "extracted_entity": null}
{"question": "are trumpets constructed of brass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no. trumpets are made of copper.", "extracted_entity": null}
{"question": "is the bass trumpet the smallest trumpet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, the smallest trumpet is the piccolo trumpet", "extracted_entity": null}
{"question": "what musical styles is the trumpet used in?", "answer": "Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ska, ska punk, classical, jazz, Rock, Blues, pop, polka, cuban music, mariachi and funk."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Jazz, Blues, Rock, Pop, R&B", "extracted_entity": null}
{"question": "what is the most common type of trumpet?", "answer": "The B trumpet.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The B trumpet."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the piccolo trumpet", "extracted_entity": null}
{"question": "how does a trumpet produce sound?", "answer": "By blowing air through closed lips.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["By blowing air through closed lips."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "when you blow into the trumpet, the air you", "extracted_entity": null}
{"question": "how do trumpets compensate for wolf tones?", "answer": "Somce trumpetes have a slide mechanism built in to compensate.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Somce trumpetes have a slide mechanism built in to compensate."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "they use a crook to make the instrument sharper", "extracted_entity": null}
{"question": "did Miles Davis play the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, he played the cornet", "extracted_entity": null}
{"question": "does a trumpet have a mellower tone than a cornet?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, they have the same tone.", "extracted_entity": null}
{"question": "is the name of the upper shell of a turtle called the plastron?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No."], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, it is called the carapace", "extracted_entity": null}
{"question": "do scutes fall away from the turtle's shell?", "answer": "Some of the scutes eventually fall away from the shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some of the scutes eventually fall away from the shell.", "yes, eventually"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, but not all of them.", "extracted_entity": null}
{"question": "do Terrestrial tortoises have short feet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, they have long legs.", "extracted_entity": null}
{"question": "how many groups are turtles broken down into?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two", "two"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "where are the only surviving giant tortoises?", "answer": "They are on the Seychelles and Gal\u00e1pagos Islands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["They are on the Seychelles and Gal\u00e1pagos Islands.", "Seychelles and Gal\u00e1pagos Islands"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Galapagos Islands", "extracted_entity": null}
{"question": "how do turtles chew food?", "answer": "Turtles use their jaws to cut and chew food.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles use their jaws to cut and chew food.", "Turtles use their jaws to cut and chew food"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "they can't chew they just swallow food whole", "extracted_entity": null}
{"question": "are tortoises land based?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, they are land based", "extracted_entity": null}
{"question": "where are turtle eggs layed?", "answer": "Turtles lay eggs on land.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turtles lay eggs on land.", "on land"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "on the beach", "extracted_entity": null}
{"question": "is turtle soup considered a delicacy?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "are Testudines the crown group of the superorder Chelonia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, Testudines is the crown group of", "extracted_entity": null}
{"question": "do turtles breathe air?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "no, they breathe water, they have gills", "extracted_entity": null}
{"question": "are harvesting wild turtles legal anywhere?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "Yes, in some areas. There are some places", "extracted_entity": null}
{"question": "approximately how many species of Testudines are alive today?", "answer": "300", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["300"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "300.", "extracted_entity": null}
{"question": "where is harvesting wild turtles legal?", "answer": "Florida", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Florida"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "on private property, with landowner permission", "extracted_entity": null}
{"question": "what was the largest ever chelonian?", "answer": "Archelon ischyros", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Archelon ischyros", "The great letherback sea tutrtle"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "the largest chelonian was a giant tortoise", "extracted_entity": null}
{"question": "is there a way to approximate the age of a turtle?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "There is a way to approximate the age of a", "extracted_entity": null}
{"question": "can turtles spend all their time underwater?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Turtle"], "answer_entity_name": null, "predicted_answer": "No, they need to come up for air.", "extracted_entity": null}
{"question": "are violinists and fiddlers the same thing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, not really. violinists are the people", "extracted_entity": null}
{"question": "are violins a single size?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, they are made in different sizes", "extracted_entity": null}
{"question": "are violas and cellos in the same family of instruments as violins?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes. they are all members of the violin family", "extracted_entity": null}
{"question": "where does the word \"violin\" come from?", "answer": "the Middle Latin word vitula, meaning \"stringed instrument\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the Middle Latin word vitula, meaning \"stringed instrument\"", "the Middle Latin word vitula"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "from the Italian word \"violino\"", "extracted_entity": null}
{"question": "what is someone who makes violins called?", "answer": "a luthier, or simply a violin maker", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a luthier, or simply a violin maker", "a luthier"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a luthier", "extracted_entity": null}
{"question": "how many strings does a violin usually have?", "answer": "four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["four", "4"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "4", "extracted_entity": null}
{"question": "what are violins made of?", "answer": "different types of wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["different types of wood", "maple, ebony, sheep gut"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "violins are made of wood", "extracted_entity": null}
{"question": "how long have people been making instruments like violins?", "answer": "since ancient times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["since ancient times", "since 1555"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "around 500 years.", "extracted_entity": null}
{"question": "whate is the usual pitch range of a violin?", "answer": "from G3 to C8", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["from G3 to C8"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the usual pitch range of a violin is 4", "extracted_entity": null}
{"question": "are xylophone bars made of rosewood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is the xylophone a precursor to the vibraphone?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "no. The xylophone is a percussion instrument", "extracted_entity": null}
{"question": "were ancient mallets made of copper?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes, but only the handle. The head was", "extracted_entity": null}
{"question": "where did the xylophone originate?", "answer": "Indonesia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Indonesia"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Central Africa", "extracted_entity": null}
{"question": "what is the earliest historical reference in Europe?", "answer": "Arnold Schlick's Spiegel der Orgelmacher und Organisten", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arnold Schlick's Spiegel der Orgelmacher und Organisten"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "1000 years before Christ", "extracted_entity": null}
{"question": "how are western-style xylophones characterised?", "answer": "by a bright, sharp tone and high register", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by a bright, sharp tone and high register"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "they have a series of bars, each of which", "extracted_entity": null}
{"question": "can a xylophone be 3 octaves?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "yes, but it would be very long", "extracted_entity": null}
{"question": "can a short bar follow a long bar?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "No, because the long bar has already been played", "extracted_entity": null}
{"question": "did vibraphones exist in 1930?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes. They had been around since the 192", "extracted_entity": null}
{"question": "was Alessandro Volta a professor of chemistry?", "answer": "Alessandro Volta was not a professor of chemistry.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was not a professor of chemistry.", "No"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, he was a professor of physics", "extracted_entity": null}
{"question": "did Alessandro Volta invent the remotely operated pistol?", "answer": "Alessandro Volta did invent the remotely operated pistol.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta did invent the remotely operated pistol.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, he invented the battery.", "extracted_entity": null}
{"question": "was Alessandro Volta taught in public schools?", "answer": "Volta was taught in public schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was taught in public schools.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no. He was a physicist, the inventor of", "extracted_entity": null}
{"question": "who did Alessandro Volta marry?", "answer": "Alessandro Volta married Teresa Peregrini.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta married Teresa Peregrini.", "Teresa Peregrini"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Maria Pellegrina Grassi", "extracted_entity": null}
{"question": "what did Alessandro Volta invent in 1800?", "answer": "In 1800, Alessandro Volta invented the voltaic pile.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800, Alessandro Volta invented the voltaic pile.", "voltaic pile"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "the battery", "extracted_entity": null}
{"question": "what is the battery made by Alessandro Volta credited as?", "answer": "The battery made by Volta is credited as the first electrochemical cell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The battery made by Volta is credited as the first electrochemical cell.", "the first electrochemical cell"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The Voltaic Pile", "extracted_entity": null}
{"question": "did Alessandro Volta die and retire in the same place?", "answer": "Alessandro Volta retired and died in the same place.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta retired and died in the same place.", "Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, Alessandro Volta died in Como,", "extracted_entity": null}
{"question": "when did Alessandro Volta improve  and popularize the electrophorus?", "answer": "Alessandro Volta improved and popularized the electrophorus in 1775.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta improved and popularized the electrophorus in 1775.", "1775"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta improved and popularized the elect", "extracted_entity": null}
{"question": "how long was Alessandro Volta a professor at the University of Pavia?", "answer": "Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Alessandro Volta was a professor at the University of Pavia for almost 25 years.", "almost 25 years"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1804-1823 (19 years)", "extracted_entity": null}
{"question": "was Alessandro Volta an Egyptian?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, he was Italian.", "extracted_entity": null}
{"question": "was Volta taught in public schools?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. The first public school in the United States", "extracted_entity": null}
{"question": "was Volta made a count in 1810?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, Volta was not made a count in", "extracted_entity": null}
{"question": "who made Volta a count?", "answer": "Napoleon", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Napoleon"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Napoleon Bonaparte.", "extracted_entity": null}
{"question": "where was Volta born?", "answer": "Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Como", "Como, Italy"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Como, Italy", "extracted_entity": null}
{"question": "when did Volta retire?", "answer": "1819", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1819", "In 1819."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1836", "extracted_entity": null}
{"question": "a year before improving and popularizing the electrophorus, what did Volta become?", "answer": "A professor of physics at the Royal School in Como", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A professor of physics at the Royal School in Como"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "a professor of physics", "extracted_entity": null}
{"question": "was Volta buried where he died or was he buried someplace else?", "answer": "where he died", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["where he died", "Yes."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "he died in Pavia, but was buried in", "extracted_entity": null}
{"question": "did Volta marry before he became professor of experimental physics at the University of Pavia?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No. He married after he became professor of experimental", "extracted_entity": null}
{"question": "what happened in 1810?", "answer": "Volta was made a count by Napoleon.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "The first Spanish colony in America was established in Florida", "extracted_entity": null}
{"question": "wasn't Alessandro Volta born in Como?", "answer": "Yes, Volta was born in Como, Italy and was taught in the public schools there.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was born in Como, Italy and was taught in the public schools there."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in 1745 in", "extracted_entity": null}
{"question": "was Alessandro Volta born in Como?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "who became a professor of physics at the Royal School in Como?", "answer": "Volta.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "Alessandro Volta, the inventor of the battery", "extracted_entity": null}
{"question": "when was Volta made a count by Napoleon?", "answer": "Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1809 (he was made a count in", "extracted_entity": null}
{"question": "is Volta's legacy celebrated by a Temple on the shore of Lake Como in the center of the town?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "no, the Temple is a monument to the memory", "extracted_entity": null}
{"question": "is the battery made by Volta credited as the first electrochemical cell?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "No, it was made by Alessandro Volta", "extracted_entity": null}
{"question": "was Volta made a count by Napoleon in 1810?", "answer": "Yes, Volta was made a count by Napoleon in 1810.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Volta was made a count by Napoleon in 1810."], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "yes, he was a count.", "extracted_entity": null}
{"question": "in what year did he become a professor of physics at the Royal School in Como?", "answer": "1774", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1774"], "entity_annotations": ["Alessandro_Volta"], "answer_entity_name": null, "predicted_answer": "1802", "extracted_entity": null}
{"question": "was Avogadro a  professor at the University of Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avogadro was a professor at the University of Turin."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was he a member of the Royal Superior Council on Public Instruction?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes,  Avogadro was a member of the Royal Superior Council on Public Instruction."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes he was.", "extracted_entity": null}
{"question": "is Avogadro's number used to compute the results of chemical reactions?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Avagadro's number is used to compute the results of chemical reactions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Yes, Avogadro's number is used to", "extracted_entity": null}
{"question": "who first calculated the value of Avogadro's number?", "answer": "Johann Josef Loschmidt", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Johann Josef Loschmidt", "Johann Josef Loschmidt first calculated the value of Avogadro's number."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Amedeo Avogadro", "extracted_entity": null}
{"question": "what does Avogadro's Law state?", "answer": "The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights", "Avogadro's Law states that the relationship between the masses of the same volume of different gases (at the same temperature and pressure) corresponds to the relationship between their respective molecular weights."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "At a given temperature and pressure, the volume of", "extracted_entity": null}
{"question": "who showed that Avogadro's theory held in dilute solutions?", "answer": "Jacobus Henricus van Hoff", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacobus Henricus van Hoff", "Jacobus Henricus van 't Hoff showed that Avogadro's theory holds in dilute solutions."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Arrhenius", "extracted_entity": null}
{"question": "in what language was his 1811 paper published?", "answer": "French", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["French", "Avogadro's 1811 paper was published in French."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "French", "extracted_entity": null}
{"question": "who was Avogadro's wife?", "answer": "Felicita Mazz", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Felicita Mazz", "Felicita Mazz\u00e9 was Avogadro's wife."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "She was a chemist", "extracted_entity": null}
{"question": "why did Avogadro lose his chair at the University of Turin?", "answer": "He was active in the revolutionary movements of 1821 against the king of Sardinia", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He was active in the revolutionary movements of 1821 against the king of Sardinia", "Avogadro lost his chair at the University of Turin because he was active in the revolutionary movements of 1821 against the king of Sardinia."], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "he was a supporter of the French Revolution and was", "extracted_entity": null}
{"question": "was Amedeo Avogadro was born in Turin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes, he was born in Turin.", "extracted_entity": null}
{"question": "is Avogadro hailed as a founder of the atomic-molecular theory?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Johann Josef Loschmidt first calculate the value of Avogadro's number?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, Avogadro's number is named after", "extracted_entity": null}
{"question": "in 1820, Avogadro became a professor of physics where?", "answer": "University of Turin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["University of Turin"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Turin.", "extracted_entity": null}
{"question": "avogadro did not actually use what word?", "answer": "atom", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["atom"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "mole", "extracted_entity": null}
{"question": "the number of elementary entities in 1 mole of a substance is known as what?", "answer": "Avogadro constant", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Avogadro constant", "Avogadro's constant"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "Avogadro's number", "extracted_entity": null}
{"question": "what would a German call Avogadro's number?", "answer": "Loschmidt number", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loschmidt number"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "the \"Avogadro number\" (the German", "extracted_entity": null}
{"question": "is Amedeo Avogadro Italian?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "No, he was Italian.", "extracted_entity": null}
{"question": "did he become a professor before the revolutionary movements against the king of Sardinia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Amedeo_Avogadro"], "answer_entity_name": null, "predicted_answer": "no, he became a professor after the revolutionary movements", "extracted_entity": null}
{"question": "do ants belong to the Hymenoptera order?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, ants belong to the Hymenoptera", "extracted_entity": null}
{"question": "are ants used in cuisine?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, ants are used in cuisine. In Mexico", "extracted_entity": null}
{"question": "does an ant's head contain sensory organs?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, it does. The ant's head is", "extracted_entity": null}
{"question": "how do most ants travel?", "answer": "most ants travel by walking", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["most ants travel by walking", "by walking"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "they fly.", "extracted_entity": null}
{"question": "in ant colonies, what are the fertile female ants called?", "answer": "queens", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["queens"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "queen ants", "extracted_entity": null}
{"question": "who wrote about ants in A Tramp Abroad?", "answer": "mark twain wrote about ants", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mark twain wrote about ants", "Mark Twain"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Mark Twain", "extracted_entity": null}
{"question": "do the ants eat plants, meats, or both?", "answer": "both", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["both"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "plants", "extracted_entity": null}
{"question": "what organs gives a bull ant its good sight?", "answer": "compound eyes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["compound eyes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "The eyes of a bull ant are located on the", "extracted_entity": null}
{"question": "what may happen to red fire ants if we use boiling water on the queen?", "answer": "nests of red fire ants may be destroyed", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["nests of red fire ants may be destroyed", "die"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "the queen will die and the fire ants will die", "extracted_entity": null}
{"question": "are ants found in Antartica?", "answer": "No, there are no ants in antarctica.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, there are no ants in antarctica."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No. They don't like cold weather.", "extracted_entity": null}
{"question": "do male ants take flight before females?", "answer": "Yes, male ants take flight before females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, male ants take flight before females."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no, males don\u2019t fly at all", "extracted_entity": null}
{"question": "do worker ants have wings?", "answer": "No, worker ants do not have wings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, worker ants do not have wings."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, they have wings, but only when they", "extracted_entity": null}
{"question": "what are the three segments of an ant?", "answer": "The head, mesosoma and metasoma are the three distinct body segments.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The head, mesosoma and metasoma are the three distinct body segments."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "head, thorax, abdomen", "extracted_entity": null}
{"question": "what are the ant colonies that lack queens called?", "answer": "Colonies that lack queens are called gamergate colonies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Colonies that lack queens are called gamergate colonies."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Queenless colonies", "extracted_entity": null}
{"question": "where are bullet ants located?", "answer": "Bullet ants are located in Central and South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bullet ants are located in Central and South America."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Central and South America", "extracted_entity": null}
{"question": "do ants belong to the same order as bees?", "answer": "Yes, ants belong to the same order as bees.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, ants belong to the same order as bees."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No. Bees belong to the order Hym", "extracted_entity": null}
{"question": "what is the most common color of ants?", "answer": "Most ants are red or black.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Most ants are red or black."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "brown", "extracted_entity": null}
{"question": "do all ants build nests?", "answer": "No, not all ants build nests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, not all ants build nests."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no, some species don't build nests", "extracted_entity": null}
{"question": "ultraviolet vision was first discovered in ants by Sir who in 1881?", "answer": "John Lubbok", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John Lubbok"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Sir John Lubbock", "extracted_entity": null}
{"question": "do ants thrive in most ecosystems?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, ants thrive in most ecosystems", "extracted_entity": null}
{"question": "are several species of ants able to use the Earth's magnetic field?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, several species of ants are able to use", "extracted_entity": null}
{"question": "are termites actually more closely related to cockroaches as well as mantids?", "answer": "Yes, termites are actually more closely related to cockroaches as well as mantids.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, termites are actually more closely related to cockroaches as well as mantids."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "Yes, termites are closely related to cockro", "extracted_entity": null}
{"question": "do some caterpillars produce vibrations as well as sounds?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes, some caterpillars produce vibrations as well", "extracted_entity": null}
{"question": "did ants evolve from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "No, the ancestor of ants is thought to have", "extracted_entity": null}
{"question": "is the ant a marsupial?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "no, it's a mammal.", "extracted_entity": null}
{"question": "are they easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist?", "answer": "Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, they are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist."], "entity_annotations": ["Ant"], "answer_entity_name": null, "predicted_answer": "yes they are", "extracted_entity": null}
{"question": "is Antwerp a city?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is Antwerp a municipality?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Yes, it is. It is the capital of", "extracted_entity": null}
{"question": "is Antwerp in Belgium?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "yes, Antwerp is in Belgium.", "extracted_entity": null}
{"question": "what is Antwerp?", "answer": "Antwerp is a city and municipality in Belgium.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is a city and municipality in Belgium."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "A city in Belgium", "extracted_entity": null}
{"question": "what is the population of the city of Antwerp?", "answer": "Antwerp's population is 472,071.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp's population is 472,071."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "1,000,000", "extracted_entity": null}
{"question": "where is the city of Antwerp?", "answer": "Antwerp is in Belgium", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is in Belgium"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is in Belgium", "extracted_entity": null}
{"question": "why is Antwerp important to fashion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Antwerp is important because it is a city that", "extracted_entity": null}
{"question": "antwerp is to the east of what river?", "answer": "Antwerp is to the east of the Scheldt river", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antwerp is to the east of the Scheldt river"], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "Scheldt river", "extracted_entity": null}
{"question": "how many municipalities does Antwerp have?", "answer": "None, Antwerp is a municipality.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["None, Antwerp is a municipality."], "entity_annotations": ["Antwerp"], "answer_entity_name": null, "predicted_answer": "103.", "extracted_entity": null}
{"question": "is Arabic a Central Semitic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "was Arabic a major vehicle of culture in Europe?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Arabic was the major vehicle of culture in Europe.", "extracted_entity": null}
{"question": "is Arabic classified alongside Semitic languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "how many people speak the Arabic language?", "answer": "280 million people.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["280 million people."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "About 200 million people", "extracted_entity": null}
{"question": "when was Arabic calligraphy invented?", "answer": "Many styles were developed after 786.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many styles were developed after 786."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "6th century AD", "extracted_entity": null}
{"question": "where is Arabic spoken?", "answer": "The Middle East and North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Middle East and North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Arab world and in the world.", "extracted_entity": null}
{"question": "why is Arabic related to Islam?", "answer": "Arabic is the liturgical language of Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic is the liturgical language of Islam"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "because Islam is an Arabic language, and it is", "extracted_entity": null}
{"question": "why does Arabic heavily influence European languages?", "answer": "Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Arabic was a major vehicle of culture in Europe, and the Arab and European civilizations are geographically close."], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "because Arabs conquered most of Europe and brought their language", "extracted_entity": null}
{"question": "is Arabic the largest member of the Semitic language family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, Arabic is the second largest member of the", "extracted_entity": null}
{"question": "does Modern Standard Arabic continue to evolve like other languages?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "yes, it does. Modern Standard Arabic is the", "extracted_entity": null}
{"question": "is Hassaniya Arabic spoken in Mauritania?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "No, it is not spoken in Mauritania.", "extracted_entity": null}
{"question": "what is the only variety of modern Arabic that has acquired official language status?", "answer": "maltese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["maltese", "Maltese"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "Egyptian Arabic (although it is not the only variety", "extracted_entity": null}
{"question": "egyptian Arabic is spoken by how many in Egypt?", "answer": "76 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["76 million"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "85% of the population.", "extracted_entity": null}
{"question": "where are the Western Arabic numerals used?", "answer": "North Africa", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["North Africa", "present-day North Africa"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "in the Arabic-speaking world, and in some other", "extracted_entity": null}
{"question": "the most active Academies of Arabic Language are found where?", "answer": "damascus and cairo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["damascus and cairo"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "the most active Academies of Arabic Language are found", "extracted_entity": null}
{"question": "hassan Massoudy is a master of what genre?", "answer": "Hassan Massoudy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hassan Massoudy", "Arabic calligraphy"], "entity_annotations": ["Arabic_language"], "answer_entity_name": null, "predicted_answer": "he is a master of calligraphy", "extracted_entity": null}
{"question": "is Berlin the capital city of Germany?", "answer": "Berlin is the capital city of Germany.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is the capital city of Germany.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no, Berlin is the largest city in Germany,", "extracted_entity": null}
{"question": "is Berlin the largest city in Germany?", "answer": "Berlin is Germany's largest city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Berlin is Germany's largest city.", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No, Hamburg is bigger.", "extracted_entity": null}
{"question": "is Schloss Charlottenburg the largest existing palace in Berlin?", "answer": "Schloss Charlottenburg is the largest existing palace in Berlin. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Schloss Charlottenburg is the largest existing palace in Berlin. ", "yes"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no, Schloss Charlottenburg is the largest", "extracted_entity": null}
{"question": "which building is the site of the German parliament?", "answer": "The Reichstag building is the site of the German parliament. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building is the site of the German parliament. ", "The Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "The Reichstag", "extracted_entity": null}
{"question": "which two sports events did the Olympiastadion host?", "answer": "The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Olympiastadion hosted the 1936 Summer Olympics and the 2006 FIFA World Cup final. ", "the 1936 Summer Olympics and the 2006 FIFA World Cup final"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the 1936 Summer Olympics and the 200", "extracted_entity": null}
{"question": "where is the Berliner Dom located?", "answer": "The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Berliner Dom is located on the Spree Island across from the site of the Berliner Stadtschloss and adjacent to the Lustgarten.", "on the Spree Island"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "on Museum Island in Berlin, Germany", "extracted_entity": null}
{"question": "which Berlin building did architect Norman Foster remodel in the 1990s?", "answer": "The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Reichstag building was remodeled by architect Norman Foster in the 1990s.", "Reichstag building"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the Reichstag (parliament building)", "extracted_entity": null}
{"question": "why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?", "answer": "Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because burning lignite produces harmful emissions, Vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources.", "Because burning lignite produces harmful emissions"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Vattenfall is a major energy company that operates", "extracted_entity": null}
{"question": "what are the names of the two zoos in Berlin?", "answer": "The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The two zoos in Berlin are the Zoologischer Garten Berlin and the Tierpark Friedrichsfelde.", "Zoologischer Garten Berlin and Tierpark Friedrichsfelde"], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Zoologischer Garten Berlin and Tierpark Berlin", "extracted_entity": null}
{"question": "is Berlin the headquarters of Springer?", "answer": "Yes, Berlin is the headquarters of Springer.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Berlin is the headquarters of Springer."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no, the headquarters of Springer is in Heidelberg", "extracted_entity": null}
{"question": "does the Gendarmenmarkt border the French Cathedral?", "answer": "Yes, the Gendarmenmarkt borders the French Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the Gendarmenmarkt borders the French Cathedral."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "no, it is the other way around", "extracted_entity": null}
{"question": "is the Nauen plain north of Berlin?", "answer": "No, the Nauen plain stretches to the west of Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, the Nauen plain stretches to the west of Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "No, it is located north of Potsdam", "extracted_entity": null}
{"question": "where is the bust of Queen Nefertiti?", "answer": "The bust of Queen Nefertiti is in the Altes Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The bust of Queen Nefertiti is in the Altes Museum."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "in the Egyptian Museum in Berlin", "extracted_entity": null}
{"question": "where does the German President live?", "answer": "The German President lives west of the center, Schloss Bellevue.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The German President lives west of the center, Schloss Bellevue."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "the German President lives in the Berliner Schloss", "extracted_entity": null}
{"question": "when did Berlin give up its status as a free Hanseatic city?", "answer": "In 1451 Berlin gave up its status as a free Hanseatic city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1451 Berlin gave up its status as a free Hanseatic city."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "when the Prussian king Frederick William I of Pr", "extracted_entity": null}
{"question": "which is the busiest airport in Berlin?", "answer": "Tegel International Airport is the busiest airport in Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tegel International Airport is the busiest airport in Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Tegel.", "extracted_entity": null}
{"question": "how many occupation zones was Germany divided into after the war?", "answer": "Germany was divided into four occupation zones after the war.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany was divided into four occupation zones after the war."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "4 (the US, the UK, France", "extracted_entity": null}
{"question": "where was Popkomm held before moving to Berlin?", "answer": "Popkomm was held in Cologne before moving to Berlin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Popkomm was held in Cologne before moving to Berlin."], "entity_annotations": ["Berlin"], "answer_entity_name": null, "predicted_answer": "Cologne", "extracted_entity": null}
{"question": "who was Blaise Pascal's father?", "answer": "His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father, \u00c3\u2030tienne Pascal (1588\u00e2\u20ac\u201c1651), who also had an interest in science and mathematics, was a local judge", "\u00c3\u2030tienne Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Etienne Pascal", "extracted_entity": null}
{"question": "was Pascal a French mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "did Pascal have poor health throughout his life?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "how old was Pascal when he lost his mother?", "answer": "at the age of three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at the age of three", "3."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "3 years and 8 months.", "extracted_entity": null}
{"question": "who was Pascal's younger sister?", "answer": "Jacqueline", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jacqueline", "Jacqueline."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "Jacqueline", "extracted_entity": null}
{"question": "what led Pascal to his religious conversion?", "answer": "Two basic influences led him to his conversion: sickness and Jansenism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two basic influences led him to his conversion: sickness and Jansenism", "sickness and Jansenism"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the death of his sister.", "extracted_entity": null}
{"question": "how old was Pascal when he died?", "answer": "39", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["39", "1662-08-19."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "39 years old.", "extracted_entity": null}
{"question": "did Pascal write about cycloid before 1658?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "No. He wrote about cycloid in 165", "extracted_entity": null}
{"question": "is there a programming language called Pascal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes, there is a programming language called Pascal.", "extracted_entity": null}
{"question": "crowds of believers came to see and kiss the thorn; all of Catholic where acclaimed a miracle?", "answer": "Paris", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "a miracle that was not a miracle", "extracted_entity": null}
{"question": "wasn't Blaise Pascal a work of Desargues on conic sections?", "answer": "Yes, it was", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it was"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "yes. He wrote that Desargues' theorem", "extracted_entity": null}
{"question": "who died the next morning?", "answer": "Pascal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pascal."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "the doctor", "extracted_entity": null}
{"question": "when did his father die?", "answer": "1651", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1651", "His father died in 1651."], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "in 1963", "extracted_entity": null}
{"question": "was he a child prodigy who was educated by his father, a civil servant?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "no, he was not a child prodigy.", "extracted_entity": null}
{"question": "was Pascal's earliest work in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": null}
{"question": "did Pascal also write in defense of the scientific method?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Blaise_Pascal"], "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "do butterflies make sounds?", "answer": "Some butterflies make sounds.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Some butterflies make sounds."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes. They make clicking sounds, especially when they", "extracted_entity": null}
{"question": "do butterflies have two eyes?", "answer": "Yes, butterflies have two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, butterflies have two eyes."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "yes, butterflies have two eyes", "extracted_entity": null}
{"question": "does a black moth mean that someone has died?", "answer": "In the Philippines a black butterfly or moth mean that someone has died.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Philippines a black butterfly or moth mean that someone has died."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "No, it does not mean that someone has died", "extracted_entity": null}
{"question": "what is the outer layer of the cuticle made of?", "answer": "The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The outer layer of the cuticle is made of of a mixture of chitin and specialized proteins."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "the outer layer of the cuticle is made of", "extracted_entity": null}
{"question": "where was there a vast swarm of butterflies?", "answer": "In Kyoto there was a vast swarm of butterflies.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Kyoto there was a vast swarm of butterflies."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "in a forest in Mexico", "extracted_entity": null}
{"question": "what butterfly is migratory?", "answer": "The Monarch butterfly is migratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Monarch butterfly is migratory."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "Monarch butterfly", "extracted_entity": null}
{"question": "do butterflies carry more pollen than Hymenoptera?", "answer": "No, butterflies carry less pollen than Hymenoptera.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, butterflies carry less pollen than Hymenoptera."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "butterflies carry more pollen than Hymenoptera", "extracted_entity": null}
{"question": "what allows the butterfly to evade predators?", "answer": "A butterfly's hind wings allow the butterfly to evade predators.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A butterfly's hind wings allow the butterfly to evade predators."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "camouflage and mimicry", "extracted_entity": null}
{"question": "what happens if a wing is removed?", "answer": "If a ring is removed the other three will grow to a larger size.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["If a ring is removed the other three will grow to a larger size."], "entity_annotations": ["Butterfly"], "answer_entity_name": null, "predicted_answer": "the plane crashes, and if the pilot is not", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb a Spanish Biologist?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, he was a French physicist."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No. He was a French physicist.", "extracted_entity": null}
{"question": "was the SI unit of charge named after Charles-Augustin de Coulomb?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, the SI unit of charge, the coulomb, was named after him."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, it was named after Georg Simon Ohm", "extracted_entity": null}
{"question": "was Charles-Augustin de Coulomb ever employed at La Rochelle?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, upon his return to France, with the rank of Captain, he was employed at La Rochelle."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, he was a member of the Royal Academy", "extracted_entity": null}
{"question": "what is Charles-Augustin de Coulomb best known for?", "answer": "He is best known for developing Coulomb's law.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He is best known for developing Coulomb's law."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Coulomb\u2019s Law", "extracted_entity": null}
{"question": "whose ideas inspired Charles-Augustin de Coulomb's experiments on the resistance of masonries?", "answer": "Pieter van Musschenbroek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pieter van Musschenbroek.", "He carried out several experiments on the resistance of masonries which were inspired by the ideas of Pieter van Musschenbroek on friction."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Pierre Bouguer", "extracted_entity": null}
{"question": "when did Charles-Augustin de Coulomb join his father's family in Montpeillier?", "answer": "From 1757 to 1759 he joined his father's family in Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["From 1757 to 1759 he joined his father's family in Montpellier."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "in 1781", "extracted_entity": null}
{"question": "did Charles-Augustin de Coulomb come from a poor family?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, his mother came from a wealthy family in the wool trade."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no, he was born in a wealthy family.", "extracted_entity": null}
{"question": "what is Coulomb's law?", "answer": "The definition of the electrostatic force of attraction and repulsion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The definition of the electrostatic force of attraction and repulsion."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "The force between two charges is directly proportional to the", "extracted_entity": null}
{"question": "where is Coll\u00e8ge des Quatre-Nations located?", "answer": "In Montpellier.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Montpellier.", "In Paris."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "In the 5th arrondissement, on", "extracted_entity": null}
{"question": "what happened in 1785?", "answer": "Coulomb presented his three reports on Electricity and Magnetism", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb presented his three reports on Electricity and Magnetism"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the french revolution began", "extracted_entity": null}
{"question": "what happened in 1757 to 1759?", "answer": "Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb joined his father's family in Montpellier and took part in the work of the academy of the city."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Seven Years War", "extracted_entity": null}
{"question": "regarding this topic, what did the Revolution do?", "answer": "The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Revolution caused Coulomb to resign his appointment as intendent des eaux et fontains and retire."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "the Revolution did not do anything, it was a", "extracted_entity": null}
{"question": "who determined to pursue mathematics?", "answer": "Charles-Augustin de Coulomb", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Charles-Augustin de Coulomb"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "I was in 8th grade and I was", "extracted_entity": null}
{"question": "when did Coulomb return to Paris?", "answer": "In 1759.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1759."], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "1778", "extracted_entity": null}
{"question": "does Coulomb leave a legacy as a pioneer in the field of geotechnical engineering for his contribution to retaining wall design?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he does. He is the first to", "extracted_entity": null}
{"question": "did the family move to Paris as well as there Coulomb studied at the prestigious Collge des Quatre-Nations?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "No, the family moved to Paris as well as", "extracted_entity": null}
{"question": "was Coulomb born in Angoulme, France to a well-to-do family?", "answer": "Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Coulomb was born in Angoul\u00c3\u00aame, France, to a well-to-do family"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is he best known for developing Coulomb's law, the definition of the electrostatic force of attraction and repulsion?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "Charles Augustin de Coulomb", "extracted_entity": null}
{"question": "the SI unit of charge, the coulomb, was named after him?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "A. Ampere", "extracted_entity": null}
{"question": "did Coulomb die in Angoul\u00c3\u00aame, France, to a well-to-do family?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Charles-Augustin_de_Coulomb"], "answer_entity_name": null, "predicted_answer": "no, he was born in Angoul\u00c3\u00aa", "extracted_entity": null}
{"question": "are all spoken varieties of Chinese tonal and analytical?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "No. The Cantonese variety of Chinese is not", "extracted_entity": null}
{"question": "is fifty percent or more of Korean vocabulary of Chinese origin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Yes. The answer is 50%.", "extracted_entity": null}
{"question": "does \"ketchup\" come from the Minnan pronunciation of \u9bad\u6c41 (koe-tsiap)?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, it does. The word \"ketchup", "extracted_entity": null}
{"question": "what is the most common romanization standard for Standard Mandarin today?", "answer": "Hanyu Pinyin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hanyu Pinyin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "hanyu pinyin, but many people use", "extracted_entity": null}
{"question": "what is the official standard language used by the People's Republic of China, the Republic of China, and Singapore (where it is called \"Huayu\")?", "answer": "Putonghua / Guoyu, often called \"Mandarin\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Putonghua / Guoyu, often called \"Mandarin\"", "Standard Mandarin"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Mandarin Chinese (or \"Guoyu\")", "extracted_entity": null}
{"question": "how many head entries for character definitions does the Zhonghua Zihai \u4e2d\u534e\u5b57\u6d77 (1994) contain?", "answer": "85,568 head entries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["85,568 head entries", "54,678"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "40,000.", "extracted_entity": null}
{"question": "what does the PRC government classify as literacy amongst workers?", "answer": "knowledge of 2,000 characters", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["knowledge of 2,000 characters", "a knowledge of 2000 characters"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "\u201cLiteracy\u201d is defined as the ability to", "extracted_entity": null}
{"question": "how is South China linguistically different from North China?", "answer": "South China displays more linguistic diversity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["South China displays more linguistic diversity"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "The dialects in South China are not mutually intellig", "extracted_entity": null}
{"question": "\u5348\u9910\u76d2 \u201clunchbox or boxed lunch\u201d (from bento) and \u6599\u7406 \u201cprepared cuisine\u201d are loan words from which language?", "answer": "Japanese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Japanese", "Chinese"], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Japanese. They are actually loan words from the Japanese", "extracted_entity": null}
{"question": "what is the standardized form of spoken chinese?", "answer": "The standardized form of spoken Chinese is Standard Mandarin.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The standardized form of spoken Chinese is Standard Mandarin.", "The standardized from of spoken Chinese is Standard Mandarin."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "putonghua (common speech, the official form", "extracted_entity": null}
{"question": "is diglossia a common feature in mainland China and Taiwan?", "answer": "Yes, diglossia is a common feature in mainland China and Taiwan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, diglossia is a common feature in mainland China and Taiwan."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "yes, diglossia is a common feature in", "extracted_entity": null}
{"question": "do linguists often view Chinese as a language family?", "answer": "Yes, linguists often view Chinese as a language family.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, linguists often view Chinese as a language family."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "no, it is not a language family", "extracted_entity": null}
{"question": "when did classical Chinese literature begin?", "answer": "Classical Chinese literature began in the Spring and Autumn period.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Classical Chinese literature began in the Spring and Autumn period.", "Classical Chinese literature began in the 14th to 11th centuires BCE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "when did classical Chinese literature begin?", "extracted_entity": null}
{"question": "how does the government of the People's Republic of China often refer to all divisions of the Sinitic language(s) besides Standard Mandarin?", "answer": "The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The government of the People's Republic of China refers to all divisions of the Sinitic language(s) besides Standard Mandarin as fangyan.", "The PRC commonly refers to all divisons of the Sinitic language(s) beside Standard Mandarin as fangyan, or \"regional tongues.\""], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "the \"national languages\"", "extracted_entity": null}
{"question": "when was Middle Chinese used?", "answer": "Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Middle Chinese was used during Southern and Northern Dynasties and the Sua, Tang, and Sang dynasties.", "Middle Chinese was the language used in the 6th through 1-th centuries CE."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "it was used from 220 to 589", "extracted_entity": null}
{"question": "how is the pronunciation of foreign words determined?", "answer": "The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pronunciation of foreign words is done by employing Chinese characters with similar pronuncations.", "Foreign words are transliterated into close-match existing Chinese morphemes."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "according to the rules of the language that the word", "extracted_entity": null}
{"question": "what is the total number of Chinese words?", "answer": "The Hanyu Da Zidian includes 54,678 head entries for characters.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Hanyu Da Zidian includes 54,678 head entries for characters."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "There are about 400,000 Chinese characters.", "extracted_entity": null}
{"question": "what language was used in the Zhou and Southern Dynasties?", "answer": "Old Chinese was the language common during the early and middle Zhou Dynasty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Old Chinese was the language common during the early and middle Zhou Dynasty."], "entity_annotations": ["Chinese_language"], "answer_entity_name": null, "predicted_answer": "Chinese", "extracted_entity": null}
{"question": "is a cougar also known as a mountain lion?", "answer": "A cougar is also known as a mountain lion.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A cougar is also known as a mountain lion.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "yes, a cougar is also known as a mountain", "extracted_entity": null}
{"question": "are cougars larger than jaguars?", "answer": "Cougars are not larger than jaguars.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cougars are not larger than jaguars.", "no"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "no, jaguars are larger than cougars", "extracted_entity": null}
{"question": "does the World Conservation Union consider the cougar a \"least concern\" species?", "answer": "The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The World Conservation Union (IUCN) currently lists the cougar as a \"least concern\" species.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "No. The IUCN Red List of Threat", "extracted_entity": null}
{"question": "what are some of the cougar's primary food sources?", "answer": "The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cougar's primary food sources include ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses and sheep, particularly in the northern part of its range.", "ungulates such as deer, elk, and bighorn sheep, as well as domestic cattle, horses, and sheep"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "deer, elk, moose, bighorn", "extracted_entity": null}
{"question": "how long are cougar adult males (from nose to tail)?", "answer": "The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The length of adult males is around 2.4 meters (8 ft) long nose to tail.", "2.4 meters"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "7-8 feet.", "extracted_entity": null}
{"question": "how long is an adult cougar's paw print?", "answer": "An adult cougar's paw print is approximately 10 cm (4 inches) long.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An adult cougar's paw print is approximately 10 cm (4 inches) long.", "4 inches"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "about 4 inches long", "extracted_entity": null}
{"question": "what are the three heaviest cats in the world?", "answer": "The three heaviest cats in the world are the tiger, lion and jaguar.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three heaviest cats in the world are the tiger, lion and jaguar.", "tiger, lion, and jaguar"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "the lion, tiger and leopard", "extracted_entity": null}
{"question": "on average, are cougar males heavier than females?", "answer": "On average, cougar males are heavier than females.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On average, cougar males are heavier than females.", "yes"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "Yes, males are typically heavier than females.", "extracted_entity": null}
{"question": "what are the three cat species that are native to Canada?", "answer": "The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three cat species that are native to Canada are the cougar, the bobcat and the Canadian lynx.", "cougar, bobcat, Canadian lynx"], "entity_annotations": ["Cougar"], "answer_entity_name": null, "predicted_answer": "bobcat, lynx, and cougar", "extracted_entity": null}
{"question": "is one of the parts of a cymbal called a disc?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "Yes, the disc is the circular part of a", "extracted_entity": null}
{"question": "are ride cymbals very common?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "no, they are not very common. They are", "extracted_entity": null}
{"question": "does Fender make cymbals?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "No, they are made by Zildjian", "extracted_entity": null}
{"question": "what part of the cymbal is the bell?", "answer": "The center of a Cymbal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The center of a Cymbal"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "the top of the cymbal", "extracted_entity": null}
{"question": "what's the timber of ancient cimbals like? disfluent?", "answer": "like that of small hand-bells or of the notes of the keyed harmonica", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["like that of small hand-bells or of the notes of the keyed harmonica"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "I don't know. I don't think it", "extracted_entity": null}
{"question": "what does it sound like to play cymbals with a cello bow?", "answer": "squealing car brakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["squealing car brakes"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "like this:", "extracted_entity": null}
{"question": "what are cymbals made of?", "answer": "brass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "cymbals are made of bronze, and the", "extracted_entity": null}
{"question": "how many cymbals typically go in a drum kit?", "answer": "at least 3", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["at least 3"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "2 cymbals", "extracted_entity": null}
{"question": "what kinds of musical groups use cymbals?", "answer": "Cymbals are used in modern orchestras and many military, marching, concert and other bands", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Cymbals are used in modern orchestras and many military, marching, concert and other bands"], "entity_annotations": ["Cymbal"], "answer_entity_name": null, "predicted_answer": "most of them, but especially rock bands and jazz", "extracted_entity": null}
{"question": "is the dragonfly associated with late summer as well as early autumn?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "yes, the dragonfly is associated with late summer", "extracted_entity": null}
{"question": "does Exposure to air cause the larva to begin breathing?", "answer": "Yes (capitalization).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (capitalization)."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "yes, if the larva is not in water", "extracted_entity": null}
{"question": "is it characterized by large multifaceted eyes, two pairs of strong transparent wings, and an elongated body?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Dragonfly"], "answer_entity_name": null, "predicted_answer": "it is an insect.", "extracted_entity": null}
{"question": "is the drum a member of the percussion group?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, it is a percussion instrument.", "extracted_entity": null}
{"question": "does each type of drum head serve its own musical purpose?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, each type of drum head serves its own", "extracted_entity": null}
{"question": "is the Aburukuwa a type of drum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "how long has the basic design of the drum remained unchanged?", "answer": "remained virtually unchanged for thousands of years", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["remained virtually unchanged for thousands of years", "thousands of years"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "since 1955.", "extracted_entity": null}
{"question": "what is the second biggest factor affecting the sound produced by a drum?", "answer": "tension", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["tension", "the tension at which the drum head is held against the shell of the drum."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the drum head", "extracted_entity": null}
{"question": "by what name are drums classified as under the percussion group?", "answer": "membranophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["membranophone", "membranous"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "membranophones", "extracted_entity": null}
{"question": "how do rock and roll drummers like their drumheads?", "answer": "rock and roll drummer may prefer drums that sound loud and deep", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["rock and roll drummer may prefer drums that sound loud and deep", "drums that sound loud and deep"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "with a hole in the middle", "extracted_entity": null}
{"question": "whare is the name for drums that have a set of wires held across some of all of the drum heads?", "answer": "snares", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["snares"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "a snare drum", "extracted_entity": null}
{"question": "had the drums been used in Sri Lanka since Before Christ?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, the drums were used in Sri Lanka since", "extracted_entity": null}
{"question": "are drums often used in music therapy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, drums are often used in music therapy.", "extracted_entity": null}
{"question": "do drums consist of at least eight membranes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "No. In the 2000s, drums", "extracted_entity": null}
{"question": "what has the most effect on how a drum sounds?", "answer": "the drumhead", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the drumhead"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the drum head", "extracted_entity": null}
{"question": "who constructed a kit using diatonic and chromatically tuned drums?", "answer": "Terry Bozzio", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Terry Bozzio"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "the American composer John Cage, in 1951", "extracted_entity": null}
{"question": "drums are usually played by what?", "answer": "the hands, or by one or two sticks", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the hands, or by one or two sticks", "the hands"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "hands", "extracted_entity": null}
{"question": "has the design of drums changed recently?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "I think the design of drums has changed quite a", "extracted_entity": null}
{"question": "can some cylindrical shell drums have no drum heads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes. for example, if you have a drum", "extracted_entity": null}
{"question": "does every drumhead make the same sound?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "no, but you can make them sound the same", "extracted_entity": null}
{"question": "aren't drums often used in music therapy?", "answer": "Yes, drums are often used in music therapy.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, drums are often used in music therapy."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "aren't drums usually played by the hands?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "Yes, but you can also play them with your", "extracted_entity": null}
{"question": "are drums usually played by the hands?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes. (drumsticks are used to play", "extracted_entity": null}
{"question": "do many such drums have six to ten tension rods?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with parts of a player's body, or with some sort of implement such as a drumstick, to produce sound?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "have other techniques been used to cause drums to make sound, such as the ``Thumb roll''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Drum"], "answer_entity_name": null, "predicted_answer": "The thumb roll is a technique used to make the", "extracted_entity": null}
{"question": "is the electric eel a true eel?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, the electric eel is not a true eel."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "no, the electric eel is a fish.", "extracted_entity": null}
{"question": "are most eels predators?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, most eels are predators."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "No, most eels are prey", "extracted_entity": null}
{"question": "is eel blood toxic to humans?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, eel blood is toxic to humans."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "Eel blood is toxic to humans. The toxin", "extracted_entity": null}
{"question": "how many species of true eels are there?", "answer": "approximately 800 species", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["approximately 800 species", "There are approximately 800 species of true eels."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "15 (and 1 more recently discovered)", "extracted_entity": null}
{"question": "where is smoked eel considered a delicacy?", "answer": "Northern Germany, The Netherlands, Denmark, Sweden", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Germany, The Netherlands, Denmark, Sweden", "Smoked eel is considered a delicacy in northern Germany, The Netherlands, Denmark and Sweden."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "the Netherlands, where it is called \"", "extracted_entity": null}
{"question": "how do eels begin life?", "answer": "As flat and transparent larvae, called leptocephali", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["As flat and transparent larvae, called leptocephali", "Eels begin life as flat and transparent larvae, called leptocephali."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "as eggs in the sea", "extracted_entity": null}
{"question": "what types of fins do eels have?", "answer": "Dorsal, anal, caudal, tail fins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dorsal, anal, caudal, tail fins", "Eels have dorsal and anal fins fused with a tail fin. Some also have  pectoral fins."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "they have a dorsal fin, a pectoral", "extracted_entity": null}
{"question": "what do leptocephali eat?", "answer": "Marine snow", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Marine snow", "Leptocephali eat small particles called marine snow."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "they eat zooplankton, algae, and", "extracted_entity": null}
{"question": "do all eels live in fresh water?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, not all eels live in fresh water."], "entity_annotations": ["Eel"], "answer_entity_name": null, "predicted_answer": "no, some live in salt water", "extracted_entity": null}
{"question": "is Finnish an official language of the European Union?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Finnish is an official language of the European Union."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "No. There are 24 official languages of the", "extracted_entity": null}
{"question": "are the dialects of Finnish divided into two groups?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, the dialects of Finnish are divided into two groups."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "yes, the dialects of Finnish are divided into", "extracted_entity": null}
{"question": "is Finnish threatened by English?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Finnish is not threatened by English."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "No, English is not a threat to Finnish.", "extracted_entity": null}
{"question": "when did Proto-Uralic language arrive in Finland?", "answer": "Around 1900 BCE", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Around 1900 BCE", "Proto-Uralic language arrived in Finland around 1900 BCE."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "3000-4000 years ago.", "extracted_entity": null}
{"question": "what are characteristic features of Finnish?", "answer": "Vowel harmony and an agglutinative morphology", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowel harmony and an agglutinative morphology", "Characteristic features of Finnish are vowel harmony and an agglutinative morphology."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "1. The Finnish language is a member of", "extracted_entity": null}
{"question": "where is the Kven language spoken?", "answer": "Norway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norway", "The Kven language is spoken in Northern Norway."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "in Norway", "extracted_entity": null}
{"question": "for how long has the classification of dialects spoken outside of Finland been a controversial issue?", "answer": "Since Finland's independence in 1917", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Since Finland's independence in 1917", "The classification of dialects spoken outside of Finland has been a controversial issue since Finland's independence in 1917."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "since the 1930s", "extracted_entity": null}
{"question": "who wrote the first novel in Finnish?", "answer": "Aleksis Kivi", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Aleksis Kivi", "The first novel in Finnish was published by Aleksis Kivi."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Minna Canth (1844-1897", "extracted_entity": null}
{"question": "what book defined the official Finnish language?", "answer": "The Dictionary of Contemporary Finnish", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Dictionary of Contemporary Finnish", "The Dictionary of Contemporary Finnish defined the official Finnish language."], "entity_annotations": ["Finnish_language"], "answer_entity_name": null, "predicted_answer": "Kalevala", "extracted_entity": null}
{"question": "what is the earliest extant transverse flute?", "answer": "it dates from 433 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["it dates from 433 BC", "Chi"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the 13th century flute of St. Catherine", "extracted_entity": null}
{"question": "when does a flute produce sound?", "answer": "when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["when a stream of air directed across a hole in the instrument creates a vibration of air at the hole", "When a stream of air directed across a hole in the instrument creats a vibration of air at the hole."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "when the air column inside the flute is vibrating.", "extracted_entity": null}
{"question": "how can a flute's volume be increased?", "answer": "a flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute's volume can generally be increased by making its resonator and tone holes larger", "A flute's volume can be increased by making its resonator and tone holes larger."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "increase the air pressure", "extracted_entity": null}
{"question": "do open-ended flutes have brighter timbres?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, because they have more overtones", "extracted_entity": null}
{"question": "where was the oldest flute ever discovered found?", "answer": "Germany", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany", "Various parts of Germany."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "in a cave in southern Germany", "extracted_entity": null}
{"question": "are Western flutes simpler than Indian flutes?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Yes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "I can't speak for Indian flutes, but", "extracted_entity": null}
{"question": "why is a police whistle very wide for its pitch?", "answer": "A flute's volume can generally be increased by making its resonator and tone holes larger", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A flute's volume can generally be increased by making its resonator and tone holes larger", "It's volume is high either because of its larger resonator or larger tone holes."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the police whistle is a very wide whistle because the", "extracted_entity": null}
{"question": "what are the two main varieties of Indian flutes?", "answer": "The first, the Bansuri. The second, the Venu or Pullanguzhal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The first, the Bansuri. The second, the Venu or Pullanguzhal", "Bansuri and the Venu or Pullanguzhal."], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the bamboo flute and the metal flute", "extracted_entity": null}
{"question": "has the flute been dated to prehistoric times?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "Yes, it has. It has been dated to", "extracted_entity": null}
{"question": "was the pan flute used in Greece?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "The pan flute was used in Greece and also in", "extracted_entity": null}
{"question": "can a flute be played with several different air sources?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "yes, as long as they are all at the", "extracted_entity": null}
{"question": "when was the pan flute used in Greece?", "answer": "the 7th century BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 7th century BC", "7th century BC"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the pan flute was used in Greece in the", "extracted_entity": null}
{"question": "when did concert flutes begin appearing in concert ensembles?", "answer": "the 16th-century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 16th-century", ""], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the 1930s. Concert flutes were", "extracted_entity": null}
{"question": "how many main varieties of Indian flutes are currently used?", "answer": "two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["two", "Two"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "what material is a chi flute fashioned from?", "answer": "lacquered bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["lacquered bamboo", "Lacquered bamboo"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "bamboo.", "extracted_entity": null}
{"question": "how does a flute player change the pitch of the sound?", "answer": "by opening and closing holes in the body of the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["by opening and closing holes in the body of the instrument", "Opening and closing holes in the body of the instrument"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "by changing the length of the air column in the", "extracted_entity": null}
{"question": "what is the most basic form of the flute?", "answer": "a flute can be an open tube which is blown like a bottle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a flute can be an open tube which is blown like a bottle", "Open tube which is blown like a bottle"], "entity_annotations": ["Flute"], "answer_entity_name": null, "predicted_answer": "the flute is the most basic form of the flute", "extracted_entity": null}
{"question": "has the dragon historically served as China's national emblem?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes, since the Shang Dynasty.", "extracted_entity": null}
{"question": "do pandas hibernate?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, they don't. But they do sleep", "extracted_entity": null}
{"question": "do giant pandas attack humans?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, they are very docile and have been", "extracted_entity": null}
{"question": "what family is the panda a part of?", "answer": "Ursidae (bear)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ursidae (bear)", "Ursidae"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "the panda is a part of the bear family.", "extracted_entity": null}
{"question": "how long does it take for the panda cubs skin to turn gray?", "answer": "one to two weeks after birth", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["one to two weeks after birth", "One to two weeks"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2-4 months", "extracted_entity": null}
{"question": "how many different names did Chinese writings give to the panda?", "answer": "20", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["20"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "24", "extracted_entity": null}
{"question": "what foods do pandas eat?", "answer": "bamboo", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["bamboo", "bamboo, honeys, eggs, fish, yams, shrub leaves, oranges, and bananas"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "bamboo, bamboo, bamboo, bamboo, bamboo,", "extracted_entity": null}
{"question": "what are the two subspecies of giant pandas?", "answer": "Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Ailuropoda melanoleuca melanoleuca and  Ailuropoda melanoleuca qinlingensis ", "Ailuropoda melanoleuca melanoleuca and The Qinling Panda, Ailuropoda melanoleuca qinlingensis"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "the giant panda subspecies are the \"Chin", "extracted_entity": null}
{"question": "what is panda diplomacy?", "answer": "Loans of Giant Pandas to American and Japanese zoos", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Loans of Giant Pandas to American and Japanese zoos"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "panda diplomacy is a term used in china to describe", "extracted_entity": null}
{"question": "is the Giant Panda a mammal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "Yes, the Giant Panda is a mammal.", "extracted_entity": null}
{"question": "is the Giant Panda a terrestrial animal?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, it is not a terrestrial animal.", "extracted_entity": null}
{"question": "is the Giant Panda an endangered species?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "where do Giant Pandas live?", "answer": "a few mountain ranges in central china", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["a few mountain ranges in central china", "Sichuan province, Shaanxi and Gansu provinces"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "In the mountains of China", "extracted_entity": null}
{"question": "when was the Giant Panda's genome sequenced?", "answer": "2009", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2009"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2009", "extracted_entity": null}
{"question": "how many subspecies of the Giant Panda are there?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "2, the Qinling and the Sich", "extracted_entity": null}
{"question": "is the Giant Panda a carnivore?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "No, it is an omnivore", "extracted_entity": null}
{"question": "why is the Giant Panda an endangered species?", "answer": "threatened by habitat loss", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["threatened by habitat loss", "It is threatened by continued habitat loss and by a very low birthrate"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "because they are a special animal that lives in China", "extracted_entity": null}
{"question": "is the population of wild Giant  Pandas growing?", "answer": "according to some reports, yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["according to some reports, yes", "Yes but uncertain"], "entity_annotations": ["Giant_Panda"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is an acoustic guitar dependent on an external device?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "No, an acoustic guitar is dependent on a human", "extracted_entity": null}
{"question": "is the battente smaller than a classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes, the battente is smaller than a classical", "extracted_entity": null}
{"question": "is the flamenco guitar similar to the classical guitar?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes, it is the same guitar but with a", "extracted_entity": null}
{"question": "where is the headstock located?", "answer": "end of the guitar neck furthest from the body", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["end of the guitar neck furthest from the body", "At the end of the guitar."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "on the neck of the guitar", "extracted_entity": null}
{"question": "whom are guitars made and repaired by?", "answer": "luthiers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["luthiers", "Luthiers."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the guitar maker", "extracted_entity": null}
{"question": "what are modern guitar strings constructed of?", "answer": "metal, polymers, animal or plant product materials", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["metal, polymers, animal or plant product materials", "Metal, polymers, or animal or plant product materials."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "steel, nylon, bronze, nickel, and other", "extracted_entity": null}
{"question": "why are Harp Guitars difficult to classify?", "answer": "as there are many variations within this type of guitar", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["as there are many variations within this type of guitar", "There are many variations within this type."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Because they are a hybrid between the Harp and", "extracted_entity": null}
{"question": "what is the bridge used for?", "answer": "transfer the vibration from the strings to the soundboard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["transfer the vibration from the strings to the soundboard", "The transfer of string vibrations."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "to connect two parts of a building", "extracted_entity": null}
{"question": "which guitars use three single-coil pickups?", "answer": "Fender Statocaster type guitars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fender Statocaster type guitars", "Fender Stratocaster type guitars."], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the telecaster, the stratocaster, and", "extracted_entity": null}
{"question": "is a guitar an instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "can guitars be divided into two broad categories?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "yes, they can be divided into two broad categories", "extracted_entity": null}
{"question": "do picks come in many shapes and sizes?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Yes, they come in many shapes and sizes.", "extracted_entity": null}
{"question": "how many strings does a guitar typically have?", "answer": "Six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Six", "six"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "6", "extracted_entity": null}
{"question": "what is located at the end of the guitar neck furthest from the body?", "answer": "Headstock", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Headstock", "headstock"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the headstock", "extracted_entity": null}
{"question": "what are most electric guitar bodies made of?", "answer": "Wood", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wood", "wood"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "wood", "extracted_entity": null}
{"question": "why do some people believe that left-handed people should learn to play guitars as right-handed people do?", "answer": "to standardise the instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["to standardise the instrument"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "Some people believe that left-handed people should learn to", "extracted_entity": null}
{"question": "how old is the oldest known representation of a guitar-like intrument being played?", "answer": "3,300 years old", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3,300 years old"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "4000 years old.", "extracted_entity": null}
{"question": "what is the point called that is bolted or glued to the body of the guitar?", "answer": "Neck Joint", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Neck Joint", "Neck joint or 'Heel'"], "entity_annotations": ["Guitar"], "answer_entity_name": null, "predicted_answer": "the bridge", "extracted_entity": null}
{"question": "did Becquerel study science at the \u00c9cole des Ponts et Chauss\u00e9es?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did he become chief engineer in the Department of Bridges and Highways in 1892?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "no, he became chief engineer in the Department of", "extracted_entity": null}
{"question": "was Becquerel a French physicist?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Yes. He was a French physicist who discovered radio", "extracted_entity": null}
{"question": "whom did he share the Nobel Prize with?", "answer": "Pierre and Marie Curie", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pierre and Marie Curie"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "John C. Harsanyi and Reinhard", "extracted_entity": null}
{"question": "what is the SI unit for radioactivity called?", "answer": "becquerel", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["becquerel", "the becquerel (Bq)"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "becquerel (Bq)", "extracted_entity": null}
{"question": "when Becquerel discovered radioactivity, whose work did he investigate?", "answer": "Wilhelm Conrad R\u00f6ntgen", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Wilhelm Conrad R\u00f6ntgen"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "Henri Becquerel discovered radioactivity when he investigated", "extracted_entity": null}
{"question": "where are there craters named Becquerel?", "answer": "Moon and Mars", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Moon and Mars", "on the moon and on mars"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "there are no craters named Becquerel on", "extracted_entity": null}
{"question": "why did Bequerel win the Nobel Prize in Physics?", "answer": "discovery of spontaneous radioactivity", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["discovery of spontaneous radioactivity", "was the discoverer of radioactivity"], "entity_annotations": ["Henri_Becquerel"], "answer_entity_name": null, "predicted_answer": "he discovered radioactivity", "extracted_entity": null}
{"question": "was Isaac Newton British?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes, he was English."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no, he was English", "extracted_entity": null}
{"question": "was Isaac Newton religious?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear.", "Yes, he was highly religious, though an unorthodox Christian."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "he was a christian, but not a very devout", "extracted_entity": null}
{"question": "did Isaac Newton die in 1898?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, Newton died in his sleep in London on 31 March 1727."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "no, he died in 1727.", "extracted_entity": null}
{"question": "where was Isaac Newton born?", "answer": "At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["At Woolsthorpe Manor in Woosthorpe-by-Colsterworth.", "He was born at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire.", "Woolsthorpe Manor in Woolsthorpe-by-Colsterworth"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Woolsthorpe Manor, Lincolnshire, England", "extracted_entity": null}
{"question": "who shares credit with Isaac Newton for developing calculus?", "answer": "Gottfried Leibniz.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gottfried Leibniz.", "Newton shares the credit with Gottfried Leibniz for the development of the differential and integral calculus."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Gottfried Leibniz", "extracted_entity": null}
{"question": "when did Isaac Newton discover the generalized binomial theorem?", "answer": "In 1665.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1665."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "he did not. The generalized binomial theorem was", "extracted_entity": null}
{"question": "did the Occult influence Newton's theory of gravitation?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "If Newton had not relied on the occult idea of action at a distance, he might not have developed his theory of gravity."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "I don't know, but I don't think", "extracted_entity": null}
{"question": "were Newton's religious views consistent with Anglicanism?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No, a conflict between Newton's religious views and Anglican orthodoxy was averted."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "I believe so, yes. He was a devout", "extracted_entity": null}
{"question": "which fruit inspired Isaac Newton's theory of gravitation?", "answer": "The apple.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The apple.", "Isaac Newton's theory of gravitation was inspired by watching the fall of an apple from a tree."], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "an apple.", "extracted_entity": null}
{"question": "was Sir Isaac Newton an English physicist and mathematician?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did he build the first practical reflecting telescope?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Isaac Newton educated at The King's Schol, Grantham?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Yes, he was educated at The King's School", "extracted_entity": null}
{"question": "when was the Principia published?", "answer": "1687", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1687"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "1687.", "extracted_entity": null}
{"question": "where is there a Newton statue on display?", "answer": "Oxford University Museum of Natural History ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Oxford University Museum of Natural History ", "Oxford University Museum of Natural History"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "on the back of a bus in Oxford", "extracted_entity": null}
{"question": "where was Isaac Newton buried?", "answer": "Westminster Abbey", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Westminster Abbey"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "Cambridge, England.", "extracted_entity": null}
{"question": "what religion did Isaac Newton follow?", "answer": "he never made a public declaration of his private faith", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["he never made a public declaration of his private faith", "unorthodox Christian"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "he was an Anglican, but he was a", "extracted_entity": null}
{"question": "what food gave Isaac Newton clues to his theory of gravity?", "answer": "apple", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apple"], "entity_annotations": ["Isaac_Newton"], "answer_entity_name": null, "predicted_answer": "an apple.", "extracted_entity": null}
{"question": "who was a farmer and later a land surveyor for the government?", "answer": " LeRoy Pollock was a farmer and later a land surveyor for the government.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" LeRoy Pollock was a farmer and later a land surveyor for the government."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "George Washington", "extracted_entity": null}
{"question": "regarding this topic, what did David Alfaro Siqueiros do?", "answer": "Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock was introduced to the use of liquid paint in 1936 at an experimental workshop operated in New York City by the Mexican muralist David Alfaro Siqueiros."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "he was the first artist to use a paint gun", "extracted_entity": null}
{"question": "who used hardened brushes, sticks,?", "answer": "Jackson Pollock used hardened brushes, sticks.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jackson Pollock used hardened brushes, sticks."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "the aborigines", "extracted_entity": null}
{"question": "when were his papers donated by Lee Krasner?", "answer": "In 1983.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1983."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "1977", "extracted_entity": null}
{"question": "did Free Jazz feature a Pollock painting as its cover artwork?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "no, but a painting by the late American artist", "extracted_entity": null}
{"question": "was Paul Jackson Pollock an influential American painter as well as a major figure in the abstract expressionist movement?", "answer": "Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Pollock was an influential American painter and a major figure in the abstract expressionist movement."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "give an example of the origins of the term action painting. ?", "answer": "Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Pollock's technique of pouring and dripping paint is thought to be one of the origins of the term action painting."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "The term action painting was first used by Harold Rosenberg", "extracted_entity": null}
{"question": "during his lifetime, did Pollock enjoy considerable fame and notoriety?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "yes. he was a member of the New York", "extracted_entity": null}
{"question": "was he regarded as a mostly reclusive artist?", "answer": "Yes, he was regarded as a mostly reclusive artist. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, he was regarded as a mostly reclusive artist. "], "entity_annotations": ["Jackson_Pollock"], "answer_entity_name": null, "predicted_answer": "He was regarded as a mostly reclusive artist", "extracted_entity": null}
{"question": "is the Wisma building the tallest building in Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes,"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes. Wisma 46 is the tallest building", "extracted_entity": null}
{"question": "is there a chinese community in jakarta?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes, the chinese community in jakarta is the", "extracted_entity": null}
{"question": "is Jakarta the 12th largest city in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No, Jakarta is the 2nd largest city", "extracted_entity": null}
{"question": "what is the capital of Indonesia?", "answer": "Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta", "The capital of Indonesia is Jakarta."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta", "extracted_entity": null}
{"question": "what is the official name of Jakarta?", "answer": "Daerah Khusus Ibukota Jakarta", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Daerah Khusus Ibukota Jakarta", "Daerah Khusus."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is the official name of the capital city of", "extracted_entity": null}
{"question": "how is the climate in the city?", "answer": "Jakarta has a hot and humid equatorial/tropical climate ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta has a hot and humid equatorial/tropical climate ", "The city is hot and humid."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "it is hot and humid in the summer and cold", "extracted_entity": null}
{"question": "where does the name of the city Jakarta come from?", "answer": "The name Jakarta is derived from the Sanskrit word \"Jayakarta.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The name Jakarta is derived from the Sanskrit word \"Jayakarta.\""], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The name of the city Jakarta is derived from the", "extracted_entity": null}
{"question": "how is the city's governor chosen?", "answer": "election", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["election", "Through election."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The city's governor is chosen by the city's", "extracted_entity": null}
{"question": "why does Jakarta suffer frequent flooding?", "answer": "Because it is located approximately eight meters above the sea level", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is located approximately eight meters above the sea level", "The fact that the northern part of Jakarta lies on a plain, approximately eight meters above the sea level."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "because the city is built on a low-lying", "extracted_entity": null}
{"question": "is the the biggest university in Jakarta the University of Indonesia?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No. The biggest university in Jakarta is the University", "extracted_entity": null}
{"question": "is Jakarta a city?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is the most important river the Ciliwung River?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "where is Jakarta located?", "answer": "northwest coast of Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["northwest coast of Java", "On the northwest coast of Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Jakarta is located in Indonesia", "extracted_entity": null}
{"question": "who created Monas Park?", "answer": "General Herman Willem Deandels", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["General Herman Willem Deandels", "Dutch Governor General Herman Willem Deandels"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the creator of the Monas Park is the architect", "extracted_entity": null}
{"question": "do mikrolets travel on the main roads?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "what contributes to frequent flooding in Jakarta?", "answer": "Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jakarta lies on a plain eight meters, approximately eight meters above the sea level.", "It lies on a plain, approximately eight meters above the sea level"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "the city is located on a delta plain, so", "extracted_entity": null}
{"question": "did the economy improve in Jakarta in 2007?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes, it improved a lot", "extracted_entity": null}
{"question": "in AD 39, King Purnawarman established Sunda Pura as a new capital city for the kingdom, located at the northern coast of where?", "answer": "Java", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Java"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "West Java, Indonesia", "extracted_entity": null}
{"question": "in 1602, the British East India Company's first voyage, commanded by Sir who, arrived in Aceh and sailed on to Banten where they were allowed to build a trading post?", "answer": "James Lancaster", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["James Lancaster"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Sir Henry Middleton", "extracted_entity": null}
{"question": "who began control of migration to the city in order to stem the overcrowding and poverty?", "answer": "Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lieutenant General Ali Sadikin began control of migration to the city in order to stem the overcrowding and poverty."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The City of London", "extracted_entity": null}
{"question": "did Jayawikarta's soldiers attack the Dutch fortress?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No, the Dutch attacked the soldiers.", "extracted_entity": null}
{"question": "did Suharto resign as president?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "No, he was forced out by the army", "extracted_entity": null}
{"question": "was the Jakarta area part of the fourth century Indianized kingdom of Tarumanagara?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes, but it was not a capital city.", "extracted_entity": null}
{"question": "do other landmarks include the Istiqlal Mosque as well as Jakarta Cathedral?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "give an example of the many Sukarno era monuments in the city. ?", "answer": "The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The West Irian Liberation Statue is one of the many Sukarno era monuments in the city. "], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "The National Monument (Monas) is a", "extracted_entity": null}
{"question": "is Jakarta the country's economic, cultural and political center?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Jakarta"], "answer_entity_name": null, "predicted_answer": "Yes, Jakarta is the country's economic, cultural", "extracted_entity": null}
{"question": "is James Watt French?", "answer": "No, he was Scottish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, he was Scottish.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "no, he's Scottish.", "extracted_entity": null}
{"question": "did James Watt improve an engine?", "answer": "Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, his improvements to the Newcomen steam engine were fundamental to the changes brought by the Industrial Revolution.", "Yes"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "yes, he improved an engine by adding a separate", "extracted_entity": null}
{"question": "is there a statue of Watt in the White House?", "answer": "No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, a colossal statue of Watt by Chantrey was placed in Westminster Abbey, and later was moved to St. Paul's Cathedral.", "No"], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "no, but there is a statue of Watt in", "extracted_entity": null}
{"question": "what was James Watt's father's job?", "answer": "His father was a shipwright, ship owner and contractor, and served as the town's chief baillie.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["His father was a shipwright, ship owner and contractor, and served as the town's chief baillie."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt's father was a ship's officer.", "extracted_entity": null}
{"question": "when did Watt retire?", "answer": "In 1800.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1800.", "Watt retired in 1800."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "when he had had enough.", "extracted_entity": null}
{"question": "where is the James Watt Memorial College?", "answer": "In Greenock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In Greenock.", "The James Watt Memorial  is in Greenock."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "James Watt Memorial College, Karratha", "extracted_entity": null}
{"question": "why is the watt named after Watt?", "answer": "The watt is named after him for his contributions to the development of the steam engine.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The watt is named after him for his contributions to the development of the steam engine.", "The watt is named after James Watt for his contributions to the development of the steam engine."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "it is named after James Watt, the inventor of", "extracted_entity": null}
{"question": "where did Watt attend university?", "answer": "Watt did not attend school regularly.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Watt did not attend school regularly.", "He did not attend a university."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "Edinburgh University", "extracted_entity": null}
{"question": "what did he invent?", "answer": "He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["He invented a new method of measuring distances by telescope, a device for copying letters, improvements in the oil lamp, a steam mangle and a machine for copying sculptures."], "entity_annotations": ["James_Watt"], "answer_entity_name": null, "predicted_answer": "the telephone", "extracted_entity": null}
{"question": "what happened in the early 20th century?", "answer": "The koalas of South Australia were largely exterminated.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "the industrial revolution took place, making the US more", "extracted_entity": null}
{"question": "do Koalas have a slow metabolism?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, koalas have a slow metabolism", "extracted_entity": null}
{"question": "did eucalypt forests grow in the place of rainforests?", "answer": "Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the koala did not specialise in a diet of eucalyptus until the climate cooled and eucalypt forests grew in the place of rainforests."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, they did. there are eucaly", "extracted_entity": null}
{"question": "does the koala fill the same ecological role as the sloth of South America?", "answer": "The koala fills the same ecological role as the sloth of South America.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koala fills the same ecological role as the sloth of South America."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "no, the koala is a herbivore", "extracted_entity": null}
{"question": "is the long term viability of the koala therefore threatened by genetic weakness?", "answer": "The long term viability of the koala is therefore threatened by genetic weakness.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The long term viability of the koala is therefore threatened by genetic weakness."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "No, because there are still plenty of koal", "extracted_entity": null}
{"question": "is the koala found in coastal regions of eastern and southern Australia, from near Adelaide to the southern part of Cape York Peninsula?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands?", "answer": "Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Populations also extend for considerable distances inland in regions with enough moisture to support suitable woodlands."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "were the koalas of South Australia largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock?", "answer": "The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The koalas of South Australia were largely exterminated during the early part of the 20th century, but the state has since been repopulated with Victorian stock."], "entity_annotations": ["Koala"], "answer_entity_name": null, "predicted_answer": "No, the koalas of South Australia were", "extracted_entity": null}
{"question": "is Korean the official language of Korea?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Yes, Korean is the official language of Korea.", "extracted_entity": null}
{"question": "is the word \"Korean\" derived from Goryeo?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no. The word is derived from the name of", "extracted_entity": null}
{"question": "are all dialects of Korean similar to each other?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no, they are not. In fact, dialect", "extracted_entity": null}
{"question": "about how many Koreans speakers are there?", "answer": "78 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["78 million", "78 million Korean speakers"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "80 million (including North and South Korea)", "extracted_entity": null}
{"question": "older English sources used the name \"Korean\" to refer to what?", "answer": "language, country, and people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["language, country, and people"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "\"Korean\" was used to refer to the", "extracted_entity": null}
{"question": "what is the official language of North Korea?", "answer": "Korean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean", "extracted_entity": null}
{"question": "why may English translation of Koreans adjectives misleadingly suggest that they are verbs?", "answer": "English does not have an identical grammatical category", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["English does not have an identical grammatical category"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "because the Korean language is a highly agglutin", "extracted_entity": null}
{"question": "where do the majority of loanwords other than Sino-Korean come from?", "answer": "modern times", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["modern times", "modern times, 90% of which are from English"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Japanese and English", "extracted_entity": null}
{"question": "how many Korean speakers are there?", "answer": "There are about 78 million Korean speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are about 78 million Korean speakers."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "77 million, or 1.6%", "extracted_entity": null}
{"question": "how many parts of speech does the Korean language contain?", "answer": "The Korean Language contains nine parts of speech. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Korean Language contains nine parts of speech. "], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "17", "extracted_entity": null}
{"question": "how many verb paradigms are there in Korean?", "answer": "There are seven verb paradigms or speech levels in Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are seven verb paradigms or speech levels in Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "4 (for the most part)", "extracted_entity": null}
{"question": "what is the Sino-Korean system of numbers based on?", "answer": "The Sino-Korean system is nearly entirely based on the Chinese numerals.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sino-Korean system is nearly entirely based on the Chinese numerals."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "the Chinese system of numbers based on the Chinese language", "extracted_entity": null}
{"question": "what is another term for Korean adjectives?", "answer": "Adjectives are also known as \"descriptive verbs\" or \"stative verbs\".", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Adjectives are also known as \"descriptive verbs\" or \"stative verbs\"."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "adverbs", "extracted_entity": null}
{"question": "is modern Korean written in columns or rows?", "answer": "It is written in rows.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is written in rows."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "columns", "extracted_entity": null}
{"question": "what percentage of the Korean language does Jeong Jae-do estimate to be Sino-Korean?", "answer": "Sino-Korean makes up 30% of the Korean language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sino-Korean makes up 30% of the Korean language."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "70% (Jeong Jae-do,", "extracted_entity": null}
{"question": "do Chinese and Japanese have spaces between words?", "answer": "No, Chinese and Japanese do not have spaces between words.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese and Japanese do not have spaces between words."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "yes. Chinese has spaces between words, and Japanese", "extracted_entity": null}
{"question": "from which languages is Korean descended?", "answer": "Korean is descended from Old Korean, Middle Korean and Modern Korean.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Korean is descended from Old Korean, Middle Korean and Modern Korean."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Korean is a language isolate, which means that it", "extracted_entity": null}
{"question": "are there seven verb paradigms or speech levels in Korean?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "yes. It is the same as Japanese.", "extracted_entity": null}
{"question": "is Modern Korean written with spaces between words?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "No, Modern Korean is written without spaces between words", "extracted_entity": null}
{"question": "are the Korean names for the language based on the names for Korea used in North as well as South Korea?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no. the names for the language are based on", "extracted_entity": null}
{"question": "is it not also one of the two official languages of the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "yes, it is one of the two official languages", "extracted_entity": null}
{"question": "is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists?", "answer": "The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists."], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "no. The only linguists who classify it as", "extracted_entity": null}
{"question": "is it also one of the two official languages in the Yanbian Korean Autonomous Prefecture in China?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Korean_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is. In fact, it is", "extracted_entity": null}
{"question": "is Kuala Lumpur the capitol of Malaysia? disfluent?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "disfluent. Kuala Lumpur is not the cap", "extracted_entity": null}
{"question": "is Kuala Lumpur in the Selangor state?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "what's the population of Kuala lumpur?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1.5 million", "extracted_entity": null}
{"question": "what language do they speak in Kuala Lumpur?", "answer": "Bahasa Melayu", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Bahasa Melayu"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "English is the main language in Kuala Lumpur.", "extracted_entity": null}
{"question": "what does the name \"Kuala Lumpur\" mean?", "answer": "muddy confluence, \"muddy estuary,\" and \"muddy city\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["muddy confluence, \"muddy estuary,\" and \"muddy city\""], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Muddy confluence", "extracted_entity": null}
{"question": "what airports does Kuala Lumpur have?", "answer": "Kuala Lumpur International Airport and Subang International Airport", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kuala Lumpur International Airport and Subang International Airport"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Kuala Lumpur International Airport (KUL) and Sultan", "extracted_entity": null}
{"question": "how long was Kuala Lumpur occupied by the Japanese?", "answer": "42 months", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["42 months"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "three years and eight months.", "extracted_entity": null}
{"question": "how many seasons does Kuala Lumpur experience?", "answer": "1", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "2 (rainy and dry)", "extracted_entity": null}
{"question": "when were the colonial buildings in Kuala Lumpur constructed?", "answer": "toward the end of the 19th and early 20th centuries", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["toward the end of the 19th and early 20th centuries"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "colonial buildings in Kuala Lumpur were constructed between 188", "extracted_entity": null}
{"question": "is it a host city for the Formula One World Championship?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "No. The only host cities for the Formula One", "extracted_entity": null}
{"question": "is the central market located in the proximity of the pertama complex?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "No, the central market is located at the other", "extracted_entity": null}
{"question": "is it home to the parliament of malaysia?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "yes, it is. it is located at the", "extracted_entity": null}
{"question": "when did kuala lumpur become a federal territory of malaysia?", "answer": "February 1, 1974", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["February 1, 1974"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1974", "extracted_entity": null}
{"question": "what is the city's population?", "answer": "1.6 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1.6 million"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "1.1 million", "extracted_entity": null}
{"question": "what is the official religion in the country?", "answer": "Islam", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Islam"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "Islam", "extracted_entity": null}
{"question": "is there a more industrialized region in malaysia?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "yes, there is a more industrialized region in", "extracted_entity": null}
{"question": "after the 2008 elections, what party had the most parliament seats?", "answer": "DAP", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["DAP"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the conservative party.", "extracted_entity": null}
{"question": "what is the name of the event that draws the worlds top riders to malaysia?", "answer": "KL Grand Prix CSI 5*", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["KL Grand Prix CSI 5*"], "entity_annotations": ["Kuala_Lumpur"], "answer_entity_name": null, "predicted_answer": "the le tour de langkawi", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci valued as an engineer?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci was valued as a painter, sculpt", "extracted_entity": null}
{"question": "wasn't Leonardo da Vinci born on April 15?", "answer": "Yes, Leonardo da Vinci was born on April 15.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Leonardo da Vinci was born on April 15."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "No, his birthday is March 15.", "extracted_entity": null}
{"question": "who left Verroccio's studio?", "answer": "Leonardo da Vinci left Verroccio's studio.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo da Vinci left Verroccio's studio."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo", "extracted_entity": null}
{"question": "when did Baldassare Castiglione, author of Il Cortegiano ( \" The Courtier \" ), write?", "answer": "1528", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1528"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1528, but he died in 152", "extracted_entity": null}
{"question": "when did he flee to Venice?", "answer": "In 1499.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1499."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "1298, when he was 16 years", "extracted_entity": null}
{"question": "was Leonardo a contemporary of Botticelli, Domenico Ghirlandaio as well as Perugino?", "answer": "Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Leonardo was a contemporary of Botticelli, Domenico Ghirlandaio and Perugino."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Yes, Leonardo was a contemporary of all these artists", "extracted_entity": null}
{"question": "is the larger work now almost universally attributed to Leonardo?", "answer": "Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Although previously attributed to Ghirlandaio, the larger work is now almost universally attributed to Leonardo."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes, and it has been for decades.", "extracted_entity": null}
{"question": "has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is he widely considered to be one of the greatest painters of all time and perhaps the most diversely talented person ever to have lived?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci", "extracted_entity": null}
{"question": "according to art historian Helen Gardner, were the scope and depth of his interests without precedent and ``his mind and personality seem to us superhuman, the man himself mysterious and remote''?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Leonardo_da_Vinci"], "answer_entity_name": null, "predicted_answer": "yes, according to art historian Helen Gardner, the", "extracted_entity": null}
{"question": "are lobsters invertebrates?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are lobsters kosher?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes, because they have fins and scales.", "extracted_entity": null}
{"question": "do lobsters have blue blood?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "no, they have green blood", "extracted_entity": null}
{"question": "how many legs do lobsters have?", "answer": "10.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10.", "10"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "10 (8 walking legs and 2 claws", "extracted_entity": null}
{"question": "where are lobsters found?", "answer": "In all oceans.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In all oceans.", "In all oceans"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "lobsters are found in the North Atlantic ocean,", "extracted_entity": null}
{"question": "what is the mean level of mercury in American lobsters?", "answer": "0.31 ppm", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["0.31 ppm"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "0.0003 parts per million", "extracted_entity": null}
{"question": "do lobsters feel pain?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "Yes. They have nociceptors.", "extracted_entity": null}
{"question": "did the Moche people worship lobsters?", "answer": "yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes.", "Yes"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "what is the caridoid escape reaction?", "answer": "Swimming backwards quickly by curling and uncurling their abdomen.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swimming backwards quickly by curling and uncurling their abdomen.", "When lobsters swim backwards quickly by curling and uncurling their abdomen"], "entity_annotations": ["Lobster"], "answer_entity_name": null, "predicted_answer": "a response to a predator that involves a predator to", "extracted_entity": null}
{"question": "were the recitations of the Ancient Greeks accompanied by lyre playing?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "does a classical lyre have a hollow body?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "No, it is a solid body, with a", "extracted_entity": null}
{"question": "is the lyre a stringed musical instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes, it is a stringed instrument.", "extracted_entity": null}
{"question": "which constellation is said to resemble a lyre shape?", "answer": "Lyra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Lyra"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "lyra", "extracted_entity": null}
{"question": "how many raised arms are extending from the sound-chest of a classical lyre?", "answer": "Two", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "7. (the 7th string is", "extracted_entity": null}
{"question": "what were the strings of a classical lyre made of?", "answer": "Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Gut"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "sheep gut", "extracted_entity": null}
{"question": "is the foot pedal framework for a piano called a lyre?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "no, that's a harp.", "extracted_entity": null}
{"question": "where was the deepest note of the classical lyre in relation to the player's body?", "answer": "It was farthest from the player's body.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was farthest from the player's body."], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "the deepest note was in the player's lap", "extracted_entity": null}
{"question": "does a standard piano have fewer strings than a harp?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Lyre"], "answer_entity_name": null, "predicted_answer": "Yes, a standard piano has 88 strings and", "extracted_entity": null}
{"question": "is Malay the official language of Iraq?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. Arabic is the official language of Iraq.", "extracted_entity": null}
{"question": "could Malay have originated from Sumatra island?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no. Malay is a language family that includes many", "extracted_entity": null}
{"question": "is Malay an agglutinative language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "no, it is an analytic language.", "extracted_entity": null}
{"question": "what family is Malay a member of?", "answer": "Austronesian family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Austronesian family", "Austronesian."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Austronesian", "extracted_entity": null}
{"question": "what is the basic word order in Malay?", "answer": "Subject Verb Object", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Subject Verb Object", "Subject Object Verb."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "subject-verb-object", "extracted_entity": null}
{"question": "what languages has Malay borrowed words from?", "answer": "Sanskrit, Arabic and English", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sanskrit, Arabic and English", "Arabic, Sanskrit, Tamil, Persian, Portuguese, Dutch, certain Chinese dialects, and English."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Sanskrit, Arabic, Tamil, Portuguese, Dutch", "extracted_entity": null}
{"question": "are there many words in Malay that use natural gender?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Yes, there are many. For example, '", "extracted_entity": null}
{"question": "how are new words formed in Malay?", "answer": "three methods.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three methods.", "Affixation, composition, and reduplication."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "by adding a prefix or a suffix or both", "extracted_entity": null}
{"question": "is Malay in the Austronesian family of languages?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is there one negation word in Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No, there are two negation words in Malay", "extracted_entity": null}
{"question": "how many parts of speech are there in Malay?", "answer": "Four", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Four"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "11 parts of speech", "extracted_entity": null}
{"question": "what is the national language in Malaysia?", "answer": "Malay ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malay "], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "English is the national language in Malaysia. It is", "extracted_entity": null}
{"question": "do linguists consider Malay to be a single language?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, there is a single Malay language.", "extracted_entity": null}
{"question": "can speakers of modern Malay understand Old Malay?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, but with some difficulty. There are many", "extracted_entity": null}
{"question": "what language besides Malay is in the Austronesia language family?", "answer": "Malagasy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malagasy"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Austronesian languages are spoken by about 300", "extracted_entity": null}
{"question": "wasn't Malay language found in Sumatra?", "answer": "The Malay language was found in Sumatra.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Malay language was found in Sumatra."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "nope, it was found in Johor and", "extracted_entity": null}
{"question": "wasn't Malay language written using Pallava?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, it was. It was written using Pall", "extracted_entity": null}
{"question": "was Malay language written using Pallava?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. Pallava was used for writing Sanskrit", "extracted_entity": null}
{"question": "had the language already been used as the lingua franca throughout the archipelago since the 15th century?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, it had been used as the lingua", "extracted_entity": null}
{"question": "do another distinguishing feature of Malay ( include Indonesian Malay )?", "answer": "Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan).", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Another distinguishing feature of Malay (include Indonesian Malay) is its use of measure words (penjodoh bilangan)."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "the Malay language is written from right to left.", "extracted_entity": null}
{"question": "is Malay spoken as local dialect of ethnic Malays?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No. It is a language, like English,", "extracted_entity": null}
{"question": "give an example of the tributary states of the Majapahit kingdom in eastern Java. ?", "answer": "Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Malayu is an example of the tributary states of the Majapahit kingdom in eastern Java."], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "Majapahit is the last of the great", "extracted_entity": null}
{"question": "is Malay the one of the most widely spoken languages in the world?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "No, it is not. Malay is the official", "extracted_entity": null}
{"question": "was it derived from Latin?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Malay_language"], "answer_entity_name": null, "predicted_answer": "yes, from Latin.", "extracted_entity": null}
{"question": "is Melbourne the capital and most populous city in the state of Victoria?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne is the capital and most populous city in the", "extracted_entity": null}
{"question": "is Melbourne home to Australia's busiest seaport?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Yes, the Port of Melbourne is the largest se", "extracted_entity": null}
{"question": "does Melbourne have a large international student community?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "yes. Melbourne is a city with a large international", "extracted_entity": null}
{"question": "what is a resident of Melbourne known as?", "answer": "Melburnian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Melburnian", "A \"Melburnian\"."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "a melburnian.", "extracted_entity": null}
{"question": "when was Melbourne officially declared as a city?", "answer": "1847", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1847", "In 1847."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "1837", "extracted_entity": null}
{"question": "how many sister cities does the City of Melbourne have?", "answer": "six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["six", "Six."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "16", "extracted_entity": null}
{"question": "where is Melbourne situated?", "answer": "boundary of the very hot inland areas and the cold southern ocean", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["boundary of the very hot inland areas and the cold southern ocean"], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "Melbourne is located in the south-eastern part of", "extracted_entity": null}
{"question": "how many domestic tourists visit Melbourne?", "answer": "7.6 million domestic visitors", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["7.6 million domestic visitors", "7.6 million."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "4.5 million domestic tourists visit Melbourne each", "extracted_entity": null}
{"question": "what religion holds majority in Melbourne?", "answer": "Christian", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Christian", "Christianity."], "entity_annotations": ["Melbourne"], "answer_entity_name": null, "predicted_answer": "No religion. Melbourne is the most secular city in", "extracted_entity": null}
{"question": "did Albert Einstein keep a photograph of Faraday on his study wall?", "answer": "Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Albert Einstein kept a photograph of Faraday on his study wall.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Faraday marry Sarah Barnard?", "answer": "Yes, Faraday married Sarah Barnard.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday married Sarah Barnard.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, he married Sarah Stoddart", "extracted_entity": null}
{"question": "did Faraday also discover the laws of electrolysis?", "answer": "Yes, Faraday also discovered the laws of  electrolysis.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday also discovered the laws of  electrolysis.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no, Faraday discovered the laws of electrolysis", "extracted_entity": null}
{"question": "who discovered benzene?", "answer": "Michael Faraday discovered benzene.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday discovered benzene.", "Michael Faraday"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "michael faraday", "extracted_entity": null}
{"question": "where was Michael Faraday born?", "answer": "Michael Faraday was born in Newington Butts.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday was born in Newington Butts.", "Newington Butts"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "London", "extracted_entity": null}
{"question": "what is the Faraday effect?", "answer": "The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Faraday effect is the phenomenon that the plane of polarisation of linearly polarised light can be rotated by the application of an external magnetic field aligned in the direction the light is moving.", "The plane of polarisation of linearly polarised light can be rotated by the application of an exernal magnetic field aligned in the direction the light is moving."], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "the rotation of the plane of polarization of light by", "extracted_entity": null}
{"question": "did Faraday discover diamagnetism?", "answer": "Yes, Faraday discovered diamagnetism.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Faraday discovered diamagnetism.", "yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no. The first to discover diamagnetism was", "extracted_entity": null}
{"question": "how many siblings did Faraday have?", "answer": "Faraday had three siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Faraday had three siblings.", "two"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "6 brothers and 3 sisters", "extracted_entity": null}
{"question": "where is Michael Faraday buried?", "answer": "Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michael Faraday is buried in the dissenters' section of Highgate Cemetery.", "in the dissenters' section of Highgate Cemetery"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Kensington Gardens, London", "extracted_entity": null}
{"question": "is faraday best known for his work with kinematics?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "no. He is best known for his work with", "extracted_entity": null}
{"question": "was he active in the area now called environmental science?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "did he discover benzene?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "who was his mentor?", "answer": "John 'Mad Jack' Fuller", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["John 'Mad Jack' Fuller"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "his mentor was\u00a0a man named", "extracted_entity": null}
{"question": "who was Faraday's wife?", "answer": "Sarah Barnard", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sarah Barnard"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "Sarah Barnard", "extracted_entity": null}
{"question": "did he advise the british government on the production of chemical weapons for use in the crimean war?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "he was a chemist and he advised them on", "extracted_entity": null}
{"question": "did faraday have strong maritime interests?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, he was a very keen sailor.", "extracted_entity": null}
{"question": "does the charge reside on the interior of a charged conductor?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Michael_Faraday"], "answer_entity_name": null, "predicted_answer": "yes, the charge is distributed on the interior surface", "extracted_entity": null}
{"question": "wasn't Michelangelo abstemious?", "answer": "Michelangelo was abstemious.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Michelangelo was abstemious."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Michelangelo was abstemious. He was a", "extracted_entity": null}
{"question": "wasn't Michelangelo a homosexual?", "answer": "It is not clear.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It is not clear."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "No. He was married, and had children.", "extracted_entity": null}
{"question": "when did Filippo Strozzi sell it to Francis I?", "answer": "In 1529.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1529."], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1530, and not 1529 as", "extracted_entity": null}
{"question": "when was his house demolished?", "answer": "1874", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1874"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "1994.", "extracted_entity": null}
{"question": "was Michelangelo's mother Francesca di Neri del Miniato di Siena?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "No. Her name was Francesca di Neri", "extracted_entity": null}
{"question": "give an example of the most renowned works of the Renaissance. ?", "answer": "The Statue of David, completed by Michelangelo in 1504", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Statue of David, completed by Michelangelo in 1504"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "Leonardo da Vinci, Michelangelo, Raphael, Bott", "extracted_entity": null}
{"question": "was Michelangelo's output in every field during his long life prodigious?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Michelangelo"], "answer_entity_name": null, "predicted_answer": "yes, his output in every field was prodigious", "extracted_entity": null}
{"question": "is English the language most often spoken in Montreal?", "answer": "The language most spoken at home in the city is French.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The language most spoken at home in the city is French.", "No, the language most spoken in Montreal is French."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes. Montreal is the second largest English-speaking city", "extracted_entity": null}
{"question": "is Montreal's economy the third largest of all cities in Canada?", "answer": "No, Montreal's economy is the second largest of all cities in Canada.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Montreal's economy is the second largest of all cities in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "yes, it's the third largest after Toronto and", "extracted_entity": null}
{"question": "is the most popular sport in Montreal ice hockey?", "answer": "yes, The most popular sport in Montreal is ice hockey.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, The most popular sport in Montreal is ice hockey.", "Yes, the most popular sport in Montreal is ice hockey."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Yes, ice hockey is the most popular sport in", "extracted_entity": null}
{"question": "how many international airports does Montreal have?", "answer": "Montreal has two international airports.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Montreal has two international airports."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "1 (Montreal-Trudeau International Airport)", "extracted_entity": null}
{"question": "who is the Mayor of Montreal?", "answer": "The mayor is G\u00c3\u00a9rald Tremblay.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mayor is G\u00c3\u00a9rald Tremblay."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Denis Coderre", "extracted_entity": null}
{"question": "what is the name of the largest church in Montreal?", "answer": "The largest church in Montreal is named Saint Joseph's Oratory.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The largest church in Montreal is named Saint Joseph's Oratory.", "Saint Joseph's Oratory is the largest church in Montreal."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Notre Dame Basilica", "extracted_entity": null}
{"question": "where was much of Montreal's industry during the late 19th and early-to-mid 20th century?", "answer": "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century. ", "The Sud-Ouest borough was home to much of the city's industry during the late 19th and early-to-mid 20th century."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "on the Island of Montreal", "extracted_entity": null}
{"question": "does Montreal contain the largest church in Canada?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Montreal contains the largest church in Canada."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "no, it is the Basilica of Sainte", "extracted_entity": null}
{"question": "what is the largest primarily French-speaking city in the western world?", "answer": "Paris is the largest primarily French-speaking city in the western world.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Paris is the largest primarily French-speaking city in the western world.", "The largest primarily French-speaking city in the western world is Paris."], "entity_annotations": ["Montreal"], "answer_entity_name": null, "predicted_answer": "Montr\u00e9al, Quebec.", "extracted_entity": null}
{"question": "is Nairobi the capital of Kenya?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes it is the capital of Kenya.", "extracted_entity": null}
{"question": "was Nairobi founded in 1899?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi was founded in 1899.", "extracted_entity": null}
{"question": "is the current estimated population of Nairobi about 6 milion?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "yes, but the city is expanding, and the", "extracted_entity": null}
{"question": "what is the current estimated population of Nairobi?", "answer": "About 3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["About 3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "3.5 million", "extracted_entity": null}
{"question": "in what year was Nairobi founded?", "answer": "1899", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1899"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "1899", "extracted_entity": null}
{"question": "which embassy in Nairobi was bombed in August 1998?", "answer": "The United States Embassy", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The United States Embassy"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "the US embassy", "extracted_entity": null}
{"question": "how many civilians died in the 1998 U.S. embassy bombing?", "answer": "Over two hundred", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Over two hundred"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "200+", "extracted_entity": null}
{"question": "how many trades can the NSE make per day?", "answer": "10 million trades", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["10 million trades"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "6.5 million", "extracted_entity": null}
{"question": "about how many people are estimated to live in Nairobi?", "answer": "3 million", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["3 million"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "3,500,000", "extracted_entity": null}
{"question": "the district is bordered to the southwest by Uhuru Park and where?", "answer": "Central Park", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Uhuru Park and the Nairobi National Museum.", "extracted_entity": null}
{"question": "when did the Catholic University of Eastern Africa which obtain its Letter of Interim Authority?", "answer": "In 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1989."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "October 21, 1989.", "extracted_entity": null}
{"question": "is Central Park adjacent to Uhuru Park?", "answer": "Central Park is adjacent to Uhuru Park.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Central Park is adjacent to Uhuru Park."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "No, it\u2019s not.", "extracted_entity": null}
{"question": "give an example of the highest growth rates of any city in Africa. ?", "answer": " The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" The growth rate of Nairobi is currently 6.9%,  an example of the highest growth rates of any city in Africa."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "The highest growth rates of any city in Africa is", "extracted_entity": null}
{"question": "give an example of the most prominent cities in Africa politically and financially. ?", "answer": "Nairobi is now one of the most prominent cities in Africa politically and financially", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is now one of the most prominent cities in Africa politically and financially"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "The most prominent cities in Africa politically and financially are", "extracted_entity": null}
{"question": "is Nairobi not the capital as well as largest city of Kenya?", "answer": "Nairobi is the capital and largest city of Kenya.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nairobi is the capital and largest city of Kenya."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Nairobi is the capital as well as largest city of", "extracted_entity": null}
{"question": "does the city and its surrounding area also form the Nairobi Province?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "No, the city and its surrounding area forms the", "extracted_entity": null}
{"question": "does the name ``Nairobi'' come from the Maasai phrase Enkare Nyirobi, which translates to ``the place of cool waters''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Nairobi"], "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "did Nikola Tesla die alone?", "answer": "Nikola Tesla died alone.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla died alone.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Nikola Tesla close friends with Mark Twain?", "answer": "Nikola Tesla was close friends with Mark Twain.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla was close friends with Mark Twain.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes, he was a friend of Mark Twain", "extracted_entity": null}
{"question": "did Nikola Tesla use a technique called picture thinking?", "answer": "Nikola Tesla used a technique called picture thinking.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla used a technique called picture thinking.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes.\u00a0 He did.\u00a0 He would see", "extracted_entity": null}
{"question": "when did Nikola Tesla appear on the cover of Time Magazine?", "answer": "On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On Tesla's 75th birthday in 1931, Time magazine put him on its cover.", "1931"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1943. (I think that\u2019s the", "extracted_entity": null}
{"question": "who became adversaries with Nikola Tesla?", "answer": "Nikola Tesla became adversaries with Edison.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla became adversaries with Edison.", "Edison"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Thomas Edison.", "extracted_entity": null}
{"question": "where did Nikola Tesla move to in 1880?", "answer": "In 1880, Nikola Tesla moved to Budapest.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1880, Nikola Tesla moved to Budapest.", "Budapest"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Budapest, Hungary", "extracted_entity": null}
{"question": "was Nikola Tesla's mother illiterate?", "answer": "Nikola Tesla's mother never learned to read.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla's mother never learned to read.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "how many siblings did Nikola Tesla have?", "answer": "Nikola Tesla had four siblings.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Nikola Tesla had four siblings.", "4"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "three, two older sisters and one younger brother.", "extracted_entity": null}
{"question": "was Nikola Tesla a vegetarian?", "answer": " In his later years Tesla became a vegetarian.", "dataset": "factoid_qa", "split": "train", "answer_aliases": [" In his later years Tesla became a vegetarian.", "Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes. Nikola Tesla was a vegetarian.", "extracted_entity": null}
{"question": "is there a monument to Tesla at Niagara Falls, New York?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No. I think there is one in Croatia,", "extracted_entity": null}
{"question": "was Tesla an American citizen?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in what is now Croatia", "extracted_entity": null}
{"question": "was Tesla on the cover of Time magazine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "no, but Nikola Tesla was on the cover of", "extracted_entity": null}
{"question": "when did Tesla receive his last patent?", "answer": "1928", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1928"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "1917, he had 300 patents in", "extracted_entity": null}
{"question": "what languages did Tesla speak?", "answer": "Serbian, Czech, English, French, German, Hungarian, Italian, and Latin", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Serbian, Czech, English, French, German, Hungarian, Italian, and Latin"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "Tesla spoke several languages, including English, French,", "extracted_entity": null}
{"question": "did Tesla win the Nobel Prize?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "No, Tesla was nominated for the Nobel Prize", "extracted_entity": null}
{"question": "was Tesla rich at the time of his death?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "what music group was named after Tesla?", "answer": "Tesla", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Tesla"], "entity_annotations": ["Nikola_Tesla"], "answer_entity_name": null, "predicted_answer": "The Tesla Coil, which is a type of high", "extracted_entity": null}
{"question": "consequently, what is not considered a \"serious painter\" by some contemporary artists, who often regard his work as bourgeois and kitsch?", "answer": "Rockwell ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "I think the answer to this question is that he", "extracted_entity": null}
{"question": "who spent the next 10 years painting for Look magazine?", "answer": "Norman Rockwell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Norman Rockwell."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Norman Rockwell.", "extracted_entity": null}
{"question": "when was his last painting for the Post published?", "answer": "1963", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1963"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "2004. The last one was a cartoon", "extracted_entity": null}
{"question": "when did Rockwell marry his first wife, Irene O'Connor,?", "answer": "In 1916.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1916."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "3/5/1930", "extracted_entity": null}
{"question": "did Norman Rockwell spend the winter months as artist-in-residence at Otis College of Art as well as Design?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "no, but he was a student there.", "extracted_entity": null}
{"question": "did Norman Rockwell publish a total of 321 original covers for The Saturday Evening Post over 47 years?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Rockwell also commissioned to illustrate over 40 books including Tom Sawyer as well as Huckleberry Finn?", "answer": "Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Rockwell was also commissioned to illustrate over 40 books including Tom Sawyer and Huckleberry Finn. "], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "do his works enjoy a broad popular appeal in the United States, where Rockwell is most famous for the cover illustrations of everyday life scenarios he created for The Saturday Evening Post magazine over more than four decades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "no, they don\u2019t.", "extracted_entity": null}
{"question": "are among the best-known of Rockwell's works the Willie Gillis series, Rosie the Riveter (although his Rosie was reproduced less than others of the day), Saying Grace (1951), and the Four Freedoms series?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes, all are well known.", "extracted_entity": null}
{"question": "is he also noted for his work for the Boy Scouts of America (BSA)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Norman_Rockwell"], "answer_entity_name": null, "predicted_answer": "yes, he was a member of the executive committee", "extracted_entity": null}
{"question": "is the octopus a cephalopod?", "answer": "Yes, the octopus is a cephalopod.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus is a cephalopod."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, the octopus is a cephalop", "extracted_entity": null}
{"question": "does the octopus have two eyes?", "answer": "Yes, the octopus has two eyes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has two eyes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no. the octopus has three eyes.", "extracted_entity": null}
{"question": "does the octopus have a hard beak?", "answer": "Yes, the octopus has a hard beak.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopus has a hard beak."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "No, it doesn't have a hard beak", "extracted_entity": null}
{"question": "where is the mouth of an octopus?", "answer": "The mouth of an octopus is at the center point of the arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The mouth of an octopus is at the center point of the arms.", "The octopus' mouth is at the center point of the arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "the anus", "extracted_entity": null}
{"question": "around how many recognized octopus species are there?", "answer": "There are around 300 recognized octopus species.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are around 300 recognized octopus species."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "300-500", "extracted_entity": null}
{"question": "how many arms does an octopus have?", "answer": "An octopus has four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["An octopus has four pairs of arms.", "The octopus has four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "8!", "extracted_entity": null}
{"question": "what is the range of lifespans of the octopus?", "answer": "Octopuses can live from six months to five years.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Octopuses can live from six months to five years.", "The octopus has a short lifespan."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "1-5 years (average 2 years", "extracted_entity": null}
{"question": "what is an example of something an octopus learned in a research experiment?", "answer": "In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In research experiments, octopus can be trained to distinguish between different shapes and patterns.", "An octopus learned to solve mazes."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "an octopus can learn to distinguish between different colors", "extracted_entity": null}
{"question": "how can an octopus defend against predators?", "answer": "To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["To defend against predators, an octopus can hide, flee quickly, expel ink, or use color-changing camouflage.", "They can hide, flee, expel ink, or use camouflage."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "by using camouflage, changing color, or by playing", "extracted_entity": null}
{"question": "are Octopus highly intelligent?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes, they are very intelligent. They are capable", "extracted_entity": null}
{"question": "do males use a specialized arm called a hectocotylus to insert spermatophores?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do the octopuses in the less familiar Cirrina suborder have two fins and an internal shell?", "answer": "Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, the octopuses in the less familiar Cirrina suborder have two fins and an internal shell."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes, yes and no.", "extracted_entity": null}
{"question": "do octopuses have two eyes and four pairs of arms?", "answer": "Yes, octopuses have two eyes and four pairs of arms.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, octopuses have two eyes and four pairs of arms."], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no, octopuses have three eyes and eight", "extracted_entity": null}
{"question": "is octopus a common food in Mediterranean cuisine as well as Portuguese cuisine?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is the collective form octopus usually reserved for animals consumed for food?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "yes. (octopodes is also correct)", "extracted_entity": null}
{"question": "has an octopus a hard beak, with its mouth at the center point of the arms?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Octopus"], "answer_entity_name": null, "predicted_answer": "no. the beak is at the front of", "extracted_entity": null}
{"question": "what happened in 1968 through 1971?", "answer": "Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Devoting his full energies to his work, Picasso became more daring, his works more colorful and expressive, and from 1968 through 1971"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The Vietnam War was going on.", "extracted_entity": null}
{"question": "what happened in 1838?", "answer": "Don Jos\u00e9 Ruiz y Blasco was born", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Don Jos\u00e9 Ruiz y Blasco was born"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The Cherokee were forced to move to Oklahoma on the", "extracted_entity": null}
{"question": "who was not long in finding another lover, Jacqueline Roque?", "answer": "Picasso.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Picasso."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "when did Picasso make his first trip to Paris?", "answer": "1900", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1900"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "1900", "extracted_entity": null}
{"question": "give an example of the most comprehensive records extant of any major artists beginnings. ?", "answer": "Museu Picasso", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Museu Picasso"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "The most comprehensive records of any major artist's beginnings", "extracted_entity": null}
{"question": "is in the 1996 movie Surviving Picasso Picasso played by actor Anthony Hopkins?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "no, Picasso was played by actor Jeremy Irons", "extracted_entity": null}
{"question": "is the U.S. copyright representative for the Picasso Administration the Artists Rights Society?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "No, the U.S. copyright representative for the", "extracted_entity": null}
{"question": "is he one of the most recognized figures in 20th-century art?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Yes, he is. His works are in the", "extracted_entity": null}
{"question": "is he best known for co-founding the Cubist movement and for the wide variety of styles embodied in his work?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "are among his most famous works the proto-Cubist Les Demoiselles d'Avignon (1907) and Guernica (1937), his portrayal of the German bombing of Guernica during the Spanish Civil War?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pablo_Picasso"], "answer_entity_name": null, "predicted_answer": "Pablo Picasso", "extracted_entity": null}
{"question": "are pianos used in Western music?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Yes. A piano is a type of keyboard instrument", "extracted_entity": null}
{"question": "are \"upright pianos\" called \"vertical pianos\"?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "no. \"upright pianos\" are called", "extracted_entity": null}
{"question": "did Bartolomeo Cristofori invent the modern piano?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "no, he invented the harpsichord.", "extracted_entity": null}
{"question": "what is the middle pedal called on grand pianos?", "answer": "the sostenuto pedal.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the sostenuto pedal.", "sostenuto pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "Sostenuto", "extracted_entity": null}
{"question": "how many black keys do modern pianos have?", "answer": "36.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["36.", "36"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "36.", "extracted_entity": null}
{"question": "what is the sustain pedal called?", "answer": "The pedal", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The pedal", "damper pedal"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "the damper pedal.", "extracted_entity": null}
{"question": "where is Irving Berlin's piano located?", "answer": "In the Smithsonian Museum.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the Smithsonian Museum.", "in the Smithsonian Museum"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "at the Irving Berlin Music Department of the Library of", "extracted_entity": null}
{"question": "what kind of piano did Irving Berlin play?", "answer": "The transposing piano.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The transposing piano.", "transposing piano"], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "a player piano.", "extracted_entity": null}
{"question": "is the left-most pedal on a grand piano called the una corda?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes, the left-most pedal on a grand piano is called the una corda."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "No, the una corda pedal is the middle", "extracted_entity": null}
{"question": "is it advantageous for a grand piano's metal plate to be quite massive?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "It is advantageous for the plate to be quite massive."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "I think so. It is a good idea to", "extracted_entity": null}
{"question": "is the mechanism in an upright piano perpendicular to its keys?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "The mechanism in upright pianos is perpendicular to the keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "No. It is parallel to the keys.", "extracted_entity": null}
{"question": "about how tall is a typical studio piano?", "answer": "Studio pianos are around 42 to 45 inches tall.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Studio pianos are around 42 to 45 inches tall."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "about 5 feet tall", "extracted_entity": null}
{"question": "what are a piano's keys generally made of?", "answer": "Piano keys are generally made of spruce or basswood.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano keys are generally made of spruce or basswood."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "wood", "extracted_entity": null}
{"question": "how many total keys does a typical modern piano have?", "answer": "88 keys.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["88 keys.", "A modern piano has 88 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "88 keys.", "extracted_entity": null}
{"question": "why are upright pianos more compact?", "answer": "Upright pianos are more compact because the frame and strings are vertical.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Upright pianos are more compact because the frame and strings are vertical."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "because the strings are not horizontal but vertical, so", "extracted_entity": null}
{"question": "do older pianos have more keys than modern pianos?", "answer": "no.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no.", "Many older pianos only have 85 keys."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "yes, older pianos have more keys than modern", "extracted_entity": null}
{"question": "what are the names of a piano's pedals?", "answer": "Piano pedals from left to right: una corda, sostenuto, and damper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piano pedals from left to right: una corda, sostenuto, and damper.", "The names of a piano's pedals are una corda, sostenuto, and damper."], "entity_annotations": ["Piano"], "answer_entity_name": null, "predicted_answer": "soft pedal, sostenuto pedal, and sustain", "extracted_entity": null}
{"question": "what happened in the January 15, 1882?", "answer": "On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["On January 15, 1882 Renoir met the composer Richard Wagner at his home in Palermo, Sicily."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "The 15th of January 1882 was", "extracted_entity": null}
{"question": "what happened in 1887?", "answer": "In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1887, a year when Queen Victoria celebrated her Golden Jubilee, and upon the request of the queen's associate, Phillip Richbourg, he donated several paintings to the \"French Impressionist Paintings\" catalog as a token of his loyalty."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "the first patent for a three-wheeled motorcycle", "extracted_entity": null}
{"question": "is one of the best known Impressionist works Renoir's 1876 Dance at Le Moulin de la Galette?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "The painting is also known as Bal du moulin", "extracted_entity": null}
{"question": "was Pierre-Auguste Renoir born in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Yes, he was born in 1841.", "extracted_entity": null}
{"question": "have two of Renoir's paintings sold for more than US0 million?", "answer": "Yes (any painting sells for more than $0 million though...)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes (any painting sells for more than $0 million though...)"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "as a celebrator of beauty, and especially feminine sensuality, has it been said that ``Renoir is the final representative of a tradition which runs directly from Rubens to Watteau''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Yes. (But Renoir is also the first", "extracted_entity": null}
{"question": "did Pierre-Auguste Renoir die in Limoges, Haute-Vienne, France, the child of a working class family?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "as a boy, did he work in a porcelain factory where his drawing talents led to him being chosen to paint designs on fine china?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Pierre-Auguste_Renoir"], "answer_entity_name": null, "predicted_answer": "Yes, in the early 1900s,", "extracted_entity": null}
{"question": "is Portuguese an official language of Andorra?", "answer": "No", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No", "No, Portuguese is not an official language of Andorra."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "No. Catalan and French are official languages of And", "extracted_entity": null}
{"question": "does the Portuguese language have its roots in the Latin language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese is derived from Latin."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes, the Portuguese language is a Romance language,", "extracted_entity": null}
{"question": "does Portuguese contain words from the Arabic language?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes, Portuguese contains words from the Arabic language."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes, some of the words are from the Arabic", "extracted_entity": null}
{"question": "where are there small Portuguese-speaking communities?", "answer": "Former overseas colonies of Portugal such as Macau and East Timor ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Former overseas colonies of Portugal such as Macau and East Timor ", "There are small Portuguese-speaking communities in Macau and East Timor."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Brazil, Mozambique, Angola, Cape Verde,", "extracted_entity": null}
{"question": "what are the two main groups of Portuguese dialects?", "answer": "Those of Brazil and those of the Old World", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Those of Brazil and those of the Old World", "The two main groups of dialects are Brazil and the Old World."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "European and Brazilian", "extracted_entity": null}
{"question": "what event marked the end of the Old Portuguese period?", "answer": "The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The publication of the Cancioneiro Geral by Garcia de Resende in 1516", "The end of the Old Portuguese period was marked by the publication of the Cancioneiro Geral by Garcia de Resende."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "the discovery of Brazil by the Portuguese", "extracted_entity": null}
{"question": "which government established Portuguese as it's third official language in order to meet the requirements to apply for full membership in the CPLP?", "answer": "Equatorial Guinea", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Equatorial Guinea", "The government of Equatorial Guinea established Portuguese as its third official language in order to meet the requirements to apply for full membership in the CPLP."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Angola", "extracted_entity": null}
{"question": "by 2050, what will the total population of Portuguese speakers in the world be?", "answer": "335 million people", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["335 million people", "By 2050, Portuguese-speaking countries will have a total population of 335 million people."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "1,500 million.", "extracted_entity": null}
{"question": "in which country is Portuguese the co-official language with Cantonese Chinese?", "answer": "Macau", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Macau", "Portuguese is the co-official language with Cantonese Chinese in Macau."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Macau", "extracted_entity": null}
{"question": "wasn't Portuguese language spread by arriving Roman soldiers?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "no, Portuguese language was spread by the Portuguese people", "extracted_entity": null}
{"question": "(where French has a similar phenomenon, with alveolar affricates instead of postalveolars?", "answer": "Quebec.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Quebec."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "In French, the palatalization of /k", "extracted_entity": null}
{"question": "give an example of the official languages of the special administrative region of Macau and East Timor, . ?", "answer": "Portuguese", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Portuguese"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "ortuguese and Chinese (Mandarin) are the", "extracted_entity": null}
{"question": "is Portuguese an official language of several international organizations?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Portuguese is an official language of the United", "extracted_entity": null}
{"question": "is it derived from the Latin spoken by the romanized pre-Roman peoples of the Iberian Peninsula (namely the Gallaeci, the Lusitanians, the Celtici and the Conii) around 2000 years ago?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "no, it is derived from the Latin spoken by", "extracted_entity": null}
{"question": "was it used as the exclusive lingua franca on the island of Sri Lanka for almost 350 years?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Portuguese_language"], "answer_entity_name": null, "predicted_answer": "yes, from the 16th century to", "extracted_entity": null}
{"question": "is the city named after the apostle Saint Peter?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "yes, the city was named after the apostle", "extracted_entity": null}
{"question": "has the terrain in the city been artificially raised?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "no, it has not.", "extracted_entity": null}
{"question": "what were some other names of the city?", "answer": "Petrograd, Leningrad, and Piter", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Petrograd, Leningrad, and Piter", "Petrograd and Leningrad."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "New Amsterdam, New Orange, New York", "extracted_entity": null}
{"question": "what festival features fireworks  celebrating the end of the school year?", "answer": "The White Nights Festival in Saint Petersburg", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The White Nights Festival in Saint Petersburg", "The White Nights Festival."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The Fourth of July", "extracted_entity": null}
{"question": "where do most people in urban Saint Petersburg live?", "answer": "apartments", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["apartments", "Apartments."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "In apartment buildings.", "extracted_entity": null}
{"question": "what was made in Russia's largest foundry?", "answer": "thousands of sculptures and statues", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["thousands of sculptures and statues", "Thousands of sculptures and statues."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "5,000,000,000 bullets for", "extracted_entity": null}
{"question": "when did the crime level become higher?", "answer": "After the October revolution.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the October revolution."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "when the police stopped to do their job", "extracted_entity": null}
{"question": "how to travel to work from the city to Moscow?", "answer": "railway", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["railway", "One way is through the Moscow-Saint Petersburg Railway."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "by train, by car, by plane, by", "extracted_entity": null}
{"question": "who laid down the Peter and Paul Fortress?", "answer": "Peter the Great ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Peter the Great "], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Peter the Great", "extracted_entity": null}
{"question": "when was the 7th symphony premiered?", "answer": "1942", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1942"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "1925, Paris", "extracted_entity": null}
{"question": "has Equestrianism been a long tradition, popular among the Tsars and aristocracy?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "yes, it has been popular since the 19", "extracted_entity": null}
{"question": "give an example of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995. ?", "answer": "Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kirov Stadium (now demolished) was one of the largest stadiums anywhere in the world, and the home to FC Zenit St. Petersburg in 1950-1993 and 1995."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The largest stadium in the world is the National Stadium", "extracted_entity": null}
{"question": "give an example of the best known symphony orchestras in the world under the leadership of conductors Yevgeny Mravinsky and Yuri Temirkanov. ?", "answer": "The Leningrad Philharmonic Orchestra", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Leningrad Philharmonic Orchestra"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The best known symphony orchestras in the world", "extracted_entity": null}
{"question": "give an example of the longest, most destructive, and most lethal sieges of major cities in modern history. ?", "answer": "The Siege of Leningrad", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Siege of Leningrad"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "The longest, most destructive, and most lethal sie", "extracted_entity": null}
{"question": "were the city's other names Petrograd (, 1914 -- 1924) and Leningrad (, 1924 -- 1991)?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "No. The city's other names were Petrograd", "extracted_entity": null}
{"question": "is it often called just Petersburg (-RRB- and is informally known as Piter (-RRB-?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "Yes, and yes.", "extracted_entity": null}
{"question": "founded by Tsar Peter I of Russia on 27 May 1703, was it the capital of the Russian Empire for more than two hundred years (1713 -- 1728, 1732 -- 1918)?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Saint_Petersburg"], "answer_entity_name": null, "predicted_answer": "St. Petersburg", "extracted_entity": null}
{"question": "is the standard of living in San Franciscio high?", "answer": "Yes, San Francisco is characterized by a high standard of living.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, San Francisco is characterized by a high standard of living.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "no, the standard of living in San Francisco is", "extracted_entity": null}
{"question": "does San Francisco have a high percentage of gay and lesbian individuals?", "answer": "Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, according to the 2005 American Community Survey, San Francisco has the highest percentage of gay and lesbian individuals of any of the 50 largest U.S. cities, at 15.4%.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes. The gay and lesbian community is very prominent", "extracted_entity": null}
{"question": "is San Franscico a popular tourist destination?", "answer": "Yes. San Francisco is a popular international tourist destination.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes. San Francisco is a popular international tourist destination.", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, San Francisco is a popular tourist destination.", "extracted_entity": null}
{"question": "how large is the population of San Francisco?", "answer": "The estimated population of San Francisco in the year 2008 was 808,976.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The estimated population of San Francisco in the year 2008 was 808,976.", "San Francisco has an estimated population of 808,976."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "800,000", "extracted_entity": null}
{"question": "how many state parks are in San Francisco?", "answer": "There is only one park managed by the California State Park system: Candlestick Point.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There is only one park managed by the California State Park system: Candlestick Point.", "There is one state park in San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "0. That\u2019s right, there are no", "extracted_entity": null}
{"question": "where is San Francisco?", "answer": "San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco is located on the West Coast of the United States at the tip of the San Francisco Peninsula.", "San Francisco is in California."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco is in California.", "extracted_entity": null}
{"question": "what is the cause of local earthquakes?", "answer": "It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["It was the San Andreas Fault which slipped and caused the earthquakes in 1906 and 1989.", "The nearby San Andreas and Hayward Faults are responsible for much earthquake activity."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "faulting in the earth's crust.", "extracted_entity": null}
{"question": "are more residents employed by small businesses than in 1977?", "answer": "The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The number of San Franciscans employed by firms of more than 1,000 employees has fallen by half since 1977.", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "yes. \u00a0In 1977,", "extracted_entity": null}
{"question": "how does poverty in San Francisco compare to the nation-wide average?", "answer": "The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The city's poverty rate is 11.8% and the number of families in poverty stands at 7.4%, both lower than the national average.", "San Francisco's poverty rate is lower than the national average."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco has the highest rate of poverty among the", "extracted_entity": null}
{"question": "was Mission Bay campus opened in 2003?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "no, it was opened in 1997.", "extracted_entity": null}
{"question": "was the University of San Fransisco founded in 1855?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "no, it was founded in 1851.", "extracted_entity": null}
{"question": "is Golden Gate Park the largest city park?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes. Golden Gate Park is 1,017", "extracted_entity": null}
{"question": "what is Northern California's most widely circulated newspaper?", "answer": "San Francisco Chronicle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco Chronicle"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The San Francisco Chronicle.", "extracted_entity": null}
{"question": "what makes San Francisco among the top-ten North American destinations for conventions and conferences?", "answer": "San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco has a large hotel infrastructure and a world-class convention facility in the Moscone Center", "a large hotel infrastructure and a world-class convention facility in the Moscone Center"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "it\u2019s the city\u2019s ability to offer an extraordinary", "extracted_entity": null}
{"question": "who manages Candlestick point?", "answer": "California State Park system", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["California State Park system", "the California State Park system"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The San Francisco Recreation and Parks Department", "extracted_entity": null}
{"question": "is Crissy field an airfield?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "yes. It was a naval air station during WW", "extracted_entity": null}
{"question": "does the de Young museum house the Asian Art Museum?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "No"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "no. The Asian Art Museum is in the Civic", "extracted_entity": null}
{"question": "what prompted the city to upgrade its building codes?", "answer": "earthquakes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["earthquakes", "The threat of major earthquakes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "the city has not changed its building codes, but", "extracted_entity": null}
{"question": "like many larger where cities, San Francisco is a minority-majority city, as non-Hispanic whites comprise less than half of the population?", "answer": "U.S.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["U.S."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "True, according to the 2000 census,", "extracted_entity": null}
{"question": "who laid out a street plan for the expanded settlement?", "answer": "William Richardson and Alcalde Francisco de Haro.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["William Richardson and Alcalde Francisco de Haro."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Sir Thomas More", "extracted_entity": null}
{"question": "when was a 43-acre Mission Bay campus opened?", "answer": "2003", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["2003"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1971", "extracted_entity": null}
{"question": "when did it sign on the air?", "answer": "In 1941.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1941."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "it signed on the air in 1999.", "extracted_entity": null}
{"question": "give an example of the largest two-year community colleges in the country. ?", "answer": "The City College of San Francisco.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The City College of San Francisco."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "The largest community colleges in the United States are the", "extracted_entity": null}
{"question": "give an example of the ten Beta World Cities. ?", "answer": "San Trancisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Trancisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "New York City, Los Angeles, Chicago, Washington", "extracted_entity": null}
{"question": "the only consolidated city-county in California, does it encompass a land area of on the northern end of the San Francisco Peninsula, making it the second most densely populated city in the United States?", "answer": "San Francisco", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["San Francisco"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "San Francisco", "extracted_entity": null}
{"question": "is San Francisco also the financial, cultural, and transportation center of the larger San Francisco Bay Area, a region of 7.4 million people?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "Yes, and San Francisco is the only consolidated city", "extracted_entity": null}
{"question": "in what year did the Spanish establish a fort at the Golden Gate and a mission named for Francis of Assisi on the site?", "answer": "In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1776, the Spanish established a fort at the Golden Gate and a mission named for Francis of Assisi on the site."], "entity_annotations": ["San_Francisco"], "answer_entity_name": null, "predicted_answer": "1776.", "extracted_entity": null}
{"question": "does Swahili have dipthongs?", "answer": "Yes, Swahili has diphthongs. (Typo)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili has diphthongs. (Typo)"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes. They are the following:", "extracted_entity": null}
{"question": "is Swahili a Bantu language?", "answer": "Yes, Swahili is a Bantu language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is a Bantu language.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes, it is a Bantu language", "extracted_entity": null}
{"question": "did Uganda make Swahili a required subject in primary schools?", "answer": "Yes, Uganda made Swahili a required subject in primary schools.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Uganda made Swahili a required subject in primary schools.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "what is the Swahili word for the Swahili language?", "answer": "Kiswahili is the Swahili word for the Swahili language.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Kiswahili is the Swahili word for the Swahili language.", "Kiswahili"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Kiswahili", "extracted_entity": null}
{"question": "is Swahili spoken in Mozambique?", "answer": "Yes, Swahili is spoken in parts of northern Mozambique.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Swahili is spoken in parts of northern Mozambique.", "yes"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "how does Swahili time run?", "answer": "Swahili time runs from dawn to dusk.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili time runs from dawn to dusk.", "from dawn to dusk"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "from 6am to 6pm, and", "extracted_entity": null}
{"question": "how many noun classes does Swahili have?", "answer": "Swahili has sixteen noun classes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swahili has sixteen noun classes.", "sixteen"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "18 noun classes.", "extracted_entity": null}
{"question": "in what year did Germany seize Tanganyika?", "answer": "Germany seized Tanganyika in 1886.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Germany seized Tanganyika in 1886.", "1886"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "1885", "extracted_entity": null}
{"question": "which is the sub-Saharan indigenous language with the greatest number of speakers?", "answer": "Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Hausa of West Africa is the sub-Saharan indigenous language with the greatest number of speakers.", "Hausa"], "entity_annotations": ["Swahili_language"], "answer_entity_name": null, "predicted_answer": "Swahili, which is spoken by more than", "extracted_entity": null}
{"question": "is Swedish a descendant of Old Norse?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "No, they are two different languages.", "extracted_entity": null}
{"question": "does Swedish use the perfect participle to form the present perfect tense?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "Yes. The present perfect tense is formed by combining", "extracted_entity": null}
{"question": "how many Swedish speakers were reported in Canada in 2001?", "answer": "16,915.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["16,915.", "There are 16,915 reported Swedish speakers in Canada."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "337,000.", "extracted_entity": null}
{"question": "how many letters are in the basic Latin alphabet?", "answer": "26.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["26.", "26"], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "26", "extracted_entity": null}
{"question": "when did a standard and common language become available to all Swedes?", "answer": "In the 20th Century.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In the 20th Century.", "It was during the 20th century that a common, standardized national language became available to all Swedes."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "when the first Swedish translation of the Bible was published", "extracted_entity": null}
{"question": "when did the Soviet military forces occupy Estonia?", "answer": "1944.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1944.", "In 1944."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "in 1940", "extracted_entity": null}
{"question": "what are the differences between English and Swedish pronouns?", "answer": "Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Swedish pronouns are basically the same as those of English but distinguish two genders and have an additional object form, derived from the old dative form, as well as a distinct genitive case."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "in Swedish, pronouns are inflected. So", "extracted_entity": null}
{"question": "how does Swedish language counterbalance the absence of cases?", "answer": "The lack of cases in Swedish is compensated by a wide variety of prepositions.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The lack of cases in Swedish is compensated by a wide variety of prepositions."], "entity_annotations": ["Swedish_language"], "answer_entity_name": null, "predicted_answer": "by using the word order.", "extracted_entity": null}
{"question": "is Taipei in a valley?", "answer": "Taipei is in the valleys of the Keelung and Xindian Rivers", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei is in the valleys of the Keelung and Xindian Rivers", "Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "no, it's on a hill.", "extracted_entity": null}
{"question": "was Taipei founded in the 18th century?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "yes, it was founded in the 18th", "extracted_entity": null}
{"question": "is the National Palace Museum in Taipei?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "no, it is in Taibei City", "extracted_entity": null}
{"question": "what is along the city's western border?", "answer": "The Danshui River borders Taipei to the west.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Danshui River borders Taipei to the west.", "Taipei has the Danshui River along its western border."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "a lake, a river, and a park", "extracted_entity": null}
{"question": "what is Taipei 101?", "answer": "Taipei 101 is a 101-floor landmark skyscraper.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei 101 is a 101-floor landmark skyscraper."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "it's a building in Taipei, Taiwan.", "extracted_entity": null}
{"question": "what are some annual festivals in Taipei?", "answer": "The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The Lantern Festival, Tomb-Sweeping Day, Dragon Boat Festival, and the Mid-Autumn Festival", "Some annual festivals include Tomb-Sweping Day, the Dragon Boat Festival, the Ghost Festival, and the Mid-Autumn Festival."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "Taipei International Film Festival, Taipei Fringe Festival,", "extracted_entity": null}
{"question": "when is the Pacific typhoon season?", "answer": "Between June and October.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Between June and October.", "It's between June and October."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "It starts in May and ends in November", "extracted_entity": null}
{"question": "who sets up shrines?", "answer": "Many homes and business set up shrines.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Many homes and business set up shrines.", "Many homes and businesses would set up shrines."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "the people", "extracted_entity": null}
{"question": "what does Taipei produce?", "answer": "Taipei produces high technology and hi-tech components", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Taipei produces high technology and hi-tech components", "Taipei produces textiles, apparel, electronics, machinery, printed materials, precision equipment, food, and beverages."], "entity_annotations": ["Taipei"], "answer_entity_name": null, "predicted_answer": "it produces a lot of plastic waste.", "extracted_entity": null}
{"question": "is the trumpet the musical instrument with the highest register in the brass family?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No. The piccolo trumpet is.", "extracted_entity": null}
{"question": "is a trumpet played by blowing air through closed lips?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, it's played by blowing air through an", "extracted_entity": null}
{"question": "is the most common type of trumpet the B trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No. The most common type of trumpet is the", "extracted_entity": null}
{"question": "what is the earliest date of the trumpet?", "answer": "1500 BC", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1500 BC", "around 1500 BC"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the earliest date of the trumpet is 2500", "extracted_entity": null}
{"question": "what is a trumpet constructed of?", "answer": "brass tubing", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["brass tubing", "brass tubing bent twice into an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "brass", "extracted_entity": null}
{"question": "what are the smallest trumpets referred to as?", "answer": "Piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Piccolo trumpets", "piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Piccolo Trumpets", "extracted_entity": null}
{"question": "why do trumpets have a bright, loud sound?", "answer": "The trumpet and trombone share a roughly cylindrical bore.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The trumpet and trombone share a roughly cylindrical bore.", "Due to the cylindrical bore"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "they have a narrow bore.", "extracted_entity": null}
{"question": "why does the cornet have a slightly mellower tone than the trumpet?", "answer": "The cornet and flugelhorn have conical bores", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The cornet and flugelhorn have conical bores", "because it has conical bores"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the cornet has a smaller bore, and a", "extracted_entity": null}
{"question": "what makes a trumpet fully chromatic?", "answer": "able to play all twelve pitches of Western music", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["able to play all twelve pitches of Western music"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a valve system", "extracted_entity": null}
{"question": "were trumpet players heavily guarded?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, but they were in the back of the", "extracted_entity": null}
{"question": "are pocket trumpets compact B trumpets?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no. The sound is different and the feel is", "extracted_entity": null}
{"question": "was Dizzy Gillespie a famous trumpeter in 1998?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, he died in 1993.", "extracted_entity": null}
{"question": "what shape is a trumpet bent into?", "answer": "oblong", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["oblong", "an oblong shape"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a trumpet", "extracted_entity": null}
{"question": "what trumpet was the first to be allowed in the Christian Church?", "answer": "slide trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["slide trumpets", "Slide trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the trumpet of the Last Judgement.", "extracted_entity": null}
{"question": "the trumpet can be confused with what close relative?", "answer": "the cornet", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the cornet", "the coronet"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the bugle. trumpets are much more versatile", "extracted_entity": null}
{"question": "what can be augmented with a fourth valve?", "answer": "the flugelhorn", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the flugelhorn", "flugelhorn"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a tuba", "extracted_entity": null}
{"question": "what was Maynard Ferguson noted for being able to play accurately?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "he was able to play the trumpet in such a", "extracted_entity": null}
{"question": "what was built to play in both B and A keys?", "answer": "piccolo trumpets", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["piccolo trumpets"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a binaural piano.", "extracted_entity": null}
{"question": "what happened in 1894?", "answer": "Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\"", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Jean-Baptiste Arban published \"Complete Conservatory Method for Trumpet.\""], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "a man named Louis Le Prince made a movie", "extracted_entity": null}
{"question": "what happened in nineteen fifty seven?", "answer": "Maynard Ferguson formed his own band.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Maynard Ferguson formed his own band."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "the first satellite was launched into space.", "extracted_entity": null}
{"question": "is the Breeze Eazy' ' method sometimes used to teach younger students?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "es, it is sometimes used to teach younger students", "extracted_entity": null}
{"question": "do many players use a smaller mouthpiece on the piccolo trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, most players use a 1/4", "extracted_entity": null}
{"question": "is the trumpet constructed of brass tubing bent twice into an oblong shape?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no, the trumpet is constructed of brass tubing bent", "extracted_entity": null}
{"question": "give an example of the most influential musicians of the 20th century. ?", "answer": "Miles Davis", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Miles Davis"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "Some of the most influential musicians of the 20", "extracted_entity": null}
{"question": "were Slide trumpets the first trumpets allowed in the Christian church?", "answer": "Slide trumpets were the first trumpets allowed in the Christian church. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Slide trumpets were the first trumpets allowed in the Christian church. "], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "no. the first trumpets were the natural trump", "extracted_entity": null}
{"question": "are they constructed of brass tubing bent twice into an oblong shape, and are played by blowing air through closed lips, producing a ``buzzing'' sound which starts a standing wave vibration in the air column inside the trumpet?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are there several types of trumpet?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Trumpet"], "answer_entity_name": null, "predicted_answer": "yes, there are many different types of trumpet.", "extracted_entity": null}
{"question": "was the Ottoman script replaced with a variant of the Latin alphabet?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes, the Ottoman script was replaced with a variant", "extracted_entity": null}
{"question": "is there a definite article in Turkish language?", "answer": "No.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No.", "No"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Yes. It is \"bir\" in singular and", "extracted_entity": null}
{"question": "is it possible to alter the word order to stress the importance of a certain phrase?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes.", "Yes"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "yes, but not always. In some cases,", "extracted_entity": null}
{"question": "what is the official language of Turkey?", "answer": "Turkish.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Turkish.", "Turkish is the official language of Turkey."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Turkish is the official language of Turkey.", "extracted_entity": null}
{"question": "when was the Turkish Language Association founded?", "answer": "In 1928.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In 1928.", "The Turkish Language Association was founded in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "in 1932", "extracted_entity": null}
{"question": "what are the most significant foreign contributors to Turkish vocabulary?", "answer": "Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. ", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. "], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "Arabic and Persian", "extracted_entity": null}
{"question": "who were the Orkhon inscriptions built for?", "answer": "For the prince Kul Tigin and his brother Emperor Bilge Khan.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["For the prince Kul Tigin and his brother Emperor Bilge Khan.", "The Orkhon inscriptions were erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "they were built for the khan of the tur", "extracted_entity": null}
{"question": "what was one of the results of the introduction of the new Turkish alphabet?", "answer": "A dramatic increase in literacy from Third World levels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["A dramatic increase in literacy from Third World levels.", "There was a dramatic increase in literacy from its original Third World levels."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The new alphabet made it easier for the people of", "extracted_entity": null}
{"question": "why does Lewis use \"absolute case\" instead of \"nominative\"?", "answer": "Because it is also used for the indefinite accusative.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Because it is also used for the indefinite accusative."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "because he is a Latinist, and Latinists", "extracted_entity": null}
{"question": "how many people speak Turkish worldwide?", "answer": "There are roughly another 10 million native speakers worldwide.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["There are roughly another 10 million native speakers worldwide."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "80 million", "extracted_entity": null}
{"question": "in Turkish, which syllable usually has the stress?", "answer": "Stress is usually on the last syllable", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stress is usually on the last syllable"], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "The last syllable.", "extracted_entity": null}
{"question": "is the spelling of Turkish phonetic?", "answer": "yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes, Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "No. It is a phonetic spelling, but", "extracted_entity": null}
{"question": "what percentage of the words in Turkish vocabulary have Turkish origins?", "answer": "86% of the Turkish vocabulary have Turkish origins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["86% of the Turkish vocabulary have Turkish origins."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "35%", "extracted_entity": null}
{"question": "after what event was the Turkish Language Association established?", "answer": "After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "after the 1923 Turkish War of Independence", "extracted_entity": null}
{"question": "what exception to the rules of vowel harmony do compound words have?", "answer": "In compound words, the vowels need not harmonize between the constituent words of the compound.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In compound words, the vowels need not harmonize between the constituent words of the compound."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "the first vowel of the second word is not harmon", "extracted_entity": null}
{"question": "what are the patterns of vowel harmony?", "answer": "Vowels are characterised by two features: front/back and rounded/unrounded.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vowels are characterised by two features: front/back and rounded/unrounded."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "there are 2 patterns of vowel harmony in Turkish", "extracted_entity": null}
{"question": "what are the ways to join groups of nouns?", "answer": "Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Two nouns, or groups of nouns, may be joined in either of two ways: definite or indefinite."], "entity_annotations": ["Turkish_language"], "answer_entity_name": null, "predicted_answer": "(1) with the conjunction \u201cand\u201d (", "extracted_entity": null}
{"question": "does Vietnamese have a comparatively large number of vowels?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no, it is rather a tonal language.", "extracted_entity": null}
{"question": "is Vietnamese the mother tongue of the Vietnamese people?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No, it is not. The Vietnamese people have", "extracted_entity": null}
{"question": "is Vietnamese the official administrative language of Vietnam?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No, Vietnamese is not the official administrative language of", "extracted_entity": null}
{"question": "was Vietnamese influenced by Chinese?", "answer": "Much vocabulary has been borrowed from Chinese.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Much vocabulary has been borrowed from Chinese.", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, the vietnamese were influenced by the Chinese", "extracted_entity": null}
{"question": "when did written Vietnamese become the official administrative language?", "answer": "the 20th century", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the 20th century", "Vietnamese became the official administrative language by the 20th century."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1919, under the French protectorate", "extracted_entity": null}
{"question": "have consonant clusters been lost?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, there are two consonant clusters that have", "extracted_entity": null}
{"question": "is the Red River Delta traditionally poor?", "answer": "Not compared to Nghe An, Ha Tinh, or Quang Binh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Not compared to Nghe An, Ha Tinh, or Quang Binh", "Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, it is traditionally poor. The delta is", "extracted_entity": null}
{"question": "what kind of words have been borrowed from Chinese?", "answer": "Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Words that denote abstract ideas in the same way European languages borrow form Latin and Greek.", "Vietnamese adopts words from Chinese which express abstract ideas."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Many words have been borrowed from Chinese, such as", "extracted_entity": null}
{"question": "is Vietnam independent from France?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Yes, Vietnam is independent from France.", "extracted_entity": null}
{"question": "was Vietnamese formally written using the Chinese writing system?", "answer": "Yes, Vietnamese was *formerly* written using the Chinese writing system.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese was *formerly* written using the Chinese writing system."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, until the 13th century", "extracted_entity": null}
{"question": "does Vietnamese have a large number of vowels?", "answer": "Yes, Vietnamese has a comparatively large number of vowels.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, Vietnamese has a comparatively large number of vowels."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no, Vietnamese has 9 vowels.", "extracted_entity": null}
{"question": "what are the three dialect regions of Vietnamese?", "answer": "The three regions are North, Central, and South.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The three regions are North, Central, and South."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "Northern, Central, and Southern.", "extracted_entity": null}
{"question": "when was the reunification of Vietnam?", "answer": "The reunification of Vietnam occurred in 1975-76.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The reunification of Vietnam occurred in 1975-76."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "1975", "extracted_entity": null}
{"question": "how many tones do Northern varieties have?", "answer": "Northern Vietnamese has six tones.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Northern Vietnamese has six tones."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "4 tones, or 5 tones, or", "extracted_entity": null}
{"question": "does Vietnamese borrow from Latin and Greek?", "answer": "No, Vietnamese does not borrow from Latin and Greek.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Vietnamese does not borrow from Latin and Greek."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "yes, but the number of words borrowed is quite", "extracted_entity": null}
{"question": "did Vietnamese come to predominate politically in 2nd century BC?", "answer": "No, Chinese came to predominate politically in the 2nd century B.C.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["No, Chinese came to predominate politically in the 2nd century B.C."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "No. There were no Vietnamese people in 2", "extracted_entity": null}
{"question": "do more northern speakers speak the southern regional dialect than southern speakers speak the northern dialect?", "answer": "More southern speakers speak the northern dialect than northerners speak the suthern dialect.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["More southern speakers speak the northern dialect than northerners speak the suthern dialect."], "entity_annotations": ["Vietnamese_language"], "answer_entity_name": null, "predicted_answer": "no, because the northern dialect is more widely spoken", "extracted_entity": null}
{"question": "what happened in 1860?", "answer": "Vincent van Gogh attended the Zundert village school from 1860.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Vincent van Gogh attended the Zundert village school from 1860."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Abraham Lincoln was elected president.", "extracted_entity": null}
{"question": "who frequented the circle of the British-Australian artist John Peter Russell?", "answer": "Van Gogh", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Van Gogh"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Paul Gauguin, Vincent van Gogh,", "extracted_entity": null}
{"question": "were the letters first annotated in 1913 by Theo's widow Johanna van Gogh-Bonger?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes. I have a copy of the letters and", "extracted_entity": null}
{"question": "was Kee seven years older than Van Gogh as well as had an eight-year-old son?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes, Kee was seven years older than Van G", "extracted_entity": null}
{"question": "did he suffer from anxiety and increasingly frequent bouts of mental illness throughout his life, and died largely unknown, at the age of 37, from a self-inflicted gunshot wound?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Vincent van Gogh.", "extracted_entity": null}
{"question": "appreciated during his lifetime, did his fame grow in the years after his death?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "Yes, his fame grew in the years after", "extracted_entity": null}
{"question": "is today, he widely regarded as one of history's greatest painters and an important contributor to the foundations of modern art?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Vincent_van_Gogh"], "answer_entity_name": null, "predicted_answer": "yes, he is one of the most important artists", "extracted_entity": null}
{"question": "does a violin have four strings?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it has two strings.", "extracted_entity": null}
{"question": "is the violin supported by the left shoulder?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it is supported by the collarbone", "extracted_entity": null}
{"question": "is the violin shaped like an hourglass?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes", "Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no, it is shaped like a pear.", "extracted_entity": null}
{"question": "what were strings first made out of?", "answer": "Sheep Gut", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Sheep Gut", "Sheep gut."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "cat gut", "extracted_entity": null}
{"question": "what is a violin called informally?", "answer": "Fiddle", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Fiddle", "A fiddle."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "fiddle", "extracted_entity": null}
{"question": "what does vitula mean?", "answer": "Stringed Instrument", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Stringed Instrument", "Stringed instrument."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "it is the latin word for 'healthy'", "extracted_entity": null}
{"question": "what is a person that makes or repairs violins called?", "answer": "Luthier", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Luthier", "A luthier, or simply a violin maker."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "a violin maker", "extracted_entity": null}
{"question": "what does the \"voice\" of a violin depend upon?", "answer": "Shape", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Shape", "Its shape."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "the quality of the wood, the craftsmanship of the", "extracted_entity": null}
{"question": "is the violin a percussion instrument?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "no. It is a string instrument.", "extracted_entity": null}
{"question": "what happened in 1980s?", "answer": "The use of traditional (non-synthesized) orchestras declined.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["The use of traditional (non-synthesized) orchestras declined."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The 1980s saw the rise of the", "extracted_entity": null}
{"question": "who features violinist Boyd Tinsley?", "answer": "Dave Matthews Band features violinist Boyd Tinsley.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Dave Matthews Band features violinist Boyd Tinsley."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Dave Matthews Band", "extracted_entity": null}
{"question": "regarding this topic, what did Antonio Stradivari do?", "answer": "Antonio Stradivari made violins.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Antonio Stradivari made violins."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "He made violins, and they were very good", "extracted_entity": null}
{"question": "when did Charles IX order Amati to construct 24 violins for him?", "answer": "1560", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["1560"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "1564.", "extracted_entity": null}
{"question": "is a string first tuned to a standard pitch?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is each node at an integer division of the string?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the tuning G-D-A-E used for most violin music?", "answer": "Yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes"], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "yes, that is the most common tuning for violin", "extracted_entity": null}
{"question": "is it the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello?", "answer": "Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes, it is the smallest and highest-pitched member of the violin family of string instruments, which also includes the viola and cello."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "The violin is the smallest and highest-pitched member", "extracted_entity": null}
{"question": "is a violin sometimes informally called a fiddle, regardless of the type of music played on it?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "Yes, the term \u201cfiddle\u201d is used", "extracted_entity": null}
{"question": "does the word ``violin'' come from the Middle Latin word vitula, meaning ``stringed instrument''?", "answer": "Yes.", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Yes."], "entity_annotations": ["Violin"], "answer_entity_name": null, "predicted_answer": "No, it comes from the Latin word vitulus", "extracted_entity": null}
{"question": "is the xylophone a musical instrument?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, it is. It is a percussion instrument", "extracted_entity": null}
{"question": "was the xylophone associated with the folk music of the United States by the 19th century?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "Yes, the xylophone was associated with the", "extracted_entity": null}
{"question": "was it likely that the xylophone reached Europe during the Crusades?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "No, because the Crusades took place from", "extracted_entity": null}
{"question": "the xylophone is an instrument from what family?", "answer": "the percussion family", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["the percussion family", "Percussion"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "percussion", "extracted_entity": null}
{"question": "how do the Java and Bali use xylophones?", "answer": "In gamelan ensembles", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["In gamelan ensembles"], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "they don't use xylophones at all", "extracted_entity": null}
{"question": "some versions of xylophones have resonators made of what?", "answer": "gourds", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["gourds", "Gourds."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "wood", "extracted_entity": null}
{"question": "which came first, the xylophone or the vibraphone?", "answer": "xylophone", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["xylophone", "The xylophone."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "the vibraphone came first. It was invented", "extracted_entity": null}
{"question": "did the golden age of xylophones come before or after the first usage of the European-derived orchestral?", "answer": "after", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["after", "After."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "after. The first usage of the European-derived orchest", "extracted_entity": null}
{"question": "does kelon increase or decrease tone quality of the xylophone?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "Decrease."], "entity_annotations": ["Xylophone"], "answer_entity_name": null, "predicted_answer": "it increases the tone quality", "extracted_entity": null}
{"question": "do the different species of zebras interbreed?", "answer": "no", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["no", "No"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "no, they do not interbreed", "extracted_entity": null}
{"question": "do zebras sleep standing up?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "yes, they sleep standing up", "extracted_entity": null}
{"question": "have plains zebras been crossed with mountain zebras?", "answer": "yes", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["yes", "Yes"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "yes, but the offspring were infertile.", "extracted_entity": null}
{"question": "how many species of zebra are there?", "answer": "three", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["three", "Three"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "two", "extracted_entity": null}
{"question": "what do zebras eat?", "answer": "mainly grass", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly grass", "Grasses, shrubs, herbs, twigs, leaves, and bark"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "grass, or any other vegetation", "extracted_entity": null}
{"question": "what are zebras hunted for?", "answer": "mainly for their skins", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["mainly for their skins", "Skins"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "lions and hyenas", "extracted_entity": null}
{"question": "what areas do the Grevy's Zebras inhabit?", "answer": "semi-arid grasslands of Ethiopia and northern Kenya", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["semi-arid grasslands of Ethiopia and northern Kenya", ""], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "they live in the grasslands of Kenya and Ethiopia", "extracted_entity": null}
{"question": "which species of zebra is known as the common zebra?", "answer": "Plains Zebra (Equus quagga, formerly Equus burchelli)", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["Plains Zebra (Equus quagga, formerly Equus burchelli)", "Plains Zebra"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "the plains zebra", "extracted_entity": null}
{"question": "at what age can a zebra breed?", "answer": "five or six", "dataset": "factoid_qa", "split": "train", "answer_aliases": ["five or six", "5 or 6"], "entity_annotations": ["Zebra"], "answer_entity_name": null, "predicted_answer": "they can breed at 2 years old", "extracted_entity": null}
